<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer085">
<h1 class="chapter-number" id="_idParaDest-153"><a id="_idTextAnchor170"/>8</h1>
<h1 id="_idParaDest-154"><a id="_idTextAnchor171"/>Deploying Applications with the Kubernetes Orchestrator</h1>
<p>Developing containers for your applications on your workstation or laptop really improves your development process by running other applications’ components while you focus on your own code. This simple standalone architecture works perfectly in your development stage, but it does not provide <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>) for your applications. Deploying container orchestrators cluster-wide will help you to constantly keep your applications running healthy. In the previous chapter, we briefly reviewed Docker Swarm, which is simpler and can be a good introductory platform before moving on to more complex orchestrators. In this chapter, we will learn how to prepare and run our applications on top of <strong class="bold">Kubernetes</strong>, which is considered a standard nowadays for running <span class="No-Break">containers cluster-wide.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Introducing the main features <span class="No-Break">of Kubernetes</span></li>
<li>Understanding <span class="No-Break">Kubernetes’ HA</span></li>
<li>Interacting with Kubernetes <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">kubectl</strong></span></li>
<li>Deploying a functional <span class="No-Break">Kubernetes cluster</span></li>
<li>Creating Pods <span class="No-Break">and Services</span></li>
<li>Deploying <span class="No-Break">orchestrated resources</span></li>
<li>Improving your applications’ security <span class="No-Break">with Kubernetes</span></li>
</ul>
<h1 id="_idParaDest-155"><a id="_idTextAnchor172"/>Technical requirements</h1>
<p>You can find the labs for this chapter at https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter8, where you will find some extended explanations, omitted in the chapter’s content to make it easier to follow. The <em class="italic">Code In Action</em> video for this chapter can be found <span class="No-Break">at </span><a href="https://packt.link/JdOIY"><span class="No-Break">https://packt.link/JdOIY</span></a><span class="No-Break">.</span></p>
<p>Now, let’s start this chapter by learning about the main features of Kubernetes and why this orchestrator has become <span class="No-Break">so popular.</span></p>
<h1 id="_idParaDest-156"><a id="_idTextAnchor173"/>Introducing the main features of Kubernetes</h1>
<p>We can say without any doubt <a id="_idIndexMarker861"/>that Kubernetes is the new standard for deploying applications based on containers. However, its success didn’t happen overnight; Kubernetes started in 2015 as a community project based on Google’s own workload orchestrator, <strong class="bold">Borg</strong>. The <a id="_idIndexMarker862"/>first commit in Kubernetes’ GitHub repository occurred in 2014, and a year later, the first release was published. Two years later, Kubernetes went mainstream thanks to its great community. I have to say that you will probably not use Kubernetes alone; you will deploy multiple components to achieve a fully functional platform, but this isn’t a bad thing, as you can customize a Kubernetes platform to your specific needs. Also, Kubernetes by default has a lot of integrations with cloud platforms, as it was designed from the very beginning with them in mind. For example, cloud storage solutions can be used without <span class="No-Break">additional components.</span></p>
<p>Let’s take a moment to briefly compare Kubernetes’ features with those of <span class="No-Break">Docker Swarm.</span></p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor174"/>Comparing Kubernetes and Docker Swarm</h2>
<p>I have to declare<a id="_idIndexMarker863"/> that, personally, my first impressions of Kubernetes<a id="_idIndexMarker864"/> weren’t good. For me, it provided a lot of features for many simple tasks that I was able to solve with Docker Swarm at that time. However, the more complex your applications are, the more features you require, and Docker Swarm eventually became too simple as it hasn’t evolved too much. Docker Swarm works well with simple projects, but microservices architecture usually requires complex interactions and a lot of portability features. Kubernetes has a really steep learning curve, and it’s continuously evolving, which means you need to follow the project almost every day. Kubernetes’ core features are usually improved upon in each new release, and lots of pluggable features and side projects also continuously appear, which makes the platform’s ecosystem <span class="No-Break">grow daily.</span></p>
<p>We will see some differences between the Docker Swarm orchestration model and the Kubernetes model. We can start with the definition of the workloads within a cluster. We mentioned in <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating with Swarm</em>, that Docker Swarm doesn’t schedule containers; in fact, it <a id="_idIndexMarker865"/>schedules <strong class="bold">Services</strong>. In Kubernetes, we schedule <strong class="bold">Pods</strong>, which is<a id="_idIndexMarker866"/> the minimum scheduling unit in this orchestrator. A Pod can contain multiple Pods, although most of <a id="_idIndexMarker867"/>them will just run one. We will explore and learn<a id="_idIndexMarker868"/> more about Pods later in <span class="No-Break">this chapter.</span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor175"/>Exploring the control plane</h2>
<p>Container orchestrators<a id="_idIndexMarker869"/> should provide a <strong class="bold">control plane</strong> for all <a id="_idIndexMarker870"/>management tasks, providing us with scheduling capabilities to execute our application workloads on a data plane and cluster-wide networking features. The Kubernetes control plane components are designed to manage every cluster component, schedule workloads, and review events that emerge in the platform. It also manages the node components, which really execute containers for us thanks to their container runtimes. Kubernetes follows the manager-worker model, as with Docker Swarm, in which two different node roles are defined. Manager nodes will manage the control plane components, while worker nodes will execute the tasks assigned by the control <span class="No-Break">plane nodes.</span></p>
<p>Now, let’s review some <span class="No-Break">key processes.</span></p>
<h3>Understanding the key processes</h3>
<p>The following is a list <a id="_idIndexMarker871"/>of the<a id="_idIndexMarker872"/> key processes that run in the Kubernetes <span class="No-Break">control plane:</span></p>
<ul>
<li><strong class="bold">kube-apiserver</strong>: The API server is <a id="_idIndexMarker873"/>a component that interacts with all other components and the user. There isn’t any direct communication between components; hence, kube-apiserver is essential in every Kubernetes cluster. All the cluster management is provided by exposing this component’s API, and we can use different clients to interact with the cluster. Different endpoints allow us to retrieve and set <span class="No-Break">Kubernetes resources.</span></li>
<li><strong class="bold">etcd</strong>: This is a<a id="_idIndexMarker874"/> component that provides data storage for all cluster components. It is a key-value store that can be consumed via its HTTP REST API. This reliable key-value store contains sensitive data, but, as mentioned before, only the kube-apiserver component can <span class="No-Break">access it.</span></li>
<li><strong class="bold">kube-scheduler</strong>: This is in charge <a id="_idIndexMarker875"/>of allocating workloads to the container runtimes deployed in the nodes. To decide which nodes will run the different containers, kube-scheduler will ask kube-apiserver for the hardware resources and availability of all nodes included in <span class="No-Break">the cluster.</span></li>
<li><strong class="bold">kube-controller-manager</strong>: Different<a id="_idIndexMarker876"/> controller processes run inside the Kubernetes cluster to maintain the status of the platform and the applications running inside. The kube-controller-manager is responsible for managing these controllers, and different tasks are delegated to <span class="No-Break">each controller:</span><ul><li>The <strong class="bold">node controller</strong> manages <a id="_idIndexMarker877"/><span class="No-Break">nodes’ statuses</span></li><li>The <strong class="bold">job controller</strong> is responsible<a id="_idIndexMarker878"/> for managing<a id="_idIndexMarker879"/> workloads’ tasks<a id="_idIndexMarker880"/> and creating Pods to <span class="No-Break">run them</span></li><li>The <strong class="bold">endpoint controller</strong> creates endpoint resources to <span class="No-Break">expose Pods</span></li><li>The <strong class="bold">service account controller</strong> and <strong class="bold">token controller</strong> manage accounts and API access <span class="No-Break">token authorizations</span></li></ul></li>
<li><strong class="bold">cloud-controller-manager</strong>: This is a<a id="_idIndexMarker881"/> separate component that manages different controllers that talk with underlying cloud <span class="No-Break">providers’ APIs:</span><ul><li>The <strong class="bold">node controller</strong> manages the state and health of nodes deployed in your <span class="No-Break">cloud provider.</span></li><li>The <strong class="bold">route controller</strong> creates routes in the cloud provider, using its specific API to access your <span class="No-Break">deployed workloads.</span></li><li>The <strong class="bold">service controller</strong> manages cloud providers’ load balancer resources. You will never deploy this component in your own local data center because it is designed for <span class="No-Break">cloud integrations.</span></li></ul></li>
</ul>
<p>Next, let’s <a id="_idIndexMarker882"/>review <strong class="bold">node components</strong>, which are in charge of<a id="_idIndexMarker883"/> executing and giving visibility to the <span class="No-Break">workload processes.</span></p>
<h3>Understanding node components</h3>
<p>Node components run <a id="_idIndexMarker884"/>on worker nodes (as in Docker<a id="_idIndexMarker885"/> Swarm, manager nodes can also have the worker role). Let’s take a closer look <span class="No-Break">at them:</span></p>
<ul>
<li><strong class="bold">Container runtime</strong>: The runtime for running containers is key, as it will execute all the workloads for us. The Kubernetes orchestrator schedules Pods on each host, and they run the containers <span class="No-Break">for us.</span></li>
<li><strong class="bold">kubelet</strong>: We can consider kubelet as the Kubernetes integration agent. All nodes with the worker role have to run kubelet in order to communicate with the control plane. In fact, the control plane will manage communications to receive the health of each worker node and the status of their running workloads. kubelet will only manage containers deployed in the Kubernetes cluster; in other words, you can still execute containers in the workers’ container runtime, but those containers will not be managed <span class="No-Break">by Kubernetes.</span></li>
<li><strong class="bold">kube-proxy</strong>: This component is responsible for Kubernetes communications. It is important to mention here that Kubernetes does not really provide full networking capabilities by itself, and this component will only manage the integration of Kubernetes Service resources within a cluster. Additional communications components will be required to have a fully functional cluster. It is fair to say that kube-proxy works at the worker-node level, publishing the applications within the cluster, but more components will be needed in order to reach other Services deployed in other <span class="No-Break">cluster nodes.</span></li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">A <strong class="bold">Service resource</strong> (or<a id="_idIndexMarker886"/> simply <strong class="bold">Service</strong>) is designed to make your applications’ Pods accessible. Different options are available to publish our applications either internally or externally for users. Service resources will get their own IP address to access the associated Pods’ endpoints. We can consider Service resources as logical components. We will use Services to access our applications because Pods can die and be recreated, acquiring new IP addresses, but Services will remain visible with their specified <span class="No-Break">IP address.</span></p>
<p>Worker nodes can be replaced when necessary; we can perform maintenance tasks whenever it’s required, moving workloads from one node to another. However, control plane components can’t be replaced. To achieve Kubernetes’ HA, we need to execute more than one replica of control plane components. In the case of etcd, we must have an odd number of replicas, which means that at least three are required for HA. This requirement leads us to a minimum of three manager nodes (or master nodes, in Kubernetes nomenclature) to deploy a Kubernetes cluster with HA, although other components will provide HA with only two replicas. Conversely, the number of worker nodes may vary. This<a id="_idIndexMarker887"/> really<a id="_idIndexMarker888"/> depends on your application’s HA, although a minimum of two workers is <span class="No-Break">always recommended.</span></p>
<h3>Networking in Kubernetes</h3>
<p>It is important to <a id="_idIndexMarker889"/>remember that Kubernetes networking differs from the Docker Swarm model. By itself, Kubernetes does not provide cluster-wide communications, but a standardized interface, the <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>), is <a id="_idIndexMarker890"/>provided. Kubernetes defines a set of rules that any project integrating a communications interface <span class="No-Break">must follow:</span></p>
<ul>
<li><strong class="bold">Container-to-container communications</strong>: This communication works at the Pod level. Containers running inside a Pod share the same network namespace; hence, they receive the same IP address from the container runtime <strong class="bold">IP Address Management</strong> (<strong class="bold">IPAM</strong>). They can also use <strong class="source-inline">localhost</strong> to resolve their<a id="_idIndexMarker891"/> internal communications. This really simplifies communications when multiple containers need to <span class="No-Break">work together.</span></li>
<li><strong class="bold">Host-to-container communications</strong>: Each host can communicate with Pods running locally using its <span class="No-Break">container runtime.</span></li>
<li><strong class="bold">Pod-to-Pod communications</strong>: These communications will work locally but not cluster-wide, and Kubernetes imposes that communications must be provided without <a id="_idIndexMarker892"/>any <strong class="bold">network address translation</strong> (<strong class="bold">NAT</strong>). This is something the CNI <span class="No-Break">must resolve.</span></li>
<li><strong class="bold">Pod-to-Service interactions</strong>: Pods will never consume other Pods as their IP address can change over time. We will use Services to expose Pods, and Kubernetes will manage their IP addresses, but the CNI must manage <span class="No-Break">them cluster-wide.</span></li>
<li><strong class="bold">Publishing Services</strong>: Different approaches exist to publish our applications, but they are resolved by Service types and Ingress resources, and cluster-wide communications must be included in <span class="No-Break">the CNI.</span></li>
</ul>
<p>Because NAT isn’t allowed, this model declares a flat network, where Pods can see each other when the CNI is included in the Kubernetes deployment. This is completely different from Docker Swarm, where applications or projects can run in isolated networks. In Kubernetes, we need to implement additional mechanisms to isolate <span class="No-Break">our applications.</span></p>
<p>There are a lot of CNI plugins available to implement these cluster-wide communications. You can use any of them, but some are more popular than others; the following list shows recommended ones <a id="_idIndexMarker893"/>with some of their <span class="No-Break">key features:</span></p>
<ul>
<li><strong class="bold">Flannel</strong> is a simple overlay network provider that works very well out of the box. It creates a VXLAN between nodes to propagate the Pods’ IP address cluster-wide, but it doesn’t provide network policies. These are Kubernetes resources that can drop or allow <span class="No-Break">Pods’ connectivity.</span></li>
<li><strong class="bold">Calico</strong> is a network plugin that supports different network configurations, including non-overlay and overlay networks, with or without <strong class="bold">Border Gateway Protocol</strong> (<strong class="bold">BGP</strong>). This <a id="_idIndexMarker894"/>plugin provides network policies, and it’s adequate for almost all <span class="No-Break">small environments.</span></li>
<li><strong class="bold">Canal</strong> is used by default in SUSE’s Rancher environments. It combines Flannel’s simplicity and Calico’s <span class="No-Break">policy features.</span></li>
<li><strong class="bold">Cilium</strong> is a very interesting network plugin because it integrates <strong class="bold">extended Berkeley Packet Filter</strong> (<strong class="bold">eBPF</strong>) Linux<a id="_idIndexMarker895"/> kernel features in Kubernetes. This network provider is intended for multi-cluster environments or when you want to integrate network observability into <span class="No-Break">your platform.</span></li>
<li><strong class="bold">Multus</strong> can be used to deploy multiple CNI plugins in <span class="No-Break">your cluster.</span></li>
<li>Cloud providers offer their own cloud-specific CNIs that allow us to implement different network scenarios and manage Pods’ IP addresses within our own private <span class="No-Break">cloud infrastructure.</span></li>
</ul>
<p>The CNI plugin should always be deployed once the Kubernetes control plane has started because some<a id="_idIndexMarker896"/> components, such as the internal DNS or kube-apiserver, need to be <span class="No-Break">reachable cluster-wide.</span></p>
<h3>Namespace scope isolation</h3>
<p>Kubernetes provides<a id="_idIndexMarker897"/> project or application isolation by <a id="_idIndexMarker898"/>using <strong class="bold">namespaces</strong>, which allow us to group resources. Kubernetes provides both cluster-scoped and <span class="No-Break">namespace-scoped resources:</span></p>
<ul>
<li><strong class="bold">Cluster-scoped</strong> resources are resources available cluster-wide, and we can consider most of them as cluster management resources, owned by the <span class="No-Break">cluster administrators.</span></li>
<li><strong class="bold">Namespace-scoped</strong> resources are those confined at the namespace level. Services and Pods, for example, are defined at the namespace level, while node resources are <span class="No-Break">available cluster-wide.</span></li>
</ul>
<p>Namespace resources are key to isolating applications and restricting users from accessing resources. Kubernetes provides different authentication and authorization methods, although we can integrate and combine additional components such as an external <strong class="bold">Lightweight Directory Access Protocol</strong> (<strong class="bold">LDAP</strong>) or<a id="_idIndexMarker899"/> Microsoft <span class="No-Break">Active Directory.</span></p>
<h3>Internal resolution</h3>
<p>The internal DNS is based <a id="_idIndexMarker900"/>on the <strong class="bold">CoreDNS</strong> project<a id="_idIndexMarker901"/> and provides autodiscovery of Services by default (additional configurations can also publish Pods’ IP addresses). This means that every time a new Service resource is created, the DNS adds an entry with its IP address, making it accessible to all Pods cluster-wide with the following <strong class="bold">Fully Qualified Domain Name</strong> (<strong class="bold">FQDN</strong>) by<a id="_idIndexMarker902"/> <span class="No-Break">default: </span><span class="No-Break"><strong class="source-inline">SERVICE_NAME.NAMESPACE.svc.cluster.local</strong></span><span class="No-Break">.</span></p>
<h3>Attaching data to containers</h3>
<p>Kubernetes includes<a id="_idIndexMarker903"/> different resource types to attach storage to <span class="No-Break">our workloads:</span></p>
<ul>
<li><strong class="bold">Volumes</strong> are Kubernetes-provided resources that include multiple cloud providers’ storage APIs, temporal storage (<strong class="source-inline">emptyDir</strong>), host storage, and <strong class="bold">Network File System</strong> (<strong class="bold">NFS</strong>), among <a id="_idIndexMarker904"/>other remote storage solutions. Other very important volume-like resources are Secrets and ConfigMaps, which can be used to manage sensitive data and configurations <span class="No-Break">cluster-wide, respectively.</span></li>
<li><strong class="bold">Persistent volumes</strong> are the preferred solution when you work on production in a local data center. Storage vendors<a id="_idIndexMarker905"/> provide their own drivers to integrate <strong class="bold">network-attached storage</strong> (<strong class="bold">NAS</strong>) and <strong class="bold">storage area network</strong> (<strong class="bold">SAN</strong>) solutions<a id="_idIndexMarker906"/> in <span class="No-Break">our applications.</span></li>
<li><strong class="bold">Projected volumes</strong> are used to map several volumes inside a unique Pod <span class="No-Break">container’s directory.</span></li>
</ul>
<p>Providing persistent storage to our applications is key in container orchestrators, and Kubernetes integrates <a id="_idIndexMarker907"/>very well with different dynamic <span class="No-Break">provisioning solutions.</span></p>
<h3>Publishing applications</h3>
<p>Finally, we will introduce<a id="_idIndexMarker908"/> the concept<a id="_idIndexMarker909"/> of <strong class="bold">Ingress</strong> resources. These resources simplify and secure the publishing of applications running in Kubernetes by linking Service resources with specific applications’ URLs. An Ingress controller is required to manage these resources, and we can integrate into this component many different options, such as NGINX, Traefik, or even more complex solutions, such as Istio. It is also remarkable that many network device vendors have also prepared their own integrations with Kubernetes platforms, improving performance <span class="No-Break">and security.</span></p>
<p>Now that we have<a id="_idIndexMarker910"/> been quickly introduced to Kubernetes, we can take a deep dive into the platform’s components <span class="No-Break">and features.</span></p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor176"/>Understanding Kubernetes’ HA</h1>
<p>Deploying our applications <a id="_idIndexMarker911"/>with HA requires a Kubernetes environment with HA. At least three replicas of etcd are required and two replicas of other control plane components. Some production architectures deploy etcd externally in dedicated hosts, while other components are deployed in additional master nodes. This isolates completely the key-value store from the rest of the control plane components, improving security, but it adds additional complexity to the environment. You will usually find three master nodes and enough worker nodes to deploy your <span class="No-Break">production applications.</span></p>
<p>A Kubernetes installation configures and manages its own internal <strong class="bold">certificate authority</strong> (<strong class="bold">CA</strong>) and then<a id="_idIndexMarker912"/> deploys certificates for the different control plane and kubelet components. This ensures TLS communications between kube-apiserver and other components. The following architecture diagram shows the different Kubernetes components in a single-master <span class="No-Break">node scenario:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<img alt="Figure 8.1 – Kubernetes cluster architecture with HA" height="639" src="image/B19845_08_01.jpg" width="1030"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Kubernetes cluster architecture with HA</p>
<p>Worker nodes are those designated to run workloads. Depending on the Kubernetes installation method, you will be able to run specific workloads on master nodes if they also run the kubelet and kube-proxy components. We can use different affinity and anti-affinity rules to identify <a id="_idIndexMarker913"/>which nodes should finally execute a container in <span class="No-Break">your cluster.</span></p>
<p>However, simply replicating the control plane does not provide HA or resilience to your applications. You will need a CNI to manage communications between your containers cluster-wide. Internal load balancing will route requests to deployed Pods within your <span class="No-Break">Kubernetes cluster.</span></p>
<p>Running your applications on different hosts requires appropriate storage solutions. Whenever a container starts with a container runtime, the required volumes should be attached. If you work on-premises, you will probably use a <strong class="bold">Container Storage Interface</strong> (<strong class="bold">CSI</strong>) in your<a id="_idIndexMarker914"/> infrastructure. However, as a developer, you should consider your storage requirements, and your infrastructure administrators will provide you with the best solution. Different providers will present filesystems, blocks, or object storage, and you can choose which best fits your application. All of them will work cluster-wide and help you <span class="No-Break">provide HA.</span></p>
<p>Finally, you have to think about how your application’s components work with multiple replicas. Your infrastructure provides resilience to your containers, but your application’s logic must <span class="No-Break">support replication.</span></p>
<p>Running a production cluster can be hard, but deploying our own cluster to learn how Kubernetes works is <a id="_idIndexMarker915"/>a task that I really recommend to anyone who wants to deploy applications on these container architectures. Using <strong class="bold">kubeadm</strong> is recommended to create Kubernetes clusters for the <span class="No-Break">first time.</span></p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor177"/>Kubeadm Kubernetes deployment</h2>
<p>Kubeadm is a tool that <a id="_idIndexMarker916"/>can be used to easily deploy a fully functional Kubernetes cluster. In fact, we can even use it to deploy <span class="No-Break">production-ready clusters.</span></p>
<p>We will initialize a cluster from the first deployment node, executing <strong class="source-inline">kubeadm init</strong>. This will create and trigger the bootstrapping processes to deploy the cluster. The node in which we execute this action will become the cluster leader, and we will join new master and worker nodes by simply executing <strong class="source-inline">kubeadm join</strong>. This really simplifies the deployment process; every step required for creating the cluster is automated. First, we will create the control plane components; hence, <strong class="source-inline">kubeadm join</strong> will be executed in the rest of the designated master nodes. And once the master nodes are installed, we will join the <span class="No-Break">worker nodes.</span></p>
<p>Kubeadm is installed as a binary in your operating system. It is important to note here that the Kubernetes master role is only available on Linux operating systems. Therefore, we can’t install a Kubernetes cluster only with Microsoft Windows or <span class="No-Break">macOS nodes.</span></p>
<p>This tool does not only install a new cluster. It can be used to modify current kubeadm-deployed cluster configurations or upgrade them to a newer release. It is a very powerful tool, and it is good to know how to use it, but unfortunately, it is out of the scope of this book. Suffice it to say that there are many command-line arguments that will help us to fully customize Kubernetes deployment, such as the IP address to be used to manage communications within the control plane, the Pods’ IP address range, and the authentication and authorization model to use. If you have time and hardware resources, it is recommended to create at least a two-node cluster with kubeadm to understand the deployment process and the components deployed by default on a <span class="No-Break">Kubernetes cluster.</span></p>
<p>Here is the link for a Kubernetes deployment process with the <a id="_idIndexMarker917"/>kubeadm tool: <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm</a>. We will not use kubeadm to deploy Kubernetes in this book. We will use Docker Desktop, Rancher Desktop, or Minikube tools, which provide fully automated deployments that work out of the box on our laptop or <span class="No-Break">desktop computers.</span></p>
<p>Docker or any other container runtime just cares about containers. We learned in <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating with Swarm</em>, how Docker provides the command line to manage Docker Swarm clusters, but this doesn’t work with Kubernetes, as it is a completely different platform. The kube-apiserver component is the only component accessible to administrators and end users. The Kubernetes community project provides its own tool to manage <a id="_idIndexMarker918"/>Kubernetes clusters and the resources deployed on them. Thus, in the next subsection, we will learn the basics of <strong class="source-inline">kubectl</strong>, the tool that we will use in a lot of examples in this book to manage configurations, content, and workloads <span class="No-Break">on clusters.</span></p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor178"/>Interacting with Kubernetes using kubectl</h1>
<p>In this section, we<a id="_idIndexMarker919"/> will learn the basics of the <strong class="source-inline">kubectl</strong> command<a id="_idIndexMarker920"/> line. It is the official Kubernetes client, and its features can be extended by <span class="No-Break">adding plugins.</span></p>
<p>The installation process for this tool is quite simple, as it is a single binary written in the Go language; therefore, we can download it from the official Kubernetes binaries repository. To download from this repository, you must include the release to use in the URL. For example, <a href="https://dl.k8s.io/release/v1.27.4/bin/linux/amd64/kubectl">https://dl.k8s.io/release/v1.27.4/bin/linux/amd64/kubectl</a> will link you to the <strong class="source-inline">kubectl</strong> Linux binary for Kubernetes 1.27.4. You will be able to manage Kubernetes clusters using binaries from a different release, although it is recommended to maintain alignment with your client and Kubernetes server releases. How to install the tool for each platform is described in https://kubernetes.io/docs/tasks/tools/#kubectl. As we will use Microsoft Windows in the <em class="italic">Labs</em> section, we will use the following link to install the tool’s <span class="No-Break">binary: </span><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-windows"><span class="No-Break">https://kubernetes.io/docs/tasks/tools/install-kubectl-windows</span></a><span class="No-Break">.</span></p>
<p>Let’s start with <strong class="source-inline">kubectl</strong> by learning the syntax for executing commands with <span class="No-Break">this tool:</span></p>
<pre class="console">
kubectl [command] [TYPE] [NAME] [flags]</pre> <p><strong class="source-inline">TYPE</strong> indicates that <strong class="source-inline">kubectl</strong> can be used with many different Kubernetes resources. We can use the singular, plural, or abbreviated forms, and we will use them in <span class="No-Break">case-insensitive format.</span></p>
<p>The first thing to know before learning some of the uses of <strong class="source-inline">kubectl</strong> is how to configure access to any cluster. The <strong class="source-inline">kubectl</strong> command uses, by default, a <strong class="source-inline">config</strong> configuration file in the home of each user, under the <strong class="source-inline">.kube</strong> directory. We can change which configuration file to use by adding the <strong class="source-inline">--kubeconfig &lt;FILE_PATH_AND_NAME&gt;</strong> argument or setting the <strong class="source-inline">KUBECONFIG</strong> variable with the configuration file location. By changing the content of the <strong class="source-inline">kubeconfig</strong> file, we can easily have different cluster configurations. However, this path change is not really needed because the configuration file structure allows different contexts. Each context is used to uniquely configure a set of user and server values, allowing<a id="_idIndexMarker921"/> us to configure a context <a id="_idIndexMarker922"/>with our authentication and a Kubernetes <span class="No-Break">cluster endpoint.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">You will usually access a Kubernetes cluster using an FQDN (or its resolved IP address). This name or its IP address will be load-balanced to all Kubernetes clusters’ available instances of kube-apiserver; hence, a load balancer will be set in front of your cluster servers. In our local environments, we will use a simple IP address associated with <span class="No-Break">our cluster.</span></p>
<p>Let’s see what a configuration file <span class="No-Break">looks like:</span></p>
<pre class="source-code">
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: BASE64_CLUSTER_CA or the CA file path
    server: https://cluster1_URL:6443
  name: cluster1
contexts:
- context:
    cluster: cluster1
    user: user1
  name: user1@cluster1
current-context: user1@cluster1
users:
- name: user1
  user:
    client-certificate-data: BASE64_USER_CERTIFICATE or the cert file path
    client-key-data: BASE64_USER_KEY or the key file path</pre> <p>We can add multiple<a id="_idIndexMarker923"/> servers <a id="_idIndexMarker924"/>and users and link them in multiple contexts. We can switch between defined contexts by using <strong class="source-inline">kubectl config </strong><span class="No-Break"><strong class="source-inline">use-context CONTEXT_NAME</strong></span><span class="No-Break">.</span></p>
<p>We can use <strong class="source-inline">kubectl api-resources</strong> to retrieve the type of resources available in the defined cluster. This is important because the <strong class="source-inline">kubectl</strong> command line retrieves data from a Kubernetes cluster; hence, its behavior changes depending on the endpoint. The following screenshot shows the API resources available in a sample <span class="No-Break">Kubernetes cluster:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="Figure 8.2 – Kubernetes API resources in a sample cluster" height="872" src="image/B19845_08_02.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Kubernetes API resources in a sample cluster</p>
<p>As you can see, there is a column that indicates whether a Kubernetes resource is namespaced or not. This shows the scope where the resource must be defined. Resources can have a cluster scope, be defined and used at the cluster level, or be namespace-scoped, in which case they exist grouped inside a Kubernetes namespace. Kubernetes<a id="_idIndexMarker925"/> namespaces<a id="_idIndexMarker926"/> are resources that allow us to isolate and group resources within a cluster. The resources defined inside a namespace are unique within the namespace, as we will use the namespaces to <span class="No-Break">identify them.</span></p>
<p>There are many commands available for <strong class="source-inline">kubectl</strong>, but we will focus in this section on just a few <span class="No-Break">of them:</span></p>
<ul>
<li><strong class="source-inline">create</strong>: This action allows us to create Kubernetes resources from a file or our <span class="No-Break">terminal </span><span class="No-Break"><strong class="source-inline">stdin</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">apply</strong>: This creates and updates resources <span class="No-Break">in Kubernetes.</span></li>
<li><strong class="source-inline">delete</strong>: We can remove already created resources using <span class="No-Break"><strong class="source-inline">kubectl delete</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">run</strong>: This action can be used to quickly deploy a simple workload and define a <span class="No-Break">container image.</span></li>
<li><strong class="source-inline">get</strong>: We can retrieve any Kubernetes resource definition by using <strong class="source-inline">kubectl get</strong>. A valid authorization is required to either create or retrieve any Kubernetes object. We can also use <strong class="source-inline">kubectl describe</strong>, which gives a detailed description of the cluster <span class="No-Break">resource retrieved.</span></li>
<li><strong class="source-inline">edit</strong>: We can modify some resources’ properties in order to change them within the cluster. This will also modify our <span class="No-Break">applications’ behavior.</span></li>
</ul>
<p>We can configure Kubernetes resources by using an <strong class="bold">imperative</strong> or <span class="No-Break"><strong class="bold">declarative </strong></span><span class="No-Break">method:</span></p>
<ul>
<li>In an imperative configuration, we describe the configuration of the Kubernetes resource in the command line, using <span class="No-Break">our terminal.</span></li>
<li>By using a declarative configuration, we will create a file describing the configuration of a resource, and then we create or apply the content of the file to the Kubernetes cluster. This method <span class="No-Break">is reproducible.</span></li>
</ul>
<p>Now that we have a basic idea<a id="_idIndexMarker927"/> of Kubernetes components, the<a id="_idIndexMarker928"/> installation process, how to interact with a cluster, and the requirements to run a functional Kubernetes platform, let’s see how to easily deploy our <span class="No-Break">own environment.</span></p>
<h1 id="_idParaDest-162"><a id="_idTextAnchor179"/>Deploying a functional Kubernetes cluster</h1>
<p>In this section, we will<a id="_idIndexMarker929"/> review different methods to deploy Kubernetes for different purposes. As a developer, you do not need to deploy a production environment, but it’s important to understand the process and be able to create a minimal environment to test your applications. If you are really interested in the full process, it’s recommended to take a look at Kelsey Hightower’s GitHub repository, <em class="italic">Kubernetes the Hard Way</em> (https://github.com/kelseyhightower/kubernetes-the-hard-way). In this repository, you will find a step-by-step complete process to deploy manually a Kubernetes cluster. Understanding how a cluster is created really helps solve problems, although it’s out of the scope of this book. Here, we will review automated Kubernetes solutions in which you can focus on your code and not on the platform itself. We will start this section with the most popular container <span class="No-Break">desktop solution.</span></p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor180"/>Docker Desktop</h2>
<p>We have used <a id="_idIndexMarker930"/>Docker Desktop in this book to <a id="_idIndexMarker931"/>create and run containers using a <strong class="bold">Windows Subsystem for Linux</strong> (<strong class="bold">WSL</strong>) terminal. Docker <a id="_idIndexMarker932"/>Desktop also includes a one-node Kubernetes environment. Let’s start using this by following <span class="No-Break">these steps:</span></p>
<ol>
<li>Click <strong class="bold">Settings</strong> | <strong class="bold">Enable Kubernetes</strong>. The following screenshot shows how Kubernetes can be set up in your Docker <span class="No-Break">Desktop environment:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 8.3 – The Docker Desktop Settings area where a Kubernetes cluster can be enabled" height="682" src="image/B19845_08_03.jpg" width="1111"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – The Docker Desktop Settings area where a Kubernetes cluster can be enabled</p>
<p class="list-inset">After the Kubernetes cluster is enabled, Docker Desktop starts the environment. The following screenshot shows the moment when <span class="No-Break">Kubernetes starts:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 8.4 – Kubernetes starting in the Docker Desktop environment" height="372" src="image/B19845_08_04.jpg" width="1138"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Kubernetes starting in the Docker Desktop environment</p>
<ol>
<li value="2">Once started, we can <a id="_idIndexMarker933"/>access the cluster from<a id="_idIndexMarker934"/> our WSL terminal by using the <strong class="source-inline">kubectl</strong> command line. As you may have noticed, we haven’t installed any additional software. Docker Desktop integrates the commands for us by attaching the required files to our <span class="No-Break">WSL environment.</span><p class="list-inset">The status of the Kubernetes cluster is shown in the lower-left side of the Docker Desktop GUI, as we can see in the <span class="No-Break">following screenshot:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt=" Figure 8.5 – Kubernetes’ status shown in Docker Desktop" height="940" src="image/B19845_08_05.jpg" width="820"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 8.5 – Kubernetes’ status shown in Docker Desktop</p>
<ol>
<li value="3">We can verify the<a id="_idIndexMarker935"/> number of nodes included in <a id="_idIndexMarker936"/>the deployed Kubernetes cluster by executing <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">get nodes</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ kubectl get nodes</strong>
<strong class="bold">NAME             STATUS   ROLES           AGE   VERSION</strong>
<strong class="bold">docker-desktop   Ready    control-plane   45m   v1.25.4</strong></pre><p class="list-inset">Notice that a Kubernetes configuration file was also added for <span class="No-Break">this environment:</span></p><pre class="source-code"><strong class="bold">$ ls -lart ~/.kube/config</strong>
<strong class="bold">-rw-r--r-- 1 frjaraur frjaraur 5632 May 27 11:21 /home/frjaraur/.kube/config</strong></pre></li> </ol>
<p>That was quite easy, and now we have a fully functional Kubernetes cluster. This cluster isn’t configurable, but it is all you need to prepare your applications’ deployments and test them. This solution doesn’t allow us to decide which Kubernetes version to deploy, but we can reset the environment at any time from the Docker Desktop Kubernetes <strong class="bold">Settings</strong> page, which is very useful when we need to start the environment afresh. It can be deployed on either Microsoft Windows (using<a id="_idIndexMarker937"/> a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) with Hyper-V or WSL, which is recommended as it consumes fewer resources), macOS (on Intel and Apple silicon <a id="_idIndexMarker938"/>architectures), or Linux (using a VM with <strong class="bold">Kernel-Based Virtual </strong><span class="No-Break"><strong class="bold">Machine</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">KVM</strong></span><span class="No-Break">)).</span></p>
<p>Docker prepares some images with all Kubernetes components and deploys them for us when we enable Kubernetes in Docker Desktop. By default, all containers created for such a purpose are hidden in the Docker Desktop GUI, but we can review them from the Docker command line. This <a id="_idIndexMarker939"/>Kubernetes solution is really<a id="_idIndexMarker940"/> suitable if you use the Docker command line to create your applications because everything necessary, from building to orchestrated execution, is provided. We can use different Kubernetes volume types because a <strong class="bold">storage class</strong> (dynamic<a id="_idIndexMarker941"/> storage provider integration) that binds the local Docker Desktop environment is provided. Therefore, we can prepare our applications as if they were to run in a production cluster. We will learn about <strong class="source-inline">storageClass</strong> resources in <a href="B19845_10.xhtml#_idTextAnchor231"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Leveraging Application Data Management</em><em class="italic"> in Kubernetes</em>. However, a few things are omitted in this Kubernetes deployment, which may impact your work, so it’s good to understand <span class="No-Break">its limitations:</span></p>
<ul>
<li>Environment internal IP addresses can’t <span class="No-Break">be changed.</span></li>
<li>You cannot ping containers; this is due to the network settings of Docker Desktop, and it also affects the <span class="No-Break">container runtime.</span></li>
<li>No CNI is provided; hence, no network policies can <span class="No-Break">be applied.</span></li>
<li>No Ingress resource is provided by default. This is not really an issue because other desktop Kubernetes environments wouldn’t provide it either, but you may need to deploy your own and modify your <strong class="source-inline">/etc/hosts</strong> file (or Microsoft Windows’s equivalent <strong class="source-inline">C:\Windows\system32\drivers\etc\hosts</strong> file) to access your applications. We will learn about Ingress resources and controllers in <a href="B19845_11.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <span class="No-Break"><em class="italic">Publishing Applications</em></span><span class="No-Break">.</span></li>
</ul>
<p>These are the more important issues you will find by using Docker Desktop for Kubernetes deployment. It is important to understand that performance will be impacted when you enable Kubernetes, and you<a id="_idIndexMarker942"/> will need at least 4 GB of RAM free and four <strong class="bold">virtual </strong><span class="No-Break"><strong class="bold">cores</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">vCores</strong></span><span class="No-Break">).</span></p>
<p>As specified in the official documentation, Docker Desktop is not an open source project, and it is licensed under the <em class="italic">Docker Subscription Service Agreement</em>. This means that it is free for small businesses (that is, fewer than 250 employees and less than $10 million in annual revenue), personal use, education, and non-commercial open source projects; otherwise, it <a id="_idIndexMarker943"/>requires<a id="_idIndexMarker944"/> a paid subscription for <span class="No-Break">professional use.</span></p>
<p>We will now review another desktop solution to deploy a simplified <span class="No-Break">Kubernetes environment.</span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor181"/>Rancher Desktop</h2>
<p>This solution comes <a id="_idIndexMarker945"/>from SUSE, and it really <a id="_idIndexMarker946"/>provides you with the Kubernetes Rancher deployment experience on your laptop or desktop computer. Rancher Desktop can be installed on Windows systems, using WSL or VMs, or macOS and Linux, using only VMs. It is an open source project and includes Moby components, <strong class="source-inline">containerd</strong>, and other components that leverage the experience of Rancher, allowing the development of new and different projects such as <strong class="bold">RancherOS</strong> (a <a id="_idIndexMarker947"/>container-oriented operating system) or <strong class="bold">K3s</strong> (a lightweight <a id="_idIndexMarker948"/>certified Kubernetes distribution). There are some interesting features on <span class="No-Break">Rancher Desktop:</span></p>
<ul>
<li>We can choose which container runtime to use for the environment. This is a key difference and makes it important to test your applications using <span class="No-Break"><strong class="source-inline">containerd</strong></span><span class="No-Break"> directly.</span></li>
<li>We can set the Kubernetes version <span class="No-Break">to deploy.</span></li>
<li>It is possible to define the resources used for the VM (on Mac <span class="No-Break">and Linux).</span></li>
<li>It provides the Rancher Dashboard, which combines perfectly with your infrastructure when your server’s environments also run Kubernetes <span class="No-Break">and Rancher.</span></li>
</ul>
<p>The following screenshot shows how can we set up a Kubernetes release from the Rancher Desktop GUI, in the <strong class="bold">Preferences</strong> area. This way, we will be able to test our applications using different API releases, which may be very interesting before moving our applications to the staging or production stages. Each Kubernetes release provides its own set of API resources; you should read each release note to find out changes in the API versions and resources that may affect your project – for example, if some <em class="italic">beta</em> resources are now included in the release, or some are deprecated. The following screenshot shows the <a id="_idIndexMarker949"/>Kubernetes releases available<a id="_idIndexMarker950"/> for deploying in <span class="No-Break">Rancher Desktop:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 8.6 – Different Kubernetes releases can be chosen" height="898" src="image/B19845_08_06.jpg" width="1388"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Different Kubernetes releases can be chosen</p>
<p>As we have seen in Docker Desktop, Rancher Desktop also provides a simple button to completely reset the Kubernetes cluster. The following screenshot shows the <strong class="bold">Troubleshooting</strong> area, where we can reset <span class="No-Break">the cluster:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 8.7 – The Troubleshooting area from the Rancher Desktop GUI" height="883" src="image/B19845_08_07.jpg" width="1379"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – The Troubleshooting area from the Rancher Desktop GUI</p>
<p>Rancher Desktop also <a id="_idIndexMarker951"/>deploys its own Ingress controller<a id="_idIndexMarker952"/> based on Traefik. This controller will help us to publish our applications, as we will learn in <a href="B19845_11.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Publishing Applications</em>. We can remove this component and deploy our own Ingress controller by unselecting the <strong class="bold">Traefik</strong> option in the <strong class="bold">Kubernetes</strong> <strong class="bold">Preferences</strong> section, but it is quite interesting to have one <span class="No-Break">by default.</span></p>
<p>The Rancher Dashboard is accessible by clicking on the Rancher Desktop notification icon and selecting <strong class="bold">Open cluster dashboard</strong>. Rancher Dashboard provides access to many Kubernetes resources graphically, which can be very useful for beginners. The following screenshot shows the Rancher Dashboard main page, where you can review and modify different deployed <span class="No-Break">Kubernetes resources:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<img alt="Figure 8.8 – The Rancher Dashboard main page" height="920" src="image/B19845_08_08.jpg" width="1413"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – The Rancher Dashboard main page</p>
<p>We can verify<a id="_idIndexMarker953"/> the <a id="_idIndexMarker954"/>Kubernetes environment from a WSL terminal by checking <span class="No-Break">its version:</span></p>
<pre class="console">
$ kubectl version --short
Flag --short has been deprecated, and will be removed in the future. The --short output will become the default.
Client Version: v1.27.2
Kustomize Version: v5.0.1
Server Version: v1.27.2+k3s1</pre> <p class="callout-heading">Important note</p>
<p class="callout">We can build images for the Kubernetes environment without using an external registry by using <strong class="source-inline">nerdctl</strong> with the  <strong class="source-inline">-–namespace k8s.io</strong> argument. This way, images will be available directly for <span class="No-Break">our deployments.</span></p>
<p>It is interesting that this Kubernetes implementation is aligned with the expected network features from Kubernetes; hence, we can ping Pods and Services, or even access the Service ports from the WSL environment. It also makes our applications accessible by adding a <strong class="source-inline">.localhost</strong> suffix to our host definitions (we will deep dive into this option in <a href="B19845_11.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Publishing Applications</em>). However, this cluster is still a standalone node, and we can’t test the behavior of our applications under certain failures or movements between nodes. If you really need to test these features, we need to go further and deploy additional nodes with <span class="No-Break">other solutions.</span></p>
<p>Both Docker Desktop<a id="_idIndexMarker955"/> and Rancher Desktop provide <a id="_idIndexMarker956"/>GUI-based Kubernetes deployments, but usually, if you don’t need any GUI, we can even deploy more <span class="No-Break">lightweight solutions.</span></p>
<p>We will now review Minikube, which may be the most complete and <span class="No-Break">pluggable solution.</span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor182"/>Minikube</h2>
<p>The Minikube<a id="_idIndexMarker957"/> Kubernetes <a id="_idIndexMarker958"/>environment is very configurable and consumes considerably fewer hardware resources than other solutions, allowing us to deploy more than one node per cluster, or even multiple clusters on one single computer host. We can create a Kubernetes cluster by using either Docker, QEMU, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMware Fusion/Workstation. We can use many different virtualization solutions or even container runtimes, and Minikube can be deployed on Microsoft Windows, macOS, or Linux <span class="No-Break">operating systems.</span></p>
<p>These are some of the features <span class="No-Break">of Minikube:</span></p>
<ul>
<li>It supports different <span class="No-Break">Kubernetes releases</span></li>
<li>Different container runtimes can <span class="No-Break">be used</span></li>
<li>A direct API endpoint improves <span class="No-Break">image management</span></li>
<li>Advanced Kubernetes customization such as the addition of <span class="No-Break"><strong class="bold">feature gates</strong></span></li>
<li>It is a pluggable solution, so we can include add-ons such as Ingress for extended <span class="No-Break">Kubernetes features</span></li>
<li>It supports integration with common <span class="No-Break">CI environments</span></li>
</ul>
<p>Kubernetes deployment is easy, and you just require a single binary for Linux systems. Different arguments can be used to set up the environment. Let’s review some of the <span class="No-Break">most important:</span></p>
<ul>
<li><strong class="source-inline">start</strong>: This action creates and starts a Kubernetes cluster. We can use the argument <strong class="source-inline">--nodes</strong> to define the number of nodes to deploy and <strong class="source-inline">--driver</strong> to specify which method to use to create a cluster. Virtual hardware resources can also be defined by using <strong class="source-inline">--cpu</strong> and <strong class="source-inline">--memory</strong>; by default, 2 CPUs and 2 GB of memory will be used. We can even choose a specific CNI to deploy with the <strong class="source-inline">--cni</strong> argument (<strong class="source-inline">auto</strong>, <strong class="source-inline">bridge</strong>, <strong class="source-inline">calico</strong>, <strong class="source-inline">cilium</strong>, <strong class="source-inline">flannel</strong>, and <strong class="source-inline">kindnet</strong> are available, but we can add our own path to a <span class="No-Break">CNI manifest).</span></li>
<li><strong class="source-inline">status</strong>: This action shows the status of the <span class="No-Break">Minikube cluster.</span></li>
<li> <strong class="source-inline">stop</strong>: This stops<a id="_idIndexMarker959"/> a<a id="_idIndexMarker960"/> running <span class="No-Break">Minikube cluster.</span></li>
<li><strong class="source-inline">delete</strong>: This action deletes a previously <span class="No-Break">created cluster.</span></li>
<li><strong class="source-inline">dashboard</strong>: An open source Kubernetes Dashboard can be deployed as an add-on, which can be accessed by using <span class="No-Break"><strong class="source-inline">minikube dashboard</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">service</strong>: This option can be very interesting to expose a deployed application Service. It returns the Service URL that can be used to <span class="No-Break">access it.</span></li>
<li><strong class="source-inline">mount</strong>: We can mount host directories into the Minikube nodes with <span class="No-Break">this option.</span></li>
<li><strong class="source-inline">ssh</strong>: We can access Kubernetes deployed hosts by using <strong class="source-inline">minikube </strong><span class="No-Break"><strong class="source-inline">ssh &lt;NODE&gt;</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">node</strong>: This action allows us to manage <span class="No-Break">cluster nodes.</span></li>
<li><strong class="source-inline">kubectl</strong>: This runs a <strong class="source-inline">kubectl</strong> binary matching the <span class="No-Break">cluster version.</span></li>
<li><strong class="source-inline">addons</strong>: One of the best features of Minikube is that we can extend its functionality with plugins to manage additional storage options for the cluster (for example, to define a specific a <strong class="source-inline">csi-hostpath-driver</strong>, or specify the default storage class to use, <strong class="source-inline">default-storageclass</strong>, or a dynamic <strong class="source-inline">storage-provisioner</strong>, among other options), Ingress controllers (<strong class="source-inline">ingress</strong>, <strong class="source-inline">ingress-dns</strong>, <strong class="source-inline">istio</strong>, and <strong class="source-inline">kong</strong>), and security (<strong class="source-inline">pod-security-policy</strong>). We can even deploy the Kubernetes Dashboard or the metrics server automatically, which recover metrics from all <span class="No-Break">running workloads.</span></li>
</ul>
<p>To create a cluster <a id="_idIndexMarker961"/>with two nodes (a master and a worker) we<a id="_idIndexMarker962"/> can simply execute <strong class="source-inline">minikube start --nodes 2</strong>. Let’s see this <span class="No-Break">in action:</span></p>
<pre class="console">
PS &gt; minikube start --nodes 2 `
--kubernetes-version=stable `
--driver=hyperv
* minikube v1.30.1 on Microsoft Windows 10 Pro 10.0.19045.2965 Build 19045.2965
* Using the hyperv driver based on user configuration
* Starting control plane node minikube in cluster minikube
...
* Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default</pre> <p>Once deployed, we can review the cluster state <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">kubectl</strong></span><span class="No-Break">:</span></p>
<pre class="console">
PS &gt; kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
minikube       Ready    control-plane   25m   v1.26.3
minikube-m02   Ready    &lt;none&gt;          22m   v1.26.3</pre> <p>Minikube is a very configurable solution that provides common Kubernetes features. In my opinion, it is the best in terms of performance <span class="No-Break">and features.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Minikube Kubernetes deployments in Microsoft Windows require administrator privileges if you use Hyper-V. Therefore, you need to open PowerShell or Command Prompt as administrator, but this may not be enough. PowerShell Hyper-V must also be included, and we will need to execute <strong class="source-inline">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V-Tools-All –All</strong> on a PowerShell console<a id="_idIndexMarker963"/> to <a id="_idIndexMarker964"/><span class="No-Break">enable it.</span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor183"/>Alternative Kubernetes desktop deployments</h2>
<p>Nowadays, there are <a id="_idIndexMarker965"/>other interesting options that use even fewer hardware resources, but they don’t provide as many features as Minikube. Let’s discuss some good candidates <span class="No-Break">to try:</span></p>
<ul>
<li><strong class="bold">kind</strong>: This solution takes advantage of a Docker runtime installation to deploy Kubernetes using containers and their own custom images. It is based on kubeadm deployment, and it really works very nicely on Linux desktop systems in which you don’t usually install Docker Desktop to <span class="No-Break">run containers.</span></li>
<li><strong class="bold">K3s</strong>: This Kubernetes deployment is the basis for the Rancher Desktop Kubernetes feature. It deploys a lightweight environment using less memory with customized binaries. This may impact your application’s deployment if you use bleeding-edge features, as they will probably not be available. This solution comes from Rancher-SUSE, which also provides K3D to deploy Kubernetes <span class="No-Break">using containers.</span></li>
<li><strong class="bold">Podman Desktop</strong>: This new solution appeared in mid-2022 and provides an open source GUI-based environment to run Kubernetes with Podman and <strong class="source-inline">containerd</strong>. It is in its early stages, but it currently seems a good solution if you don’t use any Docker tool in your <span class="No-Break">application development.</span></li>
</ul>
<p>We can now continue and deep dive into the concepts of Pods <span class="No-Break">and Services.</span></p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor184"/>Creating Pods and Services</h1>
<p>In this section, we <a id="_idIndexMarker966"/>will learn <a id="_idIndexMarker967"/>about the resources we will use in the Kubernetes orchestration platform to deploy our applications. We will start by learning how containers are implemented <span class="No-Break">inside Pods.</span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor185"/>Pods</h2>
<p>A <strong class="bold">Pod</strong> is the smallest <a id="_idIndexMarker968"/>scheduling unit in Kubernetes. We can include in a Pod multiple containers, and they will always share a uniquely defined IP address within the cluster. All containers inside a Pod will resolve <strong class="source-inline">localhost</strong>; hence, we can only use ports once. Volumes associated with a Pod are also shared between containers. We can consider a Pod as a small VM in which different processes (containers) run together. Pods are considered in a healthy state when all their containers <span class="No-Break">run correctly.</span></p>
<p>As Pods can contain many containers, we can think of using a Pod to deploy applications with multiple components. All containers associated with a Pod run on the same cluster host. This way, we can ensure that all application components run together, and their intercommunications will definitely be faster. Because Pods are the smallest unit in Kubernetes, we are only able to scale up or down complete Pods; hence, all the containers included will also be executed multiple times, which probably isn’t what we need. Not all applications’ components should follow the same scaling rules; hence, it is better to deploy multiple Pods for <span class="No-Break">an application.</span></p>
<p>The following diagram shows a Pod. Two containers are included, and thus, they share the IP address and volumes of the <span class="No-Break">same Pod:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 8.9 – A schema of a Pod with two containers included" height="357" src="image/B19845_08_09.jpg" width="819"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – A schema of a Pod with two containers included</p>
<p>We can run Pods that share the host’s resources by using the host’s namespaces (the host’s <a id="_idIndexMarker969"/>network, <strong class="bold">inter-process communications</strong> or <strong class="bold">IPCs</strong>, processes, and so on). We should limit this type<a id="_idIndexMarker970"/> of Pod because they have direct access to a host’s processes, interfaces, and <span class="No-Break">so on.</span></p>
<p>The following example shows a declarative file to execute an <span class="No-Break">example Pod:</span></p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
    name: examplepod
  labels:
    example: singlecontainer
  spec:
      containers:
      - name: examplecontainer
        image: nginx:alpine</pre> <p>We can use JSON or YAML files to define Kubernetes resources, but YAML files are more popular; hence, you have to take care of indentation when preparing your deployment files. To deploy this Pod on our Kubernetes cluster, we will simply execute <strong class="source-inline">kubectl create -</strong><span class="No-Break"><strong class="source-inline">f &lt;PATH_TO_THE_FILE&gt;</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">When you access a Kubernetes cluster, a namespace is associated with your profile or context. By default, the namespace associated is <strong class="source-inline">default</strong>; therefore, if we don’t specify any Kubernetes namespace as an argument to create a resource, the <strong class="source-inline">default</strong><em class="italic"> </em>namespace will be used. We can change the namespace for the current context to be applied to all commands hereafter, by executing <strong class="source-inline">kubectl config use-context --current --namespace &lt;NEW_NAMESPACE&gt;</strong>. The namespace for each resource can be included under the <strong class="source-inline">metadata</strong> key in the resource’s <span class="No-Break">YAML manifest:</span></p>
<p class="callout"><strong class="source-inline">...</strong></p>
<p class="callout"><span class="No-Break"><strong class="source-inline">metadata:</strong></span></p>
<p class="callout"><strong class="source-inline"> </strong><span class="No-Break"><strong class="source-inline">name: podname</strong></span></p>
<p class="callout"><strong class="source-inline"> </strong><span class="No-Break"><strong class="source-inline">namespace: my-namespace</strong></span></p>
<p class="callout"><strong class="source-inline">...</strong></p>
<p>We can modify any <a id="_idIndexMarker971"/>aspect used for the container by modifying the container image’s behavior, as we learned with the Docker command line in <a href="B19845_04.xhtml#_idTextAnchor096"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Running </em><span class="No-Break"><em class="italic">Docker Containers</em></span><span class="No-Break">.</span></p>
<p>If you are not sure of the available keys or are learning how to use a new Kubernetes resource, you can use <strong class="source-inline">kubectl explain &lt;RESOURCE&gt;</strong> to retrieve an accurate description of the keys available and the <span class="No-Break">expected values:</span></p>
<pre class="console">
PS &gt; kubectl explain pod
KIND:     Pod
VERSION:  v1
DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
...
 FIELDS:
   apiVersion   &lt;string&gt;
     APIVersion defines the versioned schema of this representation of an
 ...
   spec &lt;Object&gt;
     ...
   status       &lt;Object&gt;
     …</pre> <p>We can continue<a id="_idIndexMarker972"/> adding keys to obtain more specific definitions – for example, we can retrieve the keys <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">pod.spec.containers.resources</strong></span><span class="No-Break">:</span></p>
<pre class="console">
PS &gt; kubectl explain pod.spec.containers.resources
KIND:     Pod
...
RESOURCE: resources &lt;Object&gt;
 ...
DESCRIPTION:
     Compute Resources required by this container. Cannot be updated. More info:
     https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
 ...
FIELDS:
   claims       &lt;[]Object&gt;
     Claims lists the names of resources, defined in spec.resourceClaims, that
     are used by this container.
 ...
    limits       &lt;map[string]string&gt;
     Limits describes the maximum amount of compute resources allowed. More
   ...
   requests     &lt;map[string]string&gt;
     Requests describes the minimum amount of compute resources required. If
   ...</pre> <p>Each description shows extended information with links to the <span class="No-Break">Kubernetes documentation.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">We can retrieve all available keys at once for a specific resource by using <strong class="source-inline">kubectl explain pod --recursive</strong>. This option really helps us to fully customize <span class="No-Break">the resources.</span></p>
<p>We can test a real Pod<a id="_idIndexMarker973"/> deployment using an NGINX web server. To do this, follow <span class="No-Break">these steps:</span></p>
<ol>
<li>We will use the imperative mode with <span class="No-Break"><strong class="source-inline">kubectl run</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">PS &gt; kubectl run webserver --image=docker.io/nginx:alpine `</strong>
<strong class="bold">--port 80</strong>
<strong class="bold">pod/webserver created</strong></pre></li> <li>We can now list the Pods to verify whether the webserver <span class="No-Break">is running:</span><pre class="source-code">
<strong class="bold">PS &gt; kubectl get pods</strong>
<strong class="bold">NAME        READY   STATUS              RESTARTS   AGE</strong>
<strong class="bold">webserver   0/1     ContainerCreating   0          5s</strong></pre></li> <li>As we can see, it is starting. After a few seconds, we can verify that our web server <span class="No-Break">is running:</span><pre class="source-code">
<strong class="bold">PS &gt; kubectl get pods -o wide</strong>
<strong class="bold">NAME        READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES</strong>
<strong class="bold">webserver   1/1     Running   0          2m40s   10.244.0.3   minikube   &lt;none&gt;           &lt;none&gt;</strong></pre></li> <li>We use <strong class="source-inline">-o wide</strong> to modify the command’s output. As you can see, we now have the associated <span class="No-Break">IP address.</span></li>
<li>We can get into the <strong class="source-inline">minikube</strong> node and verify the Pod’s connectivity. The following screenshot<a id="_idIndexMarker974"/> shows the interaction with the <span class="No-Break">cluster node:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 8.10 – Testing connectivity from the minikube node" height="852" src="image/B19845_08_10.jpg" width="1268"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Testing connectivity from the minikube node</p>
<ol>
<li value="6">If we now delete the Pod and create a new one, we can easily see that that new Pods can receive a new IP address, and thus, our application may need to change the IP <span class="No-Break">addresses continuously:</span><pre class="source-code">
<strong class="bold">PS &gt; kubectl delete pod webserver</strong>
<strong class="bold">pod "webserver" deleted</strong>
<strong class="bold">PS &gt; kubectl run webserver `</strong>
<strong class="bold">--image=docker.io/nginx:alpine --port 80</strong>
<strong class="bold">pod/webserver created</strong>
<strong class="bold">PS &gt; kubectl get pods -o wide</strong>
<strong class="bold">NAME        READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES</strong>
<strong class="bold">webserver   1/1     Running   0          4s    10.244.0.4   minikube   &lt;none&gt;           &lt;none&gt;</strong></pre></li> </ol>
<p>This is a real problem, and that’s why<a id="_idIndexMarker975"/> we never use Pod IP addresses. Let’s talk now about Services and how they can help us to solve <span class="No-Break">this problem.</span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor186"/>Services</h2>
<p><strong class="bold">Services</strong> are<a id="_idIndexMarker976"/> abstract objects in Kubernetes; they are used to expose a set of Pods and, thus, they serve an application component. They will get an IP address from the internal Kubernetes IPAM system, and we will use this to access the associated Pods. We can also associate Services with external resources to make them accessible to users. Kubernetes offers different types of Services to be published either internally or outside a cluster. Let’s quickly review the different <span class="No-Break">Service types:</span></p>
<ul>
<li><strong class="source-inline">ClusterIP</strong>: This is the <a id="_idIndexMarker977"/>default Service type. Kubernetes associates an IP address from the defined Service’s IP address range, and containers will be able to access this Service by either its IP address or its name. Containers running in the same namespace will be able to simply use the Service’s name, while other containers will need to use its Kubernetes FQDN (<strong class="source-inline">SERVICE_NAME.SERVICE_NAMESPACE.svc.cluster.local</strong>, where <strong class="source-inline">cluster.local</strong> is the FQDN of the Kubernetes cluster itself). This is due to Kubernetes’<a id="_idIndexMarker978"/> internal <strong class="bold">service discovery</strong> (<strong class="bold">SD</strong>), which creates DNS entries for all <span class="No-Break">Services cluster-wide.</span></li>
<li><strong class="source-inline">Headless</strong>: These Services are a variant of the <strong class="source-inline">ClusterIP</strong> type. They don’t receive an IP address, and the Service’s name will resolve all the associated Pods’ IP addresses. We commonly use headless Services to interact with non-Kubernetes <span class="No-Break">SD solutions.</span></li>
<li><strong class="source-inline">NodePort</strong>: When we use a <strong class="source-inline">NodePort</strong> Service, we associate a set of hosts’ ports with the Service <strong class="source-inline">clusterIP</strong>’s defined IP address. This makes the Service accessible from outside a cluster. We can connect from a client computer to any of the cluster hosts in the defined port, and Kubernetes will route requests to the Service, associated with the <strong class="source-inline">ClusterIP</strong> address via internal DNS, no matter which node received the request. Thus, the Pods associated with the Service receive network traffic from <span class="No-Break">the client.</span></li>
<li><strong class="source-inline">LoadBalancer</strong>: The <strong class="source-inline">LoadBalancer</strong> Service type is used to publish a defined Service in an external load balancer. It uses the external load balancer’s API to define the required rules to reach the cluster, and indeed, this model uses a <strong class="source-inline">NodePort</strong> Service <a id="_idIndexMarker979"/>type to reach the Service cluster-wide. This Service type is mainly used to publish Services in cloud providers’ Kubernetes clusters, although some vendors also provide this <span class="No-Break">feature on-premises.</span></li>
</ul>
<p>We have seen how Services <a id="_idIndexMarker980"/>can be published outside the Kubernetes cluster to be consumed by other external applications or even our users, but we can also do the opposite. We can include external Services, available in our real network, inside our Kubernetes clusters by using the <strong class="source-inline">External</strong> <span class="No-Break">Service type.</span></p>
<p>The following schema represents a <strong class="source-inline">NodePort</strong> Service in which we publish port <strong class="source-inline">7000</strong>, attached to port <strong class="source-inline">5000</strong>, and exposed in the containers in <span class="No-Break">this example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 8.11 – NodePort Service schema" height="328" src="image/B19845_08_11.jpg" width="776"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – NodePort Service schema</p>
<p>In this example, the external requests from users are load-balanced to port <strong class="source-inline">7000</strong>, listening on all cluster hosts. All traffic from the users will be internally load-balanced to port <strong class="source-inline">5000</strong>, making it available on all Services’ <span class="No-Break">assigned Pods.</span></p>
<p>The following example shows<a id="_idIndexMarker981"/> the manifest of a Kubernetes Service, obtained by using the <strong class="bold">imperative method</strong> to<a id="_idIndexMarker982"/> retrieve the <span class="No-Break">output only:</span></p>
<pre class="source-code">
PS &gt;   kubectl  expose  pod  webserver  -o yaml  `
--port 80  --dry-run=client
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    run: webserver
  name: webserver
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: webserver
status:
  loadBalancer: {}</pre> <p>In this example, the Service resource isn’t created because we added the <strong class="source-inline">-o yaml</strong> argument to show the output in the YAML format and <strong class="source-inline">–dry-run=client</strong>. This option shows the output of the creation command executed <span class="No-Break">against kube-apiserver.</span></p>
<p>Let’s move on now to <a id="_idIndexMarker983"/>learn how to deploy workloads in a cluster because Pods don’t provide resilience; they run as unmanaged standalone workloads without <span class="No-Break">a controller.</span></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor187"/>Deploying orchestrated resources</h1>
<p>Now that we know how to <a id="_idIndexMarker984"/>deploy Pods using imperative and declarative modes, we will define new resources that can manage the Pods’ life cycle. Pods executed directly with the <strong class="source-inline">kubectl</strong> command line are not recreated if their containers die. To control the workloads within a Kubernetes cluster, we will need to deploy additional resources, managed by Kubernetes controllers. These controllers are control loops that monitor the state of different Kubernetes resources and make or request changes when needed. Each controller tracks some resources and tries to maintain their defined state. Kubernetes’ kube-controller-manager manages these controllers that maintain the overall desired state of different cluster resources. Each controller can be accessed via an API, and we will use <strong class="source-inline">kubectl</strong> to interact <span class="No-Break">with them.</span></p>
<p>In this section, we will learn the basics of Kubernetes controllers and dive deep into how to use<a id="_idIndexMarker985"/> them in <a href="B19845_09.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Implementing </em><span class="No-Break"><em class="italic">Architecture Patterns</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor188"/>ReplicaSets</h2>
<p>The most simple <a id="_idIndexMarker986"/>resource that allows us to maintain a defined state<a id="_idIndexMarker987"/> for our application is a ReplicaSet. It will keep a set of replica Pods running. To create a ReplicaSet, we will use a Pod manifest as a template to create multiple replicas. Let’s see a <span class="No-Break">quick example:</span></p>
<pre class="source-code">
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicated-webserver
spec:
  replicas: 3
  selector:
    matchLabels:
      application: webserver
  template:
     metadata:
         application: webserver
     spec:
       containers:
       - name: webserver-container
         image: docker.io/nginx:alpine</pre> <p>This ReplicaSet will run three Pods with the same <strong class="source-inline">docker.io/nginx:alpine</strong> image; the <strong class="source-inline">template</strong> section defines the specifications for these three Pod resources, with one container each. The ReplicaSet identifies Pods to manage by using the defined <strong class="source-inline">application</strong> label and its <strong class="source-inline">webserver</strong> value, defined in the Pod’s <strong class="source-inline">template</strong> section of <span class="No-Break">the manifest.</span></p>
<p>When we deploy this ReplicaSet, the cluster creates these three Pods, and whenever any of them dies, the controller manages this change and triggers the creation of a <span class="No-Break">new one.</span></p>
<p>We will continue to <a id="_idIndexMarker988"/>review more resources, but keep in mind that the<a id="_idIndexMarker989"/> basic idea of a <strong class="source-inline">template</strong> section embedded inside a more general definition applies to all <span class="No-Break">of them.</span></p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor189"/>Deployments</h2>
<p>We may think <a id="_idIndexMarker990"/>of a Deployment as an <a id="_idIndexMarker991"/>evolution of a ReplicaSet. It allows us to update them because a deployment manages a set of replicas but only runs one. Every time we create a new deployment, we create an associated ReplicaSet. And when we update this deployment, a new ReplicaSet is created with a new definition, reflecting the changes from the <span class="No-Break">previous resource.</span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor190"/>DaemonSets</h2>
<p>With a DaemonSet, we can <a id="_idIndexMarker992"/>ensure that all cluster nodes get one replica <a id="_idIndexMarker993"/>of our workload, but we cannot define the number of replicas in <span class="No-Break">a DaemonSet.</span></p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor191"/>StatefulSets</h2>
<p>A StatefulSet allows more<a id="_idIndexMarker994"/> advanced features in our <a id="_idIndexMarker995"/>workloads. It allows us to manage the order and uniqueness of Pods, ensuring that each replica gets its own unique set of resources, such as volumes. Although Pods are created from the same template section, a StatefulSet maintains a different identity for <span class="No-Break">each Pod.</span></p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor192"/>Jobs</h2>
<p>A Job creates one <a id="_idIndexMarker996"/>or<a id="_idIndexMarker997"/> more Pods at a time, but it will continue creating them until a defined number of them terminate successfully. When a Pod exits, the controller verifies whether the number of required completions was reached, and<a id="_idIndexMarker998"/> if not, it creates a <span class="No-Break">new one.</span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor193"/>CronJobs</h2>
<p>We can schedule Pods by<a id="_idIndexMarker999"/> using CronJobs, as they schedule jobs. When <a id="_idIndexMarker1000"/>the execution time comes, a Job is created and triggers the creation of defined Pods. As we will learn in <a href="B19845_09.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Implementing Architecture Patterns</em>, CronJobs manifests include two <strong class="source-inline">template</strong> sections – one to create jobs and another one to define how Pods will <span class="No-Break">be created.</span></p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor194"/>ReplicationControllers</h2>
<p>We can <a id="_idIndexMarker1001"/>consider ReplicationControllers a previous <a id="_idIndexMarker1002"/>version of the current ReplicaSet resource types. They work similarly to how we keep a number of Pod replicas alive, but they differ in how they group the monitored Pods because ReplicationControllers do not support set-based selectors. This selector method allows ReplicaSets to acquire the state management of Pods created outside of their own manifest; hence, if a Pod already running matches the ReplicaSet label’s selection, it will be automatically included in the pool of <span class="No-Break">replicated Pods.</span></p>
<p>Now that we have an overview of different resources that allow us to create orchestrated resources cluster-wide, we can learn some of the Kubernetes features that can improve our <span class="No-Break">applications’ security.</span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor195"/>Improving your applications’ security with Kubernetes</h1>
<p>Applications running in<a id="_idIndexMarker1003"/> containers offer many <a id="_idIndexMarker1004"/>new different features. We can run multiple applications’ releases at a time in a host; they start and stop in seconds. We can scale components easily, and different applications can coexist without even interaction between them. An application’s resilience is also inherited from the container runtime features (exited <span class="No-Break">containers autostarting).</span></p>
<p>However, we can also improve our applications by running them in Kubernetes. Each Kubernetes cluster is composed of multiple container runtimes running together and in coordination. Container runtimes isolate the hosts’ resources thanks to kernel namespaces<a id="_idIndexMarker1005"/> and <strong class="bold">control groups</strong> (<strong class="bold">cgroups</strong>), but Kubernetes adds some <span class="No-Break">interesting features:</span></p>
<ul>
<li><strong class="bold">Namespaces</strong>: Namespaces are<a id="_idIndexMarker1006"/> Kubernetes resources that group other resources and are designed to distribute Kubernetes resources between multiple users, grouped in teams <span class="No-Break">or projects.</span></li>
<li><strong class="bold">Authentication strategies</strong>: Requests from Kubernetes clients may use different authentication mechanisms, such as client certificates, bearer tokens, or an authenticating proxy to <span class="No-Break">authenticate them.</span></li>
<li><strong class="bold">Authorization requests</strong>: Users request the Kubernetes API after passing authentication, authorization, and different admission controllers. The authorization phase involves granting permission to access Kubernetes’ resources and features. Requests’ attributes<a id="_idIndexMarker1007"/> are evaluated <a id="_idIndexMarker1008"/>against policies, and they are allowed or denied. The user, group, API, request path, namespace, verb, and so on that are provided in the requests are used for <span class="No-Break">these validations.</span></li>
<li><strong class="bold">Admission controllers</strong>: Once the authentication and authorization have been passed, other mechanisms are in place. Validation and mutation admission controllers decide whether or not requests are accepted or even modify them to accomplish certain defined rules. For example, we can prevent the execution of any Pod or container executed as <strong class="source-inline">root</strong>, or simply change the final resulting user to a non-privileged user in order to maintain <span class="No-Break">cluster security.</span></li>
<li><strong class="bold">Role-based access control</strong> (<strong class="bold">RBAC</strong>): Several<a id="_idIndexMarker1009"/> authorization models are available in Kubernetes (a node, <strong class="bold">attribute-based access control</strong> (<strong class="bold">ABAC</strong>), RBAC, and <a id="_idIndexMarker1010"/>Webhooks). RBAC is used to control a resource’s permissions cluster-wide by assigning roles to Kubernetes users. We can have internal users (with cluster scope), such<a id="_idIndexMarker1011"/> as <strong class="bold">ServiceAccounts</strong>, and external users, such as those who execute tasks and workloads in the cluster using their clients. These roles and their users’ binding can be defined at the namespace level and cluster-wide. Also, we can use <strong class="source-inline">kubectl</strong> to check whether some verbs are available for us or even for another user, by <span class="No-Break">using impersonation:</span><pre class="source-code">
$ kubectl auth can-i create pods –namespace dev
yes
$ kubectl auth can-i create svc -n prod –as dev-user
no</pre></li> <li><strong class="bold">Secrets and ConfigMaps</strong>: We already learned how to deploy certain configurations in orchestrated environments in <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating with Swarm</em>. In Kubernetes, we also have Secrets and ConfigMaps resources, but they can be retrieved by users if they are allowed (RBAC). It is important to understand that Secrets are packaged in the Base64 format; hence, sensitive data can be accessed if we <a id="_idIndexMarker1012"/>don’t prepare appropriate roles. The kubelet Kubernetes component will mount Secrets and ConfigMaps automatically for you, and we can use them as<a id="_idIndexMarker1013"/> files <a id="_idIndexMarker1014"/>or environment variables in our application deployments. Kubernetes can encrypt Secrets at rest to ensure that operating systems administrators can’t retrieve them from the etcd database files, but this capability is disabled <span class="No-Break">by default.</span></li>
<li><strong class="bold">Security contexts</strong>: Security contexts can be defined either at the Pod or container level and allow us to specify the security features to be applied to our workloads. They are key in maintaining security in Kubernetes. We can specify whether our application requires some special capabilities, whether the containers run in read-only mode (the root filesystem keeps immutable), or the ID used for the main process execution, among many other features. We will dive deep into how they can help us protect our applications in <a href="B19845_09.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Implementing Architecture Patterns</em>. It is important to know that we can use admission controllers to enforce defined <strong class="source-inline">securityContext</strong> profiles, and this is essential because we can ensure that a Pod doesn’t run as root, privileged, or use non-read-only containers on any <span class="No-Break">defined namespace.</span></li>
<li><strong class="bold">Network policies</strong>: Kubernetes deploys a flat network by default; hence, all containers can reach each other. To avoid such a situation, Kubernetes also provides NetworkPolicies and GlobalNetworkPolicies (applied at the cluster level). Not all CNIs are able to implement this feature. Kubernetes only provides <strong class="bold">custom resource</strong> (<strong class="bold">CR</strong>) types, which <a id="_idIndexMarker1015"/>will be used to implement<a id="_idIndexMarker1016"/> the <strong class="bold">network policies</strong>. Verify that your network provider can implement them to be able to use this feature (lots of popular CNI plugins such as Calico, Canal, and Cilium are completely capable). It is a good recommendation to implement some default global policies to drop all external accesses and allow the required communications for each application at the namespace level. Network policies define both Ingress and Egress rules. These rules work at the connectivity level; hence, we don’t have raw packet logging (although some CNI plugins provide some logging features). We will learn<a id="_idIndexMarker1017"/> how to implement rules following best practices in <a href="B19845_11.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <span class="No-Break"><em class="italic">Publishing Applications</em></span><span class="No-Break">.</span></li>
</ul>
<p>Now that we have<a id="_idIndexMarker1018"/> an <a id="_idIndexMarker1019"/>overview of the most important features available in Kubernetes that help us to protect both our applications and the entire cluster, we can continue by creating some simple labs that will cover the basic usage of a <span class="No-Break">Kubernetes environment.</span></p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor196"/>Labs</h1>
<p>Now, we will have a short lab section that will help us to learn and understand the basics of deploying a local Kubernetes environment with Minikube, testing some of its resource types to validate <span class="No-Break">the cluster.</span></p>
<p>The code for the labs is available in this book’s GitHub repository at https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git. Ensure you have the latest revision available by simply executing <strong class="source-inline">git clone https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git</strong> to download all its content or <strong class="source-inline">git pull</strong> if you already downloaded the repository before. All commands and content used in these labs will be located inside the <span class="No-Break"><strong class="source-inline">Containers-for-Developers-Handbook/Chapter8</strong></span><span class="No-Break"> directory.</span></p>
<p>We will start by deploying a Minikube cluster with two nodes (one master and one worker). We will deploy them with 3 GB of RAM each, which will be more than enough to test application behavior when some of the cluster node dies, but you will probably not need two nodes for your <span class="No-Break">daily usage.</span></p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor197"/>Deploying a Minikube cluster with two nodes</h2>
<p>In this lab, we will <a id="_idIndexMarker1020"/>deploy a fully functional Kubernetes cluster locally, for testing purposes. We will continue working on a Windows 10 laptop with 16 GB of RAM, which is enough for the labs in this book. Follow <span class="No-Break">these steps:</span></p>
<ol>
<li>Install Minikube. First, download it from https://minikube.sigs.k8s.io/docs/start/, choose the appropriate installation method, and follow the simple installation steps for your operating system. We will use Hyper-V; hence, it must be enabled and running on your desktop computer <span class="No-Break">or laptop.</span></li>
<li>Once Minikube is installed, we will open an administrator PowerShell terminal. Minikube deployments using Hyper-V require execution with administrator privileges. This is due to the Hyper-V layer; hence, admin privileges won’t be required if you use VirtualBox as your hypervisor or Linux as your operating system (other hypervisors can be used, such as KVM, which works very nicely with Minikube). Admin rights are also required to remove the Minikube cluster. Once the PowerShell terminal is ready, we execute the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">minikube start –nodes 2 –memory 3G –cpus 2 `</strong>
<strong class="bold">--kubernetes-version=stable –driver=hyperv `</strong>
<strong class="bold">--cni=calico `</strong>
<strong class="bold">--addons=ingress,metrics-server,csi-hostpath-driver</strong></pre><p class="list-inset">Notice the <strong class="source-inline">`</strong> character for breaking lines in Powershell. The preceding code snippet will create a cluster with two nodes (each with 3 GB of memory) with the current stable Kubernetes release (1.26.3 at the time of writing), using Calico as the CNI and the hosts’ storage for volumes. It also delivers automatically an Ingress controller <span class="No-Break">for us.</span></p><p class="list-inset">Let’s create a two-node<a id="_idIndexMarker1021"/> cluster using <span class="No-Break"><strong class="source-inline">minikube start</strong></span><span class="No-Break">:</span></p><pre class="source-code"><strong class="bold">PS C:\Windows\system32&gt; cd c:\</strong>
<strong class="bold">PS C:\&gt;  minikube start –nodes 2 –memory 3G `</strong>
<strong class="bold">--cpus 2 –kubernetes-version=stable `</strong>
<strong class="bold">--driver=hyperv –cni=calico `</strong>
<strong class="bold">--addons=ingress,metrics-server,csi-hostpath-driver</strong>
<strong class="bold">* minikube v1.30.1 on Microsoft Windows 10 Pro 10.0.19045.2965 Build 19045.2965</strong>
<strong class="bold">* Using the hyperv driver based on user configuration</strong>
<strong class="bold">* Starting control plane node minikube in cluster minikube</strong>
<strong class="bold">* Creating hyperv VM (CPUs=2, Memory=3072MB, Disk=20000MB) …</strong>
<strong class="bold">* Preparing Kubernetes v1.26.3 on Docker 20.10.23 ...</strong>
<strong class="bold">  - Generating certificates and keys ...</strong>
<strong class="bold">...</strong>
<strong class="bold">  - Using image registry.k8s.io/metrics-server/metrics-server:v0.6.3</strong>
<strong class="bold">* Verifying ingress addon...</strong>
<strong class="bold">...</strong>
<strong class="bold">* Starting worker node minikube-m02 in cluster minikube</strong>
<strong class="bold">...</strong>
<strong class="bold">* Done! kubectl is now configured to u"e "minik"be" cluster a"d "defa"lt" namespace by default</strong>
<strong class="bold">PS C:\&gt;</strong></pre></li> <li>We can now verify the cluster status by executing <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">get nodes</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl get nodes</strong>
<strong class="bold">NAME           STATUS   ROLES           AGE   VERSION</strong>
<strong class="bold">minikube       Ready    control-plane   23m   v1.26.3</strong>
<strong class="bold">minikube-m02   Ready    &lt;none&gt;          18m   v1.26.3</strong></pre><p class="list-inset">As you can see, the <strong class="source-inline">minikube-m02</strong> node does not show any role. This is due to the fact that everything in Kubernetes is managed by labels. Remember that we saw how selectors are used to identify which Pods belong to a <span class="No-Break">specific ReplicaSet.</span></p></li> <li>We can review the node labels and create a new one for the worker node. This will show us how we can<a id="_idIndexMarker1022"/> modify the resource’s behavior by <span class="No-Break">using labels:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl get nod– --show-labels</strong>
<strong class="bold">NAME           STATUS   ROLES           AGE   VERSION   LABELS</strong>
<strong class="bold">minikube       Ready    control-plane   27m   v1.26.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=08896fd1dc362c097c925146c4a0d0dac715ace0,minikube.k8s.io/name=minikube,minikube.k8s.io/primary=true,minikube.k8s.io/updated_at=2023_05_31T10_59_55_0700,minikube.k8s.io/version=v1.30.1,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,topology.hostpath.csi/node=minikube</strong>
<strong class="bold">minikube-m02   Ready    &lt;none&gt;          22m   v1.26.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube-m02,kubernetes.io/os=linux,topology.hostpath.csi/node=minikube-m02</strong></pre><p class="list-inset">Lots of labels are assigned to both nodes, but here, we missed one defining the role of the worker node, <strong class="source-inline">node-role.kubernetes.io/worker</strong>. This label is not required, and that’s why it is not included by default, but it is good to include it to identify nodes in the node cluster review (<strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">get nodes</strong></span><span class="No-Break">).</span></p></li> <li>We now add a new label to the worker <a id="_idIndexMarker1023"/>node by using a <strong class="source-inline">kubectl</strong> label, <strong class="source-inline">&lt;</strong><span class="No-Break"><strong class="source-inline">RESOURCE&gt; &lt;LABEL_TO_ADD&gt;</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl label node minikube-m02  `</strong>
<strong class="bold">node-role.kubernetes.io/worker=</strong>
<strong class="bold">node/minikube-m02 labeled</strong>
<strong class="bold">PS C:\&gt; kubectl get nodes</strong>
<strong class="bold">NAME           STATUS   ROLES           AGE   VERSION</strong>
<strong class="bold">minikube       Ready    control-plane   33m   v1.26.3</strong>
<strong class="bold">minikube-m02   Ready    worker          28m   v1.26.3</strong></pre><p class="list-inset">We can use the <strong class="source-inline">kubectl</strong> label to add any label to any resource. In the specific case of <strong class="source-inline">node-role.kubernetes.io</strong>, it is used by Kubernetes to show the <strong class="source-inline">ROLES</strong> column, but we can use any other label to identify a set of nodes. This will help you in a production cluster to run your applications in nodes that best fit your purposes – for example, by selecting nodes with fast solid-state disks, or those with special hardware devices. You may need to ask your Kubernetes administrators <a id="_idIndexMarker1024"/>about these nodes’ <span class="No-Break">special characteristics.</span></p></li> </ol>
<p>We will now show you how you can use the deployed <span class="No-Break">Kubernetes cluster.</span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor198"/>Interacting with the Minikube deployed cluster</h2>
<p>In this lab, we will interact <a id="_idIndexMarker1025"/>with the current cluster, reviewing and creating some <span class="No-Break">new resources:</span></p>
<ol>
<li>We will start by listing all Pods deployed in the cluster using <strong class="source-inline">kubectl get pods --A</strong>. This will list all Pods in the cluster. We are able to list them after the Minikube installation because we connect as administrators. The following screenshot shows the output of <strong class="source-inline">kubectl get pods -A</strong>, followed by a list of the current namespaces, using <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">get namespaces</strong></span><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 8.12 – The output of the kubectl get pods –A and kubectl get namespace commands" height="873" src="image/B19845_08_12.jpg" width="1348"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – The output of the kubectl get pods –A and kubectl get namespace commands</p>
<ol>
<li value="2">Let’s create a new namespace, <strong class="source-inline">chapter8</strong>, by using <strong class="source-inline">kubectl create </strong><span class="No-Break"><strong class="source-inline">ns chapter8</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl create ns chapter8</strong>
<strong class="bold">namespace/chapter8 created</strong></pre></li> <li>We can list the Pods in the <strong class="source-inline">chapter8</strong> namespace by adding <strong class="source-inline">--namespace</strong> or just <strong class="source-inline">--n</strong> to <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">get pods</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl get pods -n chapter8</strong>
<strong class="bold">No resources found in chapter8 namespace.</strong></pre></li> <li>We can try now out the <strong class="source-inline">ingress-nginx</strong> namespace. We will list all the resources deployed in this namespace using <strong class="source-inline">kubectl get all</strong>, as we can see in the <span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 8.13 – The output of kubectl get all –n ingress-nginx" height="657" src="image/B19845_08_13.jpg" width="1639"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – The output of kubectl get all –n ingress-nginx</p>
<p class="list-inset">Now, we know how <a id="_idIndexMarker1026"/>we can filter resources associated with a <span class="No-Break">specific namespace.</span></p>
<ol>
<li value="5">Let’s now create a simple Pod in the <strong class="source-inline">chapter8</strong> namespace by using the imperative format. We will execute <strong class="source-inline">kubectl run webserver --image=nginx:alpine</strong> to run a Pod with one container using the <span class="No-Break"><strong class="source-inline">docker.io/nginx:alpine</strong></span><span class="No-Break"> image:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl run webserver --image=nginx:alpine `</strong>
<strong class="bold">-n chapter8</strong>
<strong class="bold">pod/webserver created</strong>
<strong class="bold">PS C:\&gt; kubectl get pods -n chapter8</strong>
<strong class="bold">NAME        READY   STATUS    RESTARTS   AGE</strong>
<strong class="bold">webserver   1/1     Running   0          11s</strong></pre><p class="list-inset">Note that we used an external registry to store the images. Kubernetes does not use local stores to pull images. As we expect in any other container orchestrator environment, a registry is needed, and the hosts’ kubelet component will pull images from this. We will never synchronize images manually between nodes in a cluster; we will use container image registries to store and pull the required <span class="No-Break">container images.</span></p></li> <li>Let’s review the <a id="_idIndexMarker1027"/>resource manifest now. It is important to understand that the <strong class="source-inline">kubectl</strong> command talks with the Kubernetes API using the credentials from our local <strong class="source-inline">kubeconfig</strong> file (this file is located in your home directory in the <strong class="source-inline">.kube</strong> directory; you can use <strong class="source-inline">$env:USERPROFILE\.kube</strong> in Microsoft Windows), and kube-apiserver gets this information from etcd before it is presented in our terminal. The following screenshot shows part of the output we get by using <strong class="source-inline">kubectl get pod &lt;PODNAME&gt; -o yaml</strong>. The <strong class="source-inline">–o yaml</strong> modifier shows the output from a current resource in the YAML format. This really helps us to understand how objects are created and managed <span class="No-Break">by Kubernetes:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 8.14 – The output of kubectl get pods --namespace chapter8 -o yaml webserver" height="985" src="image/B19845_08_14.jpg" width="1448"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – The output of kubectl get pods --namespace chapter8 -o yaml webserver</p>
<ol>
<li value="7">Let’s see which node<a id="_idIndexMarker1028"/> executes our Pod by using either <strong class="source-inline">kubectl get pods -o wide</strong>, which shows extended information, or by filtering the <strong class="source-inline">hostIP</strong> key from the <span class="No-Break">YAML output:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl get pods -n chapter8 -o wide</strong>
<strong class="bold">NAME        READY   STATUS    RESTARTS   AGE   IP          NODE           NOMINATED NODE   READINESS GATES</strong>
<strong class="bold">webserver   1/1     Running   0          27m   10.244.205.194   minikube-m02   &lt;none&gt;           &lt;none&gt;</strong></pre><p class="list-inset">This can also be done by using the JSON path <span class="No-Break">template (</span><span class="No-Break">https://kubernetes.io/docs/reference/kubectl/jsonpath/</span><span class="No-Break">):</span></p><pre class="source-code"><strong class="bold">PS C:\&gt; kubectl get pods -n chapter8 `</strong>
<strong class="bold">-o jsonpath='{ .status.hostIP }' webserver</strong>
<strong class="bold">172.19.146.184</strong></pre></li> </ol>
<p class="callout-heading">Important note</p>
<p class="callout">You can see that the node name is also available in the <strong class="source-inline">spec</strong> section (<strong class="source-inline">spec.nodeName</strong>), but this section is where Pod specifications are presented. We will learn in the next chapter how we can change the workload behavior by changing the specifications from the online manifests, directly in Kubernetes. The <strong class="source-inline">status</strong> section is read-only because it shows the actual state of the resource, while some of the sections in either the <strong class="source-inline">metadata</strong> or <strong class="source-inline">spec</strong> sections can be modified – for example, by adding new labels <span class="No-Break">or annotations.</span></p>
<p>Before ending the labs <a id="_idIndexMarker1029"/>from this chapter, we will expose the deployed Pod by adding a <strong class="source-inline">NodePort</strong> Service, which will guide our requests to the running web <span class="No-Break">server Pod.</span></p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor199"/>Exposing a Pod with a NodePort Service</h2>
<p>In this quick lab, we<a id="_idIndexMarker1030"/> will use the imperative model to<a id="_idIndexMarker1031"/> deploy a <strong class="source-inline">NodePort</strong> Service to expose the already deployed web <span class="No-Break">server Pod:</span></p>
<ol>
<li>Because we haven’t defined the container port in the <strong class="source-inline">webserver</strong> Pod, Kubernetes will not know which port must be associated with the Service; hence, we need to pass the <strong class="source-inline">--target-port 80</strong> argument to specify that the Service should link the NGINX container port that is listening. We will use port <strong class="source-inline">8080</strong> for the Service, and we will let Kubernetes choose one <strong class="source-inline">NodePort</strong> port <span class="No-Break">for us:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl expose pod webserver -n chapter8 `</strong>
<strong class="bold">--target-port 80 --port 8080 --type=NodePort</strong>
<strong class="bold">service/webserver exposed</strong></pre></li> <li>We can now review the resources in the <span class="No-Break"><strong class="source-inline">chapter8</strong></span><span class="No-Break"> namespace:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl get all -n chapter8</strong>
<strong class="bold">NAME            READY   STATUS    RESTARTS   AGE</strong>
<strong class="bold">pod/webserver   1/1     Running   0          50m</strong>
<strong class="bold">NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</strong>
<strong class="bold">service/webserver   NodePort   10.103.82.252   &lt;none&gt;        8080:32317/TCP   72s</strong></pre><p class="list-inset">Note that the hosts’ port <strong class="source-inline">32317</strong> is associated with the Service’s port, <strong class="source-inline">8080</strong>, which is associated with the <strong class="source-inline">webserver</strong> Pod’s port, 80 (the NGINX container listens on <span class="No-Break">that port).</span></p></li> <li>We can now access the published <strong class="source-inline">NodePort</strong> port on any host, even if it does not run any <a id="_idIndexMarker1032"/>Service-related <a id="_idIndexMarker1033"/>Pod. We can use the IP address of any of the Minikube cluster nodes or use <strong class="source-inline">minikube service -n chapter8 webserver</strong> to automatically open our default web browser in the <span class="No-Break">associated URL.</span><p class="list-inset">The following screenshot shows the output in both cases. First, we obtained the host’s IP addresses by using <strong class="source-inline">kubectl get nodes –o wide</strong>. We used PowerShell’s <strong class="source-inline">Invoke-WebRequest</strong> command to access a combination of IP addresses of the nodes and the <strong class="source-inline">NodePort</strong>-published port. Then, we used Minikube’s built-in DNS to resolve the Service’s URL by using the <span class="No-Break"><strong class="source-inline">minikube</strong></span><span class="No-Break"> Service:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 8.15 – The output of kubectl get nodes, different tests using the cluster nodes, and the minikube Service URL resolution" height="872" src="image/B19845_08_15.jpg" width="1638"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – The output of kubectl get nodes, different tests using the cluster nodes, and the minikube Service URL resolution</p>
<p class="list-inset">As you can see, we <a id="_idIndexMarker1034"/>used the IP addresses of both the<a id="_idIndexMarker1035"/> master and worker nodes for the tests, and they worked, even though the Pod only ran on the worker node. This output also shows how easy it is to test Services by using Minikube’s integrated Services resolution. It automatically opened our default web browser, and we can access our Service directly, as we can see in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 8.16 – The default web browser accessing the webserver Service’s NodePort port" height="842" src="image/B19845_08_16.jpg" width="1582"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – The default web browser accessing the webserver Service’s NodePort port</p>
<ol>
<li value="4">We can now remove all the resources created in this chapter. It is important to first remove the Pod, the<a id="_idIndexMarker1036"/> Service, and then the <a id="_idIndexMarker1037"/>namespace. Removing the namespace first triggers the removal of all associated resources in cascade, and there may be some issues if Kubernetes isn’t able to remove some resources. It will never happen in this simple lab, but it is a good practice to remove resources inside a namespace before deleting the <span class="No-Break">namespace itself:</span><pre class="source-code">
<strong class="bold">PS C:\&gt; kubectl delete service/webserver pod/webserver -n chapter8</strong>
<strong class="bold">service "webserver" deleted</strong>
<strong class="bold">pod "webserver" deleted</strong>
<strong class="bold">PS C:\&gt; kubectl delete ns chapter8</strong>
<strong class="bold">namespace "chapter8" deleted</strong></pre></li> </ol>
<p>You are now ready to <a id="_idIndexMarker1038"/>learn <a id="_idIndexMarker1039"/>more advanced Kubernetes topics in the <span class="No-Break">next chapter.</span></p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor200"/>Summary</h1>
<p>In this chapter, we explored Kubernetes, the most popular and extended container orchestrator. We had an overall architecture review, describing each component and how we can implement an environment with HA, and we learned the basics of some of the most important Kubernetes resources. To be able to prepare our applications to run in Kubernetes clusters, we learned some applications that will help us to implement fully functional Kubernetes environments on our desktop computers <span class="No-Break">or laptops.</span></p>
<p>In the next chapter, we will deep dive into the resource types we will use to deploy our applications, reviewing interesting use cases and examples and learning different architecture patterns to apply to our <span class="No-Break">applications’ components.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer086">
<h1 id="_idParaDest-184" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor201"/>Part 3:Application Deployment</h1>
<p>This part will describe how applications run in production, and we will use different models and Kubernetes features to help us deliver reliable <span class="No-Break">applications securely.</span></p>
<p>This part has the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B19845_09.xhtml#_idTextAnchor202"><em class="italic">Chapter 9</em></a>, <em class="italic">Implementing Architecture Patterns</em></li>
<li><a href="B19845_10.xhtml#_idTextAnchor231"><em class="italic">Chapter 10</em></a>, <em class="italic">Leveraging Application Data Management in Kubernetes</em></li>
<li><a href="B19845_11.xhtml#_idTextAnchor244"><em class="italic">Chapter 11</em></a>, <em class="italic">Publishing Applications</em></li>
<li><a href="B19845_12.xhtml#_idTextAnchor267"><em class="italic">Chapter 12</em></a>, <em class="italic">Gaining Application Insights</em></li>
</ul>
</div>
<div>
<div id="_idContainer087">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer088">
</div>
</div>
</div></body></html>