- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: OpenStack Compute – Compute Capacity and Flavors
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack计算 – 计算能力和规格
- en: “Magic is believing in yourself. If you can do that, you can make anything happen.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “魔法就是相信自己。如果你能做到，你可以让任何事情发生。”
- en: – Johann Wolfgang von Goethe
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 约翰·沃尔夫冈·冯·歌德
- en: As a private cloud provider, typical OpenStack IaaS services are the building
    blocks of its offering to end users. At the heart of the IaaS offerings is the
    compute service that runs and manages the workload life cycle in different facets.
    The OpenStack Foundation has committed to enlarging the scope of the OpenStack
    ecosystem and exposing new services by tailoring the Nova service in different
    ways. Additional new trending projects such as managed elastic data processing
    (code-named **Sahara** ), managed databases (code-named **Trove** ), and a container
    application catalog (code-named **Murano** ) are examples of services that are
    offered on top of the Nova service.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为私有云提供商，典型的OpenStack IaaS服务是其向最终用户提供的构建块。在IaaS服务的核心是计算服务，它在不同方面运行和管理工作负载的生命周期。OpenStack基金会致力于通过定制Nova服务以不同方式扩展OpenStack生态系统的范围，并推出新服务。管理弹性数据处理（代号为**Sahara**）、管理数据库（代号为**Trove**）以及容器应用目录（代号为**Murano**）等新的流行项目，都是在Nova服务之上提供的服务的例子。
- en: 'As a cloud operator, having a solid understanding of the compute farm capabilities
    and the different segregation methods within the OpenStack environment is a foundational
    requirement to run a successful private cloud journey with the desired service
    offerings. At the time of writing this edition, the Nova service had some updates,
    in addition to the emergence of more pertinent features and services around it.
    To ensure a safe compute design update based on the draft elaborated on in the
    first chapter, it will be necessary to run through the new compute service that
    came out with the latest OpenStack releases starting from the Antelope release.
    In this chapter, we will go through the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作为云操作员，在OpenStack环境中具备对计算资源和不同隔离方法的扎实理解，是成功运行私有云旅程和提供所需服务的基础要求。在撰写本版时，Nova服务已有一些更新，除了周围更多相关特性和服务的出现。为了确保基于第一章详述的草案进行安全计算设计更新，有必要详细了解从Antelope版本开始发布的最新OpenStack版本中推出的新计算服务。在本章中，我们将讨论以下主题：
- en: Revisiting the Nova compute service from an architecture perspective
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从架构角度重新审视Nova计算服务。
- en: Checking out the latest hypervisors supported by the compute service
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查计算服务支持的最新的虚拟化程序。
- en: Exploring the concept of compute segregation for massive OpenStack compute deployment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索大规模OpenStack计算部署的计算隔离概念。
- en: Demystifying the art of filtering to construct advanced ways of performing compute
    resource allocations and hosting workloads based on an array of strategies
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭秘过滤艺术，构建基于一系列策略的高级计算资源分配和托管工作负载的先进方法
- en: Discussing the **Container Orchestration Engines** ( **COEs** ) and their integration
    with Magnum
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论**容器编排引擎**（**COEs**）及其与Magnum集成的情况
- en: Learning how to manage and operate containers in the most flexible way with
    the new, trending Zun OpenStack container service
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何以最灵活的方式使用新的流行Zun OpenStack容器服务来管理和操作容器。
- en: Configuring and deploying new compute nodes in a running OpenStack environment
    using Kolla-Ansible infrastructure as a code paradigm
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kolla-Ansible基础设施即代码范式，在运行中的OpenStack环境中配置和部署新的计算节点
- en: The compute service components
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算服务组件
- en: 'The compute service is composed of multiple components that take care of incoming
    compute requests and then launch and manage instances, as depicted in the following
    diagram showing the Nova architecture:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算服务由多个组件组成，负责处理传入的计算请求，然后启动和管理实例，如下图所示展示的Nova架构：
- en: '![Figure 4.1 – The Nova component architecture](img/B21716_04_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – Nova组件架构](img/B21716_04_01.jpg)'
- en: Figure 4.1 – The Nova component architecture
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – Nova组件架构
- en: Let’s explore each compute component in the following section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下章节中探索每个计算组件。
- en: API
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API
- en: The **nova-api** component deals with different (HTTP-based) requests via the
    queuing message service. Keep in mind that the API is the first interface that
    accepts compute requests before forwarding them to the next Nova component and
    establishing a complete workflow.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-api**组件通过排队消息服务处理不同（基于HTTP的）请求。请记住，在将计算请求转发到下一个Nova组件并建立完整工作流之前，API是接受计算请求的第一个接口。'
- en: Scheduler
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度器
- en: The next station in the Nova workflow abstraction layer is the **nova-scheduler**
    component. It is a major piece of the workflow that decides in which compute node
    each incoming instance creation request will run.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Nova 工作流抽象层中的下一个站点是 **nova-scheduler** 组件。它是工作流中的一个重要部分，决定每个传入的实例创建请求将在哪个计算节点上运行。
- en: The scheduler component uses a variety of filters and preferences that can be
    customized to match different needs and reasoning for the availability of the
    existing resources. **Filtering** and **weighting** are the main mechanisms used
    by the Nova scheduler in order to customize the allocation of underlying physical
    resources. This chapter will break down the newly added filters and weighting
    references in the Antelope and later releases.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器组件使用多种过滤器和偏好设置，可以根据不同的需求和资源可用性进行定制。**过滤** 和 **加权** 是 Nova 调度器用来定制底层物理资源分配的主要机制。本章将详细拆解在
    Antelope 版本及以后的版本中新增加的过滤器和加权参考。
- en: Conductor
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Conductor
- en: Once a request is dealt with via the **nova-scheduler** process, the **nova-conductor**
    component will make sure that the reservation made in the physical host is saved
    in the shared database. The **nova-scheduler** process will be required to check
    the database beforehand and verify the available capacity before making a scheduling
    decision. The conductor component is a major addition to the Nova workflow compared
    to the earlier releases of the Nova project in the OpenStack ecosystem. As a brief
    reminder from the previous edition, the conductor comes with a security improvement,
    preventing the compute nodes from having direct access to the database. In earlier
    versions, the compute nodes would update their status directly in the database
    shared between them. That would reduce the blast radius of a possible security
    gap on the database level if one of the physical machines was compromised. Nowadays,
    the conductor takes on that responsibility on its own. Another great addition
    of the conductor component is dedicated operations dealing with the compute nodes,
    such as resizing, building, and migration.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求通过 **nova-scheduler** 进程处理，**nova-conductor** 组件将确保在物理主机上所做的预定保存在共享数据库中。**nova-scheduler**
    进程需要事先检查数据库并验证可用容量，然后才能做出调度决策。与 OpenStack 生态系统中 Nova 项目的早期版本相比，conductor 组件是 Nova
    工作流中的一项重要新增功能。作为上版的简要提醒，conductor 带来了安全性提升，防止计算节点直接访问数据库。在早期版本中，计算节点会直接更新它们在共享数据库中的状态。如果其中一台物理机器被攻破，这样可以减少数据库层面可能的安全漏洞带来的影响范围。如今，conductor
    负责这一任务。conductor 组件的另一个重要新增功能是专门处理计算节点的操作，如调整大小、构建和迁移。
- en: By having those capabilities in **nova-conductor** , the compute nodes will
    act as simple worker nodes, leaving coordination and any database operations to
    the conductor process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些能力集成到 **nova-conductor** 中，计算节点将作为简单的工作节点，只需将协调和数据库操作交给 conductor 进程处理。
- en: Compute
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: Coming to the end of the Nova workflow, the **nova-compute** component will
    take the rest of the requests by dealing with the hypervisor. The addition of
    a variety of features for different supported hypervisors in the OpenStack compute
    core makes almost every new release more exciting than the previous one. Libvirt
    KVM, VMware vSphere, XenServer, XenAPI, Microsoft HyperV, **Linux Containers**
    ( **LXC** ), Quick Emulator, and a few others are supported by the Nova service.
    The most exciting part is the continuous development and integration of each hypervisor
    feature by the vendors who own the hypervisor project. This chapter will illustrate
    more details of those hypervisors and highlight a few common architectural compute
    layouts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 来到 Nova 工作流的末尾，**nova-compute** 组件将处理剩余的请求，通过与虚拟化管理程序（hypervisor）的交互来完成。OpenStack
    计算核心为不同支持的虚拟化管理程序添加了各种新特性，使得每个新版本都比前一个版本更具吸引力。Nova 服务支持的虚拟化管理程序包括 Libvirt KVM、VMware
    vSphere、XenServer、XenAPI、Microsoft HyperV、**Linux 容器**（**LXC**）、快速模拟器（Quick Emulator）等。最令人兴奋的部分是，每个虚拟化管理程序的功能都在不断发展和集成，由拥有该虚拟化项目的供应商进行更新。本章将详细说明这些虚拟化管理程序，并突出一些常见的架构计算布局。
- en: Console access
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制台访问
- en: Access to virtual machines through OpenStack Horizon (dashboard) or via the
    **Command-Line Interface** ( **CLI** ) is well developed by the OpenStack community
    in different iterations. Console access is not a required component to run a fully
    working Nova service but is an additional aspect to interact with provisioned
    guests. Until the *Liberty* release, **nova-console** , **nova-novncproxy** ,
    and **nova-consoleauth** were mainly counted as the most stable console types.
    Now, with the Antelope and later releases, more console types have joined the
    list, such as the SPICE console and serial console as an alternative graphical
    console extension. Microsoft has also contributed to the console service with
    **Remote Desktop Protocol** ( **RDP** ). However, the latter comes with a limitation
    as it supports only Hyper-V and requires integration with the console proxy in
    OpenStack, which has not been developed. Hence, a third-party tool should be available
    along with the console configuration, such as the FreeRDP-WebConnect application,
    which can be found on GitHub. Like Microsoft RDP, VMware has also contributed
    with another console type, **Mouse, Keyboard, Mouse** ( **MKM** ). This is bound
    to a virtual machine’s access console based on Vmware’s vSphere Hypervisor, but
    independently of an existing console proxy service in OpenStack. With this growing
    list in the latest releases, one console-supported service has been deprecated
    since the Train release, which is **nova-consoleauth** .
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 OpenStack Horizon（仪表盘）或 **命令行接口**（**CLI**）访问虚拟机，在 OpenStack 社区的不同版本中得到了很好的开发。控制台访问不是运行完全工作
    Nova 服务的必需组件，但它是与已分配来宾进行交互的附加方面。直到 *Liberty* 发布，**nova-console**、**nova-novncproxy**
    和 **nova-consoleauth** 主要被认为是最稳定的控制台类型。现在，随着 Antelope 及之后的发布，更多的控制台类型已加入列表，如 SPICE
    控制台和串行控制台，作为图形控制台的替代扩展。微软还通过 **远程桌面协议**（**RDP**）为控制台服务做出了贡献。然而，后者有一个限制，即仅支持 Hyper-V
    并需要与 OpenStack 中尚未开发的控制台代理进行集成。因此，应提供第三方工具与控制台配置一起使用，例如可以在 GitHub 上找到的 FreeRDP-WebConnect
    应用程序。与微软的 RDP 类似，VMware 也通过另一种控制台类型 **鼠标、键盘、鼠标**（**MKM**）做出了贡献。它绑定到基于 VMware vSphere
    超级管理程序的虚拟机访问控制台，但与 OpenStack 中现有的控制台代理服务无关。在最新发布的版本中，随着控制台支持服务列表的不断增长，自 Train
    发布以来，一个控制台支持服务已被弃用，即 **nova-consoleauth**。
- en: The associated Nova API services, including **nova-scheduler** , **nova-conductor**
    , and **nova-api** , run as part of the controller nodes, and the **nova-compute**
    service runs on each compute node. Running the Nova compute service is based on
    the hardware platform and the selected hypervisor, which will be covered in the
    next section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的 Nova API 服务，包括 **nova-scheduler**、**nova-conductor** 和 **nova-api**，作为控制节点的一部分运行，**nova-compute**
    服务则在每个计算节点上运行。Nova 计算服务的运行基于硬件平台和所选的超级管理程序，相关内容将在下一节中介绍。
- en: Reasoning for the hypervisors
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超级管理程序的推理
- en: By navigating through the different OpenStack releases, Nova supports a range
    of **Virtual Machine Monitors** ( **VMMs** ), also known as hypervisors. Hypervisor
    software allows access to the physical machine hardware resources and provides
    different functions to create and manage virtual machines within a compute node.
    At the time of writing this book, the latest OpenStack releases including the
    Dalmatian release support **Kernel Virtual Machine** ( **KVM** ), **LXC** , **QEMU**
    , **VMware vSphere** (5.1.0 and later versions), **zVM** , **Ironic** (native
    OpenStack bare metal), **Hyper-V,** and **Virtuozzo** (7.0.0 and later versions).
    The different features and virtual machine management capabilities differ from
    one hypervisor to another. A full list of supported features per hypervisor can
    be found in the official hypervisor support matrix, which is available at [https://docs.openstack.org/nova/latest/user/support-matrix.html](https://docs.openstack.org/nova/latest/user/support-matrix.html)
    . Make sure to parse each of the provided features based on your requirements
    and needs. KVM is the most used hypervisor in the OpenStack **nova-compute** setup
    due to its early adoption and having the largest set of supported features.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过浏览不同的 OpenStack 版本，Nova 支持多种 **虚拟机监控器**（**VMM**），也称为虚拟化管理程序。虚拟化管理程序软件允许访问物理机器硬件资源，并提供不同的功能来创建和管理计算节点中的虚拟机。在编写本书时，包括
    Dalmatian 版本在内的最新 OpenStack 版本支持 **内核虚拟机**（**KVM**）、**LXC**、**QEMU**、**VMware
    vSphere**（5.1.0 及更高版本）、**zVM**、**Ironic**（原生 OpenStack 裸机）、**Hyper-V** 和 **Virtuozzo**（7.0.0
    及更高版本）。不同的功能和虚拟机管理能力因虚拟化管理程序而异。每个虚拟化管理程序所支持的完整功能列表可以在官方的虚拟化管理程序支持矩阵中找到，网址为 [https://docs.openstack.org/nova/latest/user/support-matrix.html](https://docs.openstack.org/nova/latest/user/support-matrix.html)。请根据您的需求解析每个提供的功能。KVM
    是 OpenStack **nova-compute** 设置中最常用的虚拟化管理程序，因其早期采用并且具有最大的一组支持功能。
- en: Important note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Other famous hypervisors such as **Xen** , **XCP** , **UML** , and the **Docker**
    driver are not listed in the latest OpenStack releases, including Antelope and
    Bobcat (released in October 2023). The Docker driver has been supported since
    the Havana release and moved to its own project, code-named the **Zun** project,
    which will be covered later in this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其他著名的虚拟化管理程序，如 **Xen**、**XCP**、**UML** 和 **Docker** 驱动程序，并未在最新的 OpenStack 版本中列出，包括
    Antelope 和 Bobcat（2023 年 10 月发布）。Docker 驱动程序自 Havana 版本以来已被支持，并迁移到其独立的项目中，代号为
    **Zun** 项目，后续将在本章中详细介绍。
- en: 'The Nova compute service uses **Virt Driver** to interact with the **Libvirt**
    library via API calls and manages virtual machines through the hypervisor, as
    illustrated in the following diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Nova 计算服务使用 **Virt Driver** 通过 API 调用与 **Libvirt** 库进行交互，并通过虚拟化管理程序管理虚拟机，如下图所示：
- en: '![Figure 4.2 – The Nova compute-supported hypervisors](img/B21716_04_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – Nova 支持的计算型虚拟机监控器](img/B21716_04_02.jpg)'
- en: Figure 4.2 – The Nova compute-supported hypervisors
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – Nova 支持的计算型虚拟机监控器
- en: The configuration of the hypervisor backend in Nova can be performed by adjusting
    the **compute_driver** option in the **/** **etc/nova/nova.conf** file.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Nova 中的虚拟化管理程序后端配置可以通过调整 **compute_driver** 选项来完成，配置文件位于 **/etc/nova/nova.conf**
    中。
- en: 'The KVM implementation requires **nova-compute** to be deployed on a Linux
    server with KVM virtualization modules installed. For each compute node, make
    sure the KVM modules are properly loaded by checking the following command line:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: KVM 实现要求 **nova-compute** 部署在安装了 KVM 虚拟化模块的 Linux 服务器上。对于每个计算节点，请确保通过以下命令行正确加载
    KVM 模块：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Optionally, and depending on the supported KVM module (Intel or AMD) in your
    nodes, add the listed modules in the **/etc/modules** files to persist your changes
    at reboot.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据节点中支持的 KVM 模块（Intel 或 AMD），可选择性地将列出的模块添加到 **/etc/modules** 文件中，以便在重启时保存更改。
- en: The next section considers KVM as the main adopted hypervisor of our production
    compute nodes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将 KVM 作为我们生产计算节点中采用的主要虚拟化管理程序进行讨论。
- en: Compute segregation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算隔离
- en: OpenStack is designed to scale massively to respond to the growth of the infrastructure
    size and user demand. As the compute service is the workhorse of the cloud infrastructure,
    designing for compute layer scalability and business continuity is a must. Defining
    a compute segregation strategy will help you understand your private cloud infrastructure
    limits and forecast user compute demands – that’s what will be explored in the
    following section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 设计上能够大规模扩展，以应对基础设施规模和用户需求的增长。由于计算服务是云基础设施的主力，设计计算层的可扩展性和业务连续性至关重要。定义计算隔离策略将帮助你了解私有云基础设施的限制，并预测用户的计算需求——这将在下一节中进行探讨。
- en: Infrastructure segregation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础设施隔离
- en: Depending on the available compute farm resources and inventory specifications,
    as well as the hypervisors to be used, OpenStack exposes a variety of compute
    segregation mechanisms that can be leveraged, including **Availability Zones**
    , **Regions** , **Cells** , and **Host Aggregates** . Each of the previous concepts
    will be detailed in the following section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据可用计算集群资源、库存规格以及所使用的虚拟化管理程序，OpenStack 提供了多种可以利用的计算隔离机制，包括**可用区**、**区域**、**单元**和**主机聚合**。以下各个概念将在下一节中详细说明。
- en: Regions
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域
- en: 'A region in the OpenStack glossary presents a fully independent OpenStack deployment.
    From operational and management perspectives, each region can expose its own OpenStack
    API services, including compute resources. If you’re planning a Nova multi-regional
    setup, a Nova API endpoint should be created in each region and added to the identity
    services catalog. Users can spawn instances and spread workloads between available
    regions through the available compute API. Running workloads in a multi-region
    configuration setup offers a multitude of advantages for end users, but also presents
    challenges for operators, such as maintaining a consistent setup of the OpenStack
    control plane and shared service layers. This is mainly due to the risk of having
    a **split brain** between compute resources running in more than one region. Hosting
    a user workload in a fleet of compute nodes running across multiple regions would
    require certain ways to provide consistency of shared resources such as virtual
    machine images, databases, or file shares. Another important aspect of multi-regional
    consistency is the way to handle authorization to request compute resources. One
    of the common design approaches is to consolidate all authentication calls through
    Keystone via a federated **Identity Provider** ( **IdP** ), as shown in the following
    diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 术语表中的一个区域表示一个完全独立的 OpenStack 部署。从运营和管理的角度来看，每个区域可以暴露其自己的 OpenStack
    API 服务，包括计算资源。如果你计划进行 Nova 多区域部署，则应在每个区域创建一个 Nova API 端点，并将其添加到身份服务目录中。用户可以通过可用的计算
    API 在可用的区域之间启动实例并分配工作负载。在多区域配置中运行工作负载为最终用户提供了诸多优势，但也给运营商带来了挑战，如维护 OpenStack 控制平面和共享服务层的一致性。这主要是因为计算资源在多个区域之间运行时可能会出现**脑裂**的风险。在多个区域运行的计算节点群集上托管用户工作负载需要采取一定方法来确保共享资源的一致性，如虚拟机镜像、数据库或文件共享。多区域一致性的另一个重要方面是处理请求计算资源的授权方式。常见的设计方法之一是通过联合的**身份提供者**（**IdP**）将所有身份验证调用集中到
    Keystone，如下图所示：
- en: '![Figure 4.3 – OpenStack multi-region deployment](img/B21716_04_03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – OpenStack 多区域部署](img/B21716_04_03.jpg)'
- en: Figure 4.3 – OpenStack multi-region deployment
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – OpenStack 多区域部署
- en: Each region can host one or more discrete compute structures defined as *Availability
    Zones* , which will be covered in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个区域可以托管一个或多个定义为 *可用区* 的离散计算结构，这将在下一节中详细介绍。
- en: Availability Zones
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可用区
- en: 'A region can be logically composed of one or multiple Availability Zones. Unlike
    the regional setup, there is no need to deploy a full, standalone OpenStack deployment
    and provide consistency across the control plane services. From a physical implementation
    standpoint, the compute nodes will be split into trees of logical groups called
    **fault domains** . Each fault domain will be part of a **Power Distribution Unit**
    ( **PDU** ) within a given rack and different hypervisors will run across different
    racks. This way, losing connectivity to the **Top-of-Rack** ( **ToR** ) switch
    or upon a PDU failure won’t fully bring down a user workload running in a multi-availability
    zone configuration, as shown in the following diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个区域可以逻辑上由一个或多个可用区组成。与区域设置不同，无需部署完整的独立 OpenStack 部署并在控制平面服务之间提供一致性。从物理实现的角度来看，计算节点将被拆分成称为
    **故障域** 的逻辑组树。每个故障域将是给定机架内一个 **电源分配单元**（**PDU**）的一部分，且不同的虚拟化管理程序将在不同的机架上运行。这样，在失去与
    **机架顶端交换机**（**ToR**）的连接或发生 PDU 故障时，用户工作负载不会完全崩溃，尤其是在多可用区配置中，如下图所示：
- en: '![Figure 4.4 – Extended OpenStack multi-regional and zonal deployment](img/B21716_04_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 扩展的 OpenStack 多区域和可用区部署](img/B21716_04_04.jpg)'
- en: Figure 4.4 – Extended OpenStack multi-regional and zonal deployment
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 扩展的 OpenStack 多区域和可用区部署
- en: The Nova availability zone option empowers user workloads with high availability
    that can be considered a premium option for critical workloads that require the
    highest level of SLA. A compute node can belong to only one availability zone
    as it is bound to the rack’s physical design, such as rack ID, location, and PDU.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Nova 可用区选项为用户工作负载提供高可用性，这可以视为对需要最高 SLA 的关键工作负载的高级选项。计算节点只能属于一个可用区，因为它绑定于机架的物理设计，如机架
    ID、位置和 PDU。
- en: Important note
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The logical abstraction of Availability Zones covers the network and block storage
    services besides the compute service.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 可用区的逻辑抽象除了覆盖计算服务外，还包括网络和块存储服务。
- en: Users can select which Availability Zone to launch instances in. For this reason,
    keeping an eye on the compute resource utilization for each Availability Zone
    is essential. That will ensure that each Availability Zone will be able to accommodate
    more compute allocation within the available physical resources when needed, as
    well as ensure that users can spawn workloads across multiple compute nodes in
    different zones.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以选择在哪个可用区启动实例。因此，监控每个可用区的计算资源使用情况至关重要。这将确保在需要时，每个可用区能够容纳更多的计算资源分配，同时确保用户可以在不同的区域间跨多个计算节点启动工作负载。
- en: Host aggregates
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主机聚合
- en: 'Unlike the Availability Zones concept, the host aggregates strategy groups
    compute nodes based on more granular hardware profile categories defined as metadata.
    This type of grouping can be based on the definition of features such as hypervisor,
    storage type, CPU family, network speed, and so on. Cloud operators should create
    different host aggregates in advance by attaching a set of metadata to the selected
    compute nodes. This lets users spawn instances by selecting the desired host aggregate
    that responds to the workload requirements. Metadata can be created based on plenty
    of hardware specifications, depending on what the private cloud operator has purchased.
    For example, aggregation can be configured for an optimized disk host group, based
    on SSD, GPU architecture, high-performance networking capabilities, or even by
    aggregate compute nodes for a single tenant. A compute node can be part of one
    or many host aggregates. As a best practice, it is important to list all the different
    hardware capabilities once the compute nodes are in place and sort them into trees
    by grouping them based on the expected workload that they will host. For example,
    if HPC workloads are included in your cloud business case offering for end users,
    you might consider creating a host aggregate in advance based on GPU capabilities
    and exposing it to end users. The following diagram shows a use case for providing
    four host aggregates, where some compute nodes can be part of more than one host
    aggregate:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与可用区（Availability Zones）概念不同，主机聚合策略将计算节点基于更精细的硬件配置类别（定义为元数据）进行分组。这种分组可以基于特性定义，例如虚拟化管理程序（hypervisor）、存储类型、CPU架构、网络速度等。云运营商应通过将一组元数据附加到选定的计算节点，提前创建不同的主机聚合。这样，用户可以通过选择响应工作负载需求的主机聚合来启动实例。元数据可以根据大量硬件规格创建，这取决于私有云运营商购买的硬件。例如，聚合可以为优化的磁盘主机组进行配置，基于SSD、GPU架构、高性能网络能力，甚至通过为单个租户聚合计算节点进行配置。一个计算节点可以属于一个或多个主机聚合。作为最佳实践，一旦计算节点到位，列出所有不同的硬件能力并将它们按照预期的工作负载进行分组，按树状结构进行排序是非常重要的。例如，如果HPC工作负载被包含在云业务案例中，面向最终用户，你可能会考虑提前基于GPU能力创建一个主机聚合，并将其暴露给最终用户。下图展示了提供四个主机聚合的使用案例，其中一些计算节点可以是多个主机聚合的组成部分：
- en: '![Figure 4.5 – Host aggregation in the deployment of multiple Availability
    Zones](img/B21716_04_05.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – 在多个可用区的部署中进行主机聚合](img/B21716_04_05.jpg)'
- en: Figure 4.5 – Host aggregation in the deployment of multiple Availability Zones
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 在多个可用区的部署中进行主机聚合
- en: 'The previous multi-region segregation compute layout involves four different
    host aggregates that can be spread across different Availability Zones as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的多区域隔离计算布局涉及四个不同的主机聚合，这些聚合可以跨不同的可用区进行分布，具体如下：
- en: '**Host aggregate 1** ( **HA_1** ): This involves grouping a set of compute
    nodes in the same region and Availability Zone to accommodate workloads that require
    low network-latency performance, such as HPC applications within a large network
    bandwidth and GPU architecture. Workloads in this host aggregate do not have high
    availability as a major exigency.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机聚合 1**（**HA_1**）：这涉及将一组计算节点在同一地区和可用区内进行分组，以适应需要低网络延迟性能的工作负载，如在大网络带宽和GPU架构下运行的HPC应用程序。该主机聚合中的工作负载没有高可用性作为主要要求。'
- en: '**Host aggregate 2** ( **HA_2** ): This aggregate involves grouping a set of
    compute nodes spread through two Availability Zones and within the same region
    to accommodate workloads that require large distributed workloads, such as big
    data processing running Hadoop or Cassandra. The host aggregate metadata includes
    large network bandwidth, optimized disks based on SSD, and memory capacity.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机聚合 2**（**HA_2**）：该聚合涉及将一组计算节点分布在两个可用区内，并且位于同一地区，以适应需要大规模分布式工作负载的工作负载，例如运行Hadoop或Cassandra的大数据处理。主机聚合的元数据包括大网络带宽、基于SSD的优化磁盘和内存容量。'
- en: '**Host aggregate 3** ( **HA_3** ): This involves grouping a set of compute
    nodes spread through three Availability Zones and across two regions to accommodate
    critical workloads that require the highest level of availability and scalability,
    such as web applications and webshops. The metadata describes moderate network
    bandwidth, disk, memory, and CPU standard capabilities.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机聚合 3**（**HA_3**）：这涉及将一组计算节点分布在三个可用区内，并跨越两个区域，以适应需要最高级别可用性和可扩展性的关键工作负载，例如Web应用程序和电子商务网站。元数据描述了适中的网络带宽、磁盘、内存和CPU标准能力。'
- en: '**Host aggregate 4** ( **HA_4** ): This involves grouping a set of compute
    nodes within the same region and Availability Zone to accommodate workloads that
    require enhanced networking capability within a VSphere environment. The host
    aggregate metadata includes VMware as a configured hypervisor to extend an existing
    VCenter environment and spread the application load between VMware hosts (running
    the **nova-compute** service).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机聚合 4**（**HA_4**）：这涉及将一组计算节点组合在同一区域和可用性区域内，以容纳在 VSphere 环境中需要增强网络能力的工作负载。主机聚合的元数据包括配置为超虚拟化平台的
    VMware，用以扩展现有的 VCenter 环境，并在 VMware 主机（运行 **nova-compute** 服务）之间分配应用负载。'
- en: We will demonstrate later in this chapter how the combination of the host aggregation
    approach, Placement service, and Nova scheduling can expose a gazillion ways to
    host different types of workloads in the most efficient way to cloud users.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后我们将演示，如何通过主机聚合方法、Placement 服务和 Nova 调度的结合，揭示出多种方式以最有效的方式为云用户托管不同类型的工作负载。
- en: Cells
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单元
- en: 'The concept of cells in OpenStack was introduced mainly to address the performance
    bottleneck of large-scale Nova deployment within a single OpenStack region. A
    large number of compute nodes would increase the load on the main control plane
    services, including the messaging queue and database, resulting in performance
    degradation that would limit the expansion of the entire system. The mitigation
    of performance degradation for large-scale deployment for Nova compute resources
    can be addressed by using the cell approach: compute nodes can be grouped within
    a logical boundary, where each group runs its own database and messaging queue
    service. The cell architecture has been developed through different stages, resulting
    in two versions: **CellV1** and **CellV2** . CellV1 was the first version and
    was not widely implemented due to a number of caveats. Since the OpenStack Ocata
    release, CellV2 has been introduced, with enhancements to the former version,
    and has become the official version within the **Pike** release. Let’s discuss
    the major differences between the versions next.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 中的单元概念主要是为了解决在单一 OpenStack 区域内大规模 Nova 部署的性能瓶颈。大量计算节点会增加主控制平面服务的负载，包括消息队列和数据库，导致性能下降，从而限制整个系统的扩展性。通过使用单元方法，可以解决
    Nova 计算资源大规模部署的性能下降问题：计算节点可以在逻辑边界内进行分组，每个组都运行自己的数据库和消息队列服务。单元架构已经经历了不同的阶段，最终形成了两个版本：**CellV1**
    和 **CellV2**。CellV1 是第一个版本，由于存在一些问题，未被广泛实施。自 OpenStack Ocata 发布以来，推出了 CellV2 版本，对前一版本进行了增强，并成为
    **Pike** 版本中的正式版本。接下来我们将讨论这两个版本之间的主要区别。
- en: CellV1
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CellV1
- en: 'The logical grouping of compute nodes forming each cell is arranged in a tree
    structure where the top-level cell called the **root** runs the **nova-api** service.
    The second level of cells, which run the Nova compute service, are referred to
    as child cells, as shown in the following diagram:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 形成每个单元的计算节点的逻辑分组以树形结构安排，其中顶层单元称为**根单元**，它运行**nova-api**服务。第二层单元运行 Nova 计算服务，称为子单元，如下图所示：
- en: '![Figure 4.6 – OpenStack compute CellV1 deployment](img/B21716_04_06.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – OpenStack 计算 CellV1 部署](img/B21716_04_06.jpg)'
- en: Figure 4.6 – OpenStack compute CellV1 deployment
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – OpenStack 计算 CellV1 部署
- en: The interaction between the cells running the Nova API and running the Nova
    compute goes through a dedicated messaging queue via RPC calls handled by a separate
    Nova service called **nova-cell** (a major enhancement of CellV1 introduced in
    the Juno release). Each cell runs its own messaging queue for interactive synchronization
    and is used by **nova-cell** for selecting in which cell an instance will be spawned.
    This exposes a higher level of instance scheduling but within the cell’s level.
    Once a cell is selected, the **nova-scheduler** service within that cell will
    process the rest of the request workflow to select a compute node.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Nova API 和 Nova 计算的单元之间的交互通过专用消息队列进行，使用由一个单独的 Nova 服务 **nova-cell** 处理的 RPC
    调用（这是 CellV1 在 Juno 发布中引入的一项重大增强）。每个单元都运行自己的消息队列进行交互同步，并由 **nova-cell** 用来选择在哪个单元中启动实例。这提供了更高层次的实例调度，但仅限于单元级别。一旦选择了单元，那个单元中的
    **nova-scheduler** 服务将处理请求工作流的其余部分，以选择一个计算节点。
- en: CellV2
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CellV2
- en: 'The initial cell version went through experimental phases and was not successfully
    adopted in most large compute deployments. This was mainly due to the multi-layer
    scheduling to spawn instances from the cell selection down to the compute scheduler
    service within the selected cell. *Figure 4* *.6* illustrates a simple tree composed
    of only root and child cells, but other, wider deployments could adopt more child
    cells, called **grandchild** cells. The other major shortcoming of the first version
    is the amount of data (instance and compute node information) to be mapped and
    synchronized between cells, resulting in heavy data replication processes between
    the cells. The new version, referred to as CellV2, has been developed and adopted
    officially from the Ocata release by addressing the caveats of the prior version.
    Major changes have been introduced, such as the removal of the **nova-cell** component
    and the overall cell architecture, which is no longer a tree-like structure similar
    to CellV1. This is illustrated in the following diagram:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的单元格版本经历了实验阶段，并未在大多数大型计算部署中成功采用。这主要是由于从单元格选择到所选单元格中的计算调度服务的多层次调度，以创建实例的复杂性。*图
    4* *.6* 展示了一个由根单元格和子单元格组成的简单树形结构，但其他更广泛的部署可能会采用更多的子单元格，称为 **孙子** 单元格。第一个版本的另一个主要缺点是需要在单元格之间映射和同步的数据量（实例和计算节点信息），导致单元格之间发生繁重的数据复制过程。新的版本，称为
    CellV2，已经在 Ocata 版本中被开发并正式采纳，通过解决先前版本的缺陷，引入了重大变化，例如删除了 **nova-cell** 组件和整体单元格架构，这不再是类似于
    CellV1 的树状结构。以下图示展示了这一变化：
- en: '![Figure 4.7 – OpenStack compute CellV2 deployment](img/B21716_04_07.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – OpenStack 计算 CellV2 部署](img/B21716_04_07.jpg)'
- en: Figure 4.7 – OpenStack compute CellV2 deployment
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – OpenStack 计算 CellV2 部署
- en: Sharding hypervisor nodes through the second version of cells in Nova involves
    major changes in the scheduling workflow within the multi-cell deployment, as
    illustrated in the previous diagram. A cell API runs the **nova-api** service,
    **nova-scheduler** , and more recently, the Placement service (the Placement service
    was not introduced in the OpenStack ecosystem in the Cellv1 development) to uniformly
    schedule resources across all defined cells. The cell API exposes a separate database
    called **nova_api** , which contains metadata of all global resources, such as
    instance flavors, quotas, keypairs, and so on. In multi-cell deployment, the conductor
    piece in the API cell is called **nova-super-conductor** , which deals with all
    database insulation. Additionally, from the API level, a special cell called **Cell0**
    does not run any of the services but holds only the **nova_cell0** database containing
    information about failed instances to start due to a scheduler failure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Nova 的第二版单元格（CellV2）对超融合虚拟化节点进行分片涉及到在多单元格部署中调度工作流的重大变化，如前图所示。一个单元格 API 运行
    **nova-api** 服务、**nova-scheduler**，以及最近的 Placement 服务（Placement 服务在 Cellv1 开发中没有被引入到
    OpenStack 生态系统中），以统一调度所有定义的单元格中的资源。单元格 API 暴露一个名为 **nova_api** 的独立数据库，包含所有全局资源的元数据，如实例规格、配额、密钥对等。在多单元格部署中，API
    单元格中的指挥器（conductor）称为 **nova-super-conductor**，它处理所有数据库的隔离。此外，从 API 层级来看，一个特殊的单元格称为
    **Cell0**，它不运行任何服务，仅包含名为 **nova_cell0** 的数据库，里面存储有关由于调度器故障而无法启动的失败实例的信息。
- en: 'Each cell grouping the compute nodes manages its own resources and they have
    no affiliation with each other. Each cell has its own database storing information
    about instances and uses its own dedicated messaging queue to coordinate between
    the local cell’s **nova-conductor** and **nova-compute** services. The new CellV2
    method simply involves the following workflow steps to spawn a new instance in
    a multi-cell deployment:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元格（cell）管理自己的计算节点资源，并且它们之间没有任何关联。每个单元格都有自己的数据库来存储实例信息，并使用自己专用的消息队列来协调本地单元格中的
    **nova-conductor** 和 **nova-compute** 服务。新的 CellV2 方法涉及以下工作流程步骤来在多单元格部署中创建新实例：
- en: '**nova-api** (in the API cell) receives the API REST call and forwards it to
    the **nova-scheduler** service (in the API cell).'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-api**（在 API 单元格中）接收 API REST 调用，并将其转发到 **nova-scheduler** 服务（在 API 单元格中）。'
- en: '**nova-scheduler** contacts the Placement service, applies its filters, and
    determines which compute node will be assigned.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-scheduler** 联系 Placement 服务，应用其筛选器，并确定将分配给哪个计算节点。'
- en: '**nova-api** stores the mapping instance information in its **nova_api** database
    (the **instance_mappings** table).'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-api** 将映射实例信息存储在其 **nova_api** 数据库中（**instance_mappings** 表）。'
- en: '**nova-api** stores the selected compute node record in the database target
    cell (for example, **nova_cell01** if the selected compute node resides in **Cell01**
    ).'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-api** 将选定的计算节点记录存储在目标 cell 的数据库中（例如，如果选定的计算节点位于 **Cell01**，则为 **nova_cell01**）。'
- en: '**nova-api** sends an RPC call to **nova-super-conductor** to build the instance.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-api** 向 **nova-super-conductor** 发送 RPC 调用以构建实例。'
- en: '**nova-super-conductor** initiates an RPC call to **nova-conductor** in the
    target cell to spawn the instance via the compute service.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-super-conductor** 向目标 cell 中的 **nova-conductor** 发起 RPC 调用，通过计算服务启动实例。'
- en: The CellV2 architecture has shifted drastically from the initial CellV1 architecture
    with the removal of an additional scheduling layer and the division of the data
    into multiple databases instead of carrying a replication process for each request.
    This way, expanding a cell perimeter to welcome more compute hosts can be easily
    performed without the need to go through the whole synchronization process per
    cell, as we have seen in CellV1. Moreover, the new cell architecture is simple
    as all cells are equal, making the whole multi-cell deployment a flat-like structure
    (only two layers), unlike its previous version, which was based on the tree structure.
    Once the logical layout of cells is designed and ready for deployment, operators
    will just need to choose which hypervisor nodes will be associated with which
    cell. Unlike host aggregation, regions, and Availability Zones, users are not
    aware of the concept of a cell, as the cell strategy is mainly for cloud operators
    to expand large compute environments with the best management and performance
    enhancements.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: CellV2 架构与最初的 CellV1 架构发生了显著变化，去除了额外的调度层，并将数据划分到多个数据库中，而不是为每个请求进行复制过程。这样，扩展一个
    cell 范围以迎接更多计算主机变得更加容易，而无需像 CellV1 中那样每个 cell 都经历整个同步过程。此外，新的 cell 架构简单，因为所有的
    cell 都是平等的，使得整个多 cell 部署呈现平面结构（仅有两层），与之前基于树状结构的版本不同。一旦 cell 的逻辑布局设计完成并准备好部署，操作员只需选择将哪些虚拟机监控节点与哪些
    cell 关联。与主机聚合、区域和可用区不同，用户并不需要了解 cell 的概念，因为 cell 策略主要是为了帮助云操作员在扩展大规模计算环境时提供最佳的管理和性能提升。
- en: Workload segregation
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作负载隔离
- en: The latest features of the Nova service provide different strategies to unlock
    a fully scalable OpenStack compute infrastructure by means of regions, Availability
    Zones, host aggregates, and cell concepts. To have a full picture of a large deployment,
    adopting one or more of those approaches would require the definition of an orchestrator
    element to dispatch instances in a particular set of hypervisor hosts. At the
    heart of the launch process of instances is the **nova-scheduler** service, which
    determines on which host an instance will be placed. As highlighted in *Chapters
    1* and *3* , the latest OpenStack releases, come with the Placement service, which
    was introduced in the Newton release. The Placement service works in tandem with
    the Nova scheduler for a more advanced and granular host pre-filtering selection
    process. To make the most of the segregation approaches defined in the previous
    section, it is essential to skim the surface of different compute scheduling methods
    that are mindful of the overall compute segregation strategy in your OpenStack
    deployment.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Nova 服务的最新功能通过区域、可用区、主机聚合和 cell 概念提供了不同的策略，以解锁一个完全可扩展的 OpenStack 计算基础设施。要全面了解大型部署，采用这些方法中的一种或多种将需要定义一个调度器元素来调度特定集的虚拟机监控主机上的实例。在实例启动过程中，**nova-scheduler**
    服务发挥着核心作用，决定一个实例将被放置在哪个主机上。如 *第1章* 和 *第3章* 中所强调，最新的 OpenStack 版本都带有 Placement
    服务，该服务在 Newton 版本中首次引入。Placement 服务与 Nova 调度器协同工作，为更先进、更精细的主机预过滤选择过程提供支持。为了充分利用上一节中定义的隔离方法，必须快速了解不同的计算调度方法，这些方法需要考虑到你在
    OpenStack 部署中的整体计算隔离策略。
- en: Pre-filtering
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预过滤
- en: The main motivation for having a dedicated Placement service that runs alongside
    the Nova scheduler is to resolve the dilemma of a large compute deployment. Prior
    to the introduction of the Placement service, the Nova scheduler had to loop through
    all the compute farms, including the enabled filters, in an entire large deployment,
    resulting in inefficiency and performance issues. Additionally, the sum of resources
    reported by the compute nodes lacked accuracy due to the basic functions of Nova
    in calculating, in a detailed fashion, the exact number of resources and inventory
    usage.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个专门的 Placement 服务，与 Nova 调度器一起运行，主要目的是解决大规模计算部署中的困境。在引入 Placement 服务之前，Nova
    调度器必须遍历整个大规模部署中的所有计算农场，包括启用的过滤器，导致效率低下和性能问题。此外，由于 Nova 在计算资源的详细数量和库存使用方面的基本功能，计算节点报告的资源总和缺乏准确性。
- en: With the advent of the Placement service, the logic of resource reporting has
    been changed by adding a **resource_tracker** component in a compute node that
    reports its inventory and available resources to the Placement service, which
    periodically stores a full synchronization of reported resources. As detailed
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* , the Placement service comes with the definition of **resource
    providers** and **traits** to apply prefilters to its compute node list once contacted
    by **nova-scheduler** . Under the hood, **nova-scheduler** calls the **placement-api**
    service via a **GET API** request by passing different scheduling parameters such
    as **VCPU** , **DISK_GB** , **MEMORY_MB** , and **VGPU** . These will be translated
    into **trait** requests at the **placement-api** level.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Placement 服务的出现，资源报告的逻辑发生了变化，通过在计算节点中添加 **resource_tracker** 组件，计算节点将其库存和可用资源报告给
    Placement 服务，后者定期存储报告资源的完整同步。正如在 [*第 3 章*](B21716_03.xhtml#_idTextAnchor108) 中详细描述的那样，*OpenStack
    控制平面 – 共享服务*，Placement 服务通过定义 **资源提供者** 和 **traits** 来应用预过滤器，以便在 **nova-scheduler**
    联系时对计算节点列表进行处理。在后台，**nova-scheduler** 通过传递不同的调度参数（如 **VCPU**，**DISK_GB**，**MEMORY_MB**
    和 **VGPU**）通过 **GET API** 请求调用 **placement-api** 服务。这些请求将在 **placement-api** 层转化为
    **trait** 请求。
- en: Resource provider allocation
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源提供者分配
- en: 'The Placement service, as reviewed in *Chapters 1* and *3* , is used by **nova-scheduler**
    to list a set of pre-filtered compute nodes based on specific attributes before
    evaluating their candidacy, as shown in the following diagram:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*第 1 章*和*第 3 章*中回顾的那样，Placement 服务由 **nova-scheduler** 使用，用于在评估候选节点之前，根据特定属性列出一组经过预过滤的计算节点，如下图所示：
- en: '![Figure 4.8 – The OpenStack compute placement workflow](img/B21716_04_08.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – OpenStack 计算资源分配工作流](img/B21716_04_08.jpg)'
- en: Figure 4.8 – The OpenStack compute placement workflow
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – OpenStack 计算资源分配工作流
- en: 'When the **nova-api** service receives a user request for a new instance creation,
    **nova-scheduler** will fire a **GET /allocation_candidates()** request to the
    **placement-api** service. The **allocation_candidates** method is a collection
    of a set of resource providers that will allocate resources based on the attributes
    encapsulated in the request. A sample API GET request uses the following format
    as a filter condition to embed the required resources to launch an instance:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当 **nova-api** 服务接收到用户请求创建新实例时，**nova-scheduler** 将向 **placement-api** 服务发出
    **GET /allocation_candidates()** 请求。**allocation_candidates** 方法是一个资源提供者集合，根据请求中封装的属性分配资源。一个示例
    API GET 请求使用以下格式作为筛选条件，以嵌入启动实例所需的资源：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Upon the previous request, the **placement-api** service will fetch the complying
    hosts that will accommodate instance flavors: disk capacity of 500 GB, memory
    size of 2048 MB, and 4 vCPUs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 根据先前的请求，**placement-api** 服务将获取符合要求的主机，这些主机将容纳实例规格：500 GB 磁盘容量、2048 MB 内存和 4
    个 vCPU。
- en: 'The previous request can be extended to filter based on resource traits by
    adding the **&required** query parameter at the end of the request string, so
    the allocation query will look as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的请求可以通过在请求字符串的末尾添加 **&required** 查询参数，扩展为根据资源特征进行筛选，因此分配查询将如下所示：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This way, the **placement-api** service will combine the required resource attributes
    and an additional trait for hosts that support a CPU with X86_SVM architecture.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，**placement-api** 服务将结合所需的资源属性和额外的特征，筛选出支持 X86_SVM 架构 CPU 的主机。
- en: 'Typically, cloud operators can use the OpenStack CLI to deal with placement
    configuration via the **allocation candidate** command line, as in the following
    example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，云运营商可以使用OpenStack CLI通过**allocation candidate**命令行来处理定位配置，如以下示例所示：
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here’s the output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 4.9 – The placement details of a registered allocation](img/B21716_04_09.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 注册分配的定位详情](img/B21716_04_09.jpg)'
- en: Figure 4.9 – The placement details of a registered allocation
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 注册分配的定位详情
- en: 'The main parameters that influence the logic of the subsequent scheduler pre-filtering
    process lie in **allocation_requests** and **provider_summaries** . The previous
    objects are returned by the **placement-api** service to the **nova-scheduler**
    service with two types of objects listed in JSON format with the following data
    structure:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 影响后续调度器预过滤过程逻辑的主要参数是**allocation_requests**和**provider_summaries**。这些对象由**placement-api**服务返回给**nova-scheduler**服务，以两种类型的对象以JSON格式列出，数据结构如下：
- en: '**allocation_requests** : A list of resource providers that can satisfy the
    allocation request:'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**allocation_requests**：一个资源提供者列表，用于满足分配请求：'
- en: '[PRE4]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**provider_summaries** : The total amount of resources and information usage
    across all the resource providers that satisfy the allocation request:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**provider_summaries**：满足分配请求的所有资源提供者的总资源和信息使用情况：'
- en: '[PRE5]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The JSON **provider_summaries** object output includes the associated traits
    with a resource provider. The previous output example shows the resource provider
    for the **52f6729-cd55-4747-91a1-543adfeea2a** compute node UUID, labeled with
    a trait class supporting the **HW_CPU_X86_SVM** and **HW_CPU_X86_SSE2** CPU architectures.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: JSON格式的**provider_summaries**对象输出包括与资源提供者关联的特性。前面的输出示例显示了一个计算节点UUID为**52f6729-cd55-4747-91a1-543adfeea2a**的资源提供者，并标注了支持**HW_CPU_X86_SVM**和**HW_CPU_X86_SSE2**
    CPU架构的特性类。
- en: Filtering
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤
- en: Once the **placement-api** service returns the candidate host list to the **nova-scheduler**
    service, the latter process runs a variety of filters based on the pre-configured
    filters in the Nova service. The **/etc/nova/nova.conf** file can be configured
    to run one or several schedulers by adjusting the **enabled_filters** option.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**placement-api**服务将候选主机列表返回给**nova-scheduler**服务，后者会根据Nova服务中预配置的过滤器执行多种过滤操作。可以通过调整**enabled_filters**选项配置**/etc/nova/nova.conf**文件，以运行一个或多个调度器。
- en: Important note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The scheduler filters in the **kolla-ansible** repository can be configured
    in the **filter_scheduler** section, located in the **/** **kolla-ansible/ansible/roles/nova/templates/nova.conf.j2**
    file.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible**仓库中的调度器过滤器可以在**filter_scheduler**部分进行配置，位于**/kolla-ansible/ansible/roles/nova/templates/nova.conf.j2**文件中。'
- en: 'Some of the most common filters configured and supported by default in the
    scheduler are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在调度器中默认配置和支持的常见过滤器如下：
- en: '**ComputeFilter** : This returns a list of fully operational hypervisor hosts
    that should be enabled by default.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ComputeFilter**：返回一个完全可操作的超管主机列表，默认情况下应启用。'
- en: '**ComputeCapabilitiesFilter** : This returns a list of hypervisor hosts that
    are capable of spawning an instance with the requested flavor. For example, extra
    specifications could include verifying a compute host running a KVM hypervisor.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ComputeCapabilitiesFilter**：返回能够启动具有请求规格实例的超管主机列表。例如，额外的规格可能包括验证运行KVM超管的计算主机。'
- en: '**ImagePropertiesFilter** : This returns a list of hypervisor hosts that meet
    the desired image properties defined in the instance image. Image properties can
    be added based on the architecture of the hardware or hypervisor type. For example,
    spawning an instance would require an image that should run on a host supporting
    KVM as the hypervisor:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ImagePropertiesFilter**：返回符合实例镜像中定义的期望镜像属性的超管主机列表。镜像属性可以根据硬件架构或超管类型添加。例如，启动一个实例需要一个可以在支持KVM作为超管的主机上运行的镜像：'
- en: '[PRE6]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Important note
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: One of the major updates from the Liberty release is the image properties being
    passed to the filter for checks. The new supported properties are **hw_architecture**
    (formerly named **architecture** ), which defines the hardware architecture, **img_hv_type**
    (formerly named **hypervisor_type** ), which describes the type of the hypervisor,
    **img_hv_requested_version** (formerly named **hypervisor_version_requires** ),
    which defines the required version of the hypervisor, and **hw_vm_mode** (formerly
    named **vm_mode** ), which refers to the hypervisor application binary interface.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Liberty 版本的一个主要更新是图像属性被传递到筛选器进行检查。新的支持属性包括 **hw_architecture**（之前称为 **architecture**），它定义了硬件架构；**img_hv_type**（之前称为
    **hypervisor_type**），它描述了超算类型；**img_hv_requested_version**（之前称为 **hypervisor_version_requires**），它定义了所需的超算版本；以及
    **hw_vm_mode**（之前称为 **vm_mode**），它指的是超算应用二进制接口。
- en: '**ServerGroupAntiAffinityFilter** : This dispatches instances of the same group
    in different hypervisor hosts. A common use case is to launch instances running
    the same workload and requiring an additional level of high availability. To create
    server groups using an anti-affinity policy, use the **--hint** flag in the Nova
    command-line tool as follows:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ServerGroupAntiAffinityFilter**：该筛选器将同一组的实例调度到不同的超算主机。一个常见的使用场景是启动运行相同工作负载且需要额外高可用性的实例。要使用反亲和力策略创建服务器组，请在
    Nova 命令行工具中使用 **--hint** 标志，如下所示：'
- en: '[PRE7]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The outcome of the previous command lines will ensure that **instance 01** and
    **instance 02** are part of the same application server group, **pp_webgroup**
    , have the **122ee342-3345-2234-bac4-1515321e1ebb** UUID, and run in different
    compute nodes.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令的结果将确保 **instance 01** 和 **instance 02** 是同一个应用服务器组 **pp_webgroup** 的一部分，并且具有
    **122ee342-3345-2234-bac4-1515321e1ebb** UUID，并将在不同的计算节点上运行。
- en: '**ServerGroupAffinityFilter** : This dispatches instances in the same hypervisor
    host already running an affinity server group. The affinity scheduler filter is
    recommended to run workloads that require the highest level of performance and
    low latency. To create server groups using an affinity policy, use the **--hint**
    flag in the Nova command-line tool, as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ServerGroupAffinityFilter**：该筛选器将实例调度到已经运行某个亲和力服务器组的相同超算主机。建议使用亲和力调度器筛选器来运行需要最高性能和低延迟的工作负载。要使用亲和力策略创建服务器组，请在
    Nova 命令行工具中使用 **--hint** 标志，如下所示：'
- en: '[PRE8]'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The outcome of the previous command lines will ensure that **instance 01** and
    **instance 02** , part of the same application server group, **pp_perfgroup**
    , having the **144ee217-5543-9871-cda3-1e57522ecde1** UUID, will be running in
    the same compute node.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令的结果将确保 **instance 01** 和 **instance 02**，它们是同一个应用服务器组 **pp_perfgroup** 的一部分，并且具有
    **144ee217-5543-9871-cda3-1e57522ecde1** UUID，将运行在同一计算节点上。
- en: OpenStack scheduling supports other filters, with more granular and customized
    options included in the latest releases of OpenStack. At the time of writing this
    edition, an exhaustive list of the latest filters and configurations can be found
    at [https://docs.openstack.org/nova/latest/admin/scheduling.html#filters](https://docs.openstack.org/nova/latest/admin/scheduling.html#filters)
    .
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 调度支持其他筛选器，并且在 OpenStack 的最新版本中包含了更多细粒度和自定义的选项。在撰写本版本时，最新的筛选器和配置的完整列表可以在[https://docs.openstack.org/nova/latest/admin/scheduling.html#filters](https://docs.openstack.org/nova/latest/admin/scheduling.html#filters)找到。
- en: Weighting
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加权
- en: Weighting is the last stage of the scheduling process, wherein the **nova-scheduler**
    service applies scoring for each filtered candidate host selected from the placement
    that passed all assigned filters. The compute node with the highest weight will
    be selected by the scheduler at the final stage. On the surface, the weighting
    logic might look less complex, but under the hood, additional factors should be
    considered. The compute node metrics are periodically monitored and fed to the
    database to keep track of the host usage and resource claims. The weighting process
    uses such information as factors and calculates the weights using a normalization
    function by giving each compute node a value between **0.0** and **1.0** . Compute
    nodes with the most available resources (as reported by the monitoring) will be
    assigned a maximum value of **1** and compute nodes with the lowest available
    resources will have a minimum value of **0** .
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 加权是调度过程的最后阶段，在此阶段，**nova-scheduler** 服务会对从通过所有分配过滤器的放置中选择的每个候选主机进行评分。调度器将在最后阶段选择具有最高权重的计算节点。表面上看，加权逻辑似乎不复杂，但实际上需要考虑额外的因素。计算节点的度量值会定期监控并输入到数据库中，以跟踪主机的使用情况和资源声明。加权过程使用这些信息作为因素，并通过将每个计算节点的值标准化到
    **0.0** 到 **1.0** 之间来计算权重。具有最多可用资源的计算节点（如监控所报告的）将被赋予最大值 **1** ，而具有最少可用资源的计算节点将被赋予最小值
    **0** 。
- en: 'The normalized factor is calculated based on the available resources using
    the following formula:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化因子是根据可用资源使用以下公式计算的：
- en: '**(host_availability_resource - min_value_all)/(max_value_all –** **min_value_all)**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**(host_availability_resource - min_value_all)/(max_value_all –** **min_value_all)**'
- en: 'Here, the following applies:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，适用以下规则：
- en: '**host_availability_resource** is the value of the available resource in a
    compute node'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**host_availability_resource** 是计算节点中可用资源的值'
- en: '**min_value_all** is the minimum value of resources across all compute nodes'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_value_all** 是所有计算节点中资源的最小值'
- en: '**max_value_all** is the maximum value of resources across all compute nodes'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_value_all** 是所有计算节点中资源的最大值'
- en: 'The second step of the function is to multiply the normalized value of each
    compute node for each available resource by the associated weigher multiplier.
    At the end of the weighting process, the scheduler will sum all normalized weights
    and select the compute node with the highest score, summarized as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的第二步是将每个可用资源的每个计算节点的标准化值与相关的加权乘数相乘。在加权过程结束时，调度器将汇总所有标准化的权重，并选择得分最高的计算节点，概括如下：
- en: '**Σ** **(weight(i)*norm_factor)**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**Σ** **(weight(i)*norm_factor)**'
- en: 'Here, the following applies:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，适用以下规则：
- en: '**weight(i)** is the associated weight multiplier for a resource **i**'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**weight(i)** 是资源 **i** 的相关权重乘数'
- en: '**norm_factor** is the assigned normalized weight factor for each compute node'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**norm_factor** 是为每个计算节点分配的标准化权重因子'
- en: 'The following example illustrates the weighting process by calculating the
    normalized factors and weights for available RAM capacity and CPUs across five
    compute nodes, respectively. The diagram illustrates the first subset size of
    nodes ( **subset 1** ) – the first compiled list of evaluated hosts by the Nova
    scheduler through the filters – as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过计算五个计算节点中可用的 RAM 容量和 CPU 的标准化因子和权重来说明加权过程。该图展示了第一个节点子集大小（**子集 1**）– 这是
    Nova 调度器通过过滤器评估的第一个主机列表，具体如下：
- en: '![Figure 4.10 – The OpenStack compute weighting mechanism](img/B21716_04_10.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – OpenStack 计算加权机制](img/B21716_04_10.jpg)'
- en: Figure 4.10 – The OpenStack compute weighting mechanism
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – OpenStack 计算加权机制
- en: 'The scheduler then runs the weighting mechanism and generates the most suitable
    hypervisor hosts based on the weight scoring as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，调度器运行加权机制，并根据加权评分生成最合适的虚拟化主机，具体如下：
- en: Nodes A and D accommodate the highest RAM capacity and are assigned the maximum
    weight, normalized to **1** . The weight cost is denoted as **100** (1 multiplied
    by 100).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点 A 和 D 容纳了最高的 RAM 容量，并被赋予最大权重，标准化为 **1** 。权重成本表示为 **100** （1 乘以 100）。
- en: Nodes B and C have the lowest amount of RAM capacity and are assigned the minimum
    weight normalized to **0** . The weight cost is denoted as **0** (0 multiplied
    by 100).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点 B 和 C 拥有最小的 RAM 容量，且其标准化后的权重为 **0** 。权重成本表示为 **0** （0 乘以 100）。
- en: 'Node E is assigned a weight normalized to 0.4 by applying the previous formula
    to calculate the normalized factor as follows: (350 - 250) / (500 – 250). The
    weight cost is denoted as 40 (0.4 multiplied by 100)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点 E 通过将先前的公式应用于计算归一化因子来分配归一化权重为 0.4，计算如下：(350 - 250) / (500 - 250)。权重成本表示为
    40（0.4 乘以 100）。
- en: Nodes A and D accommodate the highest number of CPUs and are assigned the maximum
    weight, normalized to **1** . The weight cost is denoted as **100** (1 multiplied
    by 100)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机 A 和 D 容纳最多的 CPU 数量，并分配了最大的权重，归一化为 **1** 。权重成本表示为 **100**（1 乘以 100）。
- en: Node C has the lowest amount of CPU capacity and is assigned the minimum weight,
    normalized to **0** . The weight cost is denoted as **0** (0 multiplied by 100)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点 C 具有最少的 CPU 容量，并分配了最小的权重，归一化为 **0** 。权重成本表示为 **0**（0 乘以 100）。
- en: 'Nodes B and E are assigned a weight normalized to 0.5 by applying the previous
    formula as follows: (10 - 5) / (20 – 10). The weight cost is denoted as 50 (0.5
    multiplied by 100)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点 B 和 E 通过将先前的公式应用于计算归一化因子来分配归一化权重为 0.5，计算如下：(10 - 5) / (20 - 10)。权重成本表示为 50（0.5
    乘以 100）。
- en: 'The final weighting for each host is calculated by summing up each weight cost
    of each CPU and RAM resource as follows: host A (100 + 100), host B (0 + 50),
    host C (0 + 0), host D (100 + 100), and host E (40 + 50)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主机的最终权重计算方式是将每个 CPU 和 RAM 资源的权重成本相加，如下所示：主机 A（100 + 100），主机 B（0 + 50），主机 C（0
    + 0），主机 D（100 + 100）和主机 E（40 + 50）。
- en: In this example, both hosts A and D won the weighting bid and the scheduler
    will simply randomly select one of them from the second node’s subset to launch
    the instance. Resource availability will be reflected in the database and the
    scheduler will update the weights accordingly on each new instance creation request.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，主机 A 和 D 均赢得了权重投标，调度程序将从第二个节点子集中随机选择一个来启动实例。资源可用性将反映在数据库中，并且调度程序将在每个新实例创建请求上相应地更新权重。
- en: The additional options brought to the filtering and scheduling mechanisms in
    the compute service can be handy in managing and distributing the workload demand
    across the compute farm. By default, the scheduling allocates resources for instances
    by spreading them across the available hosts. In some other cases, dispatching
    several instances with different flavors and sizes would create a concurrency
    issue when allocating resources, and hence prevent a subset of instances from
    being launched. The scheduling weighting policy can be adjusted to use a **stacking**
    approach. In this case, instances will be allocated to the first filtered host
    until all of its resources are fully exhausted, before moving on to the next one.
    A good example of a stacking method is the conjunction of the server affinity
    configuration and weighting mechanism.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 计算服务中引入的附加选项可以在管理和分发计算农场中的工作负载需求时非常方便。默认情况下，调度程序通过在可用主机之间分配实例来分散资源。在其他一些情况下，分派具有不同规格和大小的多个实例会在分配资源时创建并发问题，并因此阻止启动一部分实例。调度权重策略可以调整为使用
    **堆叠** 方法。在这种情况下，实例将分配给第一个经过过滤的主机，直到其所有资源完全耗尽，然后再移动到下一个主机。堆叠方法的一个良好示例是服务器关联配置和权重机制的结合。
- en: 'To customize the weighting mechanism in your deployment, make sure to apply
    the following configuration in the **/** **etc/nova/nova.conf** file:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要在部署中自定义权重机制，请确保在 **/** **etc/nova/nova.conf** 文件中应用以下配置：
- en: 'Enable monitoring on the compute nodes to gather the CPU metrics by setting
    the **compute_monitors** parameter:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过设置 **compute_monitors** 参数在计算节点上启用监控以收集 CPU 指标：
- en: '[PRE9]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Enable the **weights.all_weighers** class:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用 **weights.all_weighers** 类：
- en: '[PRE10]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Set the host subset size to **1** . Adjust the scheduler to select the host
    with the highest weight by setting the host subset size to **1** :'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将主机子集大小设置为 **1** 。通过将主机子集大小设置为 **1** 来调整调度程序以选择具有最高权重的主机：
- en: '[PRE11]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Adjust the weight multiplier to **1** . This way, the launch of a new instance
    will be spread evenly between compute nodes:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整权重乘数为 **1** 。这样，新实例的启动将在计算节点之间均匀分布：
- en: '[PRE12]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Adjust the RAM weight multiplier to **1** . This way, the launch of a new instance
    will be spread evenly between compute nodes with free memory:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整 RAM 权重乘数为 **1** 。这样，将在具有空闲内存的计算节点之间均匀分布新实例的启动：
- en: '[PRE13]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Important note
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Setting up a weighted multiple as **1.0** will spread the resource allocation
    across all available filter hosts. If the stacking option is the preferred option,
    set the associate value of the weight multiplier to a value of **-1** .
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 将加权乘数设置为**1.0**将使资源分配均匀分布在所有可用的筛选主机上。如果首选堆叠选项，则将权重乘数的关联值设置为**-1**。
- en: 'Add a new multiplier to calculate the weighting host I/O operations. Instruct
    Nova to use the active hosts evenly with the least I/O CPU metric by setting its
    weight multiplier to **1.0** as follows:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个新的乘数来计算加权主机 I/O 操作。通过将权重乘数设置为**1.0**，指示 Nova 以最少 I/O CPU 指标均匀使用活动主机，如下所示：
- en: '[PRE14]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Restart the **nova-scheduler** and **nova-compute** services in the controller
    node and each compute node to apply the changes:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制节点和每个计算节点上重新启动**nova-scheduler**和**nova-compute**服务，以应用更改：
- en: '[PRE15]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Important note
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The **kolla-ansible** repository does not, by default, provide a low-level
    configuration for Nova weighting policies. It is possible to adjust the **nova.conf**
    template file located at **/kolla-ansible/ansible/roles/nova/templates/nova.conf.j2**
    by adding the configuration lines described previously. It is also recommended
    to cast an eye on the latest supported **nova.conf** settings, as defined here
    for the latest release: [https://docs.openstack.org/nova/latest/configuration/sample-config.html](https://docs.openstack.org/nova/latest/configuration/sample-config.html)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible** 仓库默认情况下不提供 Nova 加权策略的低级配置。可以通过添加前述配置行，调整位于**/kolla-ansible/ansible/roles/nova/templates/nova.conf.j2**的**nova.conf**模板文件。还建议查看最新支持的**nova.conf**
    设置，具体内容请参见此处的最新版本：[https://docs.openstack.org/nova/latest/configuration/sample-config.html](https://docs.openstack.org/nova/latest/configuration/sample-config.html)。'
- en: The OpenStack Bobcat release introduces a new scheduler weigher, **NumInstancesWeigher**
    , which is handy for dispatching instances based on the number of active instances
    each compute node is hosting. As demonstrated in the previous example, setting
    the multiplier value to **1** will enable a **packing** strategy by favoring the
    host with the most number of active instances. A value of **-1** will favor a
    spreading strategy by checking the least busy host in terms of active instances.
    A full list of different weighers can be found at [https://docs.openstack.org/nova/latest/admin/scheduling.html#weights](https://docs.openstack.org/nova/latest/admin/scheduling.html#weights)
    .
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Bobcat 版本引入了一个新的调度器权重计算器，**NumInstancesWeigher**，该计算器有助于根据每个计算节点托管的活动实例数量来调度实例。如前面的示例所示，将乘数值设置为**1**将启用**打包**策略，优先选择具有最多活动实例的主机。将值设置为**-1**则会优先选择**分布**策略，选择活动实例最少的主机。有关不同权重计算器的完整列表，请参见
    [https://docs.openstack.org/nova/latest/admin/scheduling.html#weights](https://docs.openstack.org/nova/latest/admin/scheduling.html#weights)。
- en: Extending compute for containers
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为容器扩展计算能力
- en: Since 2014, the OpenStack community has been developing additional projects
    around containerization technology, allowing cloud users to leverage different
    sets of services to run applications on top of the compute service. The most mature
    and in-production services are **Magnum** and **Zun** , which we will cover next.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年以来，OpenStack 社区一直在围绕容器化技术开发其他项目，允许云用户利用不同的服务集在计算服务之上运行应用程序。最成熟且正在生产环境中使用的服务是**Magnum**
    和 **Zun**，我们接下来将介绍这两个服务。
- en: Magnum
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Magnum
- en: 'The main motivation to run a containerized environment is to run and deploy
    an application with minimal configuration complexity that’s portable and requires
    the least maintenance effort. Running workloads that are composed of many containers
    requires a layer of orchestration to manage the logic of the life cycle of different
    components, as well as inter-communication, in an automated fashion. This is referred
    to as a container cluster management platform, such as **Mesos** , **Kubernetes**
    , and **Docker Swarm** , to name a few. There are some well-known public cloud
    services, such as AWS **Elastic Container Service** ( **ECS** ), AWS **Elastic
    Kubernetes Service** ( **EKS** ), GCP **Google Kubernetes Engine** ( **GKE** ),
    and **Azure Kubernetes Service** ( **AKS** ). In the OpenStack world, Magnum is
    the project code name that has been integrated into the OpenStack ecosystem and
    is referred to as a COE. Like any other OpenStack service, Magnum exposes an API
    to interact with COEs as first-class resources, such as Kubernetes and Docker
    Swarm. Containers will be deployed and run on top of the hypervisor nodes. The
    exposure to different container engines unlocks cloud operators to offer an array
    of choices for users to select from, as illustrated in the following diagram:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 运行容器化环境的主要动机是以最小的配置复杂度运行和部署应用程序，这些应用程序便于移植且维护成本最低。运行由多个容器组成的工作负载需要一层编排来管理不同组件生命周期的逻辑，以及自动化的组件间通信。这被称为容器集群管理平台，例如**Mesos**、**Kubernetes**和**Docker
    Swarm**等。还有一些知名的公共云服务，如AWS **Elastic Container Service**（**ECS**）、AWS **Elastic
    Kubernetes Service**（**EKS**）、GCP **Google Kubernetes Engine**（**GKE**）和**Azure
    Kubernetes Service**（**AKS**）。在OpenStack世界中，Magnum是一个已集成到OpenStack生态系统中的项目代号，被称为COE。像其他OpenStack服务一样，Magnum暴露一个API与COE（如Kubernetes和Docker
    Swarm）作为一等资源进行交互。容器将部署并运行在虚拟化节点上。对不同容器引擎的支持让云运维人员能够为用户提供一系列可供选择的选项，如下图所示：
- en: '![Figure 4.11 – The OpenStack Magnum multi-COE architecture](img/B21716_04_11.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – OpenStack Magnum 多COE架构](img/B21716_04_11.jpg)'
- en: Figure 4.11 – The OpenStack Magnum multi-COE architecture
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – OpenStack Magnum 多COE架构
- en: 'Magnum refers to each collection of COE nodes and their associated containers
    using the following terminologies:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum使用以下术语来表示每个COE节点集合及其关联的容器：
- en: '**Bay** : This is a set of nodes running a COE. At the heart of the bay run,
    Magnum calls the Heat service to deploy the bay. Bays are simply a collection
    of Nova instances created and orchestrated by Heat using images that target both
    virtual machines and bare-metal ones.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bay**：这是一组运行COE的节点。在bay的核心运行时，Magnum调用Heat服务来部署bay。Bays只是通过Heat创建并编排的Nova实例集合，使用的镜像针对虚拟机和裸金属节点。'
- en: '**BayModel** : This is a collection of resources that constructs a bay defined
    in a simple template. A bay can use the same **BayModel** template to run across
    different COEs.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BayModel**：这是一个资源集合，用于构建在简单模板中定义的bay。一个bay可以使用相同的**BayModel**模板在不同的COE上运行。'
- en: '**Pod** : This is a group of containers running on the same COE node.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod**：这是在同一个COE节点上运行的一组容器。'
- en: '**Service** : This is an abstraction of an arrangement of bays and access policies.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Service**：这是一个抽象，表示一组bay和访问策略的配置。'
- en: '**Replication controller** : This is a dedicated process for monitoring, replicating,
    re-spawning failed containers, and scaling pods across COE nodes.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Replication controller**：这是一个专门的进程，用于监控、复制、重启失败的容器，并在COE节点之间扩展pods。'
- en: '**Magnum client** : This is a native client to query the COE that comes with
    the native **Docker CLI** for the Docker COE and **kubectl** for the Kubernetes
    COE.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Magnum client**：这是一个原生客户端，用于查询COE，并且自带原生**Docker CLI**（用于Docker COE）和**kubectl**（用于Kubernetes
    COE）。'
- en: 'Other emerging keys to Magnum’s success are the functions that are well integrated
    with other OpenStack services, which are listed here:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum成功的另一个关键因素是与其他OpenStack服务的良好集成，以下是相关功能：
- en: '**Identity** : This is an integration with Keystone for managing the authentication
    and authorization of the container cluster platform – for example, creating Kubernetes
    roles for users to run operational tasks on a cluster via Keystone.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Identity**：这是与Keystone的集成，用于管理容器集群平台的身份认证和授权——例如，通过Keystone为用户创建Kubernetes角色以便在集群上执行操作任务。'
- en: '**Networking** : The Neutron service is leveraged through supported networking
    drivers – for example, the **Flannel** overlay network is the default one for
    both the Kubernetes and Swarm COEs, allowing multi-host communication and managing
    the IP addresses of containers in each bay.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：通过支持的网络驱动程序利用Neutron服务——例如，**Flannel**覆盖网络是Kubernetes和Swarm COE的默认网络，允许多主机通信并管理每个容器池中容器的IP地址。'
- en: '**Imaging** : A prebuilt Glance image is provided to the Kubernetes and Swarm
    COEs, which can be configured and used to launch the nodes in the cluster.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**镜像**：为Kubernetes和Swarm COE提供了一个预构建的Glance镜像，可以配置并用于启动集群中的节点。'
- en: '**Storage** : The official supported storage is provided by Cinder as block
    storage in two forms: ephemeral and persistent storage options. Aside from the
    Docker storage driver, Cinder also comes with additional backend storage such
    as **Rexray** as the volume driver. At the time of writing this, Cinder only supports
    the Rexray driver for Swarm and the default Cinder for Kubernetes.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**：官方支持的存储由Cinder提供，作为块存储有两种形式：临时存储和持久存储选项。除了Docker存储驱动外，Cinder还提供了额外的后端存储，如**Rexray**作为卷驱动程序。在撰写本文时，Cinder仅支持Swarm的Rexray驱动和Kubernetes的默认Cinder驱动。'
- en: 'Other main highlights of Magnum that have been initiated or are in progress
    can be summarized as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum的其他主要亮点，已启动或正在进行中的部分，概述如下：
- en: '**Security posture** : The Magnum architecture is secure by design. Each bay
    is isolated from others, providing security for the workloads they host. Additionally,
    Magnum enables a multi-tenancy layout by preventing bays from being shared between
    OpenStack tenants.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：Magnum架构是从设计上就考虑到安全性的。每个容器池（bay）都是相互隔离的，确保它们所承载的工作负载的安全性。此外，Magnum通过防止容器池在OpenStack租户之间共享，实现了多租户布局。'
- en: '**Scalability** : The scaling of the cluster nodes is performed automatically
    by adjusting the Heat template attributes or manually via the OpenStack COE CLI.
    Note that container scaling will depend on the cluster container configuration
    as per the COE.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：集群节点的扩展是通过自动调整Heat模板属性或通过OpenStack COE CLI手动完成的。请注意，容器扩展将根据COE的集群容器配置来决定。'
- en: '**High availability** : At the time of writing this, the Magnum high availability
    criteria supports only one default Availability Zone. The group of compute nodes
    will be spawned in the same logical Availability Zone. The OpenStack community
    is considering enlarging the span of Magnum cluster nodes across more multi-Availability
    Zones in the next releases.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性**：在撰写本文时，Magnum高可用性标准仅支持一个默认的可用区。计算节点组将会在同一个逻辑可用区中启动。OpenStack社区正在考虑在未来版本中将Magnum集群节点的范围扩展到更多的多可用区。'
- en: Since the Liberty release, the Magnum service has received a lot of credit due
    to its exposure to an array of COEs and maturity, which can be seen in dozens
    of production workloads. The service has enabled engineering and architect teams
    to migrate to microservice-based architecture without the need to redesign and
    re-integrate the existing container toolsets. Other enhancements have been captured
    and can be followed by checking the release updates at [https://docs.openstack.org/magnum/latest/](https://docs.openstack.org/magnum/latest/)
    .
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 自Liberty版本以来，Magnum服务因其支持多种COE（容器编排引擎）和逐步成熟而获得了许多赞誉，这些可以在数十个生产工作负载中看到。该服务使得工程和架构团队能够迁移到基于微服务的架构，而无需重新设计和重新集成现有的容器工具集。其他增强功能已被捕获，用户可以通过查看[https://docs.openstack.org/magnum/latest/](https://docs.openstack.org/magnum/latest/)上的发布更新来了解。
- en: 'The Magnum service is not enabled by default in the current **kolla-ansible**
    infrastructure code repository. Setting **enable_magnum** to **yes** in the **/kolla-ansible/etc/kolla/globals.yml**
    file will enable the Magnum API service. Make sure to update the inventory file
    created in the previous chapter, **/ansible/inventory/multi_packtpub** , by adding
    the Magnum services, including the **magnum-api** and **magnum-conductor** services,
    in the controller node as in the following configuration stanza code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的**kolla-ansible**基础设施代码库中，Magnum服务默认未启用。通过在**/kolla-ansible/etc/kolla/globals.yml**文件中将**enable_magnum**设置为**yes**，可以启用Magnum
    API服务。请确保更新上一章节中创建的库存文件**/ansible/inventory/multi_packtpub**，通过在控制节点中添加Magnum服务，包括**magnum-api**和**magnum-conductor**服务，如以下配置代码所示：
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Important note
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: More Magnum configuration options are listed in the Ansible playbook, under
    the **/kolla-ansible/ansible/roles/magnum/default/main.yml** file. Additionally,
    Horizon supports the Magnum panel in the dashboard, which is disabled by default
    and can be added by setting **enable_horizon_magnum** to **yes** in the **/**
    **kolla-ansible/etc/kolla/** **globals** **.yml** file.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 更多 Magnum 配置选项列在 Ansible 剧本中，位于 **/kolla-ansible/ansible/roles/magnum/default/main.yml**
    文件下。此外，Horizon 支持 Magnum 面板，但默认情况下是禁用的，可以通过在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中将 **enable_horizon_magnum** 设置为 **yes** 来启用。
- en: Once both files are updated, run the CI/CD pipeline to roll out the Magnum Ansible
    playbooks and provide the Magnum service containers as part of the control plane
    in the OpenStack environment.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 更新两个文件后，运行 CI/CD 流水线以推出 Magnum Ansible 剧本，并将 Magnum 服务容器作为 OpenStack 环境中的控制平面一部分提供。
- en: Zun
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Zun
- en: 'Beyond the Magnum service and its multitude of container orchestration support
    options, there is the Zun service – a very simple service to run containers quickly
    in OpenStack without the need to manage or deploy a container orchestration environment
    based on Kubernetes Pods or Docker Swarm clusters. You can simply fire a Zun command
    line to create any type of container for any Docker repository and run your workload.
    To resolve any confusion between Magnum and Zun use cases, the latter is dedicated
    to managing and operating containers through OpenStack directly without the need
    to interface with an additional orchestration toolset layer of a COE such as Kubernetes
    or Swarm, unlike the Magnum service. Another potential source of confusion is
    the difference between Zun and the former OpenStack container originator, the
    **nova-docker** service. While **nova-docker** relies on the Nova API service
    to manage containers, Zun is independent of the Nova API and leverages its fully
    featured API interface to create and operate containers. The Zun service is well
    integrated with a number of versatile OpenStack services, including Nova, Glance,
    Keystone, Neutron, Cinder, Ceilometer, and Magnum, as illustrated in the following
    diagram:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Magnum 服务及其多种容器编排支持选项外，还有 Zun 服务——这是一个非常简单的服务，可以快速在 OpenStack 中运行容器，无需管理或部署基于
    Kubernetes Pods 或 Docker Swarm 集群的容器编排环境。你只需运行一个 Zun 命令行来创建任何类型的容器，针对任何 Docker
    仓库运行工作负载。为了消除 Magnum 和 Zun 用例之间的混淆，Zun 专注于通过 OpenStack 直接管理和操作容器，而无需与额外的编排工具层（如
    Kubernetes 或 Swarm）交互，这与 Magnum 服务不同。另一个可能的混淆源是 Zun 与前 OpenStack 容器发起者 **nova-docker**
    服务之间的区别。虽然 **nova-docker** 依赖 Nova API 服务来管理容器，但 Zun 独立于 Nova API，并利用其功能齐全的 API
    接口来创建和操作容器。Zun 服务与多个灵活的 OpenStack 服务紧密集成，包括 Nova、Glance、Keystone、Neutron、Cinder、Ceilometer
    和 Magnum，如下图所示：
- en: '![Figure 4.12 – The OpenStack Zun compute integration](img/B21716_04_12.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – OpenStack Zun 计算集成](img/B21716_04_12.jpg)'
- en: Figure 4.12 – The OpenStack Zun compute integration
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – OpenStack Zun 计算集成
- en: '*Figure 4* *.12* exposes two additional services that Zun compute uses and
    runs in tandem with Neutron and Cinder, respectively summarized as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4* *.12* 展示了 Zun 计算与 Neutron 和 Cinder 并行运行的另外两个服务，分别总结如下：'
- en: '**Kuryr** : This integrates with Neutron to provide advanced networking features
    for containers through **libnetwork** .'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kuryr**：它与 Neutron 集成，通过 **libnetwork** 提供容器的高级网络功能。'
- en: '**Fuxi** : This leverages the Docker API to manage volumes backed by Cinder
    and Manila storage services.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fuxi**：它利用 Docker API 来管理由 Cinder 和 Manila 存储服务支持的卷。'
- en: 'The Zun service is not enabled by default in the current **kolla-ansible**
    infrastructure code repository. To enable the service, the following settings
    should be adjusted in the **/kolla-ansible/etc/kolla/globals.yml** file as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 **kolla-ansible** 基础设施代码库中默认未启用 Zun 服务。要启用该服务，需在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中调整以下设置：
- en: '[PRE17]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Make sure to update the inventory file created in the previous chapter, **/ansible/inventory/
    multi_packtpub** , by adding the Zun services, including **zun-api** and optionally
    **zun-wsproxy** , as part of the control plane as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 确保通过在前一章节中创建的清单文件 **/ansible/inventory/multi_packtpub** 中添加 Zun 服务，包括 **zun-api**
    和可选的 **zun-wsproxy**，将其作为控制平面的一部分更新，如下所示：
- en: '[PRE18]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The **zun-compute** and optionally **zun-cni-daemon** services can be added
    as part of the compute node as depicted here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**zun-compute** 和可选的 **zun-cni-daemon** 服务可以作为计算节点的一部分添加，如下所示：'
- en: '[PRE19]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Important note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More Zun configuration options are listed in the Ansible playbook under the
    **/kolla-ansible/ansible/roles/zun/default/main.yml** file. Additionally, Horizon
    supports the Magnum panel in the dashboard, which is disabled by default and can
    be added by setting **enable_horizon_zun** to **yes** in the **/** **kolla-ansible/etc/kolla/globals.yml**
    file.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 更多 Zun 配置选项列在 Ansible 剧本中的 **/kolla-ansible/ansible/roles/zun/default/main.yml**
    文件中。此外，Horizon 支持仪表板中的 Magnum 面板，默认情况下该面板被禁用，可以通过在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中将 **enable_horizon_zun** 设置为 **yes** 来启用它。
- en: Once both files are updated, run the CI/CD pipeline to roll out the Magnum Ansible
    playbooks. The Zun service and its associated processes will be launched in containers
    as part of the control plane in the OpenStack environment.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦更新了这两个文件，运行 CI/CD 管道来部署 Magnum Ansible 剧本。Zun 服务及其相关进程将作为 OpenStack 环境控制平面的一部分在容器中启动。
- en: Important note
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Make sure to double-check that the **etcd** daemon is running on the controller
    node. Similarly, ensure that the Docker runtime and **kuryr-libnetwork** are properly
    installed in the compute node.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 确保**etcd**守护进程在控制节点上运行。同样，确保 Docker 运行时和**kuryr-libnetwork**在计算节点上正确安装。
- en: 'Check that the Zun service can be performed through the Zun service list command
    line:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Zun 服务是否可以通过 Zun 服务列表命令行执行：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Zun containers should be created in the compute nodes. To list the registered
    compute nodes for Zun, use the following command line:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Zun 容器应在计算节点上创建。要列出已注册的 Zun 计算节点，请使用以下命令行：
- en: '[PRE21]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Creating containers using the Zun CLI is straightforward. It is possible to
    use, by default, **Docker Hub** to pull images and rapidly run a container in
    the designated compute node. The following example command line creates a Zun
    container using a CirrOS image:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Zun CLI 创建容器是直接的。默认情况下，可以使用**Docker Hub**来拉取镜像并快速在指定的计算节点上运行容器。以下示例命令行使用
    CirrOS 镜像创建一个 Zun 容器：
- en: '[PRE22]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: An amazing hands-on feature brought by Zun is the flexibility to manage a container’s
    life cycle by using the native Docker API or the Zun API itself. For example,
    attaching a terminal to a running container can be performed by running **openstack
    appcontainer attach <container_name>** in a similar way to firing a **docker attach**
    command line.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Zun 带来的一个令人惊叹的实操特性是通过使用原生 Docker API 或 Zun API 本身来灵活管理容器生命周期。例如，附加终端到正在运行的容器，可以通过运行
    **openstack appcontainer attach <container_name>** 来执行，类似于执行 **docker attach**
    命令行。
- en: Expanding the compute farm
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展计算农场
- en: 'Managing the compute fleet hypervisors through **kolla-ansible** to spawn instances
    or containers is straightforward. After collecting your compute requirements,
    hardware capacity, types of workloads, and scheduling strategies, adding a compute
    node can be performed on the fly without incurring a complex operation overhead.
    The main requirements that should be considered are the listing of compute services
    – mainly, **nova-compute** and a networking plugin agent to provide connectivity
    with the control plane and sync with **nova-scheduler** for each resource allocation.
    The previous section demonstrated that additional components can be part of the
    compute node, including the Zun compute service, if you’re planning to offer an
    easy and quick container playground to end users. Another aspect of managing compute
    nodes from code through **kolla-ansible** is listing the available configuration
    options of the compute service, such as overcommitment ratios for CPU and memory,
    covered in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack
    – Design Considerations* . As you might deploy several compute nodes with different
    configurations, **kolla-ansible** provides ways to customize compute service configuration
    and override configuration with the global configuration defined in the **/etc/kolla/globals.yml**
    file. The following example will instruct Kolla to add a new compute host named
    **cn02.os.packtpub** (24 CPUs and 255 GB of RAM) with custom configuration supporting
    QEMU as a hypervisor, and a CPU and RAM overcommitment ratio of **8.0** and **4.0**
    , respectively:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**kolla-ansible**管理计算集群hypervisors以生成实例或容器非常简单。在收集计算需求、硬件容量、工作负载类型和调度策略后，可以在不产生复杂操作开销的情况下即时执行计算节点的添加。应考虑的主要要求是列出计算服务，主要是**nova-compute**和网络插件代理，以提供与控制平面的连接性，并与**nova-scheduler**同步以进行每个资源分配。前一节展示了其他组件可以作为计算节点的一部分，包括Zun计算服务，如果您计划为最终用户提供一个简单快速的容器游乐场。通过**kolla-ansible**从代码管理计算节点的另一个方面是列出计算服务的可用配置选项，例如[*第1章*](B21716_01.xhtml#_idTextAnchor014)，*重新访问OpenStack
    - 设计考虑*中涵盖的CPU和内存超额配额。由于可能部署具有不同配置的多个计算节点，**kolla-ansible**提供了定制计算服务配置并使用**/etc/kolla/globals.yml**文件中定义的全局配置覆盖配置的方法。以下示例将指导Kolla添加名为**cn02.os.packtpub**的新计算主机（24个CPU和255
    GB的RAM），具有支持QEMU作为hypervisor的自定义配置，并分别具有CPU和RAM超额配额比例为**8.0**和**4.0**：
- en: 'First, let’s start by populating the hostname in the inventory file:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在清单文件中填写主机名：
- en: '[PRE23]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Create a new **nova.conf** file under **/etc/kolla/config/nova/cn02.os.packtpub/**
    . Kolla looks for any possible config file under **/etc/kolla/** . If one is found,
    Kolla will override and merge the global one with the custom settings provided
    in a separate file. Note that you will need to create the path directories in
    the **kolla-ansible** repository, including the hostname of the compute node,
    following the **/etc/kolla/config/nova/COMPUTE_HOSTNAME/nova.conf** format, wherein
    **COMPUTE_HOSTNAME** is the hostname of the compute node.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**/etc/kolla/config/nova/cn02.os.packtpub/**下创建一个新的**nova.conf**文件。Kolla会查找**/etc/kolla/**下的任何可能的配置文件。如果找到一个配置文件，Kolla将使用单独文件中提供的自定义设置覆盖和合并全局配置。请注意，您需要在**kolla-ansible**存储库中创建路径目录，包括计算节点的主机名，遵循**/etc/kolla/config/nova/COMPUTE_HOSTNAME/nova.conf**格式，其中**COMPUTE_HOSTNAME**是计算节点的主机名。
- en: 'Edit the newly created **nova.conf** file by adding the CPU and RAM allocation
    ratio values of **8.0** and **4.0** , respectively, in a new section:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑新创建的**nova.conf**文件，通过在新的部分中分别添加**8.0**和**4.0**的CPU和RAM分配比例值：
- en: '[PRE24]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Add to the same file the desired **qemu** hypervisor to run on the new compute
    node:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所需的**qemu** hypervisor添加到同一文件以在新的计算节点上运行：
- en: '[PRE25]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'It is possible to overwrite the default scheduler by forcing a set of filters
    and using the **ComputeCapabilitiesFilter** filter to use any host with total
    available RAM for users of more than 250,000 MB:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过强制一组过滤器来覆盖默认调度程序，并使用**ComputeCapabilitiesFilter**过滤器来使用具有超过250,000 MB总可用RAM的任何主机：
- en: '[PRE26]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Optionally, you can use the filtering strategy for RAM criteria by stacking
    instances in the same host with a defined flavor. That way, you can make sure
    that the compute node will exactly accommodate a defined number of instances with
    specific flavors that you can predict in advance. By doing so, the 250 GB of RAM
    can be used fully without leaving smaller slots that would otherwise lay unused.
    The way to go in this case is the stacking strategy, by defining a negative weight
    multiplier in the same **custom nova.conf** file as follows:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，您可以通过在同一主机上堆叠具有定义规格的实例来使用 RAM 筛选策略。这样，您可以确保计算节点恰好容纳一个定义数量的实例和特定规格，这样您就能提前预测。通过这种方式，250
    GB 的内存可以被充分利用，不会留下任何小的空闲区域，否则这些空闲区域将无法使用。此时，采用的策略是堆叠策略，通过在同一个**custom nova.conf**文件中定义负权重乘数，如下所示：
- en: '[PRE27]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Using your CI/CD pipeline, trigger the run job to deploy the code in the staging
    environment, as demonstrated in the previous chapters, before merging to the production
    branch. The deployment stage should provision the new additional Kolla containers
    in the new compute node and start syncing with the control plane.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的 CI/CD 管道，在将代码合并到生产分支之前，触发运行作业以在暂存环境中部署代码，正如前几章所展示的那样。部署阶段应该在新的计算节点中配置新的额外
    Kolla 容器，并开始与控制平面同步。
- en: 'An example of resource allocation inspired by the filtering strategy covered
    in this chapter and [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting
    OpenStack – Design Considerations* is to dispatch a specific flavor of instances
    to a dedicated compute node. For example, a cloud operator would use the scheduler
    filter described in *step 5* to provision instances only with medium-sized allocation
    requests. For that to take effect, run the following command line in the controller
    node:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个受到本章所述筛选策略启发的资源分配示例，以及[*第 1 章*](B21716_01.xhtml#_idTextAnchor014)，《重新审视 OpenStack
    – 设计考虑因素》，是将特定规格的实例分派到专用的计算节点。例如，云操作员会使用*步骤 5* 中描述的调度器过滤器，只为中等大小的分配请求配置实例。为了使该策略生效，请在控制节点中运行以下命令行：
- en: '[PRE28]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The enabled **ComputeCapabilitiesFilter** filter should match any compute node
    attributes with a flavor, as defined against its usable amount of memory attribute
    for a specific instance flavor. If the available amount of RAM in the first compute
    node is below 250, a new instance request of the **m1.medium** size will likely
    be dispatched by the scheduler on the new compute node that has freshly joined
    the compute node farm.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启用的**ComputeCapabilitiesFilter**过滤器应匹配任何计算节点的属性与规格，依据其可用的内存属性为特定实例规格进行定义。如果第一个计算节点的可用内存低于
    250 GB，则调度器很可能会将**m1.medium**规格的新实例请求分派到刚刚加入计算节点集群的新的计算节点。
- en: By the end of the previous steps, a new compute node should join the OpenStack
    compute cluster with advanced scheduling configured based on filter and weighting
    mechanisms. Any new compute request with the **m1.medium** flavor will be launched
    in the new compute node if the amount of RAM available is at least 250 GB.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成前述步骤后，一个新的计算节点应加入 OpenStack 计算集群，并基于过滤和加权机制配置了高级调度。任何新的计算请求，只要**m1.medium**规格的内存至少为
    250 GB，就会在新的计算节点上启动。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at some powerful capabilities and additional features
    brought by the Nova service. You should be acquainted with the vast array of choices
    given by the latest updates in the compute services – from the variety of supported
    hypervisor technologies, to compute and workload segregation, to the art of filtering.
    Scheduling in OpenStack is a key element that cloud operators should pay special
    attention to when dealing with growing infrastructure and high demand for resources.
    Although we did not cover all possible scenarios, the filtering mechanism was
    introduced in the early days of the Nova project, and this chapter demonstrated
    that an extension to deploy compute power beyond a limited perimeter has since
    become possible. Thanks to the new, mature version of Nova CellV2, the host aggregation
    and Availability Zone concepts enable massive deployments if planned and designed
    properly, as we learned. At the end of the chapter, both Magnum and Zun were covered
    as popular container projects within the OpenStack ecosystem. We learned that
    cloud users can run native container applications driven by Magnum as a COE and
    Zun as a simple container API that’s well integrated with different core services
    around the OpenStack ecosystem. Furthermore, we learned that adopting microservice
    prototypes and running native cloud applications is no longer a challenge for
    an OpenStack user. With the continuous development of those services, enterprises
    can adopt a hybrid environment between private and public clouds not only due
    to the portability and flexibility of the containers but also because OpenStack
    has proven its capability to be in the container race with the other public cloud
    players. We discussed this in this chapter.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了Nova服务带来的一些强大功能和附加特性。你应该已经了解了计算服务中最新更新所提供的广泛选择——从支持的虚拟化技术种类，到计算和工作负载隔离，再到过滤的艺术。调度是OpenStack中的一个关键要素，云操作员在处理日益增长的基础设施和高资源需求时，应该特别关注这一点。尽管我们没有涵盖所有可能的场景，但过滤机制是在Nova项目的早期就被引入的，本章展示了超出有限范围部署计算能力的扩展功能如今已经成为可能。感谢新版本的成熟Nova
    CellV2，主机聚合和可用区概念使得大规模部署成为可能，前提是规划和设计得当，正如我们所学的那样。在本章的最后，我们讨论了作为OpenStack生态系统中流行容器项目的Magnum和Zun。我们了解到，云用户可以使用Magnum作为COE，运行原生容器应用程序，并且Zun作为与OpenStack生态系统周围不同核心服务高度集成的简单容器API。此外，我们还学到，采用微服务原型和运行原生云应用程序不再是OpenStack用户的挑战。随着这些服务的不断发展，企业可以在私有云和公有云之间采用混合环境，这不仅得益于容器的可移植性和灵活性，还因为OpenStack已经证明了它能够在容器竞赛中与其他公有云厂商竞争。我们在本章中讨论了这一点。
- en: Nova is not the only service that has seen some updates. Storage services including
    file share, block, and object storage have been empowered with additional features,
    which we will cover in the next chapter.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Nova并不是唯一一个经历更新的服务。包括文件共享、块存储和对象存储在内的存储服务也通过附加功能得到了增强，我们将在下一章中进行讲解。
