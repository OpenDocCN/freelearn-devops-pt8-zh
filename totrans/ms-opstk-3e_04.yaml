- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Compute – Compute Capacity and Flavors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Magic is believing in yourself. If you can do that, you can make anything happen.”
  prefs: []
  type: TYPE_NORMAL
- en: – Johann Wolfgang von Goethe
  prefs: []
  type: TYPE_NORMAL
- en: As a private cloud provider, typical OpenStack IaaS services are the building
    blocks of its offering to end users. At the heart of the IaaS offerings is the
    compute service that runs and manages the workload life cycle in different facets.
    The OpenStack Foundation has committed to enlarging the scope of the OpenStack
    ecosystem and exposing new services by tailoring the Nova service in different
    ways. Additional new trending projects such as managed elastic data processing
    (code-named **Sahara** ), managed databases (code-named **Trove** ), and a container
    application catalog (code-named **Murano** ) are examples of services that are
    offered on top of the Nova service.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a cloud operator, having a solid understanding of the compute farm capabilities
    and the different segregation methods within the OpenStack environment is a foundational
    requirement to run a successful private cloud journey with the desired service
    offerings. At the time of writing this edition, the Nova service had some updates,
    in addition to the emergence of more pertinent features and services around it.
    To ensure a safe compute design update based on the draft elaborated on in the
    first chapter, it will be necessary to run through the new compute service that
    came out with the latest OpenStack releases starting from the Antelope release.
    In this chapter, we will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the Nova compute service from an architecture perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking out the latest hypervisors supported by the compute service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the concept of compute segregation for massive OpenStack compute deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying the art of filtering to construct advanced ways of performing compute
    resource allocations and hosting workloads based on an array of strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the **Container Orchestration Engines** ( **COEs** ) and their integration
    with Magnum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to manage and operate containers in the most flexible way with
    the new, trending Zun OpenStack container service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring and deploying new compute nodes in a running OpenStack environment
    using Kolla-Ansible infrastructure as a code paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The compute service components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The compute service is composed of multiple components that take care of incoming
    compute requests and then launch and manage instances, as depicted in the following
    diagram showing the Nova architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The Nova component architecture](img/B21716_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The Nova component architecture
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore each compute component in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **nova-api** component deals with different (HTTP-based) requests via the
    queuing message service. Keep in mind that the API is the first interface that
    accepts compute requests before forwarding them to the next Nova component and
    establishing a complete workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next station in the Nova workflow abstraction layer is the **nova-scheduler**
    component. It is a major piece of the workflow that decides in which compute node
    each incoming instance creation request will run.
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler component uses a variety of filters and preferences that can be
    customized to match different needs and reasoning for the availability of the
    existing resources. **Filtering** and **weighting** are the main mechanisms used
    by the Nova scheduler in order to customize the allocation of underlying physical
    resources. This chapter will break down the newly added filters and weighting
    references in the Antelope and later releases.
  prefs: []
  type: TYPE_NORMAL
- en: Conductor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once a request is dealt with via the **nova-scheduler** process, the **nova-conductor**
    component will make sure that the reservation made in the physical host is saved
    in the shared database. The **nova-scheduler** process will be required to check
    the database beforehand and verify the available capacity before making a scheduling
    decision. The conductor component is a major addition to the Nova workflow compared
    to the earlier releases of the Nova project in the OpenStack ecosystem. As a brief
    reminder from the previous edition, the conductor comes with a security improvement,
    preventing the compute nodes from having direct access to the database. In earlier
    versions, the compute nodes would update their status directly in the database
    shared between them. That would reduce the blast radius of a possible security
    gap on the database level if one of the physical machines was compromised. Nowadays,
    the conductor takes on that responsibility on its own. Another great addition
    of the conductor component is dedicated operations dealing with the compute nodes,
    such as resizing, building, and migration.
  prefs: []
  type: TYPE_NORMAL
- en: By having those capabilities in **nova-conductor** , the compute nodes will
    act as simple worker nodes, leaving coordination and any database operations to
    the conductor process.
  prefs: []
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Coming to the end of the Nova workflow, the **nova-compute** component will
    take the rest of the requests by dealing with the hypervisor. The addition of
    a variety of features for different supported hypervisors in the OpenStack compute
    core makes almost every new release more exciting than the previous one. Libvirt
    KVM, VMware vSphere, XenServer, XenAPI, Microsoft HyperV, **Linux Containers**
    ( **LXC** ), Quick Emulator, and a few others are supported by the Nova service.
    The most exciting part is the continuous development and integration of each hypervisor
    feature by the vendors who own the hypervisor project. This chapter will illustrate
    more details of those hypervisors and highlight a few common architectural compute
    layouts.
  prefs: []
  type: TYPE_NORMAL
- en: Console access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Access to virtual machines through OpenStack Horizon (dashboard) or via the
    **Command-Line Interface** ( **CLI** ) is well developed by the OpenStack community
    in different iterations. Console access is not a required component to run a fully
    working Nova service but is an additional aspect to interact with provisioned
    guests. Until the *Liberty* release, **nova-console** , **nova-novncproxy** ,
    and **nova-consoleauth** were mainly counted as the most stable console types.
    Now, with the Antelope and later releases, more console types have joined the
    list, such as the SPICE console and serial console as an alternative graphical
    console extension. Microsoft has also contributed to the console service with
    **Remote Desktop Protocol** ( **RDP** ). However, the latter comes with a limitation
    as it supports only Hyper-V and requires integration with the console proxy in
    OpenStack, which has not been developed. Hence, a third-party tool should be available
    along with the console configuration, such as the FreeRDP-WebConnect application,
    which can be found on GitHub. Like Microsoft RDP, VMware has also contributed
    with another console type, **Mouse, Keyboard, Mouse** ( **MKM** ). This is bound
    to a virtual machine’s access console based on Vmware’s vSphere Hypervisor, but
    independently of an existing console proxy service in OpenStack. With this growing
    list in the latest releases, one console-supported service has been deprecated
    since the Train release, which is **nova-consoleauth** .
  prefs: []
  type: TYPE_NORMAL
- en: The associated Nova API services, including **nova-scheduler** , **nova-conductor**
    , and **nova-api** , run as part of the controller nodes, and the **nova-compute**
    service runs on each compute node. Running the Nova compute service is based on
    the hardware platform and the selected hypervisor, which will be covered in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning for the hypervisors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By navigating through the different OpenStack releases, Nova supports a range
    of **Virtual Machine Monitors** ( **VMMs** ), also known as hypervisors. Hypervisor
    software allows access to the physical machine hardware resources and provides
    different functions to create and manage virtual machines within a compute node.
    At the time of writing this book, the latest OpenStack releases including the
    Dalmatian release support **Kernel Virtual Machine** ( **KVM** ), **LXC** , **QEMU**
    , **VMware vSphere** (5.1.0 and later versions), **zVM** , **Ironic** (native
    OpenStack bare metal), **Hyper-V,** and **Virtuozzo** (7.0.0 and later versions).
    The different features and virtual machine management capabilities differ from
    one hypervisor to another. A full list of supported features per hypervisor can
    be found in the official hypervisor support matrix, which is available at [https://docs.openstack.org/nova/latest/user/support-matrix.html](https://docs.openstack.org/nova/latest/user/support-matrix.html)
    . Make sure to parse each of the provided features based on your requirements
    and needs. KVM is the most used hypervisor in the OpenStack **nova-compute** setup
    due to its early adoption and having the largest set of supported features.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Other famous hypervisors such as **Xen** , **XCP** , **UML** , and the **Docker**
    driver are not listed in the latest OpenStack releases, including Antelope and
    Bobcat (released in October 2023). The Docker driver has been supported since
    the Havana release and moved to its own project, code-named the **Zun** project,
    which will be covered later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Nova compute service uses **Virt Driver** to interact with the **Libvirt**
    library via API calls and manages virtual machines through the hypervisor, as
    illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – The Nova compute-supported hypervisors](img/B21716_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – The Nova compute-supported hypervisors
  prefs: []
  type: TYPE_NORMAL
- en: The configuration of the hypervisor backend in Nova can be performed by adjusting
    the **compute_driver** option in the **/** **etc/nova/nova.conf** file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The KVM implementation requires **nova-compute** to be deployed on a Linux
    server with KVM virtualization modules installed. For each compute node, make
    sure the KVM modules are properly loaded by checking the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Optionally, and depending on the supported KVM module (Intel or AMD) in your
    nodes, add the listed modules in the **/etc/modules** files to persist your changes
    at reboot.
  prefs: []
  type: TYPE_NORMAL
- en: The next section considers KVM as the main adopted hypervisor of our production
    compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Compute segregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack is designed to scale massively to respond to the growth of the infrastructure
    size and user demand. As the compute service is the workhorse of the cloud infrastructure,
    designing for compute layer scalability and business continuity is a must. Defining
    a compute segregation strategy will help you understand your private cloud infrastructure
    limits and forecast user compute demands – that’s what will be explored in the
    following section.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure segregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the available compute farm resources and inventory specifications,
    as well as the hypervisors to be used, OpenStack exposes a variety of compute
    segregation mechanisms that can be leveraged, including **Availability Zones**
    , **Regions** , **Cells** , and **Host Aggregates** . Each of the previous concepts
    will be detailed in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A region in the OpenStack glossary presents a fully independent OpenStack deployment.
    From operational and management perspectives, each region can expose its own OpenStack
    API services, including compute resources. If you’re planning a Nova multi-regional
    setup, a Nova API endpoint should be created in each region and added to the identity
    services catalog. Users can spawn instances and spread workloads between available
    regions through the available compute API. Running workloads in a multi-region
    configuration setup offers a multitude of advantages for end users, but also presents
    challenges for operators, such as maintaining a consistent setup of the OpenStack
    control plane and shared service layers. This is mainly due to the risk of having
    a **split brain** between compute resources running in more than one region. Hosting
    a user workload in a fleet of compute nodes running across multiple regions would
    require certain ways to provide consistency of shared resources such as virtual
    machine images, databases, or file shares. Another important aspect of multi-regional
    consistency is the way to handle authorization to request compute resources. One
    of the common design approaches is to consolidate all authentication calls through
    Keystone via a federated **Identity Provider** ( **IdP** ), as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – OpenStack multi-region deployment](img/B21716_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – OpenStack multi-region deployment
  prefs: []
  type: TYPE_NORMAL
- en: Each region can host one or more discrete compute structures defined as *Availability
    Zones* , which will be covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Availability Zones
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A region can be logically composed of one or multiple Availability Zones. Unlike
    the regional setup, there is no need to deploy a full, standalone OpenStack deployment
    and provide consistency across the control plane services. From a physical implementation
    standpoint, the compute nodes will be split into trees of logical groups called
    **fault domains** . Each fault domain will be part of a **Power Distribution Unit**
    ( **PDU** ) within a given rack and different hypervisors will run across different
    racks. This way, losing connectivity to the **Top-of-Rack** ( **ToR** ) switch
    or upon a PDU failure won’t fully bring down a user workload running in a multi-availability
    zone configuration, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Extended OpenStack multi-regional and zonal deployment](img/B21716_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Extended OpenStack multi-regional and zonal deployment
  prefs: []
  type: TYPE_NORMAL
- en: The Nova availability zone option empowers user workloads with high availability
    that can be considered a premium option for critical workloads that require the
    highest level of SLA. A compute node can belong to only one availability zone
    as it is bound to the rack’s physical design, such as rack ID, location, and PDU.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The logical abstraction of Availability Zones covers the network and block storage
    services besides the compute service.
  prefs: []
  type: TYPE_NORMAL
- en: Users can select which Availability Zone to launch instances in. For this reason,
    keeping an eye on the compute resource utilization for each Availability Zone
    is essential. That will ensure that each Availability Zone will be able to accommodate
    more compute allocation within the available physical resources when needed, as
    well as ensure that users can spawn workloads across multiple compute nodes in
    different zones.
  prefs: []
  type: TYPE_NORMAL
- en: Host aggregates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike the Availability Zones concept, the host aggregates strategy groups
    compute nodes based on more granular hardware profile categories defined as metadata.
    This type of grouping can be based on the definition of features such as hypervisor,
    storage type, CPU family, network speed, and so on. Cloud operators should create
    different host aggregates in advance by attaching a set of metadata to the selected
    compute nodes. This lets users spawn instances by selecting the desired host aggregate
    that responds to the workload requirements. Metadata can be created based on plenty
    of hardware specifications, depending on what the private cloud operator has purchased.
    For example, aggregation can be configured for an optimized disk host group, based
    on SSD, GPU architecture, high-performance networking capabilities, or even by
    aggregate compute nodes for a single tenant. A compute node can be part of one
    or many host aggregates. As a best practice, it is important to list all the different
    hardware capabilities once the compute nodes are in place and sort them into trees
    by grouping them based on the expected workload that they will host. For example,
    if HPC workloads are included in your cloud business case offering for end users,
    you might consider creating a host aggregate in advance based on GPU capabilities
    and exposing it to end users. The following diagram shows a use case for providing
    four host aggregates, where some compute nodes can be part of more than one host
    aggregate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Host aggregation in the deployment of multiple Availability
    Zones](img/B21716_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Host aggregation in the deployment of multiple Availability Zones
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous multi-region segregation compute layout involves four different
    host aggregates that can be spread across different Availability Zones as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Host aggregate 1** ( **HA_1** ): This involves grouping a set of compute
    nodes in the same region and Availability Zone to accommodate workloads that require
    low network-latency performance, such as HPC applications within a large network
    bandwidth and GPU architecture. Workloads in this host aggregate do not have high
    availability as a major exigency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Host aggregate 2** ( **HA_2** ): This aggregate involves grouping a set of
    compute nodes spread through two Availability Zones and within the same region
    to accommodate workloads that require large distributed workloads, such as big
    data processing running Hadoop or Cassandra. The host aggregate metadata includes
    large network bandwidth, optimized disks based on SSD, and memory capacity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Host aggregate 3** ( **HA_3** ): This involves grouping a set of compute
    nodes spread through three Availability Zones and across two regions to accommodate
    critical workloads that require the highest level of availability and scalability,
    such as web applications and webshops. The metadata describes moderate network
    bandwidth, disk, memory, and CPU standard capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Host aggregate 4** ( **HA_4** ): This involves grouping a set of compute
    nodes within the same region and Availability Zone to accommodate workloads that
    require enhanced networking capability within a VSphere environment. The host
    aggregate metadata includes VMware as a configured hypervisor to extend an existing
    VCenter environment and spread the application load between VMware hosts (running
    the **nova-compute** service).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will demonstrate later in this chapter how the combination of the host aggregation
    approach, Placement service, and Nova scheduling can expose a gazillion ways to
    host different types of workloads in the most efficient way to cloud users.
  prefs: []
  type: TYPE_NORMAL
- en: Cells
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concept of cells in OpenStack was introduced mainly to address the performance
    bottleneck of large-scale Nova deployment within a single OpenStack region. A
    large number of compute nodes would increase the load on the main control plane
    services, including the messaging queue and database, resulting in performance
    degradation that would limit the expansion of the entire system. The mitigation
    of performance degradation for large-scale deployment for Nova compute resources
    can be addressed by using the cell approach: compute nodes can be grouped within
    a logical boundary, where each group runs its own database and messaging queue
    service. The cell architecture has been developed through different stages, resulting
    in two versions: **CellV1** and **CellV2** . CellV1 was the first version and
    was not widely implemented due to a number of caveats. Since the OpenStack Ocata
    release, CellV2 has been introduced, with enhancements to the former version,
    and has become the official version within the **Pike** release. Let’s discuss
    the major differences between the versions next.'
  prefs: []
  type: TYPE_NORMAL
- en: CellV1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The logical grouping of compute nodes forming each cell is arranged in a tree
    structure where the top-level cell called the **root** runs the **nova-api** service.
    The second level of cells, which run the Nova compute service, are referred to
    as child cells, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – OpenStack compute CellV1 deployment](img/B21716_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – OpenStack compute CellV1 deployment
  prefs: []
  type: TYPE_NORMAL
- en: The interaction between the cells running the Nova API and running the Nova
    compute goes through a dedicated messaging queue via RPC calls handled by a separate
    Nova service called **nova-cell** (a major enhancement of CellV1 introduced in
    the Juno release). Each cell runs its own messaging queue for interactive synchronization
    and is used by **nova-cell** for selecting in which cell an instance will be spawned.
    This exposes a higher level of instance scheduling but within the cell’s level.
    Once a cell is selected, the **nova-scheduler** service within that cell will
    process the rest of the request workflow to select a compute node.
  prefs: []
  type: TYPE_NORMAL
- en: CellV2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The initial cell version went through experimental phases and was not successfully
    adopted in most large compute deployments. This was mainly due to the multi-layer
    scheduling to spawn instances from the cell selection down to the compute scheduler
    service within the selected cell. *Figure 4* *.6* illustrates a simple tree composed
    of only root and child cells, but other, wider deployments could adopt more child
    cells, called **grandchild** cells. The other major shortcoming of the first version
    is the amount of data (instance and compute node information) to be mapped and
    synchronized between cells, resulting in heavy data replication processes between
    the cells. The new version, referred to as CellV2, has been developed and adopted
    officially from the Ocata release by addressing the caveats of the prior version.
    Major changes have been introduced, such as the removal of the **nova-cell** component
    and the overall cell architecture, which is no longer a tree-like structure similar
    to CellV1. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – OpenStack compute CellV2 deployment](img/B21716_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – OpenStack compute CellV2 deployment
  prefs: []
  type: TYPE_NORMAL
- en: Sharding hypervisor nodes through the second version of cells in Nova involves
    major changes in the scheduling workflow within the multi-cell deployment, as
    illustrated in the previous diagram. A cell API runs the **nova-api** service,
    **nova-scheduler** , and more recently, the Placement service (the Placement service
    was not introduced in the OpenStack ecosystem in the Cellv1 development) to uniformly
    schedule resources across all defined cells. The cell API exposes a separate database
    called **nova_api** , which contains metadata of all global resources, such as
    instance flavors, quotas, keypairs, and so on. In multi-cell deployment, the conductor
    piece in the API cell is called **nova-super-conductor** , which deals with all
    database insulation. Additionally, from the API level, a special cell called **Cell0**
    does not run any of the services but holds only the **nova_cell0** database containing
    information about failed instances to start due to a scheduler failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each cell grouping the compute nodes manages its own resources and they have
    no affiliation with each other. Each cell has its own database storing information
    about instances and uses its own dedicated messaging queue to coordinate between
    the local cell’s **nova-conductor** and **nova-compute** services. The new CellV2
    method simply involves the following workflow steps to spawn a new instance in
    a multi-cell deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**nova-api** (in the API cell) receives the API REST call and forwards it to
    the **nova-scheduler** service (in the API cell).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**nova-scheduler** contacts the Placement service, applies its filters, and
    determines which compute node will be assigned.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**nova-api** stores the mapping instance information in its **nova_api** database
    (the **instance_mappings** table).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**nova-api** stores the selected compute node record in the database target
    cell (for example, **nova_cell01** if the selected compute node resides in **Cell01**
    ).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**nova-api** sends an RPC call to **nova-super-conductor** to build the instance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**nova-super-conductor** initiates an RPC call to **nova-conductor** in the
    target cell to spawn the instance via the compute service.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CellV2 architecture has shifted drastically from the initial CellV1 architecture
    with the removal of an additional scheduling layer and the division of the data
    into multiple databases instead of carrying a replication process for each request.
    This way, expanding a cell perimeter to welcome more compute hosts can be easily
    performed without the need to go through the whole synchronization process per
    cell, as we have seen in CellV1. Moreover, the new cell architecture is simple
    as all cells are equal, making the whole multi-cell deployment a flat-like structure
    (only two layers), unlike its previous version, which was based on the tree structure.
    Once the logical layout of cells is designed and ready for deployment, operators
    will just need to choose which hypervisor nodes will be associated with which
    cell. Unlike host aggregation, regions, and Availability Zones, users are not
    aware of the concept of a cell, as the cell strategy is mainly for cloud operators
    to expand large compute environments with the best management and performance
    enhancements.
  prefs: []
  type: TYPE_NORMAL
- en: Workload segregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The latest features of the Nova service provide different strategies to unlock
    a fully scalable OpenStack compute infrastructure by means of regions, Availability
    Zones, host aggregates, and cell concepts. To have a full picture of a large deployment,
    adopting one or more of those approaches would require the definition of an orchestrator
    element to dispatch instances in a particular set of hypervisor hosts. At the
    heart of the launch process of instances is the **nova-scheduler** service, which
    determines on which host an instance will be placed. As highlighted in *Chapters
    1* and *3* , the latest OpenStack releases, come with the Placement service, which
    was introduced in the Newton release. The Placement service works in tandem with
    the Nova scheduler for a more advanced and granular host pre-filtering selection
    process. To make the most of the segregation approaches defined in the previous
    section, it is essential to skim the surface of different compute scheduling methods
    that are mindful of the overall compute segregation strategy in your OpenStack
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main motivation for having a dedicated Placement service that runs alongside
    the Nova scheduler is to resolve the dilemma of a large compute deployment. Prior
    to the introduction of the Placement service, the Nova scheduler had to loop through
    all the compute farms, including the enabled filters, in an entire large deployment,
    resulting in inefficiency and performance issues. Additionally, the sum of resources
    reported by the compute nodes lacked accuracy due to the basic functions of Nova
    in calculating, in a detailed fashion, the exact number of resources and inventory
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of the Placement service, the logic of resource reporting has
    been changed by adding a **resource_tracker** component in a compute node that
    reports its inventory and available resources to the Placement service, which
    periodically stores a full synchronization of reported resources. As detailed
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* , the Placement service comes with the definition of **resource
    providers** and **traits** to apply prefilters to its compute node list once contacted
    by **nova-scheduler** . Under the hood, **nova-scheduler** calls the **placement-api**
    service via a **GET API** request by passing different scheduling parameters such
    as **VCPU** , **DISK_GB** , **MEMORY_MB** , and **VGPU** . These will be translated
    into **trait** requests at the **placement-api** level.
  prefs: []
  type: TYPE_NORMAL
- en: Resource provider allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Placement service, as reviewed in *Chapters 1* and *3* , is used by **nova-scheduler**
    to list a set of pre-filtered compute nodes based on specific attributes before
    evaluating their candidacy, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – The OpenStack compute placement workflow](img/B21716_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The OpenStack compute placement workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'When the **nova-api** service receives a user request for a new instance creation,
    **nova-scheduler** will fire a **GET /allocation_candidates()** request to the
    **placement-api** service. The **allocation_candidates** method is a collection
    of a set of resource providers that will allocate resources based on the attributes
    encapsulated in the request. A sample API GET request uses the following format
    as a filter condition to embed the required resources to launch an instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon the previous request, the **placement-api** service will fetch the complying
    hosts that will accommodate instance flavors: disk capacity of 500 GB, memory
    size of 2048 MB, and 4 vCPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous request can be extended to filter based on resource traits by
    adding the **&required** query parameter at the end of the request string, so
    the allocation query will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This way, the **placement-api** service will combine the required resource attributes
    and an additional trait for hosts that support a CPU with X86_SVM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, cloud operators can use the OpenStack CLI to deal with placement
    configuration via the **allocation candidate** command line, as in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – The placement details of a registered allocation](img/B21716_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – The placement details of a registered allocation
  prefs: []
  type: TYPE_NORMAL
- en: 'The main parameters that influence the logic of the subsequent scheduler pre-filtering
    process lie in **allocation_requests** and **provider_summaries** . The previous
    objects are returned by the **placement-api** service to the **nova-scheduler**
    service with two types of objects listed in JSON format with the following data
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**allocation_requests** : A list of resource providers that can satisfy the
    allocation request:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**provider_summaries** : The total amount of resources and information usage
    across all the resource providers that satisfy the allocation request:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The JSON **provider_summaries** object output includes the associated traits
    with a resource provider. The previous output example shows the resource provider
    for the **52f6729-cd55-4747-91a1-543adfeea2a** compute node UUID, labeled with
    a trait class supporting the **HW_CPU_X86_SVM** and **HW_CPU_X86_SSE2** CPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the **placement-api** service returns the candidate host list to the **nova-scheduler**
    service, the latter process runs a variety of filters based on the pre-configured
    filters in the Nova service. The **/etc/nova/nova.conf** file can be configured
    to run one or several schedulers by adjusting the **enabled_filters** option.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler filters in the **kolla-ansible** repository can be configured
    in the **filter_scheduler** section, located in the **/** **kolla-ansible/ansible/roles/nova/templates/nova.conf.j2**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most common filters configured and supported by default in the
    scheduler are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ComputeFilter** : This returns a list of fully operational hypervisor hosts
    that should be enabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ComputeCapabilitiesFilter** : This returns a list of hypervisor hosts that
    are capable of spawning an instance with the requested flavor. For example, extra
    specifications could include verifying a compute host running a KVM hypervisor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ImagePropertiesFilter** : This returns a list of hypervisor hosts that meet
    the desired image properties defined in the instance image. Image properties can
    be added based on the architecture of the hardware or hypervisor type. For example,
    spawning an instance would require an image that should run on a host supporting
    KVM as the hypervisor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: One of the major updates from the Liberty release is the image properties being
    passed to the filter for checks. The new supported properties are **hw_architecture**
    (formerly named **architecture** ), which defines the hardware architecture, **img_hv_type**
    (formerly named **hypervisor_type** ), which describes the type of the hypervisor,
    **img_hv_requested_version** (formerly named **hypervisor_version_requires** ),
    which defines the required version of the hypervisor, and **hw_vm_mode** (formerly
    named **vm_mode** ), which refers to the hypervisor application binary interface.
  prefs: []
  type: TYPE_NORMAL
- en: '**ServerGroupAntiAffinityFilter** : This dispatches instances of the same group
    in different hypervisor hosts. A common use case is to launch instances running
    the same workload and requiring an additional level of high availability. To create
    server groups using an anti-affinity policy, use the **--hint** flag in the Nova
    command-line tool as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The outcome of the previous command lines will ensure that **instance 01** and
    **instance 02** are part of the same application server group, **pp_webgroup**
    , have the **122ee342-3345-2234-bac4-1515321e1ebb** UUID, and run in different
    compute nodes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ServerGroupAffinityFilter** : This dispatches instances in the same hypervisor
    host already running an affinity server group. The affinity scheduler filter is
    recommended to run workloads that require the highest level of performance and
    low latency. To create server groups using an affinity policy, use the **--hint**
    flag in the Nova command-line tool, as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: The outcome of the previous command lines will ensure that **instance 01** and
    **instance 02** , part of the same application server group, **pp_perfgroup**
    , having the **144ee217-5543-9871-cda3-1e57522ecde1** UUID, will be running in
    the same compute node.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: OpenStack scheduling supports other filters, with more granular and customized
    options included in the latest releases of OpenStack. At the time of writing this
    edition, an exhaustive list of the latest filters and configurations can be found
    at [https://docs.openstack.org/nova/latest/admin/scheduling.html#filters](https://docs.openstack.org/nova/latest/admin/scheduling.html#filters)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Weighting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weighting is the last stage of the scheduling process, wherein the **nova-scheduler**
    service applies scoring for each filtered candidate host selected from the placement
    that passed all assigned filters. The compute node with the highest weight will
    be selected by the scheduler at the final stage. On the surface, the weighting
    logic might look less complex, but under the hood, additional factors should be
    considered. The compute node metrics are periodically monitored and fed to the
    database to keep track of the host usage and resource claims. The weighting process
    uses such information as factors and calculates the weights using a normalization
    function by giving each compute node a value between **0.0** and **1.0** . Compute
    nodes with the most available resources (as reported by the monitoring) will be
    assigned a maximum value of **1** and compute nodes with the lowest available
    resources will have a minimum value of **0** .
  prefs: []
  type: TYPE_NORMAL
- en: 'The normalized factor is calculated based on the available resources using
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '**(host_availability_resource - min_value_all)/(max_value_all –** **min_value_all)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**host_availability_resource** is the value of the available resource in a
    compute node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_value_all** is the minimum value of resources across all compute nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_value_all** is the maximum value of resources across all compute nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second step of the function is to multiply the normalized value of each
    compute node for each available resource by the associated weigher multiplier.
    At the end of the weighting process, the scheduler will sum all normalized weights
    and select the compute node with the highest score, summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Σ** **(weight(i)*norm_factor)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**weight(i)** is the associated weight multiplier for a resource **i**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**norm_factor** is the assigned normalized weight factor for each compute node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example illustrates the weighting process by calculating the
    normalized factors and weights for available RAM capacity and CPUs across five
    compute nodes, respectively. The diagram illustrates the first subset size of
    nodes ( **subset 1** ) – the first compiled list of evaluated hosts by the Nova
    scheduler through the filters – as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – The OpenStack compute weighting mechanism](img/B21716_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The OpenStack compute weighting mechanism
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheduler then runs the weighting mechanism and generates the most suitable
    hypervisor hosts based on the weight scoring as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Nodes A and D accommodate the highest RAM capacity and are assigned the maximum
    weight, normalized to **1** . The weight cost is denoted as **100** (1 multiplied
    by 100).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes B and C have the lowest amount of RAM capacity and are assigned the minimum
    weight normalized to **0** . The weight cost is denoted as **0** (0 multiplied
    by 100).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Node E is assigned a weight normalized to 0.4 by applying the previous formula
    to calculate the normalized factor as follows: (350 - 250) / (500 – 250). The
    weight cost is denoted as 40 (0.4 multiplied by 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes A and D accommodate the highest number of CPUs and are assigned the maximum
    weight, normalized to **1** . The weight cost is denoted as **100** (1 multiplied
    by 100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node C has the lowest amount of CPU capacity and is assigned the minimum weight,
    normalized to **0** . The weight cost is denoted as **0** (0 multiplied by 100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nodes B and E are assigned a weight normalized to 0.5 by applying the previous
    formula as follows: (10 - 5) / (20 – 10). The weight cost is denoted as 50 (0.5
    multiplied by 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final weighting for each host is calculated by summing up each weight cost
    of each CPU and RAM resource as follows: host A (100 + 100), host B (0 + 50),
    host C (0 + 0), host D (100 + 100), and host E (40 + 50)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, both hosts A and D won the weighting bid and the scheduler
    will simply randomly select one of them from the second node’s subset to launch
    the instance. Resource availability will be reflected in the database and the
    scheduler will update the weights accordingly on each new instance creation request.
  prefs: []
  type: TYPE_NORMAL
- en: The additional options brought to the filtering and scheduling mechanisms in
    the compute service can be handy in managing and distributing the workload demand
    across the compute farm. By default, the scheduling allocates resources for instances
    by spreading them across the available hosts. In some other cases, dispatching
    several instances with different flavors and sizes would create a concurrency
    issue when allocating resources, and hence prevent a subset of instances from
    being launched. The scheduling weighting policy can be adjusted to use a **stacking**
    approach. In this case, instances will be allocated to the first filtered host
    until all of its resources are fully exhausted, before moving on to the next one.
    A good example of a stacking method is the conjunction of the server affinity
    configuration and weighting mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'To customize the weighting mechanism in your deployment, make sure to apply
    the following configuration in the **/** **etc/nova/nova.conf** file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable monitoring on the compute nodes to gather the CPU metrics by setting
    the **compute_monitors** parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enable the **weights.all_weighers** class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the host subset size to **1** . Adjust the scheduler to select the host
    with the highest weight by setting the host subset size to **1** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Adjust the weight multiplier to **1** . This way, the launch of a new instance
    will be spread evenly between compute nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Adjust the RAM weight multiplier to **1** . This way, the launch of a new instance
    will be spread evenly between compute nodes with free memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a weighted multiple as **1.0** will spread the resource allocation
    across all available filter hosts. If the stacking option is the preferred option,
    set the associate value of the weight multiplier to a value of **-1** .
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a new multiplier to calculate the weighting host I/O operations. Instruct
    Nova to use the active hosts evenly with the least I/O CPU metric by setting its
    weight multiplier to **1.0** as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Restart the **nova-scheduler** and **nova-compute** services in the controller
    node and each compute node to apply the changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The **kolla-ansible** repository does not, by default, provide a low-level
    configuration for Nova weighting policies. It is possible to adjust the **nova.conf**
    template file located at **/kolla-ansible/ansible/roles/nova/templates/nova.conf.j2**
    by adding the configuration lines described previously. It is also recommended
    to cast an eye on the latest supported **nova.conf** settings, as defined here
    for the latest release: [https://docs.openstack.org/nova/latest/configuration/sample-config.html](https://docs.openstack.org/nova/latest/configuration/sample-config.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenStack Bobcat release introduces a new scheduler weigher, **NumInstancesWeigher**
    , which is handy for dispatching instances based on the number of active instances
    each compute node is hosting. As demonstrated in the previous example, setting
    the multiplier value to **1** will enable a **packing** strategy by favoring the
    host with the most number of active instances. A value of **-1** will favor a
    spreading strategy by checking the least busy host in terms of active instances.
    A full list of different weighers can be found at [https://docs.openstack.org/nova/latest/admin/scheduling.html#weights](https://docs.openstack.org/nova/latest/admin/scheduling.html#weights)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Extending compute for containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since 2014, the OpenStack community has been developing additional projects
    around containerization technology, allowing cloud users to leverage different
    sets of services to run applications on top of the compute service. The most mature
    and in-production services are **Magnum** and **Zun** , which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Magnum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main motivation to run a containerized environment is to run and deploy
    an application with minimal configuration complexity that’s portable and requires
    the least maintenance effort. Running workloads that are composed of many containers
    requires a layer of orchestration to manage the logic of the life cycle of different
    components, as well as inter-communication, in an automated fashion. This is referred
    to as a container cluster management platform, such as **Mesos** , **Kubernetes**
    , and **Docker Swarm** , to name a few. There are some well-known public cloud
    services, such as AWS **Elastic Container Service** ( **ECS** ), AWS **Elastic
    Kubernetes Service** ( **EKS** ), GCP **Google Kubernetes Engine** ( **GKE** ),
    and **Azure Kubernetes Service** ( **AKS** ). In the OpenStack world, Magnum is
    the project code name that has been integrated into the OpenStack ecosystem and
    is referred to as a COE. Like any other OpenStack service, Magnum exposes an API
    to interact with COEs as first-class resources, such as Kubernetes and Docker
    Swarm. Containers will be deployed and run on top of the hypervisor nodes. The
    exposure to different container engines unlocks cloud operators to offer an array
    of choices for users to select from, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – The OpenStack Magnum multi-COE architecture](img/B21716_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – The OpenStack Magnum multi-COE architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Magnum refers to each collection of COE nodes and their associated containers
    using the following terminologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bay** : This is a set of nodes running a COE. At the heart of the bay run,
    Magnum calls the Heat service to deploy the bay. Bays are simply a collection
    of Nova instances created and orchestrated by Heat using images that target both
    virtual machines and bare-metal ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BayModel** : This is a collection of resources that constructs a bay defined
    in a simple template. A bay can use the same **BayModel** template to run across
    different COEs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod** : This is a group of containers running on the same COE node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service** : This is an abstraction of an arrangement of bays and access policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication controller** : This is a dedicated process for monitoring, replicating,
    re-spawning failed containers, and scaling pods across COE nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Magnum client** : This is a native client to query the COE that comes with
    the native **Docker CLI** for the Docker COE and **kubectl** for the Kubernetes
    COE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other emerging keys to Magnum’s success are the functions that are well integrated
    with other OpenStack services, which are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identity** : This is an integration with Keystone for managing the authentication
    and authorization of the container cluster platform – for example, creating Kubernetes
    roles for users to run operational tasks on a cluster via Keystone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Networking** : The Neutron service is leveraged through supported networking
    drivers – for example, the **Flannel** overlay network is the default one for
    both the Kubernetes and Swarm COEs, allowing multi-host communication and managing
    the IP addresses of containers in each bay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imaging** : A prebuilt Glance image is provided to the Kubernetes and Swarm
    COEs, which can be configured and used to launch the nodes in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage** : The official supported storage is provided by Cinder as block
    storage in two forms: ephemeral and persistent storage options. Aside from the
    Docker storage driver, Cinder also comes with additional backend storage such
    as **Rexray** as the volume driver. At the time of writing this, Cinder only supports
    the Rexray driver for Swarm and the default Cinder for Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other main highlights of Magnum that have been initiated or are in progress
    can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security posture** : The Magnum architecture is secure by design. Each bay
    is isolated from others, providing security for the workloads they host. Additionally,
    Magnum enables a multi-tenancy layout by preventing bays from being shared between
    OpenStack tenants.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability** : The scaling of the cluster nodes is performed automatically
    by adjusting the Heat template attributes or manually via the OpenStack COE CLI.
    Note that container scaling will depend on the cluster container configuration
    as per the COE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability** : At the time of writing this, the Magnum high availability
    criteria supports only one default Availability Zone. The group of compute nodes
    will be spawned in the same logical Availability Zone. The OpenStack community
    is considering enlarging the span of Magnum cluster nodes across more multi-Availability
    Zones in the next releases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the Liberty release, the Magnum service has received a lot of credit due
    to its exposure to an array of COEs and maturity, which can be seen in dozens
    of production workloads. The service has enabled engineering and architect teams
    to migrate to microservice-based architecture without the need to redesign and
    re-integrate the existing container toolsets. Other enhancements have been captured
    and can be followed by checking the release updates at [https://docs.openstack.org/magnum/latest/](https://docs.openstack.org/magnum/latest/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'The Magnum service is not enabled by default in the current **kolla-ansible**
    infrastructure code repository. Setting **enable_magnum** to **yes** in the **/kolla-ansible/etc/kolla/globals.yml**
    file will enable the Magnum API service. Make sure to update the inventory file
    created in the previous chapter, **/ansible/inventory/multi_packtpub** , by adding
    the Magnum services, including the **magnum-api** and **magnum-conductor** services,
    in the controller node as in the following configuration stanza code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More Magnum configuration options are listed in the Ansible playbook, under
    the **/kolla-ansible/ansible/roles/magnum/default/main.yml** file. Additionally,
    Horizon supports the Magnum panel in the dashboard, which is disabled by default
    and can be added by setting **enable_horizon_magnum** to **yes** in the **/**
    **kolla-ansible/etc/kolla/** **globals** **.yml** file.
  prefs: []
  type: TYPE_NORMAL
- en: Once both files are updated, run the CI/CD pipeline to roll out the Magnum Ansible
    playbooks and provide the Magnum service containers as part of the control plane
    in the OpenStack environment.
  prefs: []
  type: TYPE_NORMAL
- en: Zun
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beyond the Magnum service and its multitude of container orchestration support
    options, there is the Zun service – a very simple service to run containers quickly
    in OpenStack without the need to manage or deploy a container orchestration environment
    based on Kubernetes Pods or Docker Swarm clusters. You can simply fire a Zun command
    line to create any type of container for any Docker repository and run your workload.
    To resolve any confusion between Magnum and Zun use cases, the latter is dedicated
    to managing and operating containers through OpenStack directly without the need
    to interface with an additional orchestration toolset layer of a COE such as Kubernetes
    or Swarm, unlike the Magnum service. Another potential source of confusion is
    the difference between Zun and the former OpenStack container originator, the
    **nova-docker** service. While **nova-docker** relies on the Nova API service
    to manage containers, Zun is independent of the Nova API and leverages its fully
    featured API interface to create and operate containers. The Zun service is well
    integrated with a number of versatile OpenStack services, including Nova, Glance,
    Keystone, Neutron, Cinder, Ceilometer, and Magnum, as illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – The OpenStack Zun compute integration](img/B21716_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – The OpenStack Zun compute integration
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4* *.12* exposes two additional services that Zun compute uses and
    runs in tandem with Neutron and Cinder, respectively summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kuryr** : This integrates with Neutron to provide advanced networking features
    for containers through **libnetwork** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fuxi** : This leverages the Docker API to manage volumes backed by Cinder
    and Manila storage services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Zun service is not enabled by default in the current **kolla-ansible**
    infrastructure code repository. To enable the service, the following settings
    should be adjusted in the **/kolla-ansible/etc/kolla/globals.yml** file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure to update the inventory file created in the previous chapter, **/ansible/inventory/
    multi_packtpub** , by adding the Zun services, including **zun-api** and optionally
    **zun-wsproxy** , as part of the control plane as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The **zun-compute** and optionally **zun-cni-daemon** services can be added
    as part of the compute node as depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More Zun configuration options are listed in the Ansible playbook under the
    **/kolla-ansible/ansible/roles/zun/default/main.yml** file. Additionally, Horizon
    supports the Magnum panel in the dashboard, which is disabled by default and can
    be added by setting **enable_horizon_zun** to **yes** in the **/** **kolla-ansible/etc/kolla/globals.yml**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Once both files are updated, run the CI/CD pipeline to roll out the Magnum Ansible
    playbooks. The Zun service and its associated processes will be launched in containers
    as part of the control plane in the OpenStack environment.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to double-check that the **etcd** daemon is running on the controller
    node. Similarly, ensure that the Docker runtime and **kuryr-libnetwork** are properly
    installed in the compute node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the Zun service can be performed through the Zun service list command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Zun containers should be created in the compute nodes. To list the registered
    compute nodes for Zun, use the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating containers using the Zun CLI is straightforward. It is possible to
    use, by default, **Docker Hub** to pull images and rapidly run a container in
    the designated compute node. The following example command line creates a Zun
    container using a CirrOS image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: An amazing hands-on feature brought by Zun is the flexibility to manage a container’s
    life cycle by using the native Docker API or the Zun API itself. For example,
    attaching a terminal to a running container can be performed by running **openstack
    appcontainer attach <container_name>** in a similar way to firing a **docker attach**
    command line.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the compute farm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Managing the compute fleet hypervisors through **kolla-ansible** to spawn instances
    or containers is straightforward. After collecting your compute requirements,
    hardware capacity, types of workloads, and scheduling strategies, adding a compute
    node can be performed on the fly without incurring a complex operation overhead.
    The main requirements that should be considered are the listing of compute services
    – mainly, **nova-compute** and a networking plugin agent to provide connectivity
    with the control plane and sync with **nova-scheduler** for each resource allocation.
    The previous section demonstrated that additional components can be part of the
    compute node, including the Zun compute service, if you’re planning to offer an
    easy and quick container playground to end users. Another aspect of managing compute
    nodes from code through **kolla-ansible** is listing the available configuration
    options of the compute service, such as overcommitment ratios for CPU and memory,
    covered in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack
    – Design Considerations* . As you might deploy several compute nodes with different
    configurations, **kolla-ansible** provides ways to customize compute service configuration
    and override configuration with the global configuration defined in the **/etc/kolla/globals.yml**
    file. The following example will instruct Kolla to add a new compute host named
    **cn02.os.packtpub** (24 CPUs and 255 GB of RAM) with custom configuration supporting
    QEMU as a hypervisor, and a CPU and RAM overcommitment ratio of **8.0** and **4.0**
    , respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s start by populating the hostname in the inventory file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a new **nova.conf** file under **/etc/kolla/config/nova/cn02.os.packtpub/**
    . Kolla looks for any possible config file under **/etc/kolla/** . If one is found,
    Kolla will override and merge the global one with the custom settings provided
    in a separate file. Note that you will need to create the path directories in
    the **kolla-ansible** repository, including the hostname of the compute node,
    following the **/etc/kolla/config/nova/COMPUTE_HOSTNAME/nova.conf** format, wherein
    **COMPUTE_HOSTNAME** is the hostname of the compute node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the newly created **nova.conf** file by adding the CPU and RAM allocation
    ratio values of **8.0** and **4.0** , respectively, in a new section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add to the same file the desired **qemu** hypervisor to run on the new compute
    node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is possible to overwrite the default scheduler by forcing a set of filters
    and using the **ComputeCapabilitiesFilter** filter to use any host with total
    available RAM for users of more than 250,000 MB:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Optionally, you can use the filtering strategy for RAM criteria by stacking
    instances in the same host with a defined flavor. That way, you can make sure
    that the compute node will exactly accommodate a defined number of instances with
    specific flavors that you can predict in advance. By doing so, the 250 GB of RAM
    can be used fully without leaving smaller slots that would otherwise lay unused.
    The way to go in this case is the stacking strategy, by defining a negative weight
    multiplier in the same **custom nova.conf** file as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using your CI/CD pipeline, trigger the run job to deploy the code in the staging
    environment, as demonstrated in the previous chapters, before merging to the production
    branch. The deployment stage should provision the new additional Kolla containers
    in the new compute node and start syncing with the control plane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An example of resource allocation inspired by the filtering strategy covered
    in this chapter and [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting
    OpenStack – Design Considerations* is to dispatch a specific flavor of instances
    to a dedicated compute node. For example, a cloud operator would use the scheduler
    filter described in *step 5* to provision instances only with medium-sized allocation
    requests. For that to take effect, run the following command line in the controller
    node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The enabled **ComputeCapabilitiesFilter** filter should match any compute node
    attributes with a flavor, as defined against its usable amount of memory attribute
    for a specific instance flavor. If the available amount of RAM in the first compute
    node is below 250, a new instance request of the **m1.medium** size will likely
    be dispatched by the scheduler on the new compute node that has freshly joined
    the compute node farm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By the end of the previous steps, a new compute node should join the OpenStack
    compute cluster with advanced scheduling configured based on filter and weighting
    mechanisms. Any new compute request with the **m1.medium** flavor will be launched
    in the new compute node if the amount of RAM available is at least 250 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at some powerful capabilities and additional features
    brought by the Nova service. You should be acquainted with the vast array of choices
    given by the latest updates in the compute services – from the variety of supported
    hypervisor technologies, to compute and workload segregation, to the art of filtering.
    Scheduling in OpenStack is a key element that cloud operators should pay special
    attention to when dealing with growing infrastructure and high demand for resources.
    Although we did not cover all possible scenarios, the filtering mechanism was
    introduced in the early days of the Nova project, and this chapter demonstrated
    that an extension to deploy compute power beyond a limited perimeter has since
    become possible. Thanks to the new, mature version of Nova CellV2, the host aggregation
    and Availability Zone concepts enable massive deployments if planned and designed
    properly, as we learned. At the end of the chapter, both Magnum and Zun were covered
    as popular container projects within the OpenStack ecosystem. We learned that
    cloud users can run native container applications driven by Magnum as a COE and
    Zun as a simple container API that’s well integrated with different core services
    around the OpenStack ecosystem. Furthermore, we learned that adopting microservice
    prototypes and running native cloud applications is no longer a challenge for
    an OpenStack user. With the continuous development of those services, enterprises
    can adopt a hybrid environment between private and public clouds not only due
    to the portability and flexibility of the containers but also because OpenStack
    has proven its capability to be in the container race with the other public cloud
    players. We discussed this in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Nova is not the only service that has seen some updates. Storage services including
    file share, block, and object storage have been empowered with additional features,
    which we will cover in the next chapter.
  prefs: []
  type: TYPE_NORMAL
