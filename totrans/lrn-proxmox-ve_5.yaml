- en: Chapter 5. Working with Virtual Disks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 4](ch04.html "Chapter 4. Creating Virtual Machines"), *Creating a
    Virtual Machine*, introduced the creation of virtual machines using the Proxmox
    VE management interface as well as the command line. After an outline of common
    steps in the procedure, we quickly glossed over creating two virtual machines
    with two network operating systems: **Windows Server 2012r2** and **Fedora 23
    Server**.'
  prefs: []
  type: TYPE_NORMAL
- en: Our most fundamental goal with this chapter is to empower ourselves to make
    more informed and fully-deliberated decisions affecting the efficiency and reliability
    of a Proxmox VE virtual machine guest based on its specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will accomplish this goal by achieving the following concrete objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose deliberately from among virtual disk image formats available for use
    through the Proxmox VE interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose an appropriate bus/interface by which a virtual disk will connect to
    a guest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose an appropriate cache setting for our use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, then, we elaborate on one critical, and potentially the most
    valuable, virtual machine component: the **virtual disk** that provides **secondary
    storage**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then revisit the virtual machine creation process to elaborate on specific
    options glossed over in [Chapter 4](ch04.html "Chapter 4. Creating Virtual Machines"), *Creating
    a Virtual Machine*:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a virtual disk format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing an interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting a cache option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding virtual disks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses first on the terms we should be familiar with to build
    on our understanding of virtual disks.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we''ve agreed on terms, we''ll explore virtual disk configuration options
    that we saw in [Chapter 4](ch04.html "Chapter 4. Creating Virtual Machines"), *Creating
    a Virtual Machine*, but did not explore: virtual disk image formats,bus/interface
    options, and disk **cache** options.'
  prefs: []
  type: TYPE_NORMAL
- en: Coming to terms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Secondary storage is as integral to virtual machines as it is to physical computers.
    While **hard disk drives** (**HDDs**) are hardly the most expensive hardware in
    PCs, we could compellingly argue that they are the most valuable, so far as we
    rely on them to store and provide access to our data, often the unique fruit of
    our hard labor.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, the term Virtual Disk will refer to a file or set of
    files that, to a virtual machine, represent a hard disk drive and behave just
    as a physical hard disk drive or **solid-state drive** (**SSD**) does for a physical
    computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The similarities between a physical hard disk drive and a virtual disk are,
    as you might expect, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identical file system options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same strong understanding of partitions and partition tables is required
    of administrators with fluency in using the same partition editing tools we'd
    use on physical machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identical formatting procedures and options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same support for LVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the striking differences must be made explicit. A physical hard disk
    drive unit includes more than just the media that stores data.
  prefs: []
  type: TYPE_NORMAL
- en: The media component of HDDs is constituted by a stack of double-sided, physical
    platters rotating around a common spindle within a vacuum-sealed structure.
  prefs: []
  type: TYPE_NORMAL
- en: Also within the vacuum seal is the physical, mechanical apparatus that reads
    and writes data on the platters. It includes an armature that moves read/write
    heads to specific locations on the platters.
  prefs: []
  type: TYPE_NORMAL
- en: 'A virtual disk has a storage capacity and can be written to or read from like
    a hard disk; however, the media is simply a file or series of files on the host
    that uses one of three disk image formats compatible with Proxmox VE: **QCOW2**, **RAW**,
    and **VMDK**.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A fourth option is available when using iSCSI shared storage: RAW as LVM partitions.
    In this section, we''ll address only RAW, qcow2, and VMDK images.'
  prefs: []
  type: TYPE_NORMAL
- en: As you'll learn later, each of the three virtual disk image formats explicitly
    supported by the Proxmox VE management interface provides slightly different advantages
    and disadvantages. For us, this provides extended flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in addition to the backup methods an administrator would use to
    ensure the safety and integrity of the host and the guest data, it can be a simple
    matter to make backups or snapshots of the state of one or more virtual machines.
    Restoring an image to a previous state is simple too.
  prefs: []
  type: TYPE_NORMAL
- en: Another difference between the virtual disk and a hard disk drive is that the
    bus, the interface between the motherboard and the disk, is physically incorporated
    into a hard disk unit.
  prefs: []
  type: TYPE_NORMAL
- en: This is handled quite differently on a Proxmox VE virtual machine, on which
    you choose the type of bus that's used to communicate with the virtual disk based
    on your preferences for the specific virtual machine.
  prefs: []
  type: TYPE_NORMAL
- en: The bus, then, isn't at all part of the virtual disk image; instead, it's part
    of the virtual machine configuration. On Proxmox VE, there are bus choices available
    through a simple drop-down box.
  prefs: []
  type: TYPE_NORMAL
- en: Another subcomponent of the physical hard disk drive that is not represented
    in a virtual disk is the cache subsystem (or, more accurately, *disk buffer*).
    The cache, designed to speed up the retrieval of data, constitutes, like the bus
    interface, a part of a virtual machine's configuration, rather than part of the
    virtual disk.
  prefs: []
  type: TYPE_NORMAL
- en: '![Coming to terms](img/image_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing a hard disk with a SATA interface and disk buffer on the control
    board
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we determined that *virtual disk* will be used to describe
    the file or group of files that serve as virtual machines' secondary storage devices.
    In addition, we articulated similarities between physical drives and virtual drives.
    We then contrasted the physical components of a hard disk drive and how virtualization
    with Proxmox VE abstracts these components.
  prefs: []
  type: TYPE_NORMAL
- en: Proxmox VE's configuration process completely divorces the storage media from
    both the bus and the disk cache or disk buffer, which again frees us to make more
    deliberate choices about which combination of virtual disk image format, bus,
    and cache to choose.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining subsections articulate the features of each image format, bus
    option, and disk cache option available to us through Proxmox VE.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding virtual disk configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from [Chapter 4](ch04.html "Chapter 4. Creating Virtual Machines"), *Creating
    a Virtual Machine* that we created new virtual machines from the Proxmox VE interface
    by clicking on the **Create VM** button toward the top of the page and running
    through the new VM's configuration options.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the fourth tab in the configuration dialog, **Hard Disk**, [Chapter
    4](ch04.html "Chapter 4. Creating Virtual Machines"), *Creating a Virtual Machine*
    restricted its concern to defining the size of the virtual disk.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding virtual disk configuration](img/image_05_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Hard Disk tab in the Create: Virtual Machine dialog'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we elaborate on three additional characteristics that can
    be defined through the **Hard Disk** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual disk format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bus/Device (interface)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache (disk buffer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With at least three configuration options available for each, let's explore
    how your choices can affect performance and features.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a virtual disk format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to *Mastering Proxmox*, Proxmox VE's preferred image format for virtual
    disks is RAW. However, it also supports KVM's `qcow2` format and VMDK images commonly
    associated with VMware products.
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing a virtual disk format](img/image_05_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more about image formats and their manipulation, visit [https://en.wikibooks.org/wiki/QEMU/Images](https://en.wikibooks.org/wiki/QEMU/Images).
  prefs: []
  type: TYPE_NORMAL
- en: QCOW2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: QCOW2 is the second release of QEMU's copy-on-write image format. Since Proxmox
    VE relies on KVM-QEMU for its virtual machine features, QCOW2 is its native and
    default format.
  prefs: []
  type: TYPE_NORMAL
- en: As its name suggests, this format supports **copy on write**. This feature allows
    the VM to store changes made to a base image in a separate QCOW2 file. The metadata
    (data about data) of the new QCOW2 file includes, for example, the path to the
    base image.
  prefs: []
  type: TYPE_NORMAL
- en: When the VM seeks to retrieve data, it checks first to see whether the specific
    data can be retrieved from the new image; if it is not in the new image, the data
    is retrieved from the base image referred to by the metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more about the QCOW2 image structure, visit [https://people.gnome.org/~markmc/qcow-image-format.html](https://people.gnome.org/~markmc/qcow-image-format.html).
  prefs: []
  type: TYPE_NORMAL
- en: QCOW2 images also grow as needed (**thin provisioning**), a feature that distinguishes
    them from RAW images, for which all the space requested at their creation is immediately
    allocated to a file (**thick provisioning**).
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, QCOW2 images will be smaller than RAW images in almost any case
    even when the host's file system doesn't support sparse files. However, the RAW
    image will have better throughput since it doesn't have to grow as data is written
    and because it doesn't depend on an intermediary software layer.
  prefs: []
  type: TYPE_NORMAL
- en: We should take note however, that if PVE has plenty of fast RAM and runs on
    a recent SSD drive instead of a hard drive, the difference in throughput between
    a RAW image and a QCOW2 image is much less visible. As we continue to explore
    image types, keep in mind the impact newer hardware, with ever-decreasing prices,
    can have on performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A sparse file is one that attempts to use file system space more efficiently
    when the file itself is mostly empty. This is achieved by writing metadata representing
    the empty blocks to disk, rather than the actual empty space which makes up the
    block. Consequently, less disk space is used. Only when the block contains real
    data is the full block size written to disk as its literal size ([https://en.wikipedia.org/wiki/Sparse_file](https://en.wikipedia.org/wiki/Sparse_file)).For
    a list of file systems with support for sparse files, visit [https://en.wikipedia.org/wiki/Comparison_of_file_systems#Allocation_and_layout_policies](https://en.wikipedia.org/wiki/Comparison_of_file_systems#Allocation_and_layout_policies).
  prefs: []
  type: TYPE_NORMAL
- en: QCOW2's snapshot and temporary snapshot support allows an image to contain multiple
    snapshots from prior moments in the image's history.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary snapshots store changes until the VM powers off, at which point the
    snapshot is discarded. Standard snapshots, in contrast, allow us to return to
    prior states in an image's history.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more about QCOW2 snapshots with Proxmox VE, visit their wiki page at
    [https://pve.proxmox.com/wiki/Live_Snapshots](https://pve.proxmox.com/wiki/Live_Snapshots).For
    more about the mechanics behind QCOW2's snapshot support, visit [https://kashyapc.fedorapeople.org/virt/lc-2012/snapshots-handout.html](https://kashyapc.fedorapeople.org/virt/lc-2012/snapshots-handout.html).
  prefs: []
  type: TYPE_NORMAL
- en: RAW
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RAW and QCOW2 are the two most supported Proxmox VE formats discussed in Proxmox
    VE forums.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to both QCOW2 and VMDK images, RAW virtual disks are quite simple;
    and unlike the other formats supported by Proxmox VE, RAW doesn't rely on an intermediary
    software layer.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, RAW is a more efficient option and should be given all due consideration
    when the performance of a virtual machine is of supreme importance—particularly
    RAW on LVM.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, RAW images can be directly and simply mounted on the Proxmox VE host
    for direct manipulation without requiring access through the guest.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Mounting a RAW file**To walk through this procedure, consider visiting [http://equivocation.org/node/107](http://equivocation.org/node/107)
    or [http://forensicswiki.org/wiki/Mounting_Disk_Images#To_mount_a_disk_image_on_Linux](http://forensicswiki.org/wiki/Mounting_Disk_Images#To_mount_a_disk_image_on_Linux);
    both pages recommend using the `kpartx` utility, available in the default Debian
    repositories.'
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike QCOW2 images, RAW images are not feature rich; there's no inherent
    support for snapshots, no thin provisioning, and so on. They are composed of raw
    data, built sector by sector until they reach their fixed capacity.
  prefs: []
  type: TYPE_NORMAL
- en: '*Performance*, then, is the real boon of RAW images as virtual drives; if you''re
    relying on a hard disk drive rather than a solid state drive, and reliable, snappy
    performance is critical, as it might be for a database server, for example, then
    choose RAW and forego the rich feature set offered by QCOW2.'
  prefs: []
  type: TYPE_NORMAL
- en: Although pre-allocating storage for accumulating VM guests can be an unnecessary
    strain on resources, keep in mind that RAW virtual drives too can be resized.
  prefs: []
  type: TYPE_NORMAL
- en: As is the case with QCOW2, only part of the resizing process can be accomplished
    through the management interface Proxmox VE provides.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process for resizing both RAW and QCOW2 images is documented on the Proxmox
    VE wiki page at [https://pve.proxmox.com/wiki/Resizing_disks#Enlarge_the_virtual_disk.28s.29_in_Proxmox](https://pve.proxmox.com/wiki/Resizing_disks#Enlarge_the_virtual_disk.28s.29_in_Proxmox).
  prefs: []
  type: TYPE_NORMAL
- en: VMDK
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Virtual machine disk** (**VMDK**) is the preferred virtual disk format for
    VMware virtualization products. Having said that, the format was subsequently
    opened up to other developers and vendors, and has become a popular virtual disk
    format to support.'
  prefs: []
  type: TYPE_NORMAL
- en: While KVM-QEMU currently supports versions 3, 4, and 6 of the format, and Proxmox
    VE can create VMs with VMDK images, it's recommended that PVE users rely on QEMU
    native formats—QCOW2 and RAW—whenever circumstances allow.
  prefs: []
  type: TYPE_NORMAL
- en: Realistically, however, circumstances aren't always ideal; so before we address
    bus types, let's touch on a few relevant points regarding VMDK virtual disks.
  prefs: []
  type: TYPE_NORMAL
- en: Like QCOW2, VMDK is a complex format with a rich feature set (in fact, it has
    four sub-formats).
  prefs: []
  type: TYPE_NORMAL
- en: For example, the VMDK format supports thin and thick provisioning. Thin VMDK
    images, like their QCOW2 counterparts, are slower than preallocated, or thick-provisioned,
    VMDK images. As we'd expect, they are significantly smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, both VMDK and QCOW2 formats support multiple snapshots that enable
    administrators to restore a virtual machine to a prior state.
  prefs: []
  type: TYPE_NORMAL
- en: While the format's feature set is rich, not all its features are supported by
    the Proxmox VE interface, even when the underlying virtualization layer can handle
    them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the VMDK format includes a subformat that splits a virtual disk
    into 2 GB chunks, essentially to support mobility. This subformat isn't supported
    by Proxmox VE.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Proxmox VE does invite us to create virtual machines with VMDK images,
    rely as much as possible on QCOW2 and RAW virtual disk formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming your PVE is built around a traditional hard disk drive, RAW is ideal
    for database applications, for example, because it has the performance advantage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QCOW2 offers an extremely powerful feature set that could come at the cost of
    performance if you're not relying on SSD storage. In addition, QCOW2 is more conservative
    in its use of hardware resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you've inherited a virtual disk in the VMDK format, it can be converted to
    either the QCOW2 or RAW format using the `qemu-img` command.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you're creating a Proxmox VE , reliance on the VMDK format
    should be reserved for very deliberate and purposeful edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Full documentation for the `qemu-img`  command is available on the Web at [https://www.suse.com/documentation/sles11/book_kvm/data/cha_qemu_guest_inst_qemu-img.html](https://www.suse.com/documentation/sles11/book_kvm/data/cha_qemu_guest_inst_qemu-img.html)
    (SUSE) and [https://docs.fedoraproject.org/en-US/Fedora/18/html/Virtualization_Administration_Guide/sect-Virtualization-Tips_and_tricks-Using_qemu_img.html](https://docs.fedoraproject.org/en-US/Fedora/18/html/Virtualization_Administration_Guide/sect-Virtualization-Tips_and_tricks-Using_qemu_img.html)
    (Fedora).The sparse but comprehensive GNU/Linux main page is available at [http://linux.die.net/man/1/qemu-img](http://linux.die.net/man/1/qemu-img).
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a bus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Along with **Format** options, the **Hard Disk** tab of the **Create: Virtual
    Machine** dialog offers a drop-down menu for the **Bus/Device** with which to
    interface the virtual disk.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated here, four options are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IDE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SATA**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VIRTIO**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SCSI**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Choosing a bus](img/image_05_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Bus/Device options
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that with the exception of I/O performance, the characteristics of the
    virtual SATA, IDE, and SCSI buses will be the same as their physical counterparts.
    For details about each, visit Wikipedia:**SATA**: [https://en.wikipedia.org/wiki/Serial_ATA](https://en.wikipedia.org/wiki/Serial_ATA)
    **IDE/PATA**: [https://en.wikipedia.org/wiki/Parallel_ATA](https://en.wikipedia.org/wiki/Parallel_ATA)
    **SCSI**: [https://en.wikipedia.org/wiki/SCSI](https://en.wikipedia.org/wiki/SCSI)
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the four options, two are provided as convenient support for compatibility
    with legacy systems: **IDE** and **SCSI**. (IDE is thus the dialog''s default
    option.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **SATA** option has more currency, and behaves as one would expect a SATA
    interface to behave—with one significant exception: the **SATA** option does not
    provide a performance boost over **IDE**, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: I/O performance, in this case, is determined almost entirely by the host's hardware
    configuration. The VM will not be able to read and write faster than the physical
    hardware permits.
  prefs: []
  type: TYPE_NORMAL
- en: Since the limits of the host's hardware I/O performance can't be overcome, KVM-QEMU
    addresses, instead, the overhead attached to the virtualization process, the other
    factor affecting performance in this case.
  prefs: []
  type: TYPE_NORMAL
- en: KVM-QEMU provides a paravirtualization solution called **virtio** that allows
    the guest and hypervisor to work more cooperatively and efficiently with one another
    without the virtualization overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Paravirtualization** refers to software components that are aware they are
    running in a VM. Virtio drivers for use with KVM-QEMU VMs communicate directly
    with the Proxmox VE host in our case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical of paravirtualized drivers, virtio drivers are optimized to share queues,
    buffers, and other data with Proxmox VE to improve throughput and to reduce latency
    (vTerminology: A Guide to Key Virtualization Terminology is available at [http://www.globalknowledge.com](http://www.globalknowledge.com)).'
  prefs: []
  type: TYPE_NORMAL
- en: We'll return to virtio drivers again in the context of network interfaces in
    [Chapter 6](ch06.html "Chapter 6. Networking with Proxmox VE"), *Networking with
    Proxmox VE*.
  prefs: []
  type: TYPE_NORMAL
- en: This solution, **VIRTIO**, is the only **Bus/Device** option that affects the
    I/O performance of the VM.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to take advantage of a guest's virtio devices requires that drivers
    are available for the guest's OS. As the Proxmox VE wiki page points out, recent
    Linux kernels already include the virtio drivers; therefore, any recent GNU/Linux
    distribution running on a Proxmox VE VM "should recognize virtio devices exposed
    by the KVM hypervisor" ([https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers](https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers)).
  prefs: []
  type: TYPE_NORMAL
- en: VM guests running GNU/Linux, therefore, do not require any additional explicit
    configuration steps.
  prefs: []
  type: TYPE_NORMAL
- en: VM guests running a Microsoft Windows OS will need signed drivers installed
    before the virtio device will be recognized. As you may suspect, there's a hitch
    here, since the OS install process needs the device driver in order to recognize
    and install to the virtual disk.
  prefs: []
  type: TYPE_NORMAL
- en: We can employ any of several tactics to overcome this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For additional information on Microsoft operating systems and virtio devices,
    visit [https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers](https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers).
  prefs: []
  type: TYPE_NORMAL
- en: This subsection puts a significant and just emphasis on the performance increase
    virtio paravirtualization supported by KVM-QEMU provides.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the only time we shouldn't use virtio paravirtualization is when
    the required drivers aren't available for the OS intended for the virtual machine.
    Because it will significantly improve I/O throughput and alleviate some of the
    overhead associated with full virtualization, we should rely on virtio whenever it
    is a realistic alternative.
  prefs: []
  type: TYPE_NORMAL
- en: However, let's keep in mind that IDE and SCSI are also viable bus alternatives,
    but provided primarily for legacy devices and to serve the interests of compatibility
    and flexibility. SATA, however, has significant currency at this point, so it's
    a viable alternative if circumstances just don't allow you to take advantage of
    the virtio solution.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is another case when virtio paravirtualization is not viable: when a
    VM has been converted from a physical machine and one thus needs to install the
    drivers before rebooting to rely on virtio.'
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection focuses on the five disk caching/buffering options available
    for our Proxmox VE virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cache options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Hard Disk** tab of the **Create: Virtual Machine** dialog includes a
    field labeled **Cache** that accepts five distinct values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Default (No cache)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct sync**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write through**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write back**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write back (unsafe)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, the chosen setting determines how the abstraction of a HDD's buffer
    should be handled. With Proxmox VE, the choice of cache has been demonstrated
    to significantly affect I/O performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding cache options](img/image_05_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Proxmox VE cache options
  prefs: []
  type: TYPE_NORMAL
- en: After working to somewhat articulate the function of a disk buffer, this section
    will briefly explore recommendations to optimize the performance of a VM.
  prefs: []
  type: TYPE_NORMAL
- en: On a physical hard disk, the disk buffer is a kind of memory on the controller
    board mounted outside the vacuum-sealed disk housing. Contemporary hard-disk drives
    have between 16 and 128 MB of disk buffer. (To take drives that are currently
    on the market as an example, Western Digital's Black line of HDDs has either 32
    or 64 MB of buffer, depending on the model.)
  prefs: []
  type: TYPE_NORMAL
- en: The function of this cache is primarily to sequence disk writes for optimum
    performance and manage and execute read requests from a client (such as the CPU
    or OS) in a strategic way.
  prefs: []
  type: TYPE_NORMAL
- en: Put another way, when we keep in mind that the bus attaching the physical HDD
    to the motherboard is rarely the same speed as the rotation of the hard disk platters
    and the mechanical motion of the read/write heads, the buffer stores data read
    from the disk before it's sent to the client; it likewise stores data to be written
    to the disk until the actual disk write can be executed. It's up to the buffer
    and the drive's processor to organize the data so it gets to its destination as
    efficiently as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'However accurate this explanation may be, it lacks concretion. *Linux System
    Administrator''s Guide* offers a more concrete explanation ([http://www.tldp.org/LDP/sag/html/buffer-cache.html](http://www.tldp.org/LDP/sag/html/buffer-cache.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reading from a disk is very slow compared to accessing (real) memory. In addition,
    it is common to read the same part of a disk several times during relatively short
    periods of time. For example, one might first read an e-mail message, then read
    the letter into an editor when replying to it, then make the mail program read
    it again when copying it to a folder. Or, consider how often the command ls might
    be run on a system with many users. By reading the information from disk only
    once and then keeping it in memory until no longer needed, one can speed up all
    but the first read. This is called disk buffering, and the memory used for the
    purpose is called the buffer cache.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We must choose from among five cache options to define a Proxmox VE VM: the
    default is **No Cache**. The following alternatives are available:'
  prefs: []
  type: TYPE_NORMAL
- en: Direct sync
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write Through
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write back
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write back (unsafe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There's a very clear, concise, general-purpose differentiation between write-through
    and write-back caches offered at [https://simple.wikipedia.org/wiki/Cache#Caches_for_writing](https://simple.wikipedia.org/wiki/Cache#Caches_for_writing).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Write back** is KVM-QEMU''s default; note Proxmox VE''s default is instead
    **No cache**.'
  prefs: []
  type: TYPE_NORMAL
- en: While the resources are ordered from most to least pertinent, each provides
    a helpful perspective and is included precisely because, when we rely on all three,
    we can begin to start conceptualizing use cases for each mode. The following list
    provides resources that describe each of these modes.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Virtualization_Tuning_and_Optimization_Guide/sect-Virtualization_Tuning_Optimization_Guide-BlockIO-Caching.html](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Virtualization_Tuning_and_Optimization_Guide/sect-Virtualization_Tuning_Optimization_Guide-BlockIO-Caching.html):
    From *Virtualization Tuning and Optimization Guide* from Red Hat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.suse.com/documentation/sles11/book_kvm/data/sect1_1_chapter_book_kvm.html](https://www.suse.com/documentation/sles11/book_kvm/data/sect1_1_chapter_book_kvm.html):
    From *Virtualization with KVM* and provided by SUSE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www-01.ibm.com/support/knowledgecenter/linuxonibm/liaat/liaatbpkvmguestcache.htm](https://www-01.ibm.com/support/knowledgecenter/linuxonibm/liaat/liaatbpkvmguestcache.htm):
    From *Linux on IBM Systems*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To evaluate performance, we need either consistent anecdotes or structured comparisons
    of the performances of each combination of cache mode, bus, and virtual disk format.
  prefs: []
  type: TYPE_NORMAL
- en: The most thorough and visually compelling results available on the Web are published
    at [http://jrs-s.net/2013/05/17/kvm-io-benchmarking/](http://jrs-s.net/2013/05/17/kvm-io-benchmarking/)
    (by Jim Salter, 2013) and [http://www.ilsistemista.net/index.php/virtualization/11-kvm-io-slowness-on-rhel-6.html](http://www.ilsistemista.net/index.php/virtualization/11-kvm-io-slowness-on-rhel-6.html)
    (by Ginatan Dante, 2011).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additional benchmarks, specific to Proxmox VE, are posted at [http://i51.tinypic.com/158bcl4.gif](http://i51.tinypic.com/158bcl4.gif);
    however, the results are posted without offering methodology, version, date, or
    attribution info.
  prefs: []
  type: TYPE_NORMAL
- en: The benchmarks provided in these studies from 2011 and 2013 are a helpful starting
    point, but significantly conflict with the KVM best practice statements.
  prefs: []
  type: TYPE_NORMAL
- en: These investigations sometimes resonate, but can also conflict with Proxmox
    VE or the KVMs best I/O performance tips and testimonials available on the web.
  prefs: []
  type: TYPE_NORMAL
- en: The investigations cited previously support virtio paravirtualizaiton as being
    the bus of choice whenever the choice is possible, which is anytime drivers are
    available for the guest OS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, caching adds redundant data and bus traffic and ultimately will impact
    performance negatively. For best results, choose **No Cache** for RAW images and
    avoid the **Directsync** and **Write Through** cache options with QCOW2 images
    except when working with the ZSF filesystem and a RAID array as your primary storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAW is broadly acknowledged to provide the best performance among the three
    formats available in Proxmox VE; however, it's at the cost of the significant
    bundle of features QCOW2 images offer. The benefits of the feature set should
    certainly be weighed against RAW's performance, particularly with **No Cache**
    selected in combination with **VIRTIO**. If you rely on solid state storage, much
    of the performance difference between RAW and QCOW2 becomes unnoticeable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Consensus from the Proxmox VE user community is that there is no practical
    benefit to building a VM in Proxmox VE with a VMDK disk image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proxmox VE and KVM-QEMU best practice resources
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pve.proxmox.com/wiki/Performance_Tweaks](https://pve.proxmox.com/wiki/Performance_Tweaks)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.ilsistemista.net/index.php/virtualization/23-kvm-storage-performance-and-cache-settings-on-red-hat-enterprise-linux-62.html?start=2](http://www.ilsistemista.net/index.php/virtualization/23-kvm-storage-performance-and-cache-settings-on-red-hat-enterprise-linux-62.html?start=2)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.linux-kvm.org/page/Tuning_KVM](http://www.linux-kvm.org/page/Tuning_KVM)'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the conclusion of this section, we have accomplished several objectives
    towards the goal articulated at the beginning of the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We have the resources to make informed choices about appropriate bus/interfaces
    for our Proxmox VE VM guests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now deliberately choose from among virtual disk image formats available
    for use through the Proxmox VE interface and can pursue further support as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning more
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If this chapter sends any single message clearly, it should be this: optimizing
    the I/O performance of a VM in Proxmox VE involves carefully considering and combining
    three components, each with a very rich set of options—tuning a VM to perform
    optimally and with the features you want is a complex balancing act.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, a list of helpful resources is provided here so each of us can
    pursue more information based on our specific needs. The first two resources have
    rich chapters on virtual disks. The third is a work in progress that''s thoroughly
    committed to virtual disk documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hands-On Virtual Computing*, Ted Simpson, Cengage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Virtualization from the Desktop to the Enterprise*, Chris Wolf, Apress'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Linux Sysadmins Guide to Virtual Disks: From the Basics to the Advanced*,
    Tim Bielawa, [http://lnx.cx/docs/vdg/output/Virtual-Disk-Operations.pdf](http://lnx.cx/docs/vdg/output/Virtual-Disk-Operations.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our purpose in this chapter has been to understand virtual disks in the context
    of Proxmox VE virtual machine guests (the chapter does not address anything that
    concerns container guests).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve accomplished a difficult task by focusing on how Proxmox VE, and its
    underlying virtualization technology, handle the abstractions of the components
    of physical hard disk drives: storage media, the bus interface, and the disk buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: As we proceeded, we worked hard to understand and be able to articulate how
    the choices we make when determining disk format, bus, and disk buffer preferences
    can significantly affect both features and I/O performance.
  prefs: []
  type: TYPE_NORMAL
- en: At the most fundamental level, we recognized that our VMs will not have better
    I/O proficiency than our physical host's hardware allows. However, you also learned
    that, by relying on paravirtualization drivers, you can minimize the overhead
    cost of virtualization on I/O performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, you learned that our decision making regarding virtual-disk configuration
    depends on how we answer some fundamental questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What physical hardware do you already have that affects I/O performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the performance needs of the application and/or database that the VM
    is dedicated to serving?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What features of a virtual disk format should you take advantage of? Which redundant
    features can be provided by other technologies in your datacenter?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the capacity needs of the OS and application and/or database?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the administrators already familiar with and willing to support in
    terms of file systems, virtualization generally, and virtual disks in particular?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the OS concerns and requirements in regards to file systems and bus
    drivers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we continue to build on what you learned in [Chapter 4](ch04.html
    "Chapter 4. Creating Virtual Machines"), *Creating a Virtual Machine*, about the
    creation of virtual machines by focusing on the Proxmox VE network model. To do
    this, we'll rely on our prior knowledge of TCP/IP LAN networks, switching, and
    subnets.
  prefs: []
  type: TYPE_NORMAL
- en: It'll be a great transitional chapter as we move toward understanding Proxmox
    VE and virtualization security.
  prefs: []
  type: TYPE_NORMAL
- en: Let's connect nodes and build bridges!
  prefs: []
  type: TYPE_NORMAL
