<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;Maintaining Docker Containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Maintaining Docker Containers</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Testing Docker containers with BATS</li><li class="listitem" style="list-style-type: disc">Test-Driven Development (TDD) with Docker and ServerSpec</li><li class="listitem" style="list-style-type: disc">The workflow for creating automated Docker builds from Git</li><li class="listitem" style="list-style-type: disc">The workflow for connecting the Continuous Integration (CI) system</li><li class="listitem" style="list-style-type: disc">Scanning for vulnerabilities with Quay.io and Docker Cloud</li><li class="listitem" style="list-style-type: disc">Sending Docker logs to AWS CloudWatch Logs</li><li class="listitem" style="list-style-type: disc">Monitoring and getting information out of Docker</li><li class="listitem" style="list-style-type: disc">Debugging containers using sysdig</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec109"/>Introduction</h1></div></div></div><p>In this chapter, we'll explore some advanced and highly interesting areas that probably most developers today are already used to. Infrastructure code is still code, so it should be no different than software code; the same principle should apply. This means that the Docker code should be testable, the builds automatic, and the CI systems connected to our Git servers so they could continuously apply the tests. In addition to this, security checks should be part of the mandatory release process and the logs easy to access, even if the application is scaled on multiple machines. Also note that containers shouldn't be black boxes, and highly performant debugging tools should be available for us to do our work. The good news is that these topics will be covered in this chapter, because all of this can be done easily.</p></div></div>
<div class="section" title="Testing Docker containers with BATS"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec110"/>Testing Docker containers with BATS</h1></div></div></div><p>
<span class="strong"><strong>BATS</strong></span> (<span class="strong"><strong>Bash Automated Testing System</strong></span>) allows <a id="id1055" class="indexterm"/>you <a id="id1056" class="indexterm"/>to have quick and easy tests in a very natural language, without the need of a lot of dependencies. BATS can also grow in complexity as per your requirement. In this section, we'll use Docker with Docker Compose to handle the build and a Makefile to tie the <a id="id1057" class="indexterm"/>dependencies between the build process and the BATS testing process; this will make it easier to later integrate this process into a CI system.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec277"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Docker installation</li><li class="listitem" style="list-style-type: disc">A BATS installation (it's available for all major Linux distributions and Mac OS)</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note75"/>Note</h3><p>BATS Version 0.4.0 is used in this chapter.</p></div></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec278"/>How to do it…</h2></div></div></div><p>Let's start with this simple Dockerfile that will install Apache and run it after clearing the cache:</p><div class="informalexample"><pre class="programlisting">FROM debian:stable-slim
LABEL name="apache"
LABEL maintainer="John Doe &lt;john@doe.com&gt;"
LABEL version=1.0
RUN apt-get update -y \
    &amp;&amp; apt-get install -y --no-install-recommends apache2=2.4.10-10+deb8u7 \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/*
EXPOSE 80
ENTRYPOINT ["/usr/sbin/apache2ctl"]
CMD ["-D", "FOREGROUND"]</pre></div><p>For <a id="id1058" class="indexterm"/>convenience, let's create a <code class="literal">docker-compose.yml</code> file so the image can be built and run easily:</p><div class="informalexample"><pre class="programlisting">version: '2'

services:
  http:
    build: .
    image: demo-httpd
    ports:
      - "80:80"</pre></div><p>This <a id="id1059" class="indexterm"/>way, running <code class="literal">docker-compose up</code> will <a id="id1060" class="indexterm"/>also build the image if absent. Alternatively, to just build the image, use this code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker-compose build</strong></span>
</pre></div><div class="section" title="Creating BATS tests"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec218"/>Creating BATS tests</h3></div></div></div><p>We'll now <a id="id1061" class="indexterm"/>test two of the main actions this image is supposed to do:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Install Apache 2.4.10</li><li class="listitem" style="list-style-type: disc">Clean the APT cache</li></ul></div><p>Start by creating a <code class="literal">test</code> folder at the root of our repository that will host the BATS tests: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir test</strong></span>
</pre></div><p>Our first test is to verify that the installed version of Apache is <code class="literal">2.4.10</code>, as required. How would we do it manually? We'd probably just execute the following and check the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ apache2ctl -v</strong></span>
<span class="strong"><strong>Server version: Apache/2.4.10 (Debian)</strong></span>
</pre></div><p>This translates in Docker with our image in the following command (<code class="literal">-v</code> being the command (<code class="literal">CMD</code>) for the <code class="literal">apache2ctl</code> <code class="literal">ENTRYPOINT</code> instruction):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker run --rm demo-httpd:latest -v</strong></span>
<span class="strong"><strong>Server version: Apache/2.4.10 (Debian)</strong></span>
</pre></div><p>Basically, now we just have to run <code class="literal">grep</code> for the correct version:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker run --rm demo-httpd:latest -v | grep 2.4.10</strong></span>
<span class="strong"><strong>Server version: Apache/2.4.10 (Debian)</strong></span>
</pre></div><p>If <code class="literal">grep</code> is successful, it returns <code class="literal">0</code>: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo $?</strong></span>
<span class="strong"><strong>0</strong></span>
</pre></div><p>A simple BATS test for a command return code looks like this:</p><div class="informalexample"><pre class="programlisting">@test "test title" {
  run &lt;some command&gt;
  [ $status -eq 0 ]
}</pre></div><p>We now have everything we need to write our first BATS test in <code class="literal">test/httpd.bats</code>:</p><div class="informalexample"><pre class="programlisting">@test "Apache version is correct" {
  run docker run --rm demo-httpd:latest -v \| grep 2.4.10
  [ $status -eq 0 ]
}</pre></div><p>To execute our test, let's launch BATS with the folder containing the tests as arguments:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bats test</strong></span>
<span class="strong"><strong> </strong></span>
<span class="strong"><strong></strong></span>
<span class="strong"><strong> Apache version is correct</strong></span>

<span class="strong"><strong>1 test, 0 failures  </strong></span>
</pre></div><p>Good! We're now assured that the correct Apache version is installed.</p><p>Let's ensure the APT cache is cleaned after we build the image so we don't waste precious space. Deleting the APT lists means the <code class="literal">/var/lib/apt/lists</code> folder will become <a id="id1062" class="indexterm"/>empty, so if you count the files in this folder after this, it should return <code class="literal">0</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ls -1 /var/lib/apt/lists | wc -l</strong></span>
</pre></div><p>However, we cannot just send this command to the container like we did for the Apache version; the entry point is <code class="literal">apache2ctl</code>, and it needs to be overridden by <code class="literal">sh</code> on the <code class="literal">docker run</code> command line. Here's the <code class="literal">apt.bats</code> test file, executing the shell command instead of <code class="literal">apache2ctl</code>, expecting a successful execution and an output of <code class="literal">0</code>: </p><div class="informalexample"><pre class="programlisting">@test "apt lists are empty" {
  run docker run --rm --entrypoint="/bin/sh" demo-httpd:latest -c "ls -1 /var/lib/apt/lists | wc -l"
  [ $status -eq 0 ]
  [ "$output" = "0" ]
}</pre></div><p>Execute the BATS tests: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bats test</strong></span>
<span class="strong"><strong>  apt lists are empty</strong></span>
<span class="strong"><strong>  Apache version is correct</strong></span>

2 tests, 0 failures</pre></div></div><div class="section" title="Using Makefile to glue it all together"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec219"/>Using Makefile to glue it all together</h3></div></div></div><p>Now this <a id="id1063" class="indexterm"/>whole process might be a bit tedious in CI, with some additional steps needed before the testing is done (the image needs to be built and made available before it is tested, for example). Let's create a <code class="literal">Makefile</code> that will take care of the prerequisites for us:</p><div class="informalexample"><pre class="programlisting">test: bats

bats: build
  bats test

build:
  docker-compose build</pre></div><p>Now when you execute the <code class="literal">make test</code> command, it will launch the <code class="literal">bats</code> suite, which itself depends on building the image by <code class="literal">docker-compose</code>—a much simpler command to integrate <a id="id1064" class="indexterm"/>in the CI system of your choice:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ make test</strong></span>
<span class="strong"><strong>docker-compose build</strong></span>
<span class="strong"><strong>Building http</strong></span>
<span class="strong"><strong>Step 1 : FROM debian:stable-slim</strong></span>
<span class="strong"><strong> ---&gt; d2103c196fde</strong></span>
<span class="strong"><strong>[...]</strong></span>
<span class="strong"><strong>Successfully built 1c4f46316f19</strong></span>
<span class="strong"><strong>bats test</strong></span>
<span class="strong"><strong> </strong></span>
<span class="strong"><strong>. apt lists are empty</strong></span>
<span class="strong"><strong>. Apache version is correct</strong></span>
<span class="strong"><strong> 2 tests, 0 failures</strong></span>
</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec279"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Information on <a id="id1065" class="indexterm"/>BATS at <a class="ulink" href="https://github.com/sstephenson/bats">https://github.com/sstephenson/bats</a></li></ul></div></div></div>
<div class="section" title="Test-Driven Development (TDD) with Docker and ServerSpec"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec111"/>Test-Driven Development (TDD) with Docker and ServerSpec</h1></div></div></div><p>Docker <a id="id1066" class="indexterm"/>containers might have a simpler language, but in the end, general concepts remain common and still apply. Testing is good for quality, and writing tests first ensures that we write code that would make <a id="id1067" class="indexterm"/>a test pass, instead of writing tests after the code is written, which would somehow lead to missed errors. To help us with this, we'll use ServerSpec, based on RSpec, to initiate a TDD workflow along with writing and testing a Docker container. Working like this usually ensures a very high quality of work overall and very sustainable containers.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec280"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Docker installation</li><li class="listitem" style="list-style-type: disc">A working Ruby environment (including Bundler)</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec281"/>How to do it…</h2></div></div></div><p>Our <a id="id1068" class="indexterm"/>goal is to create an NGINX container <a id="id1069" class="indexterm"/>following TDD principles. Before we start to code, let's begin by setting up our environment.</p><div class="section" title="Creating a ServerSpec environment using Bundler"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec220"/>Creating a ServerSpec environment using Bundler</h3></div></div></div><p>ServerSpec <a id="id1070" class="indexterm"/>comes as a gem (a Ruby package), and as we'll use Docker APIs, we'll need the <code class="literal">docker-api</code> gem as well. For <a id="id1071" class="indexterm"/>ease of deployment, let's create <code class="literal">Gemfile</code> containing our dependencies inside a <code class="literal">test</code> group: </p><div class="informalexample"><pre class="programlisting">source 'https://rubygems.org'

group :test do
  gem 'serverspec'
  gem 'docker-api'
end</pre></div><p>Install these dependencies using Bundler: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle install</strong></span>
<span class="strong"><strong>Using docker-api 1.33.0</strong></span>
<span class="strong"><strong>Using serverspec 2.37.2</strong></span>
<span class="strong"><strong>[...]</strong></span>
<span class="strong"><strong>Bundle complete! 2 Gemfile dependencies, 18 gems now installed.</strong></span>
</pre></div><p>Now we'll be able to execute <code class="literal">rspec</code> in our local context using Bundler:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle exec rspec</strong></span>
</pre></div></div><div class="section" title="Initializing the tests"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec221"/>Initializing the tests</h3></div></div></div><p>Let's start <a id="id1072" class="indexterm"/>by creating our first Docker Rspec test that will just, for now, initialize the libraries we need and build the Docker image before anything else. It looks like this in <code class="literal">spec/Dockerfile_spec.rb</code>:</p><div class="informalexample"><pre class="programlisting">require "serverspec"
require "docker"

describe "Docker NGINX image" do
  before(:all) do
    @image = Docker::Image.build_from_dir('.')

    set :os, family: :debian
    set :backend, :docker
    set :docker_image, @image.id
  end
end</pre></div></div><div class="section" title="TDD – using the Debian Jessie base's Docker image"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec222"/>TDD – using the Debian Jessie base's Docker image</h3></div></div></div><p>We now <a id="id1073" class="indexterm"/>want to use a Debian stable for our project, which happens to be Debian 8 at the moment. To know the current version of a Debian system, just look at the <code class="literal">/etc/debian_version</code> file (on Red-Hat-based systems, it's under <code class="literal">/etc/redhat_release</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat /etc/debian_version</strong></span>
<span class="strong"><strong>8.6</strong></span>
</pre></div><p>Good! Let's create a definition in ServerSpec, checking for the Debian version through this command:</p><div class="informalexample"><pre class="programlisting">describe "Docker NGINX image" do
[...]
  def debian_version
    command("cat /etc/debian_version").stdout
  end
end</pre></div><p>Now, the <code class="literal">debian_version</code> content can be easily queried, for example, by this check:</p><div class="informalexample"><pre class="programlisting">  it "installs Debian Jessie" do
    expect(debian_version).to include("8.")
  end</pre></div><p>If this system is running Debian 8, then the test will pass. If the <code class="literal">Dockerfile</code> is empty, the test will fail:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle exec rspec --color --format documentation</strong></span>
<span class="strong"><strong>Docker image</strong></span>
<span class="strong"><strong>  installs Debian Jessie (FAILED - 1)</strong></span>

<span class="strong"><strong>Failures:</strong></span>

<span class="strong"><strong>  1) Docker image installs Debian Jessie</strong></span>
<span class="strong"><strong>     Failure/Error: @image = Docker::Image.build_from_dir('.')</strong></span>
<span class="strong"><strong>     Docker::Error::ServerError:</strong></span>
<span class="strong"><strong>       No image was generated. Is your Dockerfile empty?</strong></span>
</pre></div><p>Good! Our test has failed. Let's write the <code class="literal">FROM</code> instruction in Dockerfile that will make it pass; this is because the current Debian stable is version 8:</p><div class="informalexample"><pre class="programlisting">FROM debian:stable-slim</pre></div><p>Save the file and launch the test again: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle exec rspec --color --format documentation</strong></span>
<span class="strong"><strong>Docker NGINX image</strong></span>
<span class="strong"><strong>  installs Debian Jessie</strong></span>

<span class="strong"><strong>Finished in 0.72234 seconds (files took 0.29061 seconds to load)</strong></span>
<span class="strong"><strong>1 example, 0 failures</strong></span>
</pre></div><p>Good job! Our <a id="id1074" class="indexterm"/>test has passed, meaning this really is Debian 8.</p></div><div class="section" title="TDD – installing the NGINX package"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec223"/>TDD – installing the NGINX package</h3></div></div></div><p>Our next <a id="id1075" class="indexterm"/>objective is to install the <code class="literal">nginx</code> package. Let's write the Rspec test in <code class="literal">Dockerfile_spec.rb</code> that will check for this:</p><div class="informalexample"><pre class="programlisting">describe "Docker NGINX image" do
[...]
  describe package('nginx') do
    it { should be_installed }
  end
end</pre></div><p>Launch the test to be sure it fails: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle exec rspec --color --format documentation</strong></span>
<span class="strong"><strong>Docker NGINX image</strong></span>
<span class="strong"><strong>  installs Debian Jessie</strong></span>
<span class="strong"><strong>  Package "nginx"</strong></span>
<span class="strong"><strong>    should be installed (FAILED - 1)</strong></span>
</pre></div><p>It's now time to add the instructions to the <code class="literal">Dockerfile</code> on how to install NGINX:</p><div class="informalexample"><pre class="programlisting">RUN apt-get update -y \
    &amp;&amp; apt-get install -y --no-install-recommends nginx=1.6.2-5+deb8u4 \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/*</pre></div><p>Relaunch the tests (it will take some time as it needs to build the image):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle exec rspec --color --format documentation</strong></span>
<span class="strong"><strong>Docker NGINX image</strong></span>
<span class="strong"><strong>  installs Debian Jessie</strong></span>
<span class="strong"><strong>  Package "nginx"</strong></span>
<span class="strong"><strong>    should be installed</strong></span>

<span class="strong"><strong>Finished in 51.89 seconds (files took 0.3032 seconds to load)</strong></span>
<span class="strong"><strong>2 examples, 0 failures</strong></span>
</pre></div><p>We're now <a id="id1076" class="indexterm"/>sure the <code class="literal">nginx</code> package is installed.</p></div><div class="section" title="TDD – running NGINX"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec224"/>TDD – running NGINX</h3></div></div></div><p>Now that <a id="id1077" class="indexterm"/>we have our image built with NGINX, execute it. Using ServerSpec, we can start a container using the <code class="literal">id</code> attribute of the image we built earlier. In the <code class="literal">Dockerfile_spec.rb</code> file, create and start the container using the image:</p><div class="informalexample"><pre class="programlisting">describe "Docker NGINX image" do
[...]
  describe 'Running the NGINX container' do
    before(:all) do
      @container = Docker::Container.create(
        'Image'      =&gt; @image.id
        )
      @container.start
    end
  end
end</pre></div><p>Using standard ServerSpec checks, verify that an NGINX process is running:</p><div class="informalexample"><pre class="programlisting">    describe process("nginx") do
      it { should be_running }
    end </pre></div><p>We can't stop here without cleaning up the container. We need to stop it when we're done with the tests and delete it:</p><div class="informalexample"><pre class="programlisting">    after(:all) do
      @container.kill
      @container.delete(:force =&gt; true)
    end</pre></div><p>Now we can run the test that will execute the container and fail upon checking for an <code class="literal">nginx</code> process (we didn't write anything that would launch <code class="literal">nginx</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle exec rspec --color --format documentation</strong></span>
<span class="strong"><strong>Docker NGINX image</strong></span>
<span class="strong"><strong>  installs Debian Jessie</strong></span>
<span class="strong"><strong>  Package "nginx"</strong></span>
<span class="strong"><strong>    should be installed</strong></span>
<span class="strong"><strong>  Running the NGINX container</strong></span>
<span class="strong"><strong>    Process "nginx"</strong></span>
<span class="strong"><strong>      should be running (FAILED - 1)</strong></span>
</pre></div><p>Now let's execute <code class="literal">/usr/bin/nginx</code> for our container in the foreground, specifically in the <code class="literal">Dockerfile</code>:</p><div class="informalexample"><pre class="programlisting">EXPOSE 80
ENTRYPOINT ["/usr/sbin/nginx"]
CMD ["-g", "daemon off;"]</pre></div><p>Rerun the tests to check whether the <code class="literal">nginx</code> process is now running as expected:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bundle exec rspec --color --format documentation</strong></span>
<span class="strong"><strong>Docker NGINX image</strong></span>
<span class="strong"><strong>  installs Debian Jessie</strong></span>
<span class="strong"><strong>  Package "nginx"</strong></span>
<span class="strong"><strong>    should be installed</strong></span>
<span class="strong"><strong>  Running the NGINX container</strong></span>
<span class="strong"><strong>    Process "nginx"</strong></span>
<span class="strong"><strong>      should be running</strong></span>

<span class="strong"><strong>Finished in 1.94 seconds (files took 0.30853 seconds to load)</strong></span>
<span class="strong"><strong>3 examples, 0 failures</strong></span>
</pre></div><p>To add <a id="id1078" class="indexterm"/>simplicity when integrating these tests in CI systems, let's create a simple <code class="literal">Makefile</code>:</p><div class="informalexample"><pre class="programlisting">test: rspec
rspec:
  bundle exec rspec --color --format documentation</pre></div><p>Now a simple <code class="literal">make test</code> command will launch the ServerSpec tests.</p><p>Good job! We've built our first simple Docker container following TDD principles. We can now build more complex and secure containers using this technique.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec282"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">RSpec <a id="id1079" class="indexterm"/>at <a class="ulink" href="http://rspec.info/">http://rspec.info/</a></li><li class="listitem" style="list-style-type: disc">Docker-api <a id="id1080" class="indexterm"/>at <a class="ulink" href="https://github.com/swipely/docker-api">https://github.com/swipely/docker-api</a></li><li class="listitem" style="list-style-type: disc">ServerSpec <a id="id1081" class="indexterm"/>at <a class="ulink" href="http://serverspec.org/">http://serverspec.org/</a></li></ul></div></div></div>
<div class="section" title="The workflow for creating automated Docker builds from Git"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec112"/>The workflow for creating automated Docker builds from Git</h1></div></div></div><p>Building <a id="id1082" class="indexterm"/>local containers is a nice thing to do, but what about its wide distribution? We can use the Docker Hub service to store and distribute our containers (or its alternative Quay.io); however, uploading each and every container and version manually will soon be a problem. Consider you need to rebuild dozens <a id="id1083" class="indexterm"/>of containers in an emergency, because of the existence of another OpenSSL security bug; nobody would want to be the one to upload them one by one, especially with the bad uplink at work. And as we're working with our Docker code using branches and tags, it will be awesome to see the same behavior reflected automatically on the remote Docker registry. This includes two of the Docker Hub (or Quay.io) features: automatically build Docker images upon changes and serve them to the world. We'll do exactly this in this section: create an automated build and distribution pipeline from our code to GitHub to the Docker Hub.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec283"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A free GitHub account</li><li class="listitem" style="list-style-type: disc">A free Docker Hub account</li><li class="listitem" style="list-style-type: disc">A Docker project</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec284"/>How to do it…</h2></div></div></div><p>Our objective is to get a fully working Docker build pipeline. To achieve this, we'll use two free, popular services: GitHub and the Docker Hub. Let's start with the code from the previous section that helped us build an NGINX container; we can alternatively use any other repository on GitHub containing at least a buildable <code class="literal">Dockerfile</code>. The code needs to actually be on GitHub not just versioned using Git locally. The repository should look like this:</p><div class="mediaobject"><img src="graphics/B05671_10_01.jpg" alt="How to do it…"/></div><p>This <a id="id1084" class="indexterm"/>repository is ready to communicate with other build services.</p><div class="section" title="Creating an automated build on the Docker Hub"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec225"/>Creating an automated build on the Docker Hub</h3></div></div></div><p>The Docker <a id="id1085" class="indexterm"/>Hub is one of the commercial services from the company that created Docker. It's both a public Docker registry service (with private or public containers, depending on your subscription) and a Docker image build service that can automatically create new images when changes occur in the code. Go to <a class="ulink" href="https://hub.docker.com">https://hub.docker.com</a> and log in or create an account if you don't have any.</p><p>Click on <span class="strong"><strong>Create Automated Build</strong></span> in the <span class="strong"><strong>Create</strong></span> menu:</p><div class="mediaobject"><img src="graphics/B05671_10_02.jpg" alt="Creating an automated build on the Docker Hub"/></div><p>Choose <a id="id1086" class="indexterm"/>the provider where the infrastructure code is hosted; in our case, it's <span class="strong"><strong>GitHub</strong></span>:</p><div class="mediaobject"><img src="graphics/B05671_10_03.jpg" alt="Creating an automated build on the Docker Hub"/></div><p>When the synchronization is done, choose the GitHub repository:</p><div class="mediaobject"><img src="graphics/B05671_10_04.jpg" alt="Creating an automated build on the Docker Hub"/></div><p>Finally, decide on a name for the image (it doesn't have to be the name of the GitHub repository) and the namespace. The namespace could either be your username or an organization <a id="id1087" class="indexterm"/>if you have one. Write a short description and choose the visibility of the image: private stuff should remain private, while public can stay public. Let's be careful about what we ship:</p><div class="mediaobject"><img src="graphics/B05671_10_05.jpg" alt="Creating an automated build on the Docker Hub"/></div><p>Navigate to <span class="strong"><strong>Build Settings</strong></span> of our Docker Hub's project to trigger an initial build:</p><div class="mediaobject"><img src="graphics/B05671_10_06.jpg" alt="Creating an automated build on the Docker Hub"/></div><p>Clicking <a id="id1088" class="indexterm"/>on the <span class="strong"><strong>Trigger</strong></span> button will create a build. This is done by having <code class="literal">master</code> as the <span class="strong"><strong>Branch</strong></span> type of our repository; tag the build with the <code class="literal">latest</code> tag. If, for some reason, the <code class="literal">Dockerfile</code> of our project wasn't at the root, we could specify it here. This build also allows us to manage different <code class="literal">Dockerfile</code> for different purposes, such as building the development and production containers differently, among other options.</p><p>Once the build is complete (should happen in minutes), navigating to the <span class="strong"><strong>Tags</strong></span> tab will show the available tags (<span class="strong"><strong>latest</strong></span> is the only one we have now) and the size of the image:</p><div class="mediaobject"><img src="graphics/B05671_10_07.jpg" alt="Creating an automated build on the Docker Hub"/></div><p>The <span class="strong"><strong>Dockerfile</strong></span> tab shows the content of the <code class="literal">Dockerfile</code> from which the image has been built, while <a id="id1089" class="indexterm"/>the <span class="strong"><strong>Build Details</strong></span> tab will list all the builds and their details, including the build output. This is very useful for debugging when things go wrong.</p></div><div class="section" title="Configuring a GitHub to a Docker Hub-automated build pipeline"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec226"/>Configuring a GitHub to a Docker Hub-automated build pipeline</h3></div></div></div><p>Now let's <a id="id1090" class="indexterm"/>make a modification to the <code class="literal">Dockerfile</code>, for example, adding a label for the image's name and version:</p><div class="informalexample"><pre class="programlisting">LABEL name="demo-nginx"
LABEL version=1.0</pre></div><p>Commit and push this change to GitHub:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git add Dockerfile</strong></span>
<span class="strong"><strong>$ git commit -m "added some missing labels"</strong></span>
<span class="strong"><strong>[master f20017b] added some missing labels</strong></span>
<span class="strong"><strong> 1 file changed, 2 insertions(+)</strong></span>
<span class="strong"><strong>$ git push</strong></span>
</pre></div><p>What's happening on the Docker Hub? It automatically starts building a new image as soon as it becomes aware of the change on GitHub:</p><div class="mediaobject"><img src="graphics/B05671_10_08.jpg" alt="Configuring a GitHub to a Docker Hub-automated build pipeline"/></div><p>A few <a id="id1091" class="indexterm"/>seconds later, our newest build is available for everyone to use:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker pull sjourdan/nginx-docker-demo</strong></span>
</pre></div></div><div class="section" title="Building Docker images using Git tags"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec227"/>Building Docker images using Git tags</h3></div></div></div><p>As we're <a id="id1092" class="indexterm"/>happy with this release, we'd like it <a id="id1093" class="indexterm"/>to be available as a <code class="literal">1.0</code> tag on the Docker Hub. To do this, we'll need to complete two actions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Configure the Docker Hub to build and tag according to Git tags and not just branches</li><li class="listitem" style="list-style-type: disc">Tag and push our release on Git</li></ul></div><p>For the Docker Hub to build images with the same tags than the ones we set on Git, let's add a new type called <span class="strong"><strong>Tag</strong></span> in the <span class="strong"><strong>Build Settings</strong></span> tab. This will now make the Docker Hub follow <a id="id1094" class="indexterm"/>the tags we set on Git. It will also build any other tag you may create in the future:</p><div class="mediaobject"><img src="graphics/B05671_10_09.jpg" alt="Building Docker images using Git tags"/></div><p>Let's tag <a id="id1095" class="indexterm"/>our code as <code class="literal">1.0</code> on Git so we can refer to it later:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git tag 1.0</strong></span>
<span class="strong"><strong>$ git push --tags</strong></span>
<span class="strong"><strong>Total 0 (delta 0), reused 0 (delta 0)</strong></span>
<span class="strong"><strong>To https://github.com/sjourdan/nginx-docker-demo.git</strong></span>
<span class="strong"><strong> * [new tag]         1.0 -&gt; 1.0 </strong></span>
</pre></div><p>This just triggered a new build on the Docker Hub, using the tag <span class="strong"><strong>1.0</strong></span>, as we asked to match:</p><div class="mediaobject"><img src="graphics/B05671_10_10.jpg" alt="Building Docker images using Git tags"/></div><p>Everyone can <a id="id1096" class="indexterm"/>now refer to this stable build and use it without fearing a breaking change from the master branch; this branch will <a id="id1097" class="indexterm"/>always be built with the latest tag:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker pull sjourdan/nginx-docker-demo:1.0</strong></span>
</pre></div><p>Even better, from now on, our future Docker projects that need both this container and the stability can simply start with the following line on the <code class="literal">Dockerfile</code>:</p><div class="informalexample"><pre class="programlisting">FROM sjourdan/nginx-docker-demo:1.0</pre></div><p>We now have a nice initial workflow for building master and tagged, stable releases of our containers.</p></div></div></div>
<div class="section" title="The workflow for connecting the Continuous Integration (CI) system"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec113"/>The workflow for connecting the Continuous Integration (CI) system</h1></div></div></div><p>As people <a id="id1098" class="indexterm"/>working with code and writing tests for it, there's no reason not to see those tests executed in CI. The same way every program has language requirements, ours need to be able to build Docker containers and execute some Ruby code. Being able to fully execute a whole pile of tests automatically, upon any code check-in, is a major quality improvement step. No one can test each and every possibility and regression and special cases from months or maybe years ago. It's true in software code, and it's the same in infrastructure code as well. Let's find an elegant and automated way to execute our infrastructure code tests in CI systematically so this could be another dot connected to the bigger map.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec285"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Docker installation</li><li class="listitem" style="list-style-type: disc">A free Travis CI account</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec286"/>How to do it…</h2></div></div></div><p>We'd like our RSpec integration tests to be executed automatically each time we commit a change on Git. This is the perfect job for a CI system, such as Jenkins, the Circle CI, or the Travis CI. Our only requirement is that the CI platform should build and execute Docker containers and run RSpec tests. Docker support is good with Travis, and it works out of the box. Jenkins would work equally well behind the firewall when properly configured, like most other CI systems. Here's how to configure our CI platform to automatically execute tests on a new commit:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a free account for the Travis CI or use your own (<a class="ulink" href="https://travis-ci.org/">https://travis-ci.org/</a>).</li><li class="listitem">Click on the <span class="strong"><strong>+</strong></span> button to add a new GitHub repository:<div class="mediaobject"><img src="graphics/B05671_10_14.jpg" alt="How to do it…"/></div></li><li class="listitem">Enable the watching of the repository by Travis:<div class="mediaobject"><img src="graphics/B05671_10_11.jpg" alt="How to do it…"/></div></li><li class="listitem">Now <a id="id1099" class="indexterm"/>add a configuration file for Travis named <code class="literal">.travis.yml</code> at the root of the repository. This file can contain a lot of information to do many things, but for now, it should simply tell Travis that we need a Ruby environment in a recent Linux distribution running Docker. Also, it should simply execute <code class="literal">make test</code> for <code class="literal">Makefile</code>. In our case, this command will execute the RSpec tests:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo: required</strong></span>
<span class="strong"><strong>language: ruby</strong></span>
<span class="strong"><strong>dist: trusty</strong></span>
<span class="strong"><strong>services:</strong></span>
<span class="strong"><strong>  - docker</strong></span>
<span class="strong"><strong>script: make test</strong></span>
</pre></div></li><li class="listitem">Commit and push this file and it will trigger our first test on Travis:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git add .travis.yml</strong></span>
<span class="strong"><strong>$ git commit -m "added travis.yml"</strong></span>
<span class="strong"><strong>$ git push</strong></span>
</pre></div></li><li class="listitem">Navigating back to the Travis CI, we can see the tests begin:<div class="mediaobject"><img src="graphics/B05671_10_12.jpg" alt="How to do it…"/></div></li><li class="listitem">A few seconds later, the tests pass successfully, assuring us the build is consistent with our expectations. Travis even gives easy access to the output of the commands:<div class="mediaobject"><img src="graphics/B05671_10_13.jpg" alt="How to do it…"/></div></li></ol></div><p>We just <a id="id1100" class="indexterm"/>initiated new steps for integrating automated tests in our workflow. This is getting increasingly important as every project or team grows, and it's getting riskier to ship untested containers into production.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note76"/>Note</h3><p>It's also highly recommended that you include any other test that can be done in this CI system, such as the Docker linters check from earlier in this book. Quality can only go higher: the more the checks, the better. Building quicker tests for a faster feedback loop will then be a new subject.</p></div></div><p>As with <a id="id1101" class="indexterm"/>every CI system, the final step after the tests are completed is to package, ship, and deploy the containers. As exciting as this step is, it's also unfortunately far beyond the scope of this book.</p></div></div>
<div class="section" title="Scanning for vulnerabilities with Quay.io and Docker Cloud"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec114"/>Scanning for vulnerabilities with Quay.io and Docker Cloud</h1></div></div></div><p>One major <a id="id1102" class="indexterm"/>issue when working with containers <a id="id1103" class="indexterm"/>is their deprecation and maintenance costs. Too often, containers are built one day, shipped to production because <a id="id1104" class="indexterm"/>they work, and forgotten there until the next rebuild (which may not happen anytime soon). Libraries are still libraries, and <a id="id1105" class="indexterm"/>security fixes are pushed every day into distributions package repositories. Sysadmins are used to patch the systems; however, now it's a total anti-pattern to update a running container. Containers need to be rebuilt, exactly like developers are used to rebuilding applications with updated libraries to get rid of bugged code. The exception is that we are lucky enough to have tools that monitor each and every layer of our Docker images and tell us how and when they are vulnerable, allowing us to simply rebuild and redeploy them.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec287"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Docker installation</li><li class="listitem" style="list-style-type: disc">A free account at Quay.io and/or a paid account at the Docker Hub</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec288"/>How to do it…</h2></div></div></div><p>Using the free Quay.io account (by the CoreOS team), push an image to their Docker Registry service after logging in using <code class="literal">docker login</code>. Here's how to do this using an earlier image from this chapter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker tag sjourdan/nginx-docker-demo:1.0 quay.io/sjourdan/nginx-docker-demo:1.0</strong></span>
<span class="strong"><strong>$ docker push quay.io/sjourdan/nginx-docker-demo:1.0</strong></span>
<span class="strong"><strong>The push refers to a repository [quay.io/sjourdan/nginx-docker-demo]</strong></span>
<span class="strong"><strong>82819c620e5d: Pushed</strong></span>
<span class="strong"><strong>d07a4f6d2067: Pushed</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note77"/>Note</h3><p>Quay.io has a very nice security feature: as Docker stores passwords in plain text on the local workstation, it's possible to generate an encrypted password from the <span class="strong"><strong>settings</strong></span> tab of your Quay.io account not only for Docker use, but also for Kubernetes, rkt, or Mesos. It's a much better option to use this encrypted password to log in to the service.</p></div></div><p>After a <a id="id1106" class="indexterm"/>while, in <a id="id1107" class="indexterm"/>the <span class="strong"><strong>Repository Tags</strong></span> tab of <a id="id1108" class="indexterm"/>our image, we'll get a <span class="strong"><strong>SECURITY SCAN</strong></span> summary:</p><div class="mediaobject"><img src="graphics/B05671_10_20.jpg" alt="How to do it…"/></div><p>In this <a id="id1109" class="indexterm"/>example, we have issues to investigate further:</p><div class="mediaobject"><img src="graphics/B05671_10_21.jpg" alt="How to do it…"/></div><p>Many <a id="id1110" class="indexterm"/>vulnerabilities are displayed, but don't be <a id="id1111" class="indexterm"/>frightened. In fact, none are fixable in <a id="id1112" class="indexterm"/>our case (click on <span class="strong"><strong>Only show fixable</strong></span> to <a id="id1113" class="indexterm"/>see what you can do). The reasons are multiple, such as no fix is available currently, the vulnerability doesn't concern the platform we're running on, and so on.</p><p>Here's a screenshot of a really vulnerable container and the Quay.io scanner giving helpful advice on the available fixes:</p><div class="mediaobject"><img src="graphics/B05671_10_22.jpg" alt="How to do it…"/></div><p>
<span class="strong"><strong>Quay.io Security Scanner</strong></span> will <a id="id1114" class="indexterm"/>also send reminders <a id="id1115" class="indexterm"/>by e-mail with a summary of the <a id="id1116" class="indexterm"/>vulnerabilities found on all the containers <a id="id1117" class="indexterm"/>it hosts on our account. So we don't have to worry too much about missing out on important security issues.</p><div class="section" title="Using Docker Security Scanning"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec228"/>Using Docker Security Scanning</h3></div></div></div><p>There's a <a id="id1118" class="indexterm"/>similar feature on the Docker Hub that uses a paid account, though still in preview at the time of this writing. By default, Docker Security Scanning is not activated, so we have to navigate to the billing tab of the account's interface and tick it to enable it:</p><div class="mediaobject"><img src="graphics/B05671_10_18.jpg" alt="Using Docker Security Scanning"/></div><p>From now on, when a new Docker image is created or pushed, the system will scan it quickly and report issues, tag by tag. To access the report summary, just click on the <span class="strong"><strong>Tag </strong></span>tab:</p><div class="mediaobject"><img src="graphics/B05671_10_19.jpg" alt="Using Docker Security Scanning"/></div><p>To see details (and the corresponding vulnerabilities), click on the tag number:</p><div class="mediaobject"><img src="graphics/B05671_10_17.jpg" alt="Using Docker Security Scanning"/></div><p>This layer <a id="id1119" class="indexterm"/>has clear issues! But don't follow this blindly and double-check the said vulnerabilities. All the critical issues in this example only concern Apple platforms and we're running Linux containers.</p></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec289"/>How it works…</h2></div></div></div><p>Under the hood, the Quay Security Scanner is based on Clair. Clair is an open source static analysis vulnerability scanner by CoreOS that we can run ourselves or build tools upon. It currently handles Debian, Ubuntu, Alpine, Oracle, and Red Hat security data sources. It gives access to a simple API. Our custom tool can send each Docker image layer we're interested in and get the corresponding vulnerabilities or fixes.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec290"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CoreOS <a id="id1120" class="indexterm"/>Clair at <a class="ulink" href="https://github.com/coreos/clair/">https://github.com/coreos/clair/</a></li></ul></div></div></div>
<div class="section" title="Sending Docker logs to AWS CloudWatch logs"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec115"/>Sending Docker logs to AWS CloudWatch logs</h1></div></div></div><p>When we <a id="id1121" class="indexterm"/>run dozens or hundreds of containers in production, hopefully on a clustered container platform, it soon becomes difficult and tedious to read, search, and process logs—just like it was before when containers with services ran on dozens or hundreds of physical or virtual servers. The problem is that traditional solutions don't work out of the box to handle Docker logs. Luckily, AWS has a nice and easy log-aggregating service, named AWS CloudWatch. Docker has a logging driver just for it. We'll send our Tomcat logs to it right away!</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec291"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Docker installation</li><li class="listitem" style="list-style-type: disc">An AWS account</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec292"/>How to do it…</h2></div></div></div><p>To use <a id="id1122" class="indexterm"/>AWS CloudWatch Logs, we need at least one <span class="strong"><strong>log group</strong></span>. Use this book's chapter on Terraform code to create a CloudWatch Logs group and a dedicated IAM user, or manually create both.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note78"/>Note</h3><p>As always, with AWS, it's highly recommended that you use a dedicated IAM user for each AWS key pair we'll use. In our case, we can associate the prebuilt IAM policy, named CloudWatchLogsFullAccess, with a new dedicated user in order to be up and running quickly in a secured way.</p></div></div><p>The Docker daemon needs to run with the AWS credentials in the memory—it's not information we pass to containers, as it's handled by the Docker daemon's log driver. To give the Docker daemon access to the keys we created, let's create an added <code class="literal">systemd</code> configuration file for the Docker service in <code class="literal">/etc/systemd/system/docker.service.d/aws.conf</code>:</p><div class="informalexample"><pre class="programlisting">[Service]
Environment="AWS_ACCESS_KEY_ID=AKIAJ..."
Environment="AWS_SECRET_ACCESS_KEY=SW+jdHKd.."</pre></div><p>Don't forget to reload the <code class="literal">systemd</code> daemon and restart Docker to apply the changes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo systemctl daemon-reload</strong></span>
<span class="strong"><strong>$ sudo systemctl restart docker</strong></span>
</pre></div><p>We're now <a id="id1123" class="indexterm"/>ready to talk to the AWS APIs through the Docker daemon.</p><div class="section" title="Using the Docker run"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec229"/>Using the Docker run</h3></div></div></div><p>Here's a <a id="id1124" class="indexterm"/>simple way to execute the Tomcat 9 container that uses the <code class="literal">awslogs</code> driver. Utilize the CloudWatch log group named <code class="literal">docker_logs</code> on the <code class="literal">us-east-1</code> data center and automatically create a new stream named <code class="literal">www</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo docker run -d -p 80:8080 --log-driver="awslogs" --log-opt awslogs-region="us-east-1" --log-opt awslogs-group="docker_logs" --log-opt awslogs-stream="www" tomcat:9</strong></span>
</pre></div><p>Navigating over the AWS Console, the new log stream will appear under <span class="strong"><strong>Search Log Group</strong></span>:</p><div class="mediaobject"><img src="graphics/B05671_10_23.jpg" alt="Using the Docker run"/></div><p>Clicking on the log stream name will give us access to all the output logs from our Tomcat container:</p><div class="mediaobject"><img src="graphics/B05671_10_24.jpg" alt="Using the Docker run"/></div><p>We now <a id="id1125" class="indexterm"/>have access to unlimited log storage and search features, and the amount of effort we put was very limited!</p></div><div class="section" title="Using docker-compose"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec230"/>Using docker-compose</h3></div></div></div><p>It's also <a id="id1126" class="indexterm"/>possible to configure the logging driver using Docker Compose. Here's how it works with creating a log stream named <code class="literal">tomcat</code> under the same log group in <code class="literal">docker-compose.yml</code>:</p><div class="informalexample"><pre class="programlisting">version: '2'

services:
  tomcat:
    image: tomcat:9
    logging:
      driver: 'awslogs'
      options:
        awslogs-region: 'us-east-1'
        awslogs-group: 'docker_logs'
        awslogs-stream: 'tomcat'</pre></div><p>Launch the compose as usual:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo docker-compose up</strong></span>
<span class="strong"><strong>Creating network "ubuntu_default" with the default driver</strong></span>
<span class="strong"><strong>[...]</strong></span>
<span class="strong"><strong>tomcat_1  | WARNING: no logs are available with the 'awslogs' log driver</strong></span>
</pre></div><p>The <code class="literal">tomcat</code> CloudWatch <a id="id1127" class="indexterm"/>log stream is now automatically created and the logs flow into it.</p></div><div class="section" title="Using systemd"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec231"/>Using systemd</h3></div></div></div><p>Another <a id="id1128" class="indexterm"/>useful way to launch containers is through the use of systemd. Here's how to create a dynamically named log stream using the systemd unit name (in this case, <code class="literal">tomcat.service</code>). This is useful on platforms that use multiple instances of the same container to let them all send their logs separately. Here's a working Tomcat systemd service that is running Docker and sending the logs to a dynamically allocated stream name in <code class="literal">/etc/systemd/system/tomcat.service</code>:</p><div class="informalexample"><pre class="programlisting">[Unit]
Description=Tomcat Container Service
After=docker.service

[Service]
TimeoutStartSec=0
Restart=always
ExecStartPre=/usr/bin/docker pull tomcat:9
ExecStartPre=-/usr/bin/docker kill %n
ExecStartPre=-/usr/bin/docker rm %n
ExecStart=/usr/bin/docker run --rm -p 80:8080 --log-driver=awslogs --log-opt awslogs-region=us-east-1 --log-opt awslogs-group=docker_logs --log-opt awslogs-stream=%n --name %n tomcat:9
ExecStop=/usr/bin/docker stop %n

[Install]
WantedBy=multi-user.target</pre></div><p>Reload systemd and start the <code class="literal">tomcat</code> unit:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo systemctl daemon-reload</strong></span>
<span class="strong"><strong>$ sudo systemctl start tomcat</strong></span>
</pre></div><p>Now a third log stream is created with the service name, with the systemd unit logs streaming into it:</p><div class="mediaobject"><img src="graphics/B05671_10_25.jpg" alt="Using systemd"/></div><p>Enjoy a <a id="id1129" class="indexterm"/>centralized and powerful way of storing and accessing logs before you eventually process them!</p></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec293"/>There's more...</h2></div></div></div><p>The Docker daemon can stream logs not only to AWS, but also to the more common syslog. This enables a lot of options (such as having traditional <code class="literal">rsyslog</code> setups and online services compatible with the traditional format). Similarly, it not only sends the logs to <code class="literal">journald</code>, but also supports the Graylog or Logstash GELF log format. The Fluentd unified logging layer is also supported, while on the platform front, we find support for Splunk and Google Cloud together with AWS CloudWatch logs.</p></div></div>
<div class="section" title="Monitoring and getting information out of Docker"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec116"/>Monitoring and getting information out of Docker</h1></div></div></div><p>It's often <a id="id1130" class="indexterm"/>important to get some quick and <a id="id1131" class="indexterm"/>useful information out of our Docker system when weird problems arise or strange issues start to cripple our performance. What's going on in the system? Is there a container taking up all of the memory? Maybe one minor container just crashed and is eating up all of the CPU. All of this information shouldn't be hard to get, but they are precious for building quality containers. We'll see two tools quite fit for the job: the first one is simply the one shipped with Docker itself, and the second one is a totally different tool by Google named cAdvisor—a web user interface with a lot of useful and easy-to-get information.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec294"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Docker installation</li></ul></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec295"/>How to do it...</h2></div></div></div><p>There's a <a id="id1132" class="indexterm"/>few ways to get information out of Docker. We'll <a id="id1133" class="indexterm"/>explore the first one through the main Docker program.</p><div class="section" title="Using docker stats"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec232"/>Using docker stats</h3></div></div></div><p>To get <a id="id1134" class="indexterm"/>live metrics about the running containers (CPU, memory, and network), we can use the simple <code class="literal">docker stats</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker stats</strong></span>
<span class="strong"><strong>CONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O               BLOCK I/O             PIDS</strong></span>
<span class="strong"><strong>c2904d5b5c89        0.01%               892.9 MB / 8.326 GB   10.72%              258.2 GB / 10.27 GB   374 MB / 0 B          16</strong></span>
<span class="strong"><strong>0641790f1b30        3.36%               894.4 MB / 8.326 GB   10.74%              258.2 GB / 11.12 GB   419.1 MB / 0 B        16</strong></span>
<span class="strong"><strong>bc8d85e05be8        112.65%             891.4 MB / 8.326 GB   10.71%              179.6 GB / 536.5 GB   326.6 MB / 0 B        10</strong></span>
<span class="strong"><strong>a7be664792b3        0.02%               45.37 MB / 8.326 GB   0.54%               17.85 GB / 17.72 GB   18.78 MB / 110.6 kB   18</strong></span>
<span class="strong"><strong>ab2d4e922949        2.37%               70.34 MB / 8.326 GB   0.84%               83.15 MB / 550 MB     459.7 MB / 143.4 kB   17</strong></span>
<span class="strong"><strong>08e685124dfd        0.01%               192 MB / 8.326 GB     2.31%               8.76 MB / 42.11 MB    1.499 MB / 14.05 MB   3</strong></span>
<span class="strong"><strong>5893c5d6f43f        0.74%               546.1 MB / 8.326 GB   6.56%               46.74 MB / 40.22 MB   160.7 MB / 317.9 MB   74</strong></span>
<span class="strong"><strong>7f21e405bdee        5.23%               8.184 MB / 8.326 GB   0.10%               30.14 GB / 30.28 GB   8.192 kB / 0 B        7</strong></span>
</pre></div><p>It's, however, not overwhelmingly helpful as it's using containers' IDs and not names, and when running many containers, it can start becoming useless because it would be unreadable. So we can use a trick: ask the stats (<code class="literal">docker stats</code>) of all the running containers (<code class="literal">docker ps</code>) whose names we extracted using a Go template formatter <code class="literal">(--format</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker stats $(docker ps --format '{{.Names}}')</strong></span>
<span class="strong"><strong>CONTAINER                   CPU %               MEM USAGE / LIMIT     MEM %               NET I/O               BLOCK I/O             PIDS</strong></span>
<span class="strong"><strong>sm_streammachine-slave_2    18.34%              889.4 MB / 8.326 GB   10.68%              258.2 GB / 10.27 GB   374 MB / 0 B          16</strong></span>
<span class="strong"><strong>sm_streammachine-slave_1    28.39%              900.1 MB / 8.326 GB   10.81%              258.2 GB / 11.12 GB   419.1 MB / 0 B        16</strong></span>
<span class="strong"><strong>sm_streammachine-master_1   1.89%               890.4 MB / 8.326 GB   10.69%              179.6 GB / 536.5 GB   326.6 MB / 0 B        10</strong></span>
<span class="strong"><strong>sm_proxy_1                  0.02%               45.37 MB / 8.326 GB   0.54%               17.85 GB / 17.72 GB   18.78 MB / 110.6 kB   18</strong></span>
<span class="strong"><strong>sm_cadvisor_1               1.62%               70.34 MB / 8.326 GB   0.84%               83.16 MB / 550 MB     459.7 MB / 143.4 kB   17</strong></span>
<span class="strong"><strong>sm_analytics_1              0.01%               192 MB / 8.326 GB     2.31%               8.76 MB / 42.11 MB    1.499 MB / 14.05 MB   3</strong></span>
<span class="strong"><strong>sm_elasticsearch_1          0.72%               546.1 MB / 8.326 GB   6.56%               46.74 MB / 40.22 MB   160.7 MB / 317.9 MB   74</strong></span>
<span class="strong"><strong>sm_streamer_1               8.17%               8.184 MB / 8.326 GB   0.10%               30.15 GB / 30.29 GB   8.192 kB / 0 B        7</strong></span>
</pre></div></div><div class="section" title="Using Google's cAdvisor tool"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec233"/>Using Google's cAdvisor tool</h3></div></div></div><p>Google <a id="id1135" class="indexterm"/>created a nice web tool to see what's going on in <a id="id1136" class="indexterm"/>machines that run containers: <span class="strong"><strong>cAdvisor</strong></span>. It collects, organizes, and displays metrics about resource usage, container by container, on a given host. Though not interactive, it's still powerful enough, given how easy it is to install and use. To install and use it, simply run the cAdvisor Docker image with volume access to all of the required system information, such as the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo docker run \</strong></span>
<span class="strong"><strong>  --volume=/:/rootfs:ro \</strong></span>
<span class="strong"><strong>  --volume=/var/run:/var/run:rw \</strong></span>
<span class="strong"><strong>  --volume=/sys:/sys:ro \</strong></span>
<span class="strong"><strong>  --volume=/var/lib/docker/:/var/lib/docker:ro \</strong></span>
<span class="strong"><strong>  --publish=8080:8080 \</strong></span>
<span class="strong"><strong>  --detach=true \</strong></span>
<span class="strong"><strong>  --name=cadvisor \</strong></span>
<span class="strong"><strong>  google/cadvisor:latest</strong></span>
</pre></div><p>Or, if using <code class="literal">docker-compose</code>:</p><div class="informalexample"><pre class="programlisting">  cadvisor:
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - "8080:8080"
    image: google/cadvisor:latest
    restart: always</pre></div><p>Navigating to the host's <code class="literal">8080</code> port (or whatever port you choose to publish) with a web browser will present a web interface where we can navigate and see graphical information <a id="id1137" class="indexterm"/>about container usage on the host:</p><div class="mediaobject"><img src="graphics/B05671_10_27.jpg" alt="Using Google's cAdvisor tool"/></div><p>Or, we may have more general gauges giving live indication of resource usage:</p><div class="mediaobject"><img src="graphics/B05671_10_26.jpg" alt="Using Google's cAdvisor tool"/></div><p>A very <a id="id1138" class="indexterm"/>useful process table with top-like data from the underlying host is also available with a container-aware context. All of these pieces of data are browsable and they help you gain more in-depth information about a specific container and its content and usage:</p><div class="mediaobject"><img src="graphics/B05671_10_36.jpg" alt="Using Google's cAdvisor tool"/></div><p>cAdvisor <a id="id1139" class="indexterm"/>can also be plugged in to many backend storage systems, such as Prometheus, ElasticSearch, InfluxDB, Redis, statsD, and so on.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note79"/>Note</h3><p>If you plan to let cAdvisor run permanently, it is a good idea to restrict access using simple HTTP authentication. This is supported out of the box by cAdvisor using <code class="literal">--http_auth_file /cadvisor.htpasswd --http_auth_realm my_message</code>.</p></div></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec296"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">cAdvisor <a id="id1140" class="indexterm"/>GitHub at <a class="ulink" href="https://github.com/google/cadvisor">https://github.com/google/cadvisor</a></li><li class="listitem" style="list-style-type: disc">cAdvisor <a id="id1141" class="indexterm"/>storage backends at <a class="ulink" href="https://github.com/google/cadvisor/blob/master/docs/storage/README.md">https://github.com/google/cadvisor/blob/master/docs/storage/README.md</a></li></ul></div></div></div>
<div class="section" title="Debugging containers using sysdig"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec117"/>Debugging containers using sysdig</h1></div></div></div><p>Sysdig is <a id="id1142" class="indexterm"/>an awesome tool that can be used for many <a id="id1143" class="indexterm"/>purposes, including monitoring, logging, process debugging, network analyzing, and exploring a system in depth. Plus, it includes fantastic Linux container support. It's also scriptable and can be fed with recorded real traffic packet captures for offline analysis. It's an incredible tool that each and every person working with containers should at least know the basics of, and as infrastructure developers used to working with code, we know how important debugging tools are. This is no different with sysdig, and we'll now discover some of its fantastic features related to containers.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec297"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Docker installation</li><li class="listitem" style="list-style-type: disc">Sysdig installed and running on the host</li></ul></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec298"/>How to do it...</h2></div></div></div><p>Installing <a id="id1144" class="indexterm"/>sysdig is easy on most platforms, including CoreOS (<a class="ulink" href="http://www.sysdig.org/install/">http://www.sysdig.org/install/</a>). However, if you're in a hurry, here's a one liner that will do the job of installing Sysdig on your Linux host. We'd probably choose a better way to deploy it programmatically though, such as Ansible or Chef, through a Docker container or not:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -s https://s3.amazonaws.com/download.draios.com/stable/install-sysdig | sudo bash</strong></span>
</pre></div><p>Here's how to get an htop-like view of all the running containers on the system:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo csysdig --view=containers</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B05671_10_28.jpg" alt="How to do it..."/></div><p>Navigating <a id="id1145" class="indexterm"/>to the <span class="strong"><strong>F2/Views</strong></span> menu helps you enter <a id="id1146" class="indexterm"/>many different options to see what's running, from processes to syslog to open files and even the Kubernetes, Marathon, or Mesos integration. Want to see which container is draining all of the IO? You're at the right place:</p><div class="mediaobject"><img src="graphics/B05671_10_32.jpg" alt="How to do it..."/></div><p>Here's an <a id="id1147" class="indexterm"/>example of a Tomcat container with a view <a id="id1148" class="indexterm"/>of all the local and remote connections, IPs, ports, protocols, bandwidth, IOs, and the corresponding commands—terribly useful to find suspicious behavior:</p><div class="mediaobject"><img src="graphics/B05671_10_30.jpg" alt="How to do it..."/></div><p>Another <a id="id1149" class="indexterm"/>useful tool is <code class="literal">F5</code>/<code class="literal">Echo</code>, grabbing what's transiting on this container: (un)encrypted content, logs, output, and more. This is also <a id="id1150" class="indexterm"/>very useful to maybe catch something wrong with a container acting weird:</p><div class="mediaobject"><img src="graphics/B05671_10_31.jpg" alt="How to do it..."/></div><p>Another <a id="id1151" class="indexterm"/>very powerful tool from sysdig is <code class="literal">F6</code>/<code class="literal">Dig</code>. This basically offers nothing less than a full-fledged <code class="literal">strace</code> for a container; imagine <a id="id1152" class="indexterm"/>the debugging power it has:</p><div class="mediaobject"><img src="graphics/B05671_10_33.jpg" alt="How to do it..."/></div><p>The <code class="literal">F8</code>/<code class="literal">Actions</code> feature is a full Docker command integration tool available right from inside <a id="id1153" class="indexterm"/>sysdig. Select a container and we'll be able to <a id="id1154" class="indexterm"/>enter it, read logs, see its image history, kill it, and more:</p><div class="mediaobject"><img src="graphics/B05671_10_34.jpg" alt="How to do it..."/></div><p>Those commands are also always available right from the main interface: want to gain a shell on this selected container? Just type <code class="literal">b</code>.</p><p>These <a id="id1155" class="indexterm"/>are just a few of the many powerful things <a id="id1156" class="indexterm"/>we can do with Sysdig using Docker containers.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec299"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">More <a id="id1157" class="indexterm"/>general sysdig usage examples at <a class="ulink" href="http://www.sysdig.org/wiki/sysdig-examples/">http://www.sysdig.org/wiki/sysdig-examples/</a></li></ul></div></div></div></body></html>