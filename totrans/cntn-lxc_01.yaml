- en: Chapter 1. Introduction to Linux Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, deploying applications inside some sort of a Linux container is a
    widely adopted practice, primarily due to the evolution of the tooling and the
    ease of use it presents. Even though Linux containers, or operating-system-level
    virtualization, in one form or another, have been around for more than a decade,
    it took some time for the technology to mature and enter mainstream operation.
    One of the reasons for this is the fact that hypervisor-based technologies such
    as KVM and Xen were able to solve most of the limitations of the Linux kernel
    during that period and the overhead it presented was not considered an issue.
    However, with the advent of kernel namespaces and **control groups** (**cgroups**)
    the notion of a *light-weight virtualization* became possible through the use
    of containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of the OS kernel and its early limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between containers and platform virtualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts and terminology related to namespaces and cgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example use of process resource isolation and management with network namespaces
    and cgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OS kernel and its early limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The current state of Linux containers is a direct result of the problems that
    early OS designers were trying to solve – managing memory, I/O, and process scheduling
    in the most efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, only a single process could be scheduled for work, wasting precious
    CPU cycles if blocked on an I/O operation. The solution to this problem was to
    develop better CPU schedulers, so more work can be allocated in a *fair* way for
    maximum CPU utilization. Even though the modern schedulers, such as the **Completely
    Fair Scheduler** (**CFS**) in Linux do a great job of allocating fair amounts
    of time to each process, there's still a strong case for being able to give higher
    or lower priority to a process and its subprocesses. Traditionally, this can be
    accomplished by the `nice()` system call, or real-time scheduling policies, however,
    there are limitations to the level of granularity or control that can be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, before the advent of virtual memory, multiple processes would allocate
    memory from a shared pool of physical memory. The virtual memory provided some
    form of memory isolation per process, in the sense that processes would have their
    own address space, and extend the available memory by means of a swap, but still
    there wasn't a good way of limiting how much memory each process and its children
    can use.
  prefs: []
  type: TYPE_NORMAL
- en: To further complicate the matter, running different workloads on the same physical
    server usually resulted in a negative impact on all running services. A memory
    leak or a kernel panic could cause one application to bring the entire operating
    system down. For example, a web server that is mostly memory bound and a database
    service that is I/O heavy running together became problematic. In an effort to
    avoid such scenarios, system administrators would separate the various applications
    between a pool of servers, leaving some machines underutilized, especially at
    certain times during the day, when there was not much work to be done. This is
    a similar problem as a single running process blocked on I/O operation is a waste
    of CPU and memory resources.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to these problems is the use of hypervisor based virtualization,
    containers, or the combination of both.
  prefs: []
  type: TYPE_NORMAL
- en: The case for Linux containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hypervisor as part of the operating system is responsible for managing the
    life cycle of virtual machines, and has been around since the early days of mainframe
    machines in the late 1960s. Most modern virtualization implementations, such as
    Xen and KVM, can trace their origins back to that era. The main reason for the
    wide adoption of these virtualization technologies around 2005 was the need to
    better control and utilize the ever-growing clusters of compute resources. The
    inherited security of having an extra layer between the virtual machine and the
    host OS was a good selling point for the security minded, though as with any other
    newly adopted technology there were security incidents.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the adoption of full virtualization and paravirtulization significantly
    improved the way servers are utilized and applications provisioned. In fact, virtualization
    such as KVM and Xen is still widely used today, especially in multitenant clouds
    and cloud technologies such as OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypervisors provide the following benefits, in the context of the problems
    outlined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: Ability to run different operating systems on the same physical server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More granular control over resource allocation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process isolation – a kernel panic on the virtual machine will not effect the
    host OS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate network stack and the ability to control traffic per virtual machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce capital and operating cost, by simplification of data center management
    and better utilization of available server resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arguably the main reason against using any sort of virtualization technology
    today is the inherited overhead of using multiple kernels in the same OS. It would
    be much better, in terms of complexity, if the host OS can provide this level
    of isolation, without the need for hardware extensions in the CPU, or the use
    of emulation software such as QEMU, or even kernel modules such as KVM. Running
    an entire operating system on a virtual machine, just to achieve a level of confinement
    for a single web server, is not the most efficient allocation of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Over the last decade, various improvements to the Linux kernel were made to
    allow for similar functionality, but with less overhead – most notably the kernel
    namespaces and cgroups. One of the first notable technologies to leverage those
    changes was LXC, since kernel 2.6.24 and around the 2008 time frame. Even though
    LXC is not the oldest container technology, it helped fuel the container revolution
    we see today.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main benefits of using LXC include:'
  prefs: []
  type: TYPE_NORMAL
- en: Lesser overheads and complexity than running a hypervisor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smaller footprint per container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start times in the millisecond range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native kernel support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth mentioning that containers are not inherently as secure as having
    a hypervisor between the virtual machine and the host OS. However, in recent years,
    great progress has been made to narrow that gap using **Mandatory Access Control**
    (**MAC**) technologies such as SELinux and AppArmor, kernel capabilities, and
    cgroups, as demonstrated in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Linux namespaces – the foundation of LXC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Namespaces are the foundation of lightweight process virtualization. They enable
    a process and its children to have different views of the underlying system. This
    is achieved by the addition of the `unshare()` and `setns()` system calls, and
    the inclusion of six new constant flags passed to the `clone()`, `unshare()`,
    and `setns()` system calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clone()`: This creates a new process and attaches it to a new specified namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unshare()`: This attaches the current process to a new specified namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setns()`: This attaches a process to an already existing namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are six namespaces currently in use by LXC, with more being developed:'
  prefs: []
  type: TYPE_NORMAL
- en: Mount namespaces, specified by the `CLONE_NEWNS` flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UTS namespaces, specified by the `CLONE_NEWUTS` flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IPC namespaces, specified by the `CLONE_NEWIPC` flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PID namespaces, specified by the `CLONE_NEWPID` flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User namespaces, specified by the `CLONE_NEWUSER` flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network namespaces, specified by the `CLONE_NEWNET` flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's have a look at each in more detail and see some userspace examples, to
    help us better understand what happens under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Mount namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mount namespaces first appeared in kernel 2.4.19 in 2002 and provided a separate
    view of the filesystem mount points for the process and its children. When mounting
    or unmounting a filesystem, the change will be noticed by all processes because
    they all share the same default namespace. When the `CLONE_NEWNS` flag is passed
    to the `clone()` system call, the new process gets a copy of the calling process
    mount tree that it can then change without affecting the parent process. From
    that point on, all mounts and unmounts in the default namespace will be visible
    in the new namespace, but changes in the per-process mount namespaces will not
    be noticed outside of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `clone()` prototype is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'An example call that creates a child process in a new mount namespace looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When the child process is created, it executes the `childFunc` function, which
    will perform its work in the new mount namespace.
  prefs: []
  type: TYPE_NORMAL
- en: The `util-linux` package provides userspace tools that implement the `unshare()`
    call, which effectively unshares the indicated namespaces from the parent process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First open a terminal and create a directory in `/tmp` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, move the current `bash` process to its own mount namespace by passing
    the mount flag to `unshare`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `bash` process is now in a separate namespace. Let''s check the associated
    inode number of the namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a temporary mount point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, make sure you can see the mount point from the newly created namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As expected, the mount point is visible because it is part of the namespace
    we created and the current `bash` process is running from.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, start a new terminal session and display the namespace inode ID from
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice how it's different from the mount namespace on the other terminal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, check if the mount point is visible in the new terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Not surprisingly, the mount point is not visible from the default namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of LXC, mount namespaces are useful because they provide a way
    for a different filesystem layout to exist inside the container. It's worth mentioning
    that before the mount namespaces, a similar process confinement could be achieved
    with the `chroot()` system call, however `chroot` does not provide the same per-process
    isolation as mount namespaces do.
  prefs: []
  type: TYPE_NORMAL
- en: UTS namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Unix Timesharing** (**UTS**) namespaces provide isolation for the hostname
    and domain name, so that each LXC container can maintain its own identifier as
    returned by the `hostname -f` command. This is needed for most applications that
    rely on a properly set hostname.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a `bash` session in a new UTS namespace, we can use the `unshare`
    utility again, which uses the `unshare()` system call to create the namespace
    and the `execve()` system call to execute `bash` in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As the preceding output shows, the hostname inside the namespace is now `uts-namespace`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, from a different terminal, check the hostname again to make sure it has
    not changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the hostname only changed in the new UTS namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the actual system calls that the `unshare` command uses, we can run
    the `strace` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the output we can see that the `unshare` command is indeed using the `unshare()`
    and `execve()` system calls and the `CLONE_NEWUTS` flag to specify new UTS namespace.
  prefs: []
  type: TYPE_NORMAL
- en: IPC namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Interprocess Communication** (**IPC**) namespaces provide isolation for
    a set of IPC and synchronization facilities. These facilities provide a way of
    exchanging data and synchronizing the actions between threads and processes. They
    provide primitives such as semaphores, file locks, and mutexes among others, that
    are needed to have true process separation in a container.
  prefs: []
  type: TYPE_NORMAL
- en: PID namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Process ID** (**PID**) namespaces provide the ability for a process to
    have an ID that already exists in the default namespace, for example an ID of
    `1`. This allows for an init system to run in a container with various other processes,
    without causing a collision with the rest of the PIDs on the same OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this concept, open up `pid_namespace.c`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we include the headers and define the `childFunc` function that the
    `clone()` system call will use. The function prints out the child PID using the
    `getpid()` system call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the `main()` function, we specify the stack size and call `clone()`, passing
    the child function `childFunc`, the stack pointer, the `CLONE_NEWPID` flag, and
    the `SIGCHLD` signal. The `CLONE_NEWPID` flag instructs `clone()` to create a
    new PID namespace and the `SIGCHLD` flag notifies the parent process when one
    of its children terminates. The parent process will block on `waitpid()` if the
    child process has not terminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and then run the program with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From the output, we can see that the child process has a PID of `1` inside its
    namespace and `17705` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that error handling has been omitted from the code examples for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: User namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The user namespaces allow a process inside a namespace to have a different user
    and group ID than that in the default namespace. In the context of LXC, this allows
    for a process to run as `root` inside the container, while having a non-privileged
    ID outside. This adds a thin layer of security, because braking out for the container
    will result in a non-privileged user. This is possible because of kernel 3.8,
    which introduced the ability for non-privileged processes to create user namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a new user namespace as a non-privileged user and have `root` inside,
    we can use the `unshare` utility. Let''s install the latest version from source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the `clone()` system call with the `CLONE_NEWUSER` flag to
    create a process in a user namespace, as demonstrated by the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After compilation and execution, the output looks similar to this when run
    as `root` - UID of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Network namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network namespaces provide isolation of the networking resources, such as network
    devices, addresses, routes, and firewall rules. This effectively creates a logical
    copy of the network stack, allowing multiple processes to listen on the same port
    from multiple namespaces. This is the foundation of networking in LXC and there
    are quite a lot of other use cases where this can come in handy.
  prefs: []
  type: TYPE_NORMAL
- en: The `iproute2` package provides very useful userspace tools that we can use
    to experiment with the network namespaces, and is installed by default on almost
    all Linux systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s always the default network namespace, referred to as the root namespace,
    where all network interfaces are initially assigned. To list the network interfaces
    that belong to the default namespace run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this case, there are two interfaces – `lo` and `eth0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To list their configuration, we can run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, to list the routes from the root network namespace, execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create two new network namespaces called `ns1` and `ns2` and list them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the new network namespaces, we can execute commands inside
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows that in the `ns1` namespace, there's only one network
    interface, the loopback - `lo` interface, and it's in a `DOWN` state.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also start a new `bash` session inside the namespace and list the interfaces
    in a similar way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This is more convenient for running multiple commands than specifying each,
    one at a time. The two network namespaces are not of much use if not connected
    to anything, so let's connect them to each other. To do this we'll use a software
    bridge called Open vSwitch.
  prefs: []
  type: TYPE_NORMAL
- en: Open vSwitch works just as a regular network bridge and then it forwards frames
    between virtual ports that we define. Virtual machines such as KVM, Xen, and LXC
    or Docker containers can then be connected to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most Debian-based distributions such as Ubuntu provide a package, so let''s
    install that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This installs and starts the Open vSwitch daemon. Time to create the bridge;
    we''ll name it `OVS-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you would like to experiment with the latest version of Open vSwitch, you
    can download the source code from [http://openvswitch.org/download/](http://openvswitch.org/download/)
    and compile it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The newly created bridge can now be seen in the root namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to connect both network namespaces, let''s first create a virtual
    pair of interfaces for each namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding two commands create four virtual interfaces `eth1-ns1`, `eth1-ns2`
    and `veth-ns1`, `veth-ns2`. The names are arbitrary.
  prefs: []
  type: TYPE_NORMAL
- en: 'To list all interfaces that are part of the root network namespace, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assign the `eth1-ns1` and `eth1-ns2` interfaces to the `ns1` and `ns2`
    namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, confirm they are visible from inside each network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Notice, how each network namespace now has two interfaces assigned – `loopback`
    and `eth1-ns*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we list the devices from the root namespace, we should see that the interfaces
    we just moved to `ns1` and `ns2` namespaces are no longer visible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s time to connect the other end of the two virtual pipes, the `veth-ns1`
    and `veth-ns2` interfaces to the bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, it's apparent that the bridge now has two ports,
    `veth-ns1` and `veth-ns2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing left to do is bring the network interfaces up and assign IP
    addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly for the `ns2` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![Network namespaces](img/image_01_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With this, we established a connection between both `ns1` and `ns2` network
    namespaces through the Open vSwitch bridge. To confirm, let''s use `ping`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Open vSwitch allows for assigning VLAN tags to network interfaces, resulting
    in traffic isolation between namespaces. This can be helpful in a scenario where
    you have multiple namespaces and you want to have connectivity between some of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how to tag the virtual interfaces on the
    `ns1` and `ns2` namespaces, so that the traffic will not be visible from each
    of the two network namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Both the namespaces should now be isolated in their own VLANs and `ping` should
    fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the `unshare` utility that we saw in the mount and UTC namespaces
    examples to create a new network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Resource management with cgroups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cgroups are kernel features that allows fine-grained control over resource allocation
    for a single process, or a group of processes, called **tasks**. In the context
    of LXC this is quite important, because it makes it possible to assign limits
    to how much memory, CPU time, or I/O, any given container can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cgroups we are most interested in are described in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Subsystem** | **Description** | **Defined in** |'
  prefs: []
  type: TYPE_TB
- en: '| `cpu` | Allocates CPU time for tasks | `kernel/sched/core.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuacct` | Accounts for CPU usage | `kernel/sched/core.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset` | Assigns CPU cores to tasks | `kernel/cpuset.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `memory` | Allocates memory for tasks | `mm/memcontrol.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio` | Limits the I/O access to devices | `block/blk-cgroup.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `devices` | Allows/denies access to devices | `security/device_cgroup.c`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `freezer` | Suspends/resumes tasks | `kernel/cgroup_freezer.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `net_cls` | Tags network packets | `net/sched/cls_cgroup.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `net_prio` | Prioritizes network traffic | `net/core/netprio_cgroup.c` |'
  prefs: []
  type: TYPE_TB
- en: '| `hugetlb` | Limits the HugeTLB | `mm/hugetlb_cgroup.c` |'
  prefs: []
  type: TYPE_TB
- en: Cgroups are organized in hierarchies, represented as directories in a **Virtual
    File System** (**VFS**). Similar to process hierarchies, where every process is
    a descendent of the `init` or `systemd` process, cgroups inherit some of the properties
    of their parents. Multiple cgroups hierarchies can exist on the system, each one
    representing a single or group of resources. It is possible to have hierarchies
    that combine two or more subsystems, for example, memory and I/O, and tasks assigned
    to a group will have limits applied on those resources.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are interested in how the different subsystems are implemented in the
    kernel, install the kernel source and have a look at the C files, shown in the
    third column of the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram helps visualize a single hierarchy that has two subsystems—CPU
    and I/O—attached to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Resource management with cgroups](img/image_01_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Cgroups can be used in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By manually manipulating files and directories on a mounted VFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using userspace tools provided by various packages such as `cgroup-bin` on Debian/Ubuntu
    and `libcgroup` on RHEL/CentOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's have a look at few practical examples on how to use cgroups to limit resources.
    This will help us get a better understanding of how containers work.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting I/O throughput
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's assume we have two applications running on a server that are heavily I/O
    bound: `app1` and `app2`. We would like to give more bandwidth to `app1` during
    the day and to `app2` during the night. This type of I/O throughput prioritization
    can be accomplished using the `blkio` subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s attach the `blkio` subsystem by mounting the `cgroup` VFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create two priority groups, which will be part of the same `blkio` hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to acquire the PIDs of the `app1` and `app2` processes and assign them
    to the `high_io` and `low_io` groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![Limiting I/O throughput](img/image_01_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The blkio hierarchy we've created
  prefs: []
  type: TYPE_NORMAL
- en: The `tasks` file is where we define what processes/tasks the limit should be
    applied on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s set a ratio of 10:1 for the `high_io` and `low_io` cgroups.
    Tasks in those cgroups will immediately use only the resources made available
    to them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `blkio.weight` file defines the weight of I/O access available to a process
    or group of processes, with values ranging from 100 to 1,000\. In this example,
    the values of `1000` and `100` create a ratio of 10:1.
  prefs: []
  type: TYPE_NORMAL
- en: With this, the low priority application, `app2` will use only about 10 percent
    of the I/O operations available, whereas the high priority application, `app1`,
    will use about 90 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you list the contents of the `high_io` directory on Ubuntu you will see
    the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output you can see that only some files are writeable. This
    depends on various OS settings, such as what I/O scheduler is being used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve already seen what the `tasks` and `blkio.weight` files are used for.
    The following is a short description of the most commonly used files in the `blkio`
    subsystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.io_merged` | Total number of reads/writes, sync, or async merged into
    requests |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.io_queued` | Total number of read/write, sync, or async requests queued
    up at any given time |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.io_service_bytes` | The number of bytes transferred to or from the
    specified device |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.io_serviced` | The number of I/O operations issued to the specified
    device |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.io_service_time` | Total amount of time between request dispatch and
    request completion in nanoseconds for the specified device |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.io_wait_time` | Total amount of time the I/O operations spent waiting
    in the scheduler queues for the specified device |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.leaf_weight` | Similar to `blkio.weight` and can be applied to the
    **Completely Fair Queuing** (**CFQ**) I/O scheduler |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.reset_stats` | Writing an integer to this file will reset all statistics
    |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.sectors` | The number of sectors transferred to or from the specified
    device |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.throttle.io_service_bytes` | The number of bytes transferred to or
    from the disk |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.throttle.io_serviced` | The number of I/O operations issued to the
    specified disk |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.time` | The disk time allocated to a device in milliseconds |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.weight` | Specifies weight for a cgroup hierarchy |'
  prefs: []
  type: TYPE_TB
- en: '| `blkio.weight_device` | Same as `blkio.weight`, but specifies a block device
    to apply the limit on |'
  prefs: []
  type: TYPE_TB
- en: '| `tasks` | Attach tasks to the cgroup |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One thing to keep in mind is that writing to the files directly to make changes
    will not persist after the server restarts. Later in this chapter, you will learn
    how to use the userspace tools to generate persistent configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting memory usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `memory` subsystem controls how much memory is presented to and available
    for use by processes. This can be particularly useful in multitenant environments
    where better control over how much memory a user process can utilize is needed,
    or to limit memory hungry applications. Containerized solutions like LXC can use
    the `memory` subsystem to manage the size of the instances, without needing to
    restart the entire container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `memory` subsystem performs resource accounting, such as tracking the utilization
    of anonymous pages, file caches, swap caches, and general hierarchical accounting,
    all of which presents an overhead. Because of this, the `memory` cgroup is disabled
    by default on some Linux distributions. If the following commands below fail you''ll
    need to enable it, by specifying the following GRUB parameter and restarting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s mount the `memory` cgroup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then set the `app1` memory to 1 GB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Limiting memory usage](img/image_01_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The memory hierarchy for the app1 process
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the `blkio` subsystem, the `tasks` file is used to specify the PID
    of the processes we are adding to the cgroup hierarchy, and the `memory.limit_in_bytes`
    specifies how much memory is to be made available in bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `app1` memory hierarchy contains the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The files and their function in the memory subsystem are described in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.failcnt` | Shows the total number of memory limit hits |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.force_empty` | If set to `0`, frees memory used by tasks |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.failcnt` | Shows the total number of kernel memory limit hits
    |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.limit_in_bytes` | Sets or shows kernel memory hard limit |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.max_usage_in_bytes` | Shows maximum kernel memory usage |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.tcp.failcnt` | Shows the number of TCP buffer memory limit hits
    |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.tcp.limit_in_bytes` | Sets or shows hard limit for TCP buffer
    memory |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.tcp.max_usage_in_bytes` | Shows maximum TCP buffer memory usage
    |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.tcp.usage_in_bytes` | Shows current TCP buffer memory |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.kmem.usage_in_bytes` | Shows current kernel memory |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.limit_in_bytes` | Sets or shows memory usage limit |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.max_usage_in_bytes` | Shows maximum memory usage |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.move_charge_at_immigrate` | Sets or shows controls of moving charges
    |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.numa_stat` | Shows the number of memory usage per NUMA node |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.oom_control` | Sets or shows the OOM controls |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.pressure_level` | Sets memory pressure notifications |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.soft_limit_in_bytes` | Sets or shows soft limit of memory usage |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.stat` | Shows various statistics |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.swappiness` | Sets or shows swappiness level |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.usage_in_bytes` | Shows current memory usage |'
  prefs: []
  type: TYPE_TB
- en: '| `memory.use_hierarchy` | Sets memory reclamation from child processes |'
  prefs: []
  type: TYPE_TB
- en: '| `tasks` | Attaches tasks to the cgroup |'
  prefs: []
  type: TYPE_TB
- en: 'Limiting the memory available to a process might trigger the **Out of Memory**
    (**OOM**) killer, which might kill the running task. If this is not the desired
    behavior and you prefer the process to be suspended waiting for memory to be freed,
    the OOM killer can be disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The `memory` cgroup presents a wide slew of accounting statistics in the `memory.stat`
    file, which can be of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need to start a new task in the `app1` memory hierarchy you can move
    the current shell process into the `tasks` file, and all other processes started
    in this shell will be direct descendants and inherit the same cgroup properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The cpu and cpuset subsystems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `cpu` subsystem schedules CPU time to cgroup hierarchies and their tasks.
    It provides finer control over CPU execution time than the default behavior of
    the CFS.
  prefs: []
  type: TYPE_NORMAL
- en: The `cpuset` subsystem allows for assigning CPU cores to a set of tasks, similar
    to the `taskset` command in Linux.
  prefs: []
  type: TYPE_NORMAL
- en: The main benefits that the `cpu` and `cpuset` subsystems provide are better
    utilization per processor core for highly CPU bound applications. They also allow
    for distributing load between cores that are otherwise idle at certain times of
    the day. In the context of multitenant environments, running many LXC containers,
    `cpu` and `cpuset` cgroups allow for creating different instance sizes and container
    flavors, for example exposing only a single core per container, with 40 percent
    scheduled work time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s assume we have two processes `app1` and `app2`, and we
    would like `app1` to use 60 percent of the CPU time and `app2` only 40 percent.
    We start by mounting the `cgroup` VFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create two child hierarchies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Also assign CPU shares for each, where `app1` will get 60 percent and `app2`
    will get 40 percent of the scheduled time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we move the PIDs in the `tasks` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cpu` subsystem contains the following control files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s a brief explanation of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `cpu.cfs_period_us` | CPU resource reallocation in microseconds |'
  prefs: []
  type: TYPE_TB
- en: '| `cpu.cfs_quota_us` | Run duration of tasks in microseconds during one `cpu.cfs_perious_us
    period` |'
  prefs: []
  type: TYPE_TB
- en: '| `cpu.shares` | Relative share of CPU time available to the tasks |'
  prefs: []
  type: TYPE_TB
- en: '| `cpu.stat` | Shows CPU time statistics |'
  prefs: []
  type: TYPE_TB
- en: '| `tasks` | Attaches tasks to the cgroup |'
  prefs: []
  type: TYPE_TB
- en: 'The `cpu.stat` file is of particular interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate how the `cpuset` subsystem works, let''s create `cpuset` hierarchies
    named `app1`, containing CPUs 0 and 1\. The `app2` cgroup will contain only CPU
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'To check if the `app1` process is pinned to CPU 0 and 1, we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cpuset app1` hierarchy contains the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'A brief description of the control files is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.cpu_exclusive` | Checks if other `cpuset` hierarchies share the settings
    defined in the current group |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.cpus` | List of the physical numbers of the CPUs on which processes
    in that `cpuset` are allowed to execute |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.mem_exclusive` | Should the `cpuset` have exclusive use of its memory
    nodes |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.mem_hardwall` | Checks if each tasks'' user allocation be kept separate
    |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.memory_migrate` | Checks if a page in memory should migrate to a
    new node if the values in `cpuset.mems` change |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.memory_pressure` | Contains running average of the memory pressure
    created by the processes |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.memory_spread_page` | Checks if filesystem buffers should spread
    evenly across the memory nodes |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.memory_spread_slab` | Checks if kernel slab caches for file I/O operations
    should spread evenly across the `cpuset` |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.mems` | Specifies the memory nodes that tasks in this cgroup are
    permitted to access |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.sched_load_balance` | Checks if the kernel balance should load across
    the CPUs in the `cpuset` by moving processes from overloaded CPUs to less utilized
    CPUs |'
  prefs: []
  type: TYPE_TB
- en: '| `cpuset.sched_relax_domain_level` | Contains the width of the range of CPUs
    across which the kernel should attempt to balance loads |'
  prefs: []
  type: TYPE_TB
- en: '| `notify_on_release` | Checks if the hierarchy should receive special handling
    after it is released and no process are using it |'
  prefs: []
  type: TYPE_TB
- en: '| `tasks` | Attaches tasks to the cgroup |'
  prefs: []
  type: TYPE_TB
- en: The cgroup freezer subsystem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `freezer` subsystem can be used to suspend the current state of running
    tasks for the purposes of analyzing them, or to create a checkpoint that can be
    used to migrate the process to a different server. Another use case is when a
    process is negatively impacting the system and needs to be temporarily paused,
    without losing its current state data.
  prefs: []
  type: TYPE_NORMAL
- en: The next example shows how to suspend the execution of the top process, check
    its state, and then resume it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, mount the `freezer` subsystem and create the new hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'In a new terminal, start the `top` process and observe how it periodically
    refreshes. Back in the original terminal, add the PID of `top` to the `frozen_group`
    task file and observe its state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'To freeze the process, echo the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the top process output is not refreshing anymore, and upon inspection
    of its status file, you can see that it is now in the blocked state.
  prefs: []
  type: TYPE_NORMAL
- en: 'To resume it, execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspecting the `frozen_group` hierarchy yields the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The few files of interest are described in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `freezer.parent_freezing` | Shows the parent-state. Shows `0` if none of
    the cgroup''s ancestors is `FROZEN`; otherwise, `1`. |'
  prefs: []
  type: TYPE_TB
- en: '| `freezer.self_freezing` | Shows the self-state. Shows `0` if the self-state
    is `THAWED`; otherwise, `1`. |'
  prefs: []
  type: TYPE_TB
- en: '| `freezer.state` | Sets the self-state of the cgroup to either `THAWED` or
    `FROZEN`. |'
  prefs: []
  type: TYPE_TB
- en: '| `tasks` | Attaches tasks to the cgroup. |'
  prefs: []
  type: TYPE_TB
- en: Using userspace tools to manage cgroups and persist changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Working with the cgroups subsystems by manipulating directories and files directly
    is a fast and convenient way to prototype and test changes, however, this comes
    with few drawbacks, namely the changes made will not persist a server restart
    and there's not much error reporting or handling.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, there are packages that provide userspace  tools and daemons
    that are quite easy to use. Let's see a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the tools on Debian/Ubuntu, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'On RHEL/CentOS, execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'To mount all subsystems, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Notice from the preceding output the location of the cgroups - `/sys/fs/cgroup`.
    This is the default location on many Linux distributions and in most cases the
    various subsystems have already been mounted.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify what cgroup subsystems are in use, we can check with the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create a `blkio` hierarchy and add an already running process
    to it with `cgclassify`. This is similar to what we did earlier, by creating the
    directories and the files by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined the `high_io` and `low_io` cgroups and added a process
    to them, let''s generate a configuration file that can be used later to reapply
    the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'To start a new process in the `high_io` group, we can use the `cgexec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we started a new `bash` process in the `high_io cgroup`,
    as confirmed by looking at the `tasks` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To move an already running process to the `memory` subsystem, first we create
    the `high_prio` and `low_prio` groups and move the task with `cgclassify`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'To set the memory and CPU limits, we can use the `cgset` command. In contrast,
    remember that we used the `echo` command to manually move the PIDs and memory
    limits to the `tasks` and the `memory.limit_in_bytes` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'To see how the cgroup hierarchies look, we can use the `lscgroup` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output confirms the existence of the `blkio`, `memory`, and `cpu`
    hierarchies and their children.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once finished, you can delete the hierarchies with `cgdelete`, which deletes
    the respective directories on the VFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'To completely clear the cgroups, we can use the `cgclear` utility, which will
    unmount the cgroup directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Managing resources with systemd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the increased adoption of `systemd` as an init system, new ways of manipulating
    cgroups were introduced. For example, if the cpu controller is enabled in the
    kernel, `systemd` will create a cgroup for each service by default. This behavior
    can be changed by adding or removing cgroup subsystems in the configuration file
    of `systemd`, usually found at `/etc/systemd/system.conf`.
  prefs: []
  type: TYPE_NORMAL
- en: If multiple services are running on the server, the CPU resources will be shared
    equally among them by default, because `systemd` assigns equal weights to each.
    To change this behavior for an application, we can edit its service file and define
    the CPU shares, allocated memory, and I/O.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how to change the CPU shares, memory, and
    I/O limits for the nginx process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the changes first reload `systemd`, then nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: This will create and update the necessary control files in `/sys/fs/cgroup/systemd`
    and apply the limits.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advent of kernel namespaces and cgroups made it possible to isolate groups
    of processes in a self-confined lightweight virtualization package; we call them
    containers. In this chapter, we saw how containers provide the same features as
    other full-fledged hypervisor-based virtualization technologies such as KVM and
    Xen, without the overhead of running multiple kernels in the same operating system.
    LXC takes full advantage of Linux cgroups and namespaces to achieve this level
    of isolation and resource control.
  prefs: []
  type: TYPE_NORMAL
- en: With the foundation gained from this chapter, you'll be able to understand better
    what's going on under the hood, which will make it much easier to troubleshoot
    and support the full life cycle of Linux containers, as we'll do in the next chapters.
  prefs: []
  type: TYPE_NORMAL
