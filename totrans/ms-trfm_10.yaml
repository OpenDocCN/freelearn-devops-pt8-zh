- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started on Azure – Building Solutions with Azure Virtual Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve built our solution end-to-end on the AWS platforms and followed
    our team’s journey from their initial VM architecture to Kubernetes and then finally
    culminating with serverless, we’re ready to switch gears and enter an alternate
    reality where Keyser has saddled up to his dear friends at Microsoft. In this
    next set of chapters, we will follow a similar path as in *Chapters 7* through
    *9*, but in this alternate version, we’ll work with Microsoft Azure.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will pick up where we started our journey on AWS; in [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), where we built a doppelgänger solution
    using AWS. In that chapter, we went into great detail about elements of the solution
    that are 100% cloud agnostic. This included a detailed explanation of exactly
    how we use Packer to provision our .NET-based application code to a Linux VM and
    a detailed explanation of how to set up GitHub Actions for a VM-based CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Since an overwhelming majority of this remains the same when we move to Azure,
    we won’t be revisiting these topics in this chapter at the same length. However,
    I would encourage you to put a bookmark in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365)
    and reference it frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our team at Söze Enterprises applauds their achievement of responding to the
    whimsical technical course correction of their fearless leader, Keyser Söze, and
    marvels at their success and fortune in launching their product successfully on
    AWS. Here, they used VMs, Kubernetes, and serverless technology. The comforting
    orange complexion of the AWS console begins to melt away when suddenly, the air
    fills with an eerie yet familiar sound: doodle-oo doodle-oo doodle-oo. An unexpected
    duo appears – one with shoulder-length brown hair under a black baseball cap and
    a simple black T-shirt; the other with a nerdy charm with tousled blond hair,
    thick black-rimmed glasses, and a red and blue plaid flannel over a white Aerosmith
    T-shirt. They start the familiar chant: doodle-oo doodle-oo doodle-oo. Suddenly,
    we’re transported to another world – another universe, perhaps, where Azure’s
    deep blue replaces AWS’s bright orange. Söze Enterprises has partnered with Microsoft
    for their next-generation autonomous vehicle platform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as before, we have inherited a team from one of Söze Enterprises’ other
    divisions that has a strong core team of C# .NET developers, so we’ll be building
    version 1.0 of the platform using .NET technologies. The elusive CEO, Keyser,
    was seen hobnobbing with Satya Nadella during the glitz and glamor of the Met
    Gala in New York City over the weekend, and word has come down from corporate
    that we will be using Microsoft Azure to host the platform. Since the team doesn’t
    have a ton of experience with containers and timelines are tight, we’ve decided
    to build a simple three-tier architecture and host on Azure VMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Logical architecture for the autonomous vehicle platform](img/B21183_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Logical architecture for the autonomous vehicle platform
  prefs: []
  type: TYPE_NORMAL
- en: The platform will need a frontend, which will be a web UI built using ASP.NET
    Core Blazor. The frontend will be powered by a REST API backend, which will be
    built using ASP.NET Core Web API. Having our core functionality encapsulated into
    a REST API will allow autonomous vehicles to communicate directly with the platform
    and allow us to expand by adding client interfaces with additional frontend technologies
    such as native mobile apps and virtual or mixed reality in the future. The backend
    will use a PostgreSQL database for persistent storage since it’s lightweight,
    industry-standard, and relatively inexpensive.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the tight timelines the team is facing, we want to keep the cloud architecture
    simple. Therefore, we’ll keep it simple and use the tried and tested services
    of Microsoft Azure to implement the platform instead of trying to learn something
    new. The first decision we must make is what Azure service each component of our
    logical architecture will be hosted on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application architecture consists of three components: a frontend, a backend,
    and a database. The frontend and backend are application components and need to
    be hosted on a cloud service that provides general computing, while the database
    needs to be hosted on a cloud database service. There are many options for both
    types of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Logical architecture for the autonomous vehicle platform](img/B21183_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Logical architecture for the autonomous vehicle platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have decided we’re going to use VMs to host our application, we have
    narrowed down the different services that we can use to host our application,
    and we have decided that the Azure VM service is the ideal choice for our current
    situation. Other options, such as Azure App Service, also use VMs but we want
    to have total control over the solution and maintain as many cross-platform capabilities
    as we can in case we ever have to migrate to a different cloud platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Source control structure of our repository](img/B21183_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Source control structure of our repository
  prefs: []
  type: TYPE_NORMAL
- en: In this solution, we’ll have six parts. We still have the application code and
    Packer templates for both the frontend and backend. Then, we have GitHub Actions
    to implement our CI/CD process and Terraform to provision our Azure infrastructure
    and reference the Packer-built VM images for our Azure VMs.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we developed a similar solution
    using AWS and its equivalent offerings concerning VMs. As a result, our design
    for Azure will look rather similar. Many of the cloud services we use on AWS have
    equivalents to Microsoft Azure. This is largely because VMs, networks, and network
    security have stabilized in terms of how the industry views them. Don’t expect
    to see radical differences in naming conventions and how things work. When working
    with this cloud computing paradigm, the differences between platforms are usually
    very subtle. Throughout this book, I will attempt to highlight synonymous terms
    across clouds to help you better translate your conceptual knowledge from one
    cloud to another.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VMs must be deployed within a virtual network. As you may recall from [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365) when we provisioned this solution on AWS,
    we needed to set up multiple subnets for our solution to span Availability Zones.
    That’s because of the structure of virtual networks on AWS, how the virtual network
    is scoped to an AWS region, and how a subnet is scoped to an AWS Availability
    Zone. Azure is different.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Azure, the virtual network and the subnets are scoped to a region. Zonal
    resiliency is built into the virtual network. Azure has two resiliency modes:
    one based on fault domains or *regional* and another based on Availability Zones
    or *zonal*. VMs can be provisioned in either of these two modes.'
  prefs: []
  type: TYPE_NORMAL
- en: To provision a regional VM solution, you need to provision an availability set
    and specify how many fault domains you want to distribute your VMs across. When
    VMs are provisioned within this availability set, the Azure platform takes care
    to ensure that they are provisioned to hardware that does not share a common source
    of power and network switch, thus making it less likely that the entire workload
    will fail in the case of an outage isolated to a single fault domain. If you don’t
    use an availability set, Azure will allocate your VMs based on available capacity
    and make no guarantee that your VMs won’t be in the same fault domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provision a zonal VM solution, you simply need to specify which Availability
    Zone to use to provision your VMs and ensure that you have more than one VM spread
    across multiple Availability Zones. An Availability Zone offers much more resiliency
    than a fault domain as instead of the Azure platform guaranteeing your VM doesn’t
    share the same power source and network switch, it guarantees your VM is in a
    different physical data center within the region. In this book, we will focus
    on ensuring that our solution achieves zonal resiliency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Resource Azure virtual network architecture](img/B21183_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Resource Azure virtual network architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, you can see that our virtual network and both its
    subnets can support VMs across all Availability Zones within the region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Isolated subnets for frontend and backend application components](img/B21183_10_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Isolated subnets for frontend and backend application components
  prefs: []
  type: TYPE_NORMAL
- en: This means that we don’t need to design our subnets based on the constraints
    of the cloud platform’s resiliency boundaries as we do on AWS; we can design our
    subnets to match our workload’s needs. In this case, we need a subnet for our
    solution’s frontend, which hosts the ASP.NET Core Blazor web application, and
    we need a subnet for our solution’s backend, which hosts the ASP.NET Core Web
    API. Whether we choose to provision VMs regionally, taking advantage of Azure’s
    fault domains, or zonally, taking advantage of Azure’s Availability Zones, does
    not affect the network design. Both options are available to us when we decide
    to provision VMs.
  prefs: []
  type: TYPE_NORMAL
- en: Network routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), when we set up this solution
    on AWS, we needed to configure an internet gateway, NAT gateways, and route tables
    for our VMs to have outbound access to the internet. On Azure, we don’t need to
    configure equivalent components because Azure provides a default gateway and automatically
    configures VMs to use it. If we wanted to block internet access or route internet
    traffic another way, we would need to configure additional resources.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When discussing **load balancers** as a component of our architecture, we will
    inevitably use some well-established and familiar terms, but we will be using
    them in a different context. This can be confusing. Therefore, I hope to tackle
    the elephant in the room. Our solution has a frontend – the web application that
    serves up web pages for the end user’s web browser. Our solution also has a backend
    – the REST Web API that our web application calls to talk to the database and
    perform stateful operations. Our solution will also leverage two load balancers:
    one to distribute load across our frontend web servers running the web application
    and another to distribute load across our backend web servers running the Web
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Too many frontends and backends](img/B21183_10_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Too many frontends and backends
  prefs: []
  type: TYPE_NORMAL
- en: Within the context of each load balancer, each load balancer will have a frontend
    and backend. It’s important to note the context when using these terms as the
    frontend of our solution refers to a different architectural component at an altogether
    different architectural granularity. We need to understand that when we refer
    to the frontend of our solution, we are talking about all of the components that
    make up the frontend of our solution function properly, and when we are talking
    about the frontend of the *frontend* load balancer, we are talking about the networking
    endpoint that accepts traffic for the *frontend* of our solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), when we set up this solution
    on AWS, we used the AWS **Application Load Balancer** (**ALB**) service. On Azure,
    we’ll use the Azure Load Balancer service. Both services function very similarly
    but they are structured a little differently and use different terminology to
    describe similar concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS** | **Azure** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| ALB | Azure Load Balancer | Load balancer |'
  prefs: []
  type: TYPE_TB
- en: '| Listener | Frontend IP Configuration | The singular endpoint that accepts
    incoming traffic on a load balancer |'
  prefs: []
  type: TYPE_TB
- en: '| Target Group | Backend Address Pool | A collection of VMs that incoming traffic
    is forwarded to |'
  prefs: []
  type: TYPE_TB
- en: '| Health Check | Health Probe | An endpoint published by each of the backend
    VMs that indicates it is healthy and ready to handle traffic |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – Mapping of synonymous load balancer components between AWS and
    Azure
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), a load
    balancer provides a singular frontend endpoint and distributes network traffic
    across a multitude of backend VMs. On AWS, while they call this frontend endpoint
    a **listener**, on Azure, it is called the **frontend IP configuration**. Likewise,
    the backend VMs are called the **target group** in AWS, while they are called
    the **backend address pool** on Azure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Resource-isolated subnets for frontend and backend application
    components](img/B21183_10_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Resource-isolated subnets for frontend and backend application
    components
  prefs: []
  type: TYPE_NORMAL
- en: Azure Load Balancer uses rules to determine how incoming traffic is routed to
    backend pools.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Load Balancer organizes how it routes incoming traffic using rules. Each
    rule has a protocol, a frontend component, and a backend component. The rule’s
    frontend component configures where and how the network traffic should come into
    the load balancer. This includes a port to expose, which frontend IP configuration
    to expose the port on, and what health probe it should use to determine which
    backend nodes are healthy and ready to receive traffic. The backend component
    of the rule specifies which backend address pool to route traffic to and what
    port to use.
  prefs: []
  type: TYPE_NORMAL
- en: The `/`) and the backend – the REST Web API – will continue to use the custom
    health check endpoint we setup at `/health`.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) we set up four security
    groups in AWS for each logical stop that network traffic makes within our solution
    architecture. In Azure, we only need two security groups because Azure Load Balancer
    is automatically granted access to our VMs using the rules that we configured
    in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Frontend node pool network traffic flow](img/B21183_10_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Frontend node pool network traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of VMs handling traffic within the frontend, they will
    receive traffic on port `5000` using the HTTP protocol. The C# application will
    make requests to the REST Web API hosted in the backend, but we’ll be routing
    all our requests to the backend through the backend load balancer on port `80`
    using the HTTP protocol. On Azure, we don’t need to explicitly allow this egress
    traffic within the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Backend node pool network traffic flow](img/B21183_10_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Backend node pool network traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of the VMs handling traffic within the backend, they will
    be receiving traffic on port `5000` using the HTTP protocol. The C# application
    code will be making requests to the PostgreSQL database on port `5432` using the
    HTTPS protocol. On Azure, we don’t need to explicitly allow this egress traffic
    within the network.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Secrets such as database credentials or service access keys need to be stored
    securely. Each cloud platform has a service that provides this functionality.
    On Azure, this service is called **Azure** **Key Vault**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS** | **Azure** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| IAM | Microsoft Entra | Identity provider |'
  prefs: []
  type: TYPE_TB
- en: '| Secrets Manager | Key Vault | Secure secret storage |'
  prefs: []
  type: TYPE_TB
- en: '| IAM role | User-assigned managed identity | Identity for machine-to-machine
    interaction |'
  prefs: []
  type: TYPE_TB
- en: '| IAM policy | **Role-based access** **control** (**RBAC**) | Provides permission
    to perform specific operations on specific services or resources |'
  prefs: []
  type: TYPE_TB
- en: '| IAM role policy | Role assignment | Associates specific permissions with
    specific identities |'
  prefs: []
  type: TYPE_TB
- en: Table 10.2 – Mapping synonymous identity and access management components between
    AWS and Azure
  prefs: []
  type: TYPE_NORMAL
- en: 'Secrets stored in Azure Key Vault can be accessed by VMs once they have the
    necessary RBAC granted. In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we
    used an AWS IAM role assignment to allow a VM to do this. Azure works similarly
    by attaching one or more user-assigned managed identities to the VMs and then
    creating role assignments for the managed identities so that they have specific
    roles that grant the necessary permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Key Vault architecture](img/B21183_10_10.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Key Vault architecture
  prefs: []
  type: TYPE_NORMAL
- en: Granting the managed identity that is attached to the VMs access to the **Key
    Vault Secrets User** role will allow the VMs to read the secret values from Key
    Vault. This does not put the secrets on the machine. The VM will need to use the
    Azure CLI to access the Key Vault secrets.
  prefs: []
  type: TYPE_NORMAL
- en: VMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have everything we need for our solution, we can finish by talking
    about where our application components will run: VMs provisioned on Azure’s VM
    service. When provisioning VMs on Azure, you have two options. First, you can
    provision static VMs. In this approach, you need to specify key characteristics
    for every VM. The second option is to provision a **Virtual Machine Scale Set**
    (**VMSS**). This will allow you to dynamically scale up and down based on demand
    as well as auto-heal VMs that fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS** | **Azure** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| EC2 | VMs | VM service |'
  prefs: []
  type: TYPE_TB
- en: '| AMI | VM image | VM image either from Marketplace or custom build (e.g.,
    using tools such as Packer) |'
  prefs: []
  type: TYPE_TB
- en: '| IAM role | User-assigned managed identity | Identity for machine-to-machine
    interaction |'
  prefs: []
  type: TYPE_TB
- en: '| **Auto Scaling** **group** (**ASG**) | VMSS | Set of dynamically provisioned
    VMs that can be scaled up/down using a VM configuration template |'
  prefs: []
  type: TYPE_TB
- en: '| Launch template | VM profile | Configuration template used to create new
    VMs |'
  prefs: []
  type: TYPE_TB
- en: Table 10.3 – Mapping synonymous VM service components between AWS and Azure
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we provisioned our solution
    using AWS **Elastic Cloud Compute** (**EC2**). Azure VMs share a similar structure
    to EC2 instances. Like on AWS, Azure VMs are connected to their corresponding
    subnet by way of a virtual network interface. However, on Azure, we have two types
    of network security rules: **network security groups** (**NSGs**) and **application
    security groups** (**ASGs**). While both are used to control traffic on Azure,
    NSGs focus on specifying lower-level network rules such as port and protocol filtering
    for network-level resources defined as IP address ranges. AGSs, on the other hand,
    provide a higher level of abstraction that allows you to group resources based
    on the role they play within the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Azure VM architecture](img/B21183_10_11.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – Azure VM architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can use an Azure VMSS to dynamically provision and manage
    the VMs. In this approach, you provide the VMSS with some configuration and parameters
    on when to scale up and when to scale down, and the VMSS will take care of everything
    else:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Azure VMSS architecture](img/B21183_10_12.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Azure VMSS architecture
  prefs: []
  type: TYPE_NORMAL
- en: Azure VMSS allows you to provide fine-grained configuration for each of the
    VMs that it will spin up on your behalf. It also provides a set of policies that
    allow you to control the behavior of the VMSS relating to when instances fail
    unexpectedly, when Azure needs to update them, or whether to scale up or down
    the number of VMs.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on Azure, we need to come up with a plan for how to provision
    our environments and deploy our code.
  prefs: []
  type: TYPE_NORMAL
- en: VM configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our solution, we have two VM roles: the frontend role, which is responsible
    for handling web page requests from the end user’s web browser, and the backend
    role, which is responsible for handling REST API requests from the web application.
    Each of these roles has a different code and different configuration that needs
    to be set. Each will require its own Packer template to build a VM image that
    we can use to launch a VM on Azure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Packer pipeline to build a VM image for the frontend](img/B21183_10_13.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – Packer pipeline to build a VM image for the frontend
  prefs: []
  type: TYPE_NORMAL
- en: A GitHub Actions workflow that triggers off changes to the frontend application
    code and the frontend packer template will execute `packer build` and create a
    new VM image for the solution’s frontend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the frontend and the backend will have identical GitHub Actions workflows
    that execute `packer build`. The key difference between the workflows is the code
    bases that they execute against. Both the frontend and the backend might have
    slightly different operating system configurations, and both require different
    deployment packages for their respective application components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Packer pipeline to build a VM image for the backend](img/B21183_10_14.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Packer pipeline to build a VM image for the backend
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the application code will be baked into the VM image
    rather than copied to an already running VM. This means that to update the software
    running on the VMs, each VM will need to be restarted so that it can be restarted
    with a new VM image containing the latest copy of the code.
  prefs: []
  type: TYPE_NORMAL
- en: This approach makes the VM image itself an immutable deployment artifact that
    is versioned and updated each time there is a release of the application code
    that needs to be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud environment configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the VM images have been built for both the frontend and the backend, we
    can execute the final workflow that will both provision and deploy our solution
    to Azure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – VM images as inputs to the Terraform code, which provisions
    the environment on Azure](img/B21183_10_15.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – VM images as inputs to the Terraform code, which provisions the
    environment on Azure
  prefs: []
  type: TYPE_NORMAL
- en: The Terraform code base will have two input variables for the version of the
    VM image for both the frontend and the backend. When new versions of the application
    software need to be deployed, the input parameters for these versions will be
    incremented to reflect the target version for deployment. When the workflow is
    executed, `terraform apply` will simply replace the existing VMs with VMs using
    the new VM image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a solid plan for how we will implement both the cloud architecture
    using Azure and the deployment architecture using GitHub Actions, let’s start
    building! In the next section, we’ll break down the HCL code that we’ll use to
    implement the Terraform and Packer solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a solid design for our solution, we can begin building it.
    As discussed in the previous section, we’ll be using an Azure VM. As we did with
    AWS in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we’ll need to package
    our application into VM images using Packer and then provision an environment
    that provisions an environment using these VM images.
  prefs: []
  type: TYPE_NORMAL
- en: Packer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll learn how to implement our Packer template provisioners
    so that we can install our .NET application code on a Linux VM. If you skipped
    *Chapters 7* through *9* due to a lack of interest in AWS, I can’t hold that against
    you – particularly if your primary interest in reading this book is working on
    the Microsoft Azure cloud platform. However, I would encourage you to review the
    corresponding section within [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) to
    see how we use Packer’s provisioners to configure a Debian-based Linux VM with
    our .NET application code.
  prefs: []
  type: TYPE_NORMAL
- en: Azure plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), Packer
    – like Terraform – is an extensible command-line executable. Each cloud platform
    provides a plugin for Packer that encapsulates the integration with its services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we saw how to declare the
    Packer plugin for AWS as a required plugin. The preceding code demonstrates how
    to declare Azure’s plugin instead – at the time of writing, the latest version
    is `2.0.0`.
  prefs: []
  type: TYPE_NORMAL
- en: The Azure plugin for Packer provides an `azure-arm` builder that will generate
    Azure VM images by creating a new VM from a base image, executing the provisioners,
    taking a snapshot of the Azure managed disk, and creating an Azure managed image
    from it. Like the AWS plugin, this behavior is encapsulated within the Azure builder.
  prefs: []
  type: TYPE_NORMAL
- en: Just as the plugin for AWS encapsulated the logic to build VMs on AWS and its
    configuration was in AWS-centric terminology, so does the Azure plugin encapsulate
    all the logic to build VMs on Azure, and its configuration is in Azure-centric
    terminology. Packer does not try to create a standard builder interface across
    cloud platforms – rather, it isolates the cloud-specific configuration within
    the builders. This keeps things simple for users who know the target platform
    well and allows the builder to take advantage of any platform-specific features
    without additional layers of complexity by trying to rationalize the syntax across
    every platform.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the structure of the AWS and Azure builders is radically different
    in almost every way – from how they authenticate to how they look at marketplace
    images. There are some common fields and similarities, but they are very different
    animals.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first big difference is how they pass authentication credentials. As we
    saw in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), the AWS plugin allows
    us to use environment variables to pass in the AWS access key and secret key to
    authenticate with AWS. The Azure provider does not support this method and requires
    you to pass in all four attributes to authenticate using a Microsoft Entra (formerly
    Azure Active Directory) service principal. Those four attributes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tenant ID**: Unique identifier for the Microsoft Entra tenant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subscription ID**: Unique identifier for the Microsoft Azure subscription'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Client ID**: Unique identifier for the Microsoft Entra service principal
    that we will use as the identity of Terraform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Client secret**: Secret key for the Microsoft Entra service principal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code shows how the four Microsoft Azure credential attributes
    are passed into the Azure builder using input variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows how we reference the Azure marketplace version of
    the Ubuntu 22.04 Virtual Machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice how, unlike in the AWS version, where we used a data source of `amazon-ami`
    to look up the same image in a specific AWS region, we don’t need to do this on
    Microsoft Azure. Because of the way Azure structures marketplace images, there’s
    no need to look up the region-specific unique identifier for the VM image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final part of the Azure builder should look very familiar to the AWS version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we see the same `communicator` attribute set to `ssh`,
    a `vm_size` attribute that corresponds to the AWS equivalent, `instance_type`,
    and an `allowed_inbound_ip_addresses` attribute that corresponds to the AWS equivalent,
    `temporary_security_group_source_cidrs`, which pokes a hole in the security group
    to allow the machine that GitHub Actions is executing on access to the temporary
    VM that Packer provisions.
  prefs: []
  type: TYPE_NORMAL
- en: Operating system configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To configure the operating system, we must install software dependencies (such
    as .NET 6.0), copy and deploy our application code’s deployment package to the
    correct location in the local filesystem, configure a Linux service that runs
    on boot, and set up a local user and group with necessary access for the service
    to run as.
  prefs: []
  type: TYPE_NORMAL
- en: I expanded on these steps in detail in the corresponding section in [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), so I encourage you to review this section
    if you want to refresh your memory.
  prefs: []
  type: TYPE_NORMAL
- en: Platform-specific build tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Packer provides a way for you to execute provisioners only on particular builders.
    This allows you to accommodate platform-specific differences even within the operating
    system configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Microsoft Azure, we need to execute a platform-specific command as the last
    and final step before Packer shuts down the VM and creates the image. Those of
    you with experience setting up Microsoft Windows VM images will be familiar with
    a utility called `sysprep`. This tool is used to prepare a VM so that we can have
    an image created from its disk. Although we are not using a Windows operating
    system, Microsoft Azure needs us to execute a similar command so that we can prepare
    our Linux VM to have an image made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The archaic `waagent` command is of little importance. You just need to know
    that this command needs to be executed last for the VM image that Packer builds
    to be bootable when you launch a new VM from the image. However, do take notice
    of the `only` attribute, which takes a `list` value of `string`. The only value
    we have set in this `list` is `azure-arm`. This indicates to Packer that this
    provisioner only needs to be executed when we’re building images using that plugin.
    As we know, the same Packer template can be used to do multi-targeting, which
    means you can build multiple images in the same template while targeting multiple
    different cloud platforms or regions. This means you can build the same VM image
    simultaneously on AWS, Azure, and Google Cloud. You could even build the same
    VM image on AWS in all 30+ regions. This isn’t exactly practical as there are
    much better ways to replicate VM images across regions, but it can be done.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has an application code base that needs to
    be deployed. Since this is the first time we will be using the `azurerm` provider,
    we’ll look at the basic provider setup and the configuration of the backend before
    we cover the nuts and bolts of each component of our architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Provider setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to specify all the providers that we intend to use in this solution
    within the `required_providers` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We must also configure the Azure provider. Unlike the AWS provider, the Azure
    provider is not scoped to a particular region. This means you can provision resources
    across all Azure regions without declaring different Azure provider blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Azure provider requires some additional parameters to specify the credentials
    to use to connect to Azure, but because these are sensitive values, we don’t want
    to embed them in the code. We’ll pass those values in later when we automate the
    deployment using the standard Azure credentials environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ARM_TENANT_ID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ARM_SUBSCRIPTION_ID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ARM_CLIENT_ID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ARM_CLIENT_SECRET`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because we will be using a CI/CD pipeline to provision and maintain our environment
    in the long term, we need to set up a remote backend for our Terraform state.
    Because our solution will be hosted on Azure, we’ll use the Azure Blob storage
    backend to store our Terraform state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the Azure provider, we don’t want to hard code the backend configuration
    in our code, so we’ll simply set up a placeholder for the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We’ll configure the backend’s parameters using the `-backend-config` parameters
    when we run `terraform init` in our CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Input variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s good practice to pass in short names that identify the application’s name
    and the application’s environment. This allows you to embed consistent naming
    conventions across the resources that make up your solution. This makes it easier
    to identify and track resources from the Azure portal.
  prefs: []
  type: TYPE_NORMAL
- en: The `primary_region`, `vnet_cidr_block`, and `az_count` input variables drive
    key architectural characteristics of the deployment. They can’t be hard-coded
    as it would limit the reusability of the Terraform code base.
  prefs: []
  type: TYPE_NORMAL
- en: The `vnet_cidr_block` input variable establishes the virtual network address
    space, which is often tightly regulated by an enterprise governance body. There
    is usually a process to ensure that teams across an organization do not use IP
    address ranges that conflict, thus making it impossible in the future to allow
    those two applications to integrate or integrate with shared network resources
    within the enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: The `az_count` input variable allows us to configure how much redundancy we
    want within our solution. This will affect the high availability of the solution
    but also the cost of the deployment. As you can imagine, cost is also a tightly
    regulated characteristic of cloud infrastructure deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Consistent naming and tagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike the AWS console, Azure is designed in such a way that it is extremely
    easy to get an application-centric view of your deployment. For this, you can
    use resource groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It’s still important to tag the resources that you deploy that indicate what
    application and what environment they belong to. This helps with other reporting
    needs, such as budgets and compliance. Almost all resources within the Azure provider
    have a `map` attribute called `tags`. Unlike AWS, each resource has a `name` value
    as a required attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just as we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we need
    to construct a virtual network and keep its address space as tight as possible
    to avoid gobbling up unnecessary address space for the broader organization in
    the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Network creation in Azure is simpler than what we did with AWS because we don’t
    have to segment our subnets based on Availability Zone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in the design, the Azure Load Balancer service is structured
    quite a bit differently than AWS’s equivalent offering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to call out that to achieve zonal resiliency, we need to ensure
    that all components of our architecture are deployed in a zone-resilient way.
    This often requires setting the `zones` attribute and specifying which Availability
    Zones we want to provision into.
  prefs: []
  type: TYPE_NORMAL
- en: 'The backend configuration of Azure Load Balancer is a simple logical container
    for the backend address pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This logical container must be linked to either static VMs or a VMSS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding backend address pool association resource, we are iterating
    over `var.az_count`. This is the same number that we iterate over the VMs, which
    allows us to put a single VM into each Availability Zone. Unlike AWS, where the
    load balancer rules are split between a listener and a target group configuration,
    an Azure load balancer rule combines the two and then links them to a corresponding
    health probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the load balancer rule connects many of the components, including
    the frontend IP configuration, the listener on AWS, the health probe, and the
    backend address pool – the target group on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we need to set up the logical ASG for each application architectural
    component. We’ll have one for the frontend and one for the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create NSGs that allow the necessary traffic into each of
    the ASGs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll set up Key Vault:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we’ll set up a managed identity for each application architectural component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll grant the managed identity the necessary privileges using Azure
    role assignments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: VMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll obtain the VM image from our input variables. We built this VM
    image with Packer and provisioned it into a different Azure resource group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we’ll create the network interface for each VM by iterating over the
    `var.az_count` input variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll set up the VM with all the necessary attributes and link it
    to the network interface, the VM image, and the managed identity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With that, we’ve implemented the Packer and Terraform solutions and have a working
    code base that will build VM images for both our frontend and backend application
    components and provision our cloud environment into Azure. In the next section,
    we’ll dive into the YAML and Bash and implement the GitHub Actions workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base consisting of application code
    and operating system configuration encapsulated within a Packer template. These
    two application components are then deployed into a cloud environment on Azure
    that’s defined within our Terraform code base.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with the
    AWS solution, there is an additional code base that we have to discuss: our automation
    pipelines on GitHub Actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went over the folder
    structure for our code base and where our GitHub Actions fit in so that we know
    that our automation pipelines are called workflows, and they’re stored in `/.github/workflows`.
    Each of our code bases is stored in its respective folder. Our solutions source
    code repository’s folder structure will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.``github`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workflows`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dotnet`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frontend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packer`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frontend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terraform`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As per our design, we will have GitHub Actions workflows that will execute
    Packer and build VM images for both the frontend (for example, `packer-frontend.yaml`)
    and the backend (for example, `packer-backend.yaml`). We’ll also have workflows
    that will run `terraform plan` and `terraform apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.``github`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workflows`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packer-backend.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packer-frontend.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terraform-apply.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terraform-plan.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went into greater detail
    on the GitFlow process and how it interacts with our GitHub Actions workflows,
    so for now, let’s dig into how these pipelines will differ when targeting the
    Azure platform.
  prefs: []
  type: TYPE_NORMAL
- en: Packer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went into great detail
    about each step of the GitHub Actions workflow that executes Packer to build VM
    images. Thanks to the nature of Packer’s cloud-agnostic architecture, this overwhelmingly
    stays the same. The only thing that changes is the final step where we execute
    Packer.
  prefs: []
  type: TYPE_NORMAL
- en: Because Packer needs to be configured to build a VM on Microsoft Azure, we need
    to pass in different input variables that are Azure-specific. This includes the
    Microsoft Azure credential attributes, an Azure region, and an Azure resource
    group name.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we did with the input variables for the Packer template for AWS, we
    must ensure that all Azure input variables are prefixed with `azure_`. This will
    help if we ever want to introduce multi-targeting as many cloud platforms will
    have similar required inputs, such as target region and VM size. While most clouds
    will have similar required inputs, the input values are not interchangeable.
  prefs: []
  type: TYPE_NORMAL
- en: For example, both Azure and AWS require you to specify the region that you want
    Packer to provide the temporary VM into and the resulting VM image to be stored.
    On Azure, the region has a value of `westus2`, while on AWS, it has a value of
    `us-west-2`. They may seem very similar, but they are miles apart (pun intended).
    Azure West US 2 region is completely different than AWS’s West US 2 region – in
    fact, besides just being on different cloud platforms, they are physically different
    locations, with Azure’s West US 2 region being located in Washington State and
    AWS’s West US 2 region being located in Oregon. Neighbors, yes, the same thing
    – hardly.
  prefs: []
  type: TYPE_NORMAL
- en: This goes back to Packer’s strategy of isolating platform-specific configuration
    within the builders. Therefore, if we are going to do multi-targeting, the AWS
    plugin is going to need input variables that are AWS-specific and the Azure plugin
    is going to need input variables that are Azure-specific. Hence, when we merge
    these plugins into one Packer template, we’ll need input variables for both.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, our `aws_primary_region`, which has a value of `us-west-2`, can
    sit right next to our `azure_primary_region`, which has a value of `westus2`,
    without any conflicts or confusion. Likewise, our `aws_instance_type` with a value
    of `t2.small` can sit right next to our `azure_vm_size` with a value of `Standard_DS2_v2`.
    The differences can get even more radical as you take advantage of more platform-specific
    capabilities within the builders.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GitHub Actions workflow YAML files are identical to Azure, except for the
    additional input variables that need to be specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code references the four Azure credential attributes, which are
    stored as GitHub Actions variables and secrets, and transfers them to Packer using
    environment variables with the `PKR_VAR_` prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With both of our VM images built and their versions input into our `tfvars`
    file, our Terraform automation pipeline is ready to take the reigns and not only
    provision our environment but deploy our solution as well (although not technically).
    The deployment was technically done within the `packer build` process, with the
    physical deployment packages being copied to the home directory and the Linux
    service setup primed and ready. Terraform is finishing the job by actually launching
    VMs using these images.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went into great detail
    about each step of the GitHub Actions workflow that executes Terraform to provision
    the cloud environment and deploy the application code. Thanks to the nature of
    Terraform’s cloud-agnostic architecture, this overwhelmingly stays the same. The
    only thing that changes is the final step where we execute Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with the
    AWS provider, we can set the authentication context using environment variables
    that are specific to the `azurerm` provider. In this case, the four Azure credentials
    attributes are passed in with the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ARM_TENANT_ID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ARM_SUBSCRIPTION_ID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ARM_CLIENT_ID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ARM_CLIENT_SECRET`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with the
    AWS provider, we need to configure the Azure-specific backend that stores the
    Terraform state using the `-backend-config` command-line arguments to the `terraform
    init` command. Unlike AWS, which only specifies an S3 bucket name to configure
    the backend to save the Terraform state to S3, to configure the Azure backend,
    we need to specify three fields to triangulate a location in Azure Blob storage
    to save the Terraform state – a resource group, storage account, and Blob storage
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hierarchy of Azure resources looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Resource group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage account
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Blob storage container
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Terraform state files
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Like with the AWS provider, the backend uses a *key* and the Terraform workspace
    name to uniquely identify the location to store state files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice how, unlike with the AWS solution, we don’t need to perform a targeted
    `terraform apply`. This is because we don’t need to do dynamic calculations based
    on the number of Availability Zones in the region to configure our virtual network.
    This is due to Azure Virtual Network and its subnets spanning all Availability
    Zones within the region whereas, on AWS, a subnet is constrained to a specific
    Availability Zone within the parent virtual network’s region.
  prefs: []
  type: TYPE_NORMAL
- en: These subtle architectural differences between the cloud platforms can create
    radical structural changes even when deploying the same solution using the same
    technologies. It is a sobering reminder that while knowledge of the core concepts
    we looked at in *Chapters 4* through *6* will help us transcend to a multi-cloud
    point of view, to implement practical solutions, we need to understand the subtle
    nuances of each platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a multi-tier cloud architecture using Azure VMs with
    a fully operation GitFlow process and an end-to-end CI/CD pipeline using GitHub
    Actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, our fearless leader at Söze Enterprises will be throwing
    us into turmoil with some big new ideas, and we’ll have to respond to his call
    to action. It turns out our CEO, Keyser, has been up late watching some YouTube
    videos about the next big thing – containers – and after talking with his pal
    Satya on his superyacht, he has decided that we need to refactor our whole solution
    to run on Docker and Kubernetes. Luckily, the good people at Microsoft have a
    service that might help us out: **Azure Kubernetes** **Service** (**AKS**).'
  prefs: []
  type: TYPE_NORMAL
