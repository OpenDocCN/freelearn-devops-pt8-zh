- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Getting Started on Azure – Building Solutions with Azure Virtual Machines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Azure上入门——使用Azure虚拟机构建解决方案
- en: Now that we’ve built our solution end-to-end on the AWS platforms and followed
    our team’s journey from their initial VM architecture to Kubernetes and then finally
    culminating with serverless, we’re ready to switch gears and enter an alternate
    reality where Keyser has saddled up to his dear friends at Microsoft. In this
    next set of chapters, we will follow a similar path as in *Chapters 7* through
    *9*, but in this alternate version, we’ll work with Microsoft Azure.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经在AWS平台上从头到尾构建了解决方案，并跟随团队的旅程，从他们最初的虚拟机架构，到Kubernetes，最后到无服务器，我们准备切换思路，进入另一个现实，在那里Keyser与他在微软的亲密朋友们并肩作战。在接下来的几章中，我们将遵循与*第7章*到*第9章*类似的路径，但在这个替代版本中，我们将与微软Azure合作。
- en: This chapter will pick up where we started our journey on AWS; in [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), where we built a doppelgänger solution
    using AWS. In that chapter, we went into great detail about elements of the solution
    that are 100% cloud agnostic. This included a detailed explanation of exactly
    how we use Packer to provision our .NET-based application code to a Linux VM and
    a detailed explanation of how to set up GitHub Actions for a VM-based CI/CD pipeline.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从我们在AWS上的旅程开始的地方接续；在[*第7章*](B21183_07.xhtml#_idTextAnchor365)，我们使用AWS构建了一个双胞胎解决方案。在那一章中，我们详细讲解了完全不依赖云平台的解决方案要素。包括如何使用Packer将基于.NET的应用程序代码部署到Linux虚拟机上，以及如何为虚拟机上的CI/CD管道设置GitHub
    Actions的详细说明。
- en: Since an overwhelming majority of this remains the same when we move to Azure,
    we won’t be revisiting these topics in this chapter at the same length. However,
    I would encourage you to put a bookmark in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365)
    and reference it frequently.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数内容在转移到Azure时保持不变，我们不会在本章中以相同的长度重新讨论这些话题。然而，我建议你在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中做个书签，并经常参考它。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及以下主题：
- en: Laying the foundation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打好基础
- en: Designing the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Building the solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: Automating the deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化部署
- en: Laying the foundation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打好基础
- en: 'Our team at Söze Enterprises applauds their achievement of responding to the
    whimsical technical course correction of their fearless leader, Keyser Söze, and
    marvels at their success and fortune in launching their product successfully on
    AWS. Here, they used VMs, Kubernetes, and serverless technology. The comforting
    orange complexion of the AWS console begins to melt away when suddenly, the air
    fills with an eerie yet familiar sound: doodle-oo doodle-oo doodle-oo. An unexpected
    duo appears – one with shoulder-length brown hair under a black baseball cap and
    a simple black T-shirt; the other with a nerdy charm with tousled blond hair,
    thick black-rimmed glasses, and a red and blue plaid flannel over a white Aerosmith
    T-shirt. They start the familiar chant: doodle-oo doodle-oo doodle-oo. Suddenly,
    we’re transported to another world – another universe, perhaps, where Azure’s
    deep blue replaces AWS’s bright orange. Söze Enterprises has partnered with Microsoft
    for their next-generation autonomous vehicle platform.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Söze企业的团队为他们成功应对无畏领导者Keyser Söze的技术路线调整而喝彩，并对他们成功在AWS上成功推出产品的成就与运气感到惊叹。在这里，他们使用了虚拟机、Kubernetes和无服务器技术。当AWS控制台的舒适橙色外观开始消退时，空气中突然弥漫着一种既陌生又熟悉的声音：doodle-oo
    doodle-oo doodle-oo。一个意外的二人组合出现在眼前——一位是戴着黑色棒球帽、披肩发的棕发男，穿着简单的黑色T恤；另一位则是典型的书呆子魅力，金色乱发，戴着厚重的黑框眼镜，穿着红蓝格子法兰绒衬衫，下面是白色Aerosmith
    T恤。他们开始熟悉的吟唱：doodle-oo doodle-oo doodle-oo。突然间，我们被带到了另一个世界——也许是另一个宇宙，在那里Azure的深蓝色取代了AWS的亮橙色。Söze企业与微软合作开发他们的下一代自动驾驶平台。
- en: 'Just as before, we have inherited a team from one of Söze Enterprises’ other
    divisions that has a strong core team of C# .NET developers, so we’ll be building
    version 1.0 of the platform using .NET technologies. The elusive CEO, Keyser,
    was seen hobnobbing with Satya Nadella during the glitz and glamor of the Met
    Gala in New York City over the weekend, and word has come down from corporate
    that we will be using Microsoft Azure to host the platform. Since the team doesn’t
    have a ton of experience with containers and timelines are tight, we’ve decided
    to build a simple three-tier architecture and host on Azure VMs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们从Söze企业的其他部门接管了一支拥有强大核心团队的C# .NET开发者团队，因此我们将使用.NET技术构建平台的1.0版本。那位神秘的CEO
    Keyser上周末在纽约市的Met Gala盛会上被看到与Satya Nadella亲密交谈，企业总部已经下令我们将使用Microsoft Azure来托管平台。由于团队对容器的经验并不丰富，而且时间紧迫，我们决定构建一个简单的三层架构并托管在Azure虚拟机上：
- en: '![Figure 10.1 – Logical architecture for the autonomous vehicle platform](img/B21183_10_1.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 自动驾驶平台的逻辑架构](img/B21183_10_1.jpg)'
- en: Figure 10.1 – Logical architecture for the autonomous vehicle platform
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 自动驾驶平台的逻辑架构
- en: The platform will need a frontend, which will be a web UI built using ASP.NET
    Core Blazor. The frontend will be powered by a REST API backend, which will be
    built using ASP.NET Core Web API. Having our core functionality encapsulated into
    a REST API will allow autonomous vehicles to communicate directly with the platform
    and allow us to expand by adding client interfaces with additional frontend technologies
    such as native mobile apps and virtual or mixed reality in the future. The backend
    will use a PostgreSQL database for persistent storage since it’s lightweight,
    industry-standard, and relatively inexpensive.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该平台将需要一个前端，前端将是一个使用ASP.NET Core Blazor构建的Web UI。前端将由REST API后端提供支持，后端将使用ASP.NET
    Core Web API构建。将核心功能封装成REST API将使自动驾驶汽车能够直接与平台进行通信，并允许我们通过在未来添加客户端接口（如原生移动应用和虚拟或混合现实）来扩展功能。后端将使用PostgreSQL数据库进行持久化存储，因为它轻量级、行业标准且相对便宜。
- en: Designing the solution
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Due to the tight timelines the team is facing, we want to keep the cloud architecture
    simple. Therefore, we’ll keep it simple and use the tried and tested services
    of Microsoft Azure to implement the platform instead of trying to learn something
    new. The first decision we must make is what Azure service each component of our
    logical architecture will be hosted on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于团队面临紧迫的时间表，我们希望保持云架构的简单性。因此，我们将保持简单，使用经过验证的Microsoft Azure服务来实现该平台，而不是尝试学习新东西。我们必须做出的第一个决定是，每个逻辑架构组件将托管在Azure的哪个服务上。
- en: 'Our application architecture consists of three components: a frontend, a backend,
    and a database. The frontend and backend are application components and need to
    be hosted on a cloud service that provides general computing, while the database
    needs to be hosted on a cloud database service. There are many options for both
    types of services:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用架构由三个组件组成：前端、后端和数据库。前端和后端是应用程序组件，需要托管在提供通用计算的云服务上，而数据库则需要托管在云数据库服务上。两种服务都有许多选择：
- en: '![Figure 10.2 – Logical architecture for the autonomous vehicle platform](img/B21183_10_2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 自动驾驶平台的逻辑架构](img/B21183_10_2.jpg)'
- en: Figure 10.2 – Logical architecture for the autonomous vehicle platform
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 自动驾驶平台的逻辑架构
- en: 'Since we have decided we’re going to use VMs to host our application, we have
    narrowed down the different services that we can use to host our application,
    and we have decided that the Azure VM service is the ideal choice for our current
    situation. Other options, such as Azure App Service, also use VMs but we want
    to have total control over the solution and maintain as many cross-platform capabilities
    as we can in case we ever have to migrate to a different cloud platform:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经决定使用虚拟机（VMs）来托管我们的应用程序，我们已经缩小了可以用来托管应用程序的不同服务，并决定Azure虚拟机服务是当前情况下的理想选择。其他选项，如Azure应用服务，也使用虚拟机，但我们希望对解决方案有完全的控制，并尽可能保持跨平台的能力，以防我们将来需要迁移到其他云平台：
- en: '![Figure 10.3 – Source control structure of our repository](img/B21183_10_3.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 我们的代码库的源代码控制结构](img/B21183_10_3.jpg)'
- en: Figure 10.3 – Source control structure of our repository
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 我们的代码库的源代码控制结构
- en: In this solution, we’ll have six parts. We still have the application code and
    Packer templates for both the frontend and backend. Then, we have GitHub Actions
    to implement our CI/CD process and Terraform to provision our Azure infrastructure
    and reference the Packer-built VM images for our Azure VMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个解决方案中，我们将有六个部分。我们仍然需要前端和后端的应用程序代码以及 Packer 模板。然后，我们需要 GitHub Actions 来实现
    CI/CD 过程，使用 Terraform 来部署 Azure 基础设施，并引用 Packer 构建的虚拟机镜像用于我们的 Azure 虚拟机。
- en: Cloud architecture
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云架构
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we developed a similar solution
    using AWS and its equivalent offerings concerning VMs. As a result, our design
    for Azure will look rather similar. Many of the cloud services we use on AWS have
    equivalents to Microsoft Azure. This is largely because VMs, networks, and network
    security have stabilized in terms of how the industry views them. Don’t expect
    to see radical differences in naming conventions and how things work. When working
    with this cloud computing paradigm, the differences between platforms are usually
    very subtle. Throughout this book, I will attempt to highlight synonymous terms
    across clouds to help you better translate your conceptual knowledge from one
    cloud to another.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，我们使用 AWS 和其相应的虚拟机服务开发了一个类似的解决方案。因此，我们为
    Azure 的设计将会非常相似。我们在 AWS 上使用的许多云服务在 Microsoft Azure 上都有对应的服务。这主要是因为虚拟机、网络和网络安全在行业中的认知已经趋于稳定。不要期望在命名约定和工作方式上看到太大的差异。在使用这种云计算范式时，各个平台之间的差异通常非常细微。在本书中，我将尽力突出不同云平台之间的同义词，帮助您更好地将概念知识从一个云平台迁移到另一个云平台。
- en: Virtual network
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: VMs must be deployed within a virtual network. As you may recall from [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365) when we provisioned this solution on AWS,
    we needed to set up multiple subnets for our solution to span Availability Zones.
    That’s because of the structure of virtual networks on AWS, how the virtual network
    is scoped to an AWS region, and how a subnet is scoped to an AWS Availability
    Zone. Azure is different.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机必须部署在虚拟网络中。正如您可能记得的，在我们[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，当我们在 AWS
    上部署这个解决方案时，我们需要为我们的解决方案设置多个子网，以便跨可用区展开。这是因为 AWS 上虚拟网络的结构、虚拟网络的范围与 AWS 区域的关系，以及子网的范围与
    AWS 可用区的关系。Azure 不同。
- en: 'On Azure, the virtual network and the subnets are scoped to a region. Zonal
    resiliency is built into the virtual network. Azure has two resiliency modes:
    one based on fault domains or *regional* and another based on Availability Zones
    or *zonal*. VMs can be provisioned in either of these two modes.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 上，虚拟网络和子网的范围限定在一个区域内。区域弹性内置于虚拟网络中。Azure 有两种弹性模式：一种基于故障域或*区域*，另一种基于可用区或*区域性*。虚拟机可以在这两种模式下进行部署。
- en: To provision a regional VM solution, you need to provision an availability set
    and specify how many fault domains you want to distribute your VMs across. When
    VMs are provisioned within this availability set, the Azure platform takes care
    to ensure that they are provisioned to hardware that does not share a common source
    of power and network switch, thus making it less likely that the entire workload
    will fail in the case of an outage isolated to a single fault domain. If you don’t
    use an availability set, Azure will allocate your VMs based on available capacity
    and make no guarantee that your VMs won’t be in the same fault domain.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要提供区域虚拟机解决方案，您需要创建一个可用性集并指定要分布虚拟机的故障域数量。当虚拟机在此可用性集中部署时，Azure 平台会确保它们部署到不共享相同电源和网络交换机的硬件上，从而降低整个工作负载因单一故障域中的故障而发生故障的可能性。如果不使用可用性集，Azure
    将根据可用容量分配虚拟机，并不会保证您的虚拟机不会位于同一个故障域。
- en: 'To provision a zonal VM solution, you simply need to specify which Availability
    Zone to use to provision your VMs and ensure that you have more than one VM spread
    across multiple Availability Zones. An Availability Zone offers much more resiliency
    than a fault domain as instead of the Azure platform guaranteeing your VM doesn’t
    share the same power source and network switch, it guarantees your VM is in a
    different physical data center within the region. In this book, we will focus
    on ensuring that our solution achieves zonal resiliency:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要提供区域虚拟机解决方案，您只需要指定要使用的可用区来部署虚拟机，并确保您的虚拟机分布在多个可用区中。与故障域相比，可用区提供了更高的弹性，因为它不仅保证您的虚拟机不会共享相同的电源和网络交换机，还保证您的虚拟机位于区域内不同的物理数据中心。在本书中，我们将专注于确保我们的解决方案实现区域弹性：
- en: '![Figure 10.4 – Resource Azure virtual network architecture](img/B21183_10_4.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – 资源 Azure 虚拟网络架构](img/B21183_10_4.jpg)'
- en: Figure 10.4 – Resource Azure virtual network architecture
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 资源 Azure 虚拟网络架构
- en: 'In the preceding diagram, you can see that our virtual network and both its
    subnets can support VMs across all Availability Zones within the region:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，您可以看到我们的虚拟网络及其两个子网可以支持该区域内所有可用性区域中的虚拟机：
- en: '![Figure 10.5 – Isolated subnets for frontend and backend application components](img/B21183_10_5.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – 前端和后端应用组件的隔离子网](img/B21183_10_5.jpg)'
- en: Figure 10.5 – Isolated subnets for frontend and backend application components
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 前端和后端应用组件的隔离子网
- en: This means that we don’t need to design our subnets based on the constraints
    of the cloud platform’s resiliency boundaries as we do on AWS; we can design our
    subnets to match our workload’s needs. In this case, we need a subnet for our
    solution’s frontend, which hosts the ASP.NET Core Blazor web application, and
    we need a subnet for our solution’s backend, which hosts the ASP.NET Core Web
    API. Whether we choose to provision VMs regionally, taking advantage of Azure’s
    fault domains, or zonally, taking advantage of Azure’s Availability Zones, does
    not affect the network design. Both options are available to us when we decide
    to provision VMs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不需要像在 AWS 上那样，根据云平台的可靠性边界来设计我们的子网；我们可以根据工作负载的需求来设计我们的子网。在这种情况下，我们需要为解决方案的前端设计一个子网，前端托管了
    ASP.NET Core Blazor Web 应用程序；还需要为解决方案的后端设计一个子网，后端托管了 ASP.NET Core Web API。无论我们选择按区域部署虚拟机，利用
    Azure 的故障域，还是按可用区部署虚拟机，利用 Azure 的可用性区域，都不会影响网络设计。我们在决定部署虚拟机时，都可以选择这两种方式。
- en: Network routing
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络路由
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), when we set up this solution
    on AWS, we needed to configure an internet gateway, NAT gateways, and route tables
    for our VMs to have outbound access to the internet. On Azure, we don’t need to
    configure equivalent components because Azure provides a default gateway and automatically
    configures VMs to use it. If we wanted to block internet access or route internet
    traffic another way, we would need to configure additional resources.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 7 章*](B21183_07.xhtml#_idTextAnchor365)中，当我们在 AWS 上设置此解决方案时，我们需要为我们的虚拟机配置互联网网关、NAT
    网关和路由表，以便虚拟机能够访问互联网。而在 Azure 上，我们不需要配置等效的组件，因为 Azure 提供了默认网关并自动配置虚拟机使用它。如果我们想阻止互联网访问或以其他方式路由互联网流量，我们则需要配置额外的资源。
- en: Load balancing
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'When discussing **load balancers** as a component of our architecture, we will
    inevitably use some well-established and familiar terms, but we will be using
    them in a different context. This can be confusing. Therefore, I hope to tackle
    the elephant in the room. Our solution has a frontend – the web application that
    serves up web pages for the end user’s web browser. Our solution also has a backend
    – the REST Web API that our web application calls to talk to the database and
    perform stateful operations. Our solution will also leverage two load balancers:
    one to distribute load across our frontend web servers running the web application
    and another to distribute load across our backend web servers running the Web
    API:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论**负载均衡器**作为我们架构的一部分时，我们不可避免地会使用一些成熟且熟悉的术语，但我们会在不同的上下文中使用它们，这可能会让人感到困惑。因此，我希望能先解决这个难题。我们的解决方案有前端——为最终用户的
    Web 浏览器提供网页的 Web 应用程序。我们的解决方案还有后端——Web 应用程序调用的 REST Web API，用于与数据库交互并执行有状态操作。我们的解决方案还将使用两个负载均衡器：一个分配负载到运行
    Web 应用程序的前端 Web 服务器，另一个分配负载到运行 Web API 的后端 Web 服务器：
- en: '![Figure 10.6 – Too many frontends and backends](img/B21183_10_6.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 前端和后端过多](img/B21183_10_6.jpg)'
- en: Figure 10.6 – Too many frontends and backends
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 前端和后端过多
- en: Within the context of each load balancer, each load balancer will have a frontend
    and backend. It’s important to note the context when using these terms as the
    frontend of our solution refers to a different architectural component at an altogether
    different architectural granularity. We need to understand that when we refer
    to the frontend of our solution, we are talking about all of the components that
    make up the frontend of our solution function properly, and when we are talking
    about the frontend of the *frontend* load balancer, we are talking about the networking
    endpoint that accepts traffic for the *frontend* of our solution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个负载均衡器的上下文中，每个负载均衡器都会有一个前端和后端。需要注意的是，使用这些术语时要理解上下文，因为我们解决方案的前端指的是一个完全不同架构粒度的不同架构组件。我们需要明白，当我们提到我们解决方案的前端时，我们是在谈论所有确保解决方案前端正常工作的组件，而当我们提到*前端*负载均衡器的前端时，我们指的是接受流量的网络端点，专门为我们解决方案的*前端*服务。
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), when we set up this solution
    on AWS, we used the AWS **Application Load Balancer** (**ALB**) service. On Azure,
    we’ll use the Azure Load Balancer service. Both services function very similarly
    but they are structured a little differently and use different terminology to
    describe similar concepts:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，当我们在 AWS 上设置此解决方案时，我们使用了 AWS **应用负载均衡器**（**ALB**）服务。在
    Azure 上，我们将使用 Azure Load Balancer 服务。这两项服务功能非常相似，但它们的结构略有不同，并且使用不同的术语来描述相似的概念：
- en: '| **AWS** | **Azure** | **Description** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **AWS** | **Azure** | **描述** |'
- en: '| ALB | Azure Load Balancer | Load balancer |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ALB | Azure Load Balancer | 负载均衡器 |'
- en: '| Listener | Frontend IP Configuration | The singular endpoint that accepts
    incoming traffic on a load balancer |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 监听器 | 前端 IP 配置 | 接受负载均衡器传入流量的单一端点 |'
- en: '| Target Group | Backend Address Pool | A collection of VMs that incoming traffic
    is forwarded to |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 目标组 | 后端地址池 | 一组将接收传入流量的虚拟机 |'
- en: '| Health Check | Health Probe | An endpoint published by each of the backend
    VMs that indicates it is healthy and ready to handle traffic |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 健康检查 | 健康探针 | 每个后端虚拟机发布的一个端点，表明它是健康的并且准备好处理流量 |'
- en: Table 10.1 – Mapping of synonymous load balancer components between AWS and
    Azure
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1 – AWS 和 Azure 之间负载均衡器组件的同义映射
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), a load
    balancer provides a singular frontend endpoint and distributes network traffic
    across a multitude of backend VMs. On AWS, while they call this frontend endpoint
    a **listener**, on Azure, it is called the **frontend IP configuration**. Likewise,
    the backend VMs are called the **target group** in AWS, while they are called
    the **backend address pool** on Azure:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第4章*](B21183_04.xhtml#_idTextAnchor239)中讨论的，负载均衡器提供了一个单一的前端端点，并将网络流量分配到多个后端虚拟机。在
    AWS 中，虽然他们称这个前端端点为**监听器**，而在 Azure 中，它被称为**前端 IP 配置**。同样，AWS 中的后端虚拟机被称为**目标组**，而在
    Azure 中，它们被称为**后端地址池**：
- en: '![Figure 10.7 – Resource-isolated subnets for frontend and backend application
    components](img/B21183_10_7.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 前端和后端应用组件的资源隔离子网](img/B21183_10_7.jpg)'
- en: Figure 10.7 – Resource-isolated subnets for frontend and backend application
    components
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 前端和后端应用组件的资源隔离子网
- en: Azure Load Balancer uses rules to determine how incoming traffic is routed to
    backend pools.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Load Balancer 使用规则来决定如何将传入的流量路由到后端池。
- en: Azure Load Balancer organizes how it routes incoming traffic using rules. Each
    rule has a protocol, a frontend component, and a backend component. The rule’s
    frontend component configures where and how the network traffic should come into
    the load balancer. This includes a port to expose, which frontend IP configuration
    to expose the port on, and what health probe it should use to determine which
    backend nodes are healthy and ready to receive traffic. The backend component
    of the rule specifies which backend address pool to route traffic to and what
    port to use.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Load Balancer 使用规则来组织如何路由传入流量。每个规则都有一个协议、一个前端组件和一个后端组件。规则的前端组件配置了网络流量应如何进入负载均衡器。这包括暴露的端口，在哪个前端
    IP 配置上暴露端口，以及使用哪个健康探针来判断哪些后端节点是健康的并且准备好接收流量。规则的后端组件指定了将流量路由到哪个后端地址池，并且指定了使用的端口。
- en: The `/`) and the backend – the REST Web API – will continue to use the custom
    health check endpoint we setup at `/health`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`/`）以及后端 – REST Web API – 将继续使用我们在 `/health` 设置的自定义健康检查端点。'
- en: Network security
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) we set up four security
    groups in AWS for each logical stop that network traffic makes within our solution
    architecture. In Azure, we only need two security groups because Azure Load Balancer
    is automatically granted access to our VMs using the rules that we configured
    in it:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365)中，我们为 AWS 设置了四个安全组，用于管理网络流量在解决方案架构中的每个逻辑节点。Azure
    中我们只需要两个安全组，因为 Azure 负载均衡器会根据我们在其中配置的规则自动授予虚拟机访问权限：
- en: '![Figure 10.8 – Frontend node pool network traffic flow](img/B21183_10_8.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 前端节点池网络流量流向](img/B21183_10_8.jpg)'
- en: Figure 10.8 – Frontend node pool network traffic flow
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 前端节点池网络流量流向
- en: 'From the perspective of VMs handling traffic within the frontend, they will
    receive traffic on port `5000` using the HTTP protocol. The C# application will
    make requests to the REST Web API hosted in the backend, but we’ll be routing
    all our requests to the backend through the backend load balancer on port `80`
    using the HTTP protocol. On Azure, we don’t need to explicitly allow this egress
    traffic within the network:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前端处理流量的虚拟机（VMs）角度来看，它们将通过 HTTP 协议在端口 `5000` 上接收流量。C# 应用程序将向后端托管的 REST Web API
    发起请求，但我们会通过后端负载均衡器在端口 `80` 上使用 HTTP 协议将所有请求路由到后端。在 Azure 中，我们不需要在网络中显式允许这种出站流量：
- en: '![Figure 10.9 – Backend node pool network traffic flow](img/B21183_10_9.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9 – 后端节点池网络流量流向](img/B21183_10_9.jpg)'
- en: Figure 10.9 – Backend node pool network traffic flow
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 后端节点池网络流量流向
- en: From the perspective of the VMs handling traffic within the backend, they will
    be receiving traffic on port `5000` using the HTTP protocol. The C# application
    code will be making requests to the PostgreSQL database on port `5432` using the
    HTTPS protocol. On Azure, we don’t need to explicitly allow this egress traffic
    within the network.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从后端处理流量的虚拟机（VMs）角度来看，它们将通过 HTTP 协议在端口 `5000` 上接收流量。C# 应用程序代码将通过 HTTPS 协议向端口
    `5432` 上的 PostgreSQL 数据库发起请求。在 Azure 中，我们不需要在网络中显式允许这种出站流量。
- en: Secrets management
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机密管理
- en: 'Secrets such as database credentials or service access keys need to be stored
    securely. Each cloud platform has a service that provides this functionality.
    On Azure, this service is called **Azure** **Key Vault**:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 像数据库凭据或服务访问密钥这样的机密需要被安全地存储。每个云平台都有提供此功能的服务。在 Azure 中，这项服务称为 **Azure** **Key
    Vault**：
- en: '| **AWS** | **Azure** | **Description** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **AWS** | **Azure** | **描述** |'
- en: '| IAM | Microsoft Entra | Identity provider |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| IAM | Microsoft Entra | 身份提供者 |'
- en: '| Secrets Manager | Key Vault | Secure secret storage |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Secrets Manager | Key Vault | 安全的机密存储 |'
- en: '| IAM role | User-assigned managed identity | Identity for machine-to-machine
    interaction |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| IAM 角色 | 用户分配的托管身份 | 机器间交互的身份 |'
- en: '| IAM policy | **Role-based access** **control** (**RBAC**) | Provides permission
    to perform specific operations on specific services or resources |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| IAM 策略 | **基于角色的访问** **控制** (**RBAC**) | 提供对特定服务或资源执行特定操作的权限 |'
- en: '| IAM role policy | Role assignment | Associates specific permissions with
    specific identities |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| IAM 角色策略 | 角色分配 | 将特定权限与特定身份关联 |'
- en: Table 10.2 – Mapping synonymous identity and access management components between
    AWS and Azure
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2 – 映射 AWS 和 Azure 之间的同义身份和访问管理组件
- en: 'Secrets stored in Azure Key Vault can be accessed by VMs once they have the
    necessary RBAC granted. In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we
    used an AWS IAM role assignment to allow a VM to do this. Azure works similarly
    by attaching one or more user-assigned managed identities to the VMs and then
    creating role assignments for the managed identities so that they have specific
    roles that grant the necessary permissions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在 Azure Key Vault 中的机密可以在虚拟机获得必要的 RBAC 权限后访问。在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365)中，我们使用
    AWS IAM 角色分配来允许虚拟机执行此操作。Azure 也类似，通过将一个或多个用户分配的托管身份附加到虚拟机，然后为这些托管身份创建角色分配，以便它们拥有授予必要权限的特定角色：
- en: '![Figure 10.10 – Key Vault architecture](img/B21183_10_10.0.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.10 – 密钥保管库架构](img/B21183_10_10.0.jpg)'
- en: Figure 10.10 – Key Vault architecture
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 密钥保管库架构
- en: Granting the managed identity that is attached to the VMs access to the **Key
    Vault Secrets User** role will allow the VMs to read the secret values from Key
    Vault. This does not put the secrets on the machine. The VM will need to use the
    Azure CLI to access the Key Vault secrets.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将附加到虚拟机的托管身份授予 **Key Vault Secrets User** 角色，将允许虚拟机从密钥保管库中读取机密值。这不会将机密存储在机器上。虚拟机将需要使用
    Azure CLI 来访问密钥保管库中的机密。
- en: VMs
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机
- en: 'Now that we have everything we need for our solution, we can finish by talking
    about where our application components will run: VMs provisioned on Azure’s VM
    service. When provisioning VMs on Azure, you have two options. First, you can
    provision static VMs. In this approach, you need to specify key characteristics
    for every VM. The second option is to provision a **Virtual Machine Scale Set**
    (**VMSS**). This will allow you to dynamically scale up and down based on demand
    as well as auto-heal VMs that fail:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为解决方案准备好了所需的一切，我们可以继续讨论应用程序组件将运行的位置：在 Azure 的虚拟机服务上配置的虚拟机。当在 Azure 上配置虚拟机时，你有两种选择。首先，你可以配置静态虚拟机。在这种方式下，你需要为每个虚拟机指定关键特性。第二种选择是配置一个**虚拟机规模集**（**VMSS**）。这样，你可以根据需求动态扩展或缩减，并且在虚拟机失败时自动修复：
- en: '| **AWS** | **Azure** | **Description** |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **AWS** | **Azure** | **描述** |'
- en: '| EC2 | VMs | VM service |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| EC2 | 虚拟机 | 虚拟机服务 |'
- en: '| AMI | VM image | VM image either from Marketplace or custom build (e.g.,
    using tools such as Packer) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| AMI | 虚拟机镜像 | 来自市场或自定义构建的虚拟机镜像（例如，使用工具如 Packer） |'
- en: '| IAM role | User-assigned managed identity | Identity for machine-to-machine
    interaction |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| IAM 角色 | 用户分配的托管身份 | 用于机器间交互的身份 |'
- en: '| **Auto Scaling** **group** (**ASG**) | VMSS | Set of dynamically provisioned
    VMs that can be scaled up/down using a VM configuration template |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **自动扩展** **组**（**ASG**） | VMSS | 一组可以通过虚拟机配置模板动态扩展/缩减的虚拟机 |'
- en: '| Launch template | VM profile | Configuration template used to create new
    VMs |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 启动模板 | 虚拟机配置文件 | 用于创建新虚拟机的配置模板 |'
- en: Table 10.3 – Mapping synonymous VM service components between AWS and Azure
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.3 – AWS 与 Azure 之间相似虚拟机服务组件的映射
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we provisioned our solution
    using AWS **Elastic Cloud Compute** (**EC2**). Azure VMs share a similar structure
    to EC2 instances. Like on AWS, Azure VMs are connected to their corresponding
    subnet by way of a virtual network interface. However, on Azure, we have two types
    of network security rules: **network security groups** (**NSGs**) and **application
    security groups** (**ASGs**). While both are used to control traffic on Azure,
    NSGs focus on specifying lower-level network rules such as port and protocol filtering
    for network-level resources defined as IP address ranges. AGSs, on the other hand,
    provide a higher level of abstraction that allows you to group resources based
    on the role they play within the application:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 中，我们使用 AWS **弹性云计算**（**EC2**）配置了我们的解决方案。Azure
    虚拟机的结构与 EC2 实例类似。像在 AWS 上一样，Azure 虚拟机通过虚拟网络接口连接到它们对应的子网。然而，在 Azure 上，我们有两种类型的网络安全规则：**网络安全组**（**NSG**）和**应用程序安全组**（**ASG**）。虽然这两者都用于控制
    Azure 上的流量，NSG 侧重于指定低级别的网络规则，如端口和协议过滤，适用于定义为 IP 地址范围的网络级资源。而 ASG 提供了更高层次的抽象，允许你根据应用程序内角色将资源分组：
- en: '![Figure 10.11 – Azure VM architecture](img/B21183_10_11.0.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.11 – Azure VM 架构](img/B21183_10_11.0.jpg)'
- en: Figure 10.11 – Azure VM architecture
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – Azure VM 架构
- en: 'Alternatively, you can use an Azure VMSS to dynamically provision and manage
    the VMs. In this approach, you provide the VMSS with some configuration and parameters
    on when to scale up and when to scale down, and the VMSS will take care of everything
    else:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用 Azure VMSS 来动态配置和管理虚拟机。在这种方式下，你为 VMSS 提供一些配置和参数，指明何时扩展和何时缩减，VMSS 会处理其他所有事情：
- en: '![Figure 10.12 – Azure VMSS architecture](img/B21183_10_12.0.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – Azure VMSS 架构](img/B21183_10_12.0.jpg)'
- en: Figure 10.12 – Azure VMSS architecture
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – Azure VMSS 架构
- en: Azure VMSS allows you to provide fine-grained configuration for each of the
    VMs that it will spin up on your behalf. It also provides a set of policies that
    allow you to control the behavior of the VMSS relating to when instances fail
    unexpectedly, when Azure needs to update them, or whether to scale up or down
    the number of VMs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Azure VMSS 允许你为它将代表你启动的每个虚拟机提供细粒度的配置。它还提供了一组策略，让你控制 VMSS 的行为，比如在实例意外失败时、Azure
    需要更新它们时，或是否根据需求扩展或缩减虚拟机的数量。
- en: Deployment architecture
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署架构
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on Azure, we need to come up with a plan for how to provision
    our environments and deploy our code.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对我们的解决方案在 Azure 上的云架构有了一个清晰的概念，我们需要制定一个计划，来配置我们的环境并部署我们的代码。
- en: VM configuration
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机配置
- en: 'In our solution, we have two VM roles: the frontend role, which is responsible
    for handling web page requests from the end user’s web browser, and the backend
    role, which is responsible for handling REST API requests from the web application.
    Each of these roles has a different code and different configuration that needs
    to be set. Each will require its own Packer template to build a VM image that
    we can use to launch a VM on Azure:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们有两个虚拟机角色：前端角色，负责处理来自最终用户浏览器的网页请求；后端角色，负责处理来自网页应用的 REST API 请求。每个角色都有不同的代码和不同的配置需要设置。每个角色都需要自己的
    Packer 模板来构建虚拟机镜像，以便我们在 Azure 上启动虚拟机：
- en: '![Figure 10.13 – Packer pipeline to build a VM image for the frontend](img/B21183_10_13.0.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.13 – 使用 Packer 管道构建前端虚拟机镜像](img/B21183_10_13.0.jpg)'
- en: Figure 10.13 – Packer pipeline to build a VM image for the frontend
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – 使用 Packer 管道构建前端虚拟机镜像
- en: A GitHub Actions workflow that triggers off changes to the frontend application
    code and the frontend packer template will execute `packer build` and create a
    new VM image for the solution’s frontend.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 GitHub Actions 工作流会在前端应用代码和前端 Packer 模板发生变化时触发，执行 `packer build`，并为解决方案的前端创建一个新的虚拟机镜像。
- en: 'Both the frontend and the backend will have identical GitHub Actions workflows
    that execute `packer build`. The key difference between the workflows is the code
    bases that they execute against. Both the frontend and the backend might have
    slightly different operating system configurations, and both require different
    deployment packages for their respective application components:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前端和后端将有相同的 GitHub Actions 工作流，执行 `packer build`。工作流之间的主要区别在于它们执行的代码库。前端和后端可能有稍微不同的操作系统配置，并且它们分别需要不同的部署包来部署各自的应用组件：
- en: '![Figure 10.14 – Packer pipeline to build a VM image for the backend](img/B21183_10_14.0.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.14 – 使用 Packer 管道构建后端虚拟机镜像](img/B21183_10_14.0.jpg)'
- en: Figure 10.14 – Packer pipeline to build a VM image for the backend
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – 使用 Packer 管道构建后端虚拟机镜像
- en: It’s important to note that the application code will be baked into the VM image
    rather than copied to an already running VM. This means that to update the software
    running on the VMs, each VM will need to be restarted so that it can be restarted
    with a new VM image containing the latest copy of the code.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，应用代码会被集成到虚拟机镜像中，而不是复制到已经运行的虚拟机上。这意味着，要更新虚拟机上运行的软件，每台虚拟机都需要重新启动，以便使用包含最新代码的新的虚拟机镜像重新启动。
- en: This approach makes the VM image itself an immutable deployment artifact that
    is versioned and updated each time there is a release of the application code
    that needs to be deployed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使得虚拟机镜像本身成为一个不可变的部署产物，每次有应用代码发布时，都会对其进行版本控制和更新。
- en: Cloud environment configuration
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云环境配置
- en: 'Once the VM images have been built for both the frontend and the backend, we
    can execute the final workflow that will both provision and deploy our solution
    to Azure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦前端和后端的虚拟机镜像构建完成，我们就可以执行最终的工作流，既配置环境又将我们的解决方案部署到 Azure 上：
- en: '![Figure 10.15 – VM images as inputs to the Terraform code, which provisions
    the environment on Azure](img/B21183_10_15.0.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.15 – 虚拟机镜像作为输入到 Terraform 代码中，Terraform 会在 Azure 上配置环境](img/B21183_10_15.0.jpg)'
- en: Figure 10.15 – VM images as inputs to the Terraform code, which provisions the
    environment on Azure
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15 – 虚拟机镜像作为输入到 Terraform 代码中，Terraform 会在 Azure 上配置环境
- en: The Terraform code base will have two input variables for the version of the
    VM image for both the frontend and the backend. When new versions of the application
    software need to be deployed, the input parameters for these versions will be
    incremented to reflect the target version for deployment. When the workflow is
    executed, `terraform apply` will simply replace the existing VMs with VMs using
    the new VM image.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 代码库将有两个输入变量，分别用于前端和后端虚拟机镜像的版本。当需要部署新的应用软件版本时，这些版本的输入参数将会递增，以反映目标版本。当工作流执行时，`terraform
    apply` 将会用新的虚拟机镜像替换现有的虚拟机。
- en: Now that we have a solid plan for how we will implement both the cloud architecture
    using Azure and the deployment architecture using GitHub Actions, let’s start
    building! In the next section, we’ll break down the HCL code that we’ll use to
    implement the Terraform and Packer solutions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个完整的计划，说明如何使用 Azure 实现云架构，如何使用 GitHub Actions 实现部署架构，让我们开始构建吧！在下一节中，我们将详细解析用于实现
    Terraform 和 Packer 解决方案的 HCL 代码。
- en: Building the solution
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: Now that we have a solid design for our solution, we can begin building it.
    As discussed in the previous section, we’ll be using an Azure VM. As we did with
    AWS in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we’ll need to package
    our application into VM images using Packer and then provision an environment
    that provisions an environment using these VM images.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个稳固的解决方案设计，可以开始构建它了。正如上一节所述，我们将使用 Azure 虚拟机。就像我们在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365)
    中使用 AWS 一样，我们需要使用 Packer 将我们的应用程序打包成虚拟机镜像，然后使用这些虚拟机镜像来配置一个环境。
- en: Packer
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Packer
- en: In this section, we’ll learn how to implement our Packer template provisioners
    so that we can install our .NET application code on a Linux VM. If you skipped
    *Chapters 7* through *9* due to a lack of interest in AWS, I can’t hold that against
    you – particularly if your primary interest in reading this book is working on
    the Microsoft Azure cloud platform. However, I would encourage you to review the
    corresponding section within [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) to
    see how we use Packer’s provisioners to configure a Debian-based Linux VM with
    our .NET application code.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何实现 Packer 模板中的配置工具，以便我们可以在 Linux 虚拟机上安装我们的 .NET 应用程序代码。如果你因对 AWS
    不感兴趣而跳过了 *第 7 章* 到 *第 9 章*，我不会因此责怪你——尤其是如果你读这本书的主要目的是在 Microsoft Azure 云平台上工作。然而，我还是建议你回顾一下
    [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 中的相关部分，看看我们如何使用 Packer 的配置工具配置基于 Debian
    的 Linux 虚拟机，并在其上部署 .NET 应用程序代码。
- en: Azure plugin
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure 插件
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), Packer
    – like Terraform – is an extensible command-line executable. Each cloud platform
    provides a plugin for Packer that encapsulates the integration with its services:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [*第 4 章*](B21183_04.xhtml#_idTextAnchor239) 中讨论的那样，Packer —— 就像 Terraform
    —— 是一个可扩展的命令行可执行文件。每个云平台都为 Packer 提供了一个插件，封装了与其服务的集成：
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we saw how to declare the
    Packer plugin for AWS as a required plugin. The preceding code demonstrates how
    to declare Azure’s plugin instead – at the time of writing, the latest version
    is `2.0.0`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 中，我们学习了如何将 Packer 插件声明为 AWS 所需的插件。前面的代码演示了如何声明
    Azure 插件——截至本文撰写时，最新版本是 `2.0.0`。
- en: The Azure plugin for Packer provides an `azure-arm` builder that will generate
    Azure VM images by creating a new VM from a base image, executing the provisioners,
    taking a snapshot of the Azure managed disk, and creating an Azure managed image
    from it. Like the AWS plugin, this behavior is encapsulated within the Azure builder.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Packer 的 Azure 插件提供了一个 `azure-arm` 构建器，它通过从基础镜像创建一个新的虚拟机，执行配置工具，拍摄 Azure 托管磁盘的快照，并从中创建一个
    Azure 托管镜像，从而生成 Azure 虚拟机镜像。与 AWS 插件类似，这种行为被封装在 Azure 构建器中。
- en: Just as the plugin for AWS encapsulated the logic to build VMs on AWS and its
    configuration was in AWS-centric terminology, so does the Azure plugin encapsulate
    all the logic to build VMs on Azure, and its configuration is in Azure-centric
    terminology. Packer does not try to create a standard builder interface across
    cloud platforms – rather, it isolates the cloud-specific configuration within
    the builders. This keeps things simple for users who know the target platform
    well and allows the builder to take advantage of any platform-specific features
    without additional layers of complexity by trying to rationalize the syntax across
    every platform.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 AWS 插件封装了在 AWS 上构建虚拟机的逻辑，并且其配置采用 AWS 中心的术语一样，Azure 插件也封装了在 Azure 上构建虚拟机的所有逻辑，并且其配置采用
    Azure 中心的术语。Packer 并不试图为不同的云平台创建一个标准化的构建器接口——而是将特定平台的配置封装在构建器中。这使得熟悉目标平台的用户能够简化操作，并允许构建器在不增加额外复杂性的情况下利用任何平台特有的功能，而不是通过在每个平台之间尝试统一语法来增加复杂度。
- en: As a result, the structure of the AWS and Azure builders is radically different
    in almost every way – from how they authenticate to how they look at marketplace
    images. There are some common fields and similarities, but they are very different
    animals.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，AWS 和 Azure 构建器的结构几乎在所有方面都截然不同——从它们如何进行身份验证，到它们如何看待市场镜像。虽然它们有一些共同的字段和相似之处，但它们是截然不同的存在。
- en: 'The first big difference is how they pass authentication credentials. As we
    saw in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), the AWS plugin allows
    us to use environment variables to pass in the AWS access key and secret key to
    authenticate with AWS. The Azure provider does not support this method and requires
    you to pass in all four attributes to authenticate using a Microsoft Entra (formerly
    Azure Active Directory) service principal. Those four attributes are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个重大区别是它们传递身份验证凭证的方式。正如我们在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 中看到的，AWS
    插件允许我们使用环境变量传递 AWS 访问密钥和密钥进行身份验证，而 Azure 提供者不支持这种方法，要求传递所有四个属性以使用 Microsoft Entra（前身为
    Azure Active Directory）服务主体进行身份验证。这四个属性如下：
- en: '**Tenant ID**: Unique identifier for the Microsoft Entra tenant'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**租户 ID**：Microsoft Entra 租户的唯一标识符'
- en: '**Subscription ID**: Unique identifier for the Microsoft Azure subscription'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**订阅 ID**：Microsoft Azure 订阅的唯一标识符'
- en: '**Client ID**: Unique identifier for the Microsoft Entra service principal
    that we will use as the identity of Terraform'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端 ID**：Microsoft Entra 服务主体的唯一标识符，我们将其作为 Terraform 的身份使用'
- en: '**Client secret**: Secret key for the Microsoft Entra service principal'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端密钥**：Microsoft Entra 服务主体的密钥'
- en: 'The following code shows how the four Microsoft Azure credential attributes
    are passed into the Azure builder using input variables:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何通过输入变量将四个 Microsoft Azure 凭证属性传递到 Azure 构建器：
- en: '[PRE1]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following code shows how we reference the Azure marketplace version of
    the Ubuntu 22.04 Virtual Machine:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何引用 Azure 市场版的 Ubuntu 22.04 虚拟机：
- en: '[PRE2]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice how, unlike in the AWS version, where we used a data source of `amazon-ami`
    to look up the same image in a specific AWS region, we don’t need to do this on
    Microsoft Azure. Because of the way Azure structures marketplace images, there’s
    no need to look up the region-specific unique identifier for the VM image.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与 AWS 版本不同，在 AWS 中我们使用 `amazon-ami` 数据源来查找特定 AWS 区域中的相同镜像，而在 Microsoft Azure
    中我们无需这么做。由于 Azure 市场镜像的结构方式，我们不需要查找 VM 镜像的区域特定唯一标识符。
- en: 'The final part of the Azure builder should look very familiar to the AWS version:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 构建器的最后部分应该与 AWS 版本非常相似：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code, we see the same `communicator` attribute set to `ssh`,
    a `vm_size` attribute that corresponds to the AWS equivalent, `instance_type`,
    and an `allowed_inbound_ip_addresses` attribute that corresponds to the AWS equivalent,
    `temporary_security_group_source_cidrs`, which pokes a hole in the security group
    to allow the machine that GitHub Actions is executing on access to the temporary
    VM that Packer provisions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们看到相同的 `communicator` 属性被设置为 `ssh`，一个与 AWS 等效的 `vm_size` 属性，对应于 AWS
    的 `instance_type`，以及一个与 AWS 等效的 `allowed_inbound_ip_addresses` 属性，对应于 AWS 的 `temporary_security_group_source_cidrs`，这个属性在安全组中打了一个孔，允许
    GitHub Actions 执行的机器访问 Packer 部署的临时虚拟机。
- en: Operating system configuration
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作系统配置
- en: To configure the operating system, we must install software dependencies (such
    as .NET 6.0), copy and deploy our application code’s deployment package to the
    correct location in the local filesystem, configure a Linux service that runs
    on boot, and set up a local user and group with necessary access for the service
    to run as.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置操作系统，我们必须安装软件依赖项（如 .NET 6.0），将应用程序代码的部署包复制并部署到本地文件系统的正确位置，配置一个在启动时运行的 Linux
    服务，并设置一个具有必要访问权限的本地用户和组，以便该服务能够以该身份运行。
- en: I expanded on these steps in detail in the corresponding section in [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), so I encourage you to review this section
    if you want to refresh your memory.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 的相关章节中详细展开了这些步骤，因此如果你想刷新记忆，建议回顾这一章节。
- en: Platform-specific build tasks
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平台特定的构建任务
- en: Packer provides a way for you to execute provisioners only on particular builders.
    This allows you to accommodate platform-specific differences even within the operating
    system configuration.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Packer 提供了一种只在特定构建器上执行配置程序的方法。这使得即使在操作系统配置中，也能适应平台特定的差异。
- en: 'In Microsoft Azure, we need to execute a platform-specific command as the last
    and final step before Packer shuts down the VM and creates the image. Those of
    you with experience setting up Microsoft Windows VM images will be familiar with
    a utility called `sysprep`. This tool is used to prepare a VM so that we can have
    an image created from its disk. Although we are not using a Windows operating
    system, Microsoft Azure needs us to execute a similar command so that we can prepare
    our Linux VM to have an image made:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Microsoft Azure 中，我们需要执行一个平台特定的命令，作为 Packer 关闭虚拟机并创建镜像之前的最后一步。那些有设置 Microsoft
    Windows 虚拟机镜像经验的朋友应该会熟悉一个叫做 `sysprep` 的工具。这个工具用于准备虚拟机，以便我们可以从其磁盘创建镜像。尽管我们并没有使用
    Windows 操作系统，但 Microsoft Azure 需要我们执行一个类似的命令，以便我们能够准备好 Linux 虚拟机以创建镜像：
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The archaic `waagent` command is of little importance. You just need to know
    that this command needs to be executed last for the VM image that Packer builds
    to be bootable when you launch a new VM from the image. However, do take notice
    of the `only` attribute, which takes a `list` value of `string`. The only value
    we have set in this `list` is `azure-arm`. This indicates to Packer that this
    provisioner only needs to be executed when we’re building images using that plugin.
    As we know, the same Packer template can be used to do multi-targeting, which
    means you can build multiple images in the same template while targeting multiple
    different cloud platforms or regions. This means you can build the same VM image
    simultaneously on AWS, Azure, and Google Cloud. You could even build the same
    VM image on AWS in all 30+ regions. This isn’t exactly practical as there are
    much better ways to replicate VM images across regions, but it can be done.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 过时的 `waagent` 命令并不重要。你只需要知道，这个命令需要作为最后一步执行，以确保从 Packer 构建的虚拟机镜像在启动新虚拟机时可以正常引导。但是，请注意
    `only` 属性，它接受一个 `list` 类型的 `string` 值。在这个 `list` 中，我们设置的唯一值是 `azure-arm`。这表示 Packer
    只在我们使用该插件构建镜像时执行此配置器。正如我们所知，同一个 Packer 模板可以用于多目标构建，这意味着你可以在同一个模板中构建多个镜像，同时针对不同的云平台或区域进行构建。这也意味着你可以在
    AWS、Azure 和 Google Cloud 上同时构建相同的虚拟机镜像，甚至可以在 AWS 的所有 30 多个区域中构建相同的虚拟机镜像。尽管这并不实用，因为有更好的方法可以跨区域复制虚拟机镜像，但它是可行的。
- en: Terraform
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Terraform
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has an application code base that needs to
    be deployed. Since this is the first time we will be using the `azurerm` provider,
    we’ll look at the basic provider setup and the configuration of the backend before
    we cover the nuts and bolts of each component of our architecture.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中讨论的那样，我们的解决方案由两个应用组件组成：前端和后端。每个组件都有需要部署的应用程序代码库。由于这是我们第一次使用 `azurerm`
    提供商，我们将在介绍架构各组件的具体内容之前，先了解基本的提供商设置和后端配置。
- en: Provider setup
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提供商设置
- en: 'We need to specify all the providers that we intend to use in this solution
    within the `required_providers` block:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 `required_providers` 区块中指定我们打算在此解决方案中使用的所有提供商：
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We must also configure the Azure provider. Unlike the AWS provider, the Azure
    provider is not scoped to a particular region. This means you can provision resources
    across all Azure regions without declaring different Azure provider blocks:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要配置 Azure 提供商。与 AWS 提供商不同，Azure 提供商不局限于特定区域。这意味着您可以在所有 Azure 区域中配置资源，而无需声明不同的
    Azure 提供商区块：
- en: '[PRE6]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Azure provider requires some additional parameters to specify the credentials
    to use to connect to Azure, but because these are sensitive values, we don’t want
    to embed them in the code. We’ll pass those values in later when we automate the
    deployment using the standard Azure credentials environment variables:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 提供商需要一些额外的参数来指定用于连接 Azure 的凭据，但由于这些是敏感信息，我们不希望将它们嵌入到代码中。我们稍后会在自动化部署时，通过标准的
    Azure 凭据环境变量传递这些值：
- en: '`ARM_TENANT_ID`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_TENANT_ID`'
- en: '`ARM_SUBSCRIPTION_ID`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_SUBSCRIPTION_ID`'
- en: '`ARM_CLIENT_ID`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_CLIENT_ID`'
- en: '`ARM_CLIENT_SECRET`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_CLIENT_SECRET`'
- en: Backend
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后端
- en: Because we will be using a CI/CD pipeline to provision and maintain our environment
    in the long term, we need to set up a remote backend for our Terraform state.
    Because our solution will be hosted on Azure, we’ll use the Azure Blob storage
    backend to store our Terraform state.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将使用 CI/CD 流水线来长期配置和维护我们的环境，所以我们需要为 Terraform 状态设置一个远程后端。由于我们的解决方案将托管在 Azure
    上，我们将使用 Azure Blob 存储后端来存储 Terraform 状态。
- en: 'Just like the Azure provider, we don’t want to hard code the backend configuration
    in our code, so we’ll simply set up a placeholder for the backend:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Azure 提供商一样，我们不想在代码中硬编码后端配置，所以我们将简单地设置一个后端占位符：
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We’ll configure the backend’s parameters using the `-backend-config` parameters
    when we run `terraform init` in our CI/CD pipeline.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 CI/CD 流水线中运行 `terraform init` 时，使用 `-backend-config` 参数来配置后端的参数。
- en: Input variables
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入变量
- en: It’s good practice to pass in short names that identify the application’s name
    and the application’s environment. This allows you to embed consistent naming
    conventions across the resources that make up your solution. This makes it easier
    to identify and track resources from the Azure portal.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是传入能够标识应用程序名称和环境的简短名称。这可以让你在组成解决方案的资源中嵌入一致的命名规范，从而更容易在 Azure 门户中识别和跟踪资源。
- en: The `primary_region`, `vnet_cidr_block`, and `az_count` input variables drive
    key architectural characteristics of the deployment. They can’t be hard-coded
    as it would limit the reusability of the Terraform code base.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`primary_region`、`vnet_cidr_block` 和 `az_count` 输入变量驱动部署的关键架构特性。它们不能被硬编码，因为这会限制
    Terraform 代码库的重用性。'
- en: The `vnet_cidr_block` input variable establishes the virtual network address
    space, which is often tightly regulated by an enterprise governance body. There
    is usually a process to ensure that teams across an organization do not use IP
    address ranges that conflict, thus making it impossible in the future to allow
    those two applications to integrate or integrate with shared network resources
    within the enterprise.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`vnet_cidr_block` 输入变量用于建立虚拟网络地址空间，这通常由企业治理机构严格管理。通常会有一个流程，确保组织内部的团队不会使用冲突的
    IP 地址范围，这样将来就无法将这两个应用程序集成，或与企业内部的共享网络资源进行集成。'
- en: The `az_count` input variable allows us to configure how much redundancy we
    want within our solution. This will affect the high availability of the solution
    but also the cost of the deployment. As you can imagine, cost is also a tightly
    regulated characteristic of cloud infrastructure deployments.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`az_count` 输入变量允许我们配置解决方案中所需的冗余程度。这将影响解决方案的高可用性，也会影响部署的成本。正如你可以想象的那样，成本也是云基础设施部署中一个严格管理的特性。'
- en: Consistent naming and tagging
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一致的命名和标签
- en: 'Unlike the AWS console, Azure is designed in such a way that it is extremely
    easy to get an application-centric view of your deployment. For this, you can
    use resource groups:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS 控制台不同，Azure 的设计使得获得应用程序中心的部署视图变得非常容易。为此，你可以使用资源组：
- en: '[PRE8]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s still important to tag the resources that you deploy that indicate what
    application and what environment they belong to. This helps with other reporting
    needs, such as budgets and compliance. Almost all resources within the Azure provider
    have a `map` attribute called `tags`. Unlike AWS, each resource has a `name` value
    as a required attribute.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然很重要的是对你部署的资源进行标签标注，指明它们属于哪个应用程序和环境。这有助于满足其他报告需求，比如预算和合规性。几乎所有 Azure 提供商中的资源都有一个
    `map` 属性叫做 `tags`。与 AWS 不同，每个资源都有一个 `name` 值作为必需属性。
- en: Virtual network
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: 'Just as we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we need
    to construct a virtual network and keep its address space as tight as possible
    to avoid gobbling up unnecessary address space for the broader organization in
    the future:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中做的那样，我们需要构建一个虚拟网络，并将其地址空间保持尽可能紧凑，以避免未来为更广泛的组织浪费不必要的地址空间：
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Network creation in Azure is simpler than what we did with AWS because we don’t
    have to segment our subnets based on Availability Zone:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 中创建网络比我们在 AWS 中做的要简单，因为我们不需要根据可用区来细分子网：
- en: '[PRE10]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Load balancing
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'As we discussed in the design, the Azure Load Balancer service is structured
    quite a bit differently than AWS’s equivalent offering:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中讨论的那样，Azure 负载均衡器服务与 AWS 对应服务的结构差异很大：
- en: '[PRE11]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s important to call out that to achieve zonal resiliency, we need to ensure
    that all components of our architecture are deployed in a zone-resilient way.
    This often requires setting the `zones` attribute and specifying which Availability
    Zones we want to provision into.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 需要特别指出的是，要实现区域弹性，我们需要确保架构中的所有组件都以区域弹性的方式部署。这通常需要设置 `zones` 属性，并指定我们希望部署的可用区。
- en: 'The backend configuration of Azure Load Balancer is a simple logical container
    for the backend address pool:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 负载均衡器的后端配置是一个简单的逻辑容器，用于存放后端地址池：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This logical container must be linked to either static VMs or a VMSS:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个逻辑容器必须链接到静态虚拟机或虚拟机规模集（VMSS）：
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding backend address pool association resource, we are iterating
    over `var.az_count`. This is the same number that we iterate over the VMs, which
    allows us to put a single VM into each Availability Zone. Unlike AWS, where the
    load balancer rules are split between a listener and a target group configuration,
    an Azure load balancer rule combines the two and then links them to a corresponding
    health probe:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的后端地址池关联资源中，我们正在遍历 `var.az_count`。这个数字与我们遍历虚拟机的数量相同，这使得我们可以将每个虚拟机放入每个可用区。与
    AWS 不同，AWS 的负载均衡规则将监听器和目标组配置拆分开来，而 Azure 的负载均衡规则将二者结合，并将其链接到相应的健康探针：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice how the load balancer rule connects many of the components, including
    the frontend IP configuration, the listener on AWS, the health probe, and the
    backend address pool – the target group on AWS.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意负载均衡规则是如何连接多个组件的，包括前端 IP 配置、AWS 上的监听器、健康探针和后端地址池——AWS 上的目标组。
- en: Network security
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: 'First, we need to set up the logical ASG for each application architectural
    component. We’ll have one for the frontend and one for the backend:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为每个应用程序架构组件设置逻辑 ASG。我们将为前端和后端分别设置一个：
- en: '[PRE15]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we need to create NSGs that allow the necessary traffic into each of
    the ASGs:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建 NSG，以允许必要的流量进入每个 ASG：
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Secrets management
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 秘密管理
- en: 'First, we’ll set up Key Vault:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将设置 Key Vault：
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we’ll set up a managed identity for each application architectural component:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将为每个应用程序架构组件设置托管身份：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we’ll grant the managed identity the necessary privileges using Azure
    role assignments:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Azure 角色分配授予托管身份必要的权限：
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: VMs
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机
- en: 'First, we’ll obtain the VM image from our input variables. We built this VM
    image with Packer and provisioned it into a different Azure resource group:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从输入变量中获取虚拟机镜像。我们使用 Packer 构建了这个虚拟机镜像，并将其部署到一个不同的 Azure 资源组中：
- en: '[PRE20]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we’ll create the network interface for each VM by iterating over the
    `var.az_count` input variable:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过遍历 `var.az_count` 输入变量来为每个虚拟机创建网络接口：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we’ll set up the VM with all the necessary attributes and link it
    to the network interface, the VM image, and the managed identity:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将设置虚拟机，配置所有必要的属性，并将其链接到网络接口、虚拟机镜像和托管身份：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With that, we’ve implemented the Packer and Terraform solutions and have a working
    code base that will build VM images for both our frontend and backend application
    components and provision our cloud environment into Azure. In the next section,
    we’ll dive into the YAML and Bash and implement the GitHub Actions workflows.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们已实现了 Packer 和 Terraform 解决方案，并且有了一个可工作的代码库，该代码库将为我们的前端和后端应用程序组件构建虚拟机镜像，并将我们的云环境部署到
    Azure。接下来，我们将深入研究 YAML 和 Bash，并实现 GitHub Actions 工作流。
- en: Automating the deployment
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化部署
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base consisting of application code
    and operating system configuration encapsulated within a Packer template. These
    two application components are then deployed into a cloud environment on Azure
    that’s defined within our Terraform code base.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中讨论的那样，我们的解决方案由两个应用程序组件组成：前端和后端。每个组件都包含应用程序代码和操作系统配置，这些内容都被封装在 Packer
    模板中。这两个应用程序组件随后被部署到定义在 Terraform 代码库中的 Azure 云环境中。
- en: 'Just as we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with the
    AWS solution, there is an additional code base that we have to discuss: our automation
    pipelines on GitHub Actions.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 讨论 AWS 解决方案时所做的那样，还有一个额外的代码库需要讨论：我们的
    GitHub Actions 自动化流水线。
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went over the folder
    structure for our code base and where our GitHub Actions fit in so that we know
    that our automation pipelines are called workflows, and they’re stored in `/.github/workflows`.
    Each of our code bases is stored in its respective folder. Our solutions source
    code repository’s folder structure will look like this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 中，我们回顾了代码库的文件夹结构以及 GitHub Actions
    如何适应其中，确保我们知道我们的自动化流水线被称为工作流，并存储在 `/.github/workflows` 中。我们的每个代码库都存储在各自的文件夹中。我们的解决方案源代码仓库的文件夹结构将如下所示：
- en: '`.``github`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.``github`'
- en: '`workflows`'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workflows`'
- en: '`dotnet`'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dotnet`'
- en: '`backend`'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend`'
- en: '`frontend`'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frontend`'
- en: '`packer`'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packer`'
- en: '`backend`'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend`'
- en: '`frontend`'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frontend`'
- en: '`terraform`'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terraform`'
- en: 'As per our design, we will have GitHub Actions workflows that will execute
    Packer and build VM images for both the frontend (for example, `packer-frontend.yaml`)
    and the backend (for example, `packer-backend.yaml`). We’ll also have workflows
    that will run `terraform plan` and `terraform apply`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，我们将会有 GitHub Actions 工作流来执行 Packer 并为前端（例如 `packer-frontend.yaml`）和后端（例如
    `packer-backend.yaml`）构建虚拟机镜像。我们还会有工作流来运行 `terraform plan` 和 `terraform apply`：
- en: '`.``github`'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.``github`'
- en: '`workflows`'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workflows`'
- en: '`packer-backend.yaml`'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packer-backend.yaml`'
- en: '`packer-frontend.yaml`'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packer-frontend.yaml`'
- en: '`terraform-apply.yaml`'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terraform-apply.yaml`'
- en: '`terraform-plan.yaml`'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terraform-plan.yaml`'
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went into greater detail
    on the GitFlow process and how it interacts with our GitHub Actions workflows,
    so for now, let’s dig into how these pipelines will differ when targeting the
    Azure platform.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，我们更详细地讲解了 GitFlow 流程及其如何与我们的 GitHub
    Actions 工作流交互，因此现在让我们深入探讨这些流水线在面向 Azure 平台时的差异。
- en: Packer
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Packer
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went into great detail
    about each step of the GitHub Actions workflow that executes Packer to build VM
    images. Thanks to the nature of Packer’s cloud-agnostic architecture, this overwhelmingly
    stays the same. The only thing that changes is the final step where we execute
    Packer.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，我们详细介绍了执行 Packer 来构建虚拟机镜像的 GitHub
    Actions 工作流的每一个步骤。由于 Packer 的云无关架构的特性，这些内容基本上保持不变。唯一变化的是我们执行 Packer 的最后一步。
- en: Because Packer needs to be configured to build a VM on Microsoft Azure, we need
    to pass in different input variables that are Azure-specific. This includes the
    Microsoft Azure credential attributes, an Azure region, and an Azure resource
    group name.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Packer 需要配置以在 Microsoft Azure 上构建虚拟机，因此我们需要传入一些特定于 Azure 的输入变量。这些变量包括 Microsoft
    Azure 凭证属性、Azure 区域以及 Azure 资源组名称。
- en: Just as we did with the input variables for the Packer template for AWS, we
    must ensure that all Azure input variables are prefixed with `azure_`. This will
    help if we ever want to introduce multi-targeting as many cloud platforms will
    have similar required inputs, such as target region and VM size. While most clouds
    will have similar required inputs, the input values are not interchangeable.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们为 AWS 的 Packer 模板准备输入变量一样，我们必须确保所有 Azure 输入变量都以 `azure_` 为前缀。如果我们将来想要引入多目标支持，这将非常有帮助，因为许多云平台将具有相似的必需输入，比如目标区域和虚拟机大小。虽然大多数云平台有相似的输入要求，但输入值并不是可以互换的。
- en: For example, both Azure and AWS require you to specify the region that you want
    Packer to provide the temporary VM into and the resulting VM image to be stored.
    On Azure, the region has a value of `westus2`, while on AWS, it has a value of
    `us-west-2`. They may seem very similar, but they are miles apart (pun intended).
    Azure West US 2 region is completely different than AWS’s West US 2 region – in
    fact, besides just being on different cloud platforms, they are physically different
    locations, with Azure’s West US 2 region being located in Washington State and
    AWS’s West US 2 region being located in Oregon. Neighbors, yes, the same thing
    – hardly.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Azure 和 AWS 都要求您指定 Packer 将临时虚拟机放置在的区域，并且最终的虚拟机镜像将存储在哪里。在 Azure 上，区域的值为 `westus2`，而在
    AWS 上，则为 `us-west-2`。它们看起来非常相似，但实际上相距甚远（玩笑话）。Azure 的西部美国 2 区域与 AWS 的西部美国 2 区域完全不同——事实上，除了在不同的云平台上，它们物理位置也不同，Azure
    的西部美国 2 区域位于华盛顿州，而 AWS 的西部美国 2 区域则位于俄勒冈州。虽然邻近，但绝对不是同一个地方。
- en: This goes back to Packer’s strategy of isolating platform-specific configuration
    within the builders. Therefore, if we are going to do multi-targeting, the AWS
    plugin is going to need input variables that are AWS-specific and the Azure plugin
    is going to need input variables that are Azure-specific. Hence, when we merge
    these plugins into one Packer template, we’ll need input variables for both.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这又回到了 Packer 将平台特定配置隔离在构建器中的策略。因此，如果我们要做多目标支持，AWS 插件需要特定于 AWS 的输入变量，而 Azure
    插件需要特定于 Azure 的输入变量。因此，当我们将这些插件合并成一个 Packer 模板时，我们需要为两者都准备输入变量。
- en: As a result, our `aws_primary_region`, which has a value of `us-west-2`, can
    sit right next to our `azure_primary_region`, which has a value of `westus2`,
    without any conflicts or confusion. Likewise, our `aws_instance_type` with a value
    of `t2.small` can sit right next to our `azure_vm_size` with a value of `Standard_DS2_v2`.
    The differences can get even more radical as you take advantage of more platform-specific
    capabilities within the builders.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，我们的 `aws_primary_region`，其值为 `us-west-2`，可以与 `azure_primary_region`（值为 `westus2`）并排显示，而不会产生任何冲突或混淆。同样，我们的
    `aws_instance_type`，值为 `t2.small`，也可以与 `azure_vm_size`，值为 `Standard_DS2_v2`，并排显示。随着我们在构建器中利用更多平台特定的功能，差异可能会更加明显。
- en: 'The GitHub Actions workflow YAML files are identical to Azure, except for the
    additional input variables that need to be specified:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Actions 工作流 YAML 文件与 Azure 相同，唯一不同的是需要指定的额外输入变量：
- en: '[PRE23]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding code references the four Azure credential attributes, which are
    stored as GitHub Actions variables and secrets, and transfers them to Packer using
    environment variables with the `PKR_VAR_` prefix.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码引用了四个 Azure 凭证属性，这些属性作为 GitHub Actions 变量和秘密存储，并通过带有 `PKR_VAR_` 前缀的环境变量传递给
    Packer。
- en: Terraform
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Terraform
- en: With both of our VM images built and their versions input into our `tfvars`
    file, our Terraform automation pipeline is ready to take the reigns and not only
    provision our environment but deploy our solution as well (although not technically).
    The deployment was technically done within the `packer build` process, with the
    physical deployment packages being copied to the home directory and the Linux
    service setup primed and ready. Terraform is finishing the job by actually launching
    VMs using these images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建完我们的两个虚拟机镜像并将其版本输入到 `tfvars` 文件后，我们的 Terraform 自动化流水线已经准备好接管工作，不仅能够配置我们的环境，还能部署我们的解决方案（尽管从技术上讲，部署过程是在
    `packer build` 中完成的，物理部署包被复制到主目录，并且 Linux 服务已准备就绪）。Terraform 完成了工作，通过实际启动这些镜像来创建虚拟机。
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went into great detail
    about each step of the GitHub Actions workflow that executes Terraform to provision
    the cloud environment and deploy the application code. Thanks to the nature of
    Terraform’s cloud-agnostic architecture, this overwhelmingly stays the same. The
    only thing that changes is the final step where we execute Terraform.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，我们详细介绍了执行 Terraform 来配置云环境和部署应用程序代码的
    GitHub Actions 工作流的每一步。得益于 Terraform 的云无关架构，这些步骤大体保持不变。唯一变化的是我们执行 Terraform 的最后一步。
- en: 'Just like we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with the
    AWS provider, we can set the authentication context using environment variables
    that are specific to the `azurerm` provider. In this case, the four Azure credentials
    attributes are passed in with the following environment variables:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中与 AWS 提供商一起做的一样，我们可以使用特定于 `azurerm`
    提供商的环境变量来设置身份验证上下文。在这种情况下，四个 Azure 凭证属性通过以下环境变量传递：
- en: '`ARM_TENANT_ID`'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_TENANT_ID`'
- en: '`ARM_SUBSCRIPTION_ID`'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_SUBSCRIPTION_ID`'
- en: '`ARM_CLIENT_ID`'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_CLIENT_ID`'
- en: '`ARM_CLIENT_SECRET`'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARM_CLIENT_SECRET`'
- en: Just like we did in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with the
    AWS provider, we need to configure the Azure-specific backend that stores the
    Terraform state using the `-backend-config` command-line arguments to the `terraform
    init` command. Unlike AWS, which only specifies an S3 bucket name to configure
    the backend to save the Terraform state to S3, to configure the Azure backend,
    we need to specify three fields to triangulate a location in Azure Blob storage
    to save the Terraform state – a resource group, storage account, and Blob storage
    container.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中与 AWS 提供商一起做的一样，我们需要使用 `-backend-config`
    命令行参数配置用于存储 Terraform 状态的 Azure 特定后端。与只需指定一个 S3 存储桶名称来配置 AWS 后端保存 Terraform 状态到
    S3 不同，为了配置 Azure 后端，我们需要指定三个字段，以便在 Azure Blob 存储中三角定位存储 Terraform 状态的位置——资源组、存储账户和
    Blob 存储容器。
- en: 'The hierarchy of Azure resources looks like this:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 资源的层次结构如下所示：
- en: Resource group
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源组
- en: Storage account
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储账户
- en: Blob storage container
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blob 存储容器
- en: Terraform state files
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Terraform 状态文件
- en: 'Like with the AWS provider, the backend uses a *key* and the Terraform workspace
    name to uniquely identify the location to store state files:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS 提供商类似，后端使用*密钥*和 Terraform 工作区名称来唯一标识存储状态文件的位置：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notice how, unlike with the AWS solution, we don’t need to perform a targeted
    `terraform apply`. This is because we don’t need to do dynamic calculations based
    on the number of Availability Zones in the region to configure our virtual network.
    This is due to Azure Virtual Network and its subnets spanning all Availability
    Zones within the region whereas, on AWS, a subnet is constrained to a specific
    Availability Zone within the parent virtual network’s region.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与 AWS 解决方案不同的是，我们不需要执行特定的 `terraform apply`。这是因为我们不需要根据区域中的可用区数量来动态计算，从而配置我们的虚拟网络。这是因为
    Azure 虚拟网络及其子网跨越了区域内所有的可用区，而在 AWS 中，子网被限制在父虚拟网络区域的特定可用区内。
- en: These subtle architectural differences between the cloud platforms can create
    radical structural changes even when deploying the same solution using the same
    technologies. It is a sobering reminder that while knowledge of the core concepts
    we looked at in *Chapters 4* through *6* will help us transcend to a multi-cloud
    point of view, to implement practical solutions, we need to understand the subtle
    nuances of each platform.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 云平台之间这些微妙的架构差异，即使使用相同的技术部署相同的解决方案，也可能会带来根本性的结构变化。这提醒我们，虽然我们在*第 4 章*至*第 6 章*中学习的核心概念有助于我们超越单一云平台，走向多云视角，但要实施实际解决方案，我们需要理解每个平台的微妙差异。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we built a multi-tier cloud architecture using Azure VMs with
    a fully operation GitFlow process and an end-to-end CI/CD pipeline using GitHub
    Actions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用 Azure 虚拟机构建了一个多层次云架构，并配备了完整的 GitFlow 流程和基于 GitHub Actions 的端到端 CI/CD
    管道。
- en: 'In the next chapter, our fearless leader at Söze Enterprises will be throwing
    us into turmoil with some big new ideas, and we’ll have to respond to his call
    to action. It turns out our CEO, Keyser, has been up late watching some YouTube
    videos about the next big thing – containers – and after talking with his pal
    Satya on his superyacht, he has decided that we need to refactor our whole solution
    to run on Docker and Kubernetes. Luckily, the good people at Microsoft have a
    service that might help us out: **Azure Kubernetes** **Service** (**AKS**).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们在 Söze Enterprises 的无畏领导者将带领我们进入动荡的新局面，提出一些大新想法，我们将不得不回应他的号召。原来，我们的
    CEO Keyser 最近熬夜看了一些关于下一大热点——容器——的 YouTube 视频，在与他的朋友 Satya 一起乘坐超级游艇时，他决定我们需要重新构建整个解决方案，使其能够在
    Docker 和 Kubernetes 上运行。幸运的是，微软的好心人提供了一项可能帮助我们的服务：**Azure Kubernetes** **服务**（**AKS**）。
