- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Implementing Architecture Patterns
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现架构模式
- en: 'Kubernetes is the most popular container orchestrator in production. This platform
    provides different resources that allow us to deploy our applications with high
    resilience and with their components distributed cluster-wide while the platform
    itself runs with high availability. In this chapter, we will learn how these resources
    can provide different application architecture patterns, along with use cases
    and best practices to implement them. We will also review different options for
    managing application data and learn how to manage the health of our applications
    to make them respond to possible health and performance issues in the most effective
    way. At the end of this chapter, we will review how Kubernetes provides security
    patterns that improve application security. This chapter will give you a good
    overview of which Kubernetes resources will fit your applications’ requirements
    most accurately. The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是生产环境中最流行的容器调度器。该平台提供了不同的资源，允许我们以高韧性和分布式的方式部署应用程序，同时平台本身具有高可用性。在本章中，我们将学习这些资源如何提供不同的应用架构模式，并结合用例和最佳实践来实施它们。我们还将回顾不同的应用数据管理选项，并学习如何管理应用程序的健康，以最有效的方式应对可能出现的健康和性能问题。在本章的最后，我们将回顾
    Kubernetes 提供的安全模式，以提高应用程序的安全性。本章将为您提供一个关于哪些 Kubernetes 资源最适合您应用需求的良好概览。本章将涵盖以下主题：
- en: Applying Kubernetes resources to common application patterns
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Kubernetes 资源应用于常见应用模式
- en: Understanding advanced Pod application patterns
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解高级 Pod 应用模式
- en: Verifying application health
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证应用健康状况
- en: Resource management and scalability
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源管理与可扩展性
- en: Improving application security with Pods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pods 提升应用安全性
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the labs for this chapter at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9),
    where you will find some extended explanations that have been omitted in this
    chapter’s content to make it easier to follow. The *Code In Action* video for
    this chapter can be found at [https://packt.link/JdOIY](https://packt.link/JdOIY).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的实验可以在 [https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9)
    找到，您将在那里找到一些扩展的解释，这些内容在本章中被省略，以便更容易理解。本章的 *Code In Action* 视频可以在 [https://packt.link/JdOIY](https://packt.link/JdOIY)
    找到。
- en: We will start this chapter by reviewing some of the resource types that were
    presented in [*Chapter 8*](B19845_08.xhtml#_idTextAnchor170), *Deploying Applications
    with the Kubernetes Orchestrator*, and present some common use cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从回顾 [*第 8 章*](B19845_08.xhtml#_idTextAnchor170)，《使用 Kubernetes 调度器部署应用》，开始，并介绍一些常见的用例。
- en: Applying Kubernetes resources to common application patterns
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Kubernetes 资源应用于常见应用模式
- en: 'The Kubernetes container orchestrator is based on resources that are managed
    by different controllers. By default, our applications can use one of the following
    to run our processes as containers:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 容器调度器基于由不同控制器管理的资源。默认情况下，我们的应用程序可以使用以下其中之一来运行容器中的进程：
- en: Pods
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods
- en: ReplicaSets
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaSets
- en: ReplicaControllers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaControllers
- en: Deployments
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deployments
- en: StatefulSets
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSets
- en: DaemonSets
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSets
- en: Jobs
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jobs
- en: CronJobs
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CronJobs
- en: This list shows the standard or default resources that are allowed on every
    Kubernetes installation, but we can create custom resources to implement any non-standard
    or more specific application behavior. In this section, we are going to learn
    about these standard resources so that we can decide which one will fit our application’s
    needs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表显示了每个 Kubernetes 安装中允许的标准或默认资源，但我们可以创建自定义资源来实现任何非标准或更具体的应用行为。在本节中，我们将学习这些标准资源，以便我们能够决定哪个最适合我们的应用需求。
- en: Pods
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pods
- en: Pods are the minimal unit for deploying workloads on a Kubernetes cluster. A
    Pod can contain multiple containers, and we have different mechanisms for doing
    this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是在 Kubernetes 集群中部署工作负载的最小单元。一个 Pod 可以包含多个容器，我们有不同的机制来实现这一点。
- en: 'The following applies to all the containers running inside a Pod by default:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，以下内容适用于 Pod 内运行的所有容器：
- en: They all share the network namespace, so they all refer to the same localhost
    and run with the same IP address.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们共享网络命名空间，因此它们都指向相同的本地主机，并使用相同的 IP 地址运行。
- en: They are all scheduled together in the same host.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都在同一主机上调度。
- en: They all share the namespaces that can be defined at the container level. We
    will define resource limits (using cgroups) for each container, although we can
    also define resource limits at the Pod level. Pod resource limits will be applied
    for all the containers.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们共享可以在容器级别定义的命名空间。我们将为每个容器定义资源限制（使用 cgroups），尽管我们也可以在 Pod 级别定义资源限制。Pod 资源限制将应用于所有容器。
- en: Volumes attached to a Pod are available to all containers running inside.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到 Pod 的卷对所有在 Pod 内运行的容器都是可用的。
- en: So, we can see that Pods are groups of containers running together that share
    kernel namespaces and compute resources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以看到 Pods 是一组一起运行的容器，它们共享内核命名空间和计算资源。
- en: You will find a lot of Pods running a single container, and this is completely
    fine. The distribution of containers inside different Pods depends on your application’s
    components distribution. You need to ask yourself whether or not some processes
    must run together. For example, you can put together two containers that need
    to communicate fast, or you may need to keep track of files created by one of
    them without sharing a remote data volume. But it is very important to understand
    that all the containers running in a Pod scale and replicate together. This means
    that multiple replicas of a Pod will execute the same number of replicas of their
    containers, and thus your application’s behavior can be impacted because the same
    type of process will run multiple times, and it will also access your files at
    the same time. This, for example, will break the data in your database or may
    lead to data inconsistency. Therefore, you need to decide wisely whether or not
    your application should distribute your application’s containers into different
    Pods or run them together.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现许多 Pods 只运行一个容器，这完全是可以接受的。容器在不同 Pods 之间的分布取决于你的应用程序组件的分布。你需要问自己，某些进程是否必须一起运行。例如，你可以将需要快速通信的两个容器放在一起，或者你可能需要在不共享远程数据卷的情况下跟踪其中一个容器创建的文件。但非常重要的是要理解，所有在
    Pod 中运行的容器是一起扩展和复制的。这意味着，Pod 的多个副本将执行相同数量的容器副本，因此应用程序的行为可能会受到影响，因为相同类型的进程会多次运行，并且它们也会同时访问你的文件。例如，这会破坏你的数据库中的数据，或者可能导致数据不一致。因此，你需要明智地决定是否将应用程序的容器分布到不同的
    Pods 中，或者将它们一起运行。
- en: Kubernetes will keep track of the status of each container running inside a
    Pod by executing the Pod’s defined probes. Each container should have its own
    probes (different types of probes exist, as we will learn in the *Verifying application
    health* section). But at this point, we have to understand that the health of
    all the containers inside a Pod controls the overall behavior of the Pod. If one
    of the containers dies, the entire Pod is set as unhealthy, triggering the defined
    Kubernetes event and resource’s behavior. Thus, we can execute multiple Service
    containers in parallel or prepare our application by executing some pre-processes
    that can, for example, populate some minimal filesystem resources, binaries, permissions,
    and so on. These types of containers, which run before the actual application
    processes, are called `initContainers`, which is the key that’s used to define
    them) sequentially before any other container; therefore, if any of these initial
    containers fail, the Pod will not run as expected.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 会通过执行 Pod 定义的探针来跟踪每个容器在 Pod 中的状态。每个容器应该有自己的探针（存在不同类型的探针，我们将在*验证应用程序健康*部分学习）。但是此时，我们必须理解，Pod
    内所有容器的健康状况会控制整个 Pod 的行为。如果其中一个容器崩溃，整个 Pod 会被标记为不健康，从而触发定义的 Kubernetes 事件和资源行为。因此，我们可以并行执行多个服务容器，或者通过执行一些预处理来准备我们的应用程序，例如填充一些最小的文件系统资源、二进制文件、权限等等。这些在实际应用程序进程之前运行的容器称为
    `initContainers`（它是定义它们的关键字），并且会在其他容器之前按顺序运行；因此，如果这些初始容器中的任何一个失败，Pod 将无法按预期运行。
- en: Important note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The Kubernetes 1.25 release introduced `kubectl debug` action followed by the
    name of the Pod to which your terminal should attach (shared kernel namespaces).
    We will provide a quick example of this in the *Labs* section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.25 版本引入了 `kubectl debug` 操作，后跟 Pod 的名称，你的终端应该连接到该 Pod（共享内核命名空间）。我们将在
    *实验室* 部分提供一个快速示例。
- en: 'Let’s review how we write the manifest required for creating a Pod:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下如何编写创建 Pod 所需的清单：
- en: '![Figure 9.1 – Pod manifest](img/B19845_09_1.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – Pod 清单](img/B19845_09_1.jpg)'
- en: Figure 9.1 – Pod manifest
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – Pod 清单
- en: All the manifests for Kubernetes have at least the `apiVersion`, `kind`, and
    `metadata` keys, and these are used to define which API version will be used to
    reach the associated API server path, which type of resource we are defining,
    and information that uniquely describes the resource within the Kubernetes cluster,
    respectively. We can access all the resource manifest information via the Kubernetes
    API by using the JSON or YAML keys hierarchy; for example, to retrieve a Pod’s
    name, we can use `.metadata.name` to access its key. The properties of the resource
    should usually be written in the `spec` or `data` section. Kubernetes roles, role
    bindings (in cluster and namespaces scopes), Service accounts, and other resources
    do not include either `data` or `spec` keys for declaring their functionality.
    And we can even create custom resources, with custom definitions for declaring
    their properties. In the default workload resources, we will always use the `spec`
    section to define the behavior of our resource.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Kubernetes 的清单文件至少包含 `apiVersion`、`kind` 和 `metadata` 这三个键，它们分别用于定义将使用哪个
    API 版本来访问关联的 API 服务器路径，定义我们正在定义的资源类型，以及描述资源在 Kubernetes 集群内唯一标识的信息。我们可以通过 Kubernetes
    API 使用 JSON 或 YAML 键层次结构访问所有资源清单信息；例如，要获取 Pod 的名称，我们可以使用 `.metadata.name` 来访问其键。资源的属性通常应写入
    `spec` 或 `data` 部分。Kubernetes 角色、角色绑定（在集群和命名空间范围内）、服务帐户以及其他资源没有包含 `data` 或 `spec`
    键来声明其功能。我们甚至可以创建自定义资源，使用自定义定义来声明其属性。在默认的工作负载资源中，我们总是会使用 `spec` 部分来定义资源的行为。
- en: Notice that in the previous code snippet, the `containers` key is an array.
    This allows us to define multiple containers, as we already mentioned, and the
    same happens with initial containers; we will define a list of containers in both
    cases and we will need at least the image that the container runtime must use
    and the name for the container.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码片段中，`containers` 键是一个数组。这允许我们定义多个容器，正如我们之前提到的，初始容器也是如此；在两种情况下，我们都将定义一个容器列表，并且我们至少需要容器运行时必须使用的镜像和容器的名称。
- en: Important note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We can use `kubectl explain pod.spec.containers --recursive` to retrieve all
    the existent keys under the `spec` section for a defined resource. The `explain`
    action allows you to retrieve all the keys for each resource directly from the
    Kubernetes cluster; this is important as it doesn’t depend on your `kubectl` binary
    version. The output of this action also shows which keys can be changed at runtime,
    once the resource has been created in the cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `kubectl explain pod.spec.containers --recursive` 来检索定义资源下 `spec` 部分的所有现有键。`explain`
    操作允许你直接从 Kubernetes 集群中检索每个资源的所有键；这很重要，因为它不依赖于你 `kubectl` 二进制文件的版本。该操作的输出还显示哪些键可以在运行时更改，一旦资源在集群中创建。
- en: It is important to mention here that Pods by themselves don’t have cluster-wide
    auto-healing. This means that when you run a Pod and it is considered unhealthy
    for whatever reason (any of the containers is considered unhealthy) in a host
    within the cluster, it will not execute on another host. Pods include the `restartPolicy`
    property to manage the behavior of Pods when they die. We can set this property
    to `Always` (always restart the container’s Pods), `OnFailure` (only restart containers
    when they fail), or `Never`. A new Pod will never be recreated on another cluster
    host. We will need more advanced resources for managing the containers’ life cycle
    cluster-wide; these will be discussed in the subsequent sections.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要特别提到的是，Pod 本身没有集群范围的自动修复功能。这意味着，当你运行一个 Pod，并且它由于某种原因（任何容器被认为不健康）被认为不健康时，它不会在集群中的其他主机上执行。Pod
    包含 `restartPolicy` 属性来管理 Pod 死亡后的行为。我们可以将该属性设置为 `Always`（始终重启容器的 Pod）、`OnFailure`（仅在容器失败时重启）、或
    `Never`。新的 Pod 永远不会在其他集群主机上重新创建。我们需要更高级的资源来管理集群范围内容器的生命周期，这些将在后续章节中讨论。
- en: Pods are used to run a test for an application or one of its components, but
    we never use them to run actual Services because Kubernetes just keeps them running;
    it doesn’t manage their updates or reconfiguration. Let’s review how ReplicaSets
    solve these situations when we need to keep our application’s containers up and
    running.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 用于运行应用程序或其组件的测试，但我们从不使用它们来运行实际的服务，因为 Kubernetes 只是保持它们运行；它不管理它们的更新或重新配置。让我们回顾一下副本集是如何解决在需要保持应用程序容器持续运行时的这些问题。
- en: ReplicaSets
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 副本集（ReplicaSets）
- en: 'A ReplicaSet is a set of Pods that should be running at the same time for an
    application’s components (or for the application itself if it just has one component).
    To define a ReplicaSet resource, we need to write the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 是一组应该同时运行的 Pods，用于应用的组件（或者如果应用只有一个组件，则用于应用本身）。为了定义一个 ReplicaSet 资源，我们需要编写以下内容：
- en: A `selector` section in which we define which Pods are part of the resource
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `selector` 部分，用于定义哪些 Pods 是资源的一部分
- en: The number of replicas required to keep the resource healthy
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维持资源健康所需的副本数量
- en: A Pod template in which we define how new Pods should be created when one of
    the Pods in the set dies
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Pod 模板，用于定义当集合中的某个 Pod 死亡时，如何创建新的 Pods
- en: 'Let’s review the syntax of these resources:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些资源的语法：
- en: '![Figure 9.2 – ReplicaSet manifest](img/B19845_09_2.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – ReplicaSet 清单](img/B19845_09_2.jpg)'
- en: Figure 9.2 – ReplicaSet manifest
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – ReplicaSet 清单
- en: As you can see, the `template` section describes a Pod definition inside the
    ReplicaSet’s `spec` section. This `spec` section also includes the `selector`
    section, which defines what Pods will be included. We can use `matchLabels` to
    include exact label-key pairs from Pods, and `matchExpressions` to include advanced
    rules such as the existence of a defined label or its value included in a list
    of strings.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`template` 部分描述了 ReplicaSet 中 `spec` 部分的 Pod 定义。这个 `spec` 部分还包括 `selector`
    部分，用于定义哪些 Pods 会被包含。我们可以使用 `matchLabels` 来包含来自 Pods 的精确标签键值对，使用 `matchExpressions`
    来包括一些高级规则，如定义的标签是否存在，或者其值是否包含在字符串列表中。
- en: Important note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Selectors from ReplicaSet resources apply to running Pods too. This means that
    you have to take care of the labels that uniquely identify your application’s
    components. ReplicaSet resources are namespaced, so we can use `kubectl get pods
    -–show-labels` before actually creating a ReplicaSet to ensure that the right
    Pods will be included in the set.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 资源的选择器也适用于正在运行的 Pods。这意味着你需要注意那些唯一标识你应用组件的标签。ReplicaSet 资源是有命名空间的，因此我们可以在实际创建
    ReplicaSet 之前使用 `kubectl get pods --show-labels` 来确保正确的 Pods 会被包含在集合中。
- en: In the Pod template, we will define the volumes to be attached to the different
    containers created by the ReplicaSet, but it is important to understand that these
    volumes are common to all the replicas. Therefore, all container replicas will
    attach the same volumes (in fact, the hosts where they run mount the volumes and
    the kubelet makes them available to the Pods’ containers), which may generate
    issues if your application does not allow such a situation. For example, if you
    are deploying a database, running more than one replica that’s attaching the same
    volume will probably break your data files. We should ensure that our application
    can run more than one replicated process at a time, and if not, ensure we apply
    the appropriate `ReadWriteOnce` mode flag in the `accessMode` key. We will deep
    dive into this key, its importance, and its meaning for our workloads in *Chapter
    10*, *Leveraging Application Data Management* *in Kubernetes*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pod 模板中，我们将定义要附加到 ReplicaSet 创建的不同容器上的卷，但需要理解的是，这些卷对于所有副本都是共享的。因此，所有容器副本都会附加相同的卷（实际上，运行它们的主机挂载这些卷，kubelet
    会将它们提供给 Pods 的容器），如果你的应用不允许这种情况，可能会产生问题。例如，如果你正在部署数据库，运行多个副本并附加相同的卷可能会破坏你的数据文件。我们应该确保我们的应用可以同时运行多个复制进程，如果不能，请确保在
    `accessMode` 键中应用适当的 `ReadWriteOnce` 模式标志。我们将在 *第 10 章*，*在 Kubernetes 中利用应用数据管理*
    中深入探讨这个键、它的重要性以及它对我们工作负载的意义。
- en: The most important key in ReplicaSets is the `replicas` key, which defines the
    number of active healthy Pods that should be running. This allows us to scale
    up or down the number of instances for our application’s processes. The names
    of the Pods associated with a ReplicaSet will follow `<REPLICASET_NAME>-<POD_RANDOM_UNIQUE_GENERATED_ID>`.
    This helps us understand which ReplicaSet generated them. We also can review the
    ReplicaSet creator by using `kubectl get pod –o yaml`. The `metadata.OwnerReferences`
    key shows the ReplicaSet that finally created each Pod resource.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSets 中最重要的键是 `replicas` 键，它定义了应该运行的活动健康 Pods 数量。这允许我们对应用的进程数量进行扩展或缩减。与
    ReplicaSet 关联的 Pods 的名称将遵循 `<REPLICASET_NAME>-<POD_RANDOM_UNIQUE_GENERATED_ID>`
    格式。这有助于我们理解哪些 ReplicaSet 创建了它们。我们还可以通过使用 `kubectl get pod –o yaml` 来查看 ReplicaSet
    的创建者。`metadata.OwnerReferences` 键显示了最终创建每个 Pod 资源的 ReplicaSet。
- en: 'We can modify the number of replicas of a running ReplicaSet resource using
    any of the following methods:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下任一方法修改正在运行的ReplicaSet资源的副本数：
- en: Editing the running ReplicaSet resource directly in Kubernetes using `kubectl`
    `edit <REPLICASET_NAME>`
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl`直接编辑Kubernetes中正在运行的ReplicaSet资源：`edit <REPLICASET_NAME>`
- en: Patching the current ReplicaSet resource using `kubectl patch`
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl patch`修补当前的ReplicaSet资源
- en: 'Using the `scale` action with `kubectl`, setting the number of replicas: `kubectl
    scale rs --replicas <``NUMBER_OF_REPLICAS> <REPLICASET_NAME>`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl`的`scale`命令，设置副本数：`kubectl scale rs --replicas <``NUMBER_OF_REPLICAS>
    <REPLICASET_NAME>`
- en: Although changing the number of replicas works automatically, other changes
    don’t work so well. In the Pod template, if we change the image to use for creating
    containers, the resource will show this change, but the current associated Pods
    will not change. This is because ReplicaSets do not manage their changes; we need
    to work with Deployment resources, which are more advanced. To make any change
    available in a ReplicaSet, we need to recreate the Pods manually by just removing
    the current Pods (using `kubectl delete pod <REPLICASET_POD_NAMES>`) or scaling
    the replicas down to zero and scaling up after all are deleted. Any of these methods
    will create fresh new replicas, using the new ReplicaSet definition.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然更改副本数会自动生效，但其他更改则效果不佳。在Pod模板中，如果我们更改用于创建容器的镜像，资源会显示此更改，但当前关联的Pods不会改变。这是因为ReplicaSets并不管理这些更改；我们需要使用更高级的Deployment资源来进行操作。要在ReplicaSet中使任何更改生效，我们需要手动重新创建Pods，可以通过删除当前Pods（使用`kubectl
    delete pod <REPLICASET_POD_NAMES>`）或将副本数缩减为零，待所有Pods删除后再扩容。任何一种方法都会创建新的副本，使用新的ReplicaSet定义。
- en: Important note
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can use `kubectl delete pod --selector <LABEL_SELECTOR>`, with the current
    ReplicaSet selectors that were used to create them, to delete all associated Pod
    resources.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kubectl delete pod --selector <LABEL_SELECTOR>`，结合创建当前ReplicaSet时使用的标签选择器，删除所有关联的Pod资源。
- en: ReplicaSets, by default, don’t publish any Service; we need to create a Service
    resource to consume the deployed containers. When we create a Service associated
    with a ReplicaSet (using the Service’s label selector with the appropriate ReplicaSet’s
    labels), all the ReplicaSet instances will be accessible by using the Service’s
    `ClusterIP` address (default Service mode). All replicas get the same number of
    requests because the internal load balancing provides round-robin access.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，ReplicaSets不会发布任何Service；我们需要创建一个Service资源来访问部署的容器。当我们创建与ReplicaSet关联的Service（使用Service的标签选择器与相应ReplicaSet的标签），所有ReplicaSet实例都可以通过Service的`ClusterIP`地址访问（默认的Service模式）。所有副本会接收相同数量的请求，因为内部负载均衡提供了轮询访问。
- en: We will probably not use ReplicaSets as standalone resources in production as
    we have seen that any change in their definition requires additional interaction
    from our side, and that’s not ideal in dynamic environments such as Kubernetes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在生产环境中可能不会单独使用ReplicaSets，因为我们已经看到，任何对其定义的更改都需要我们进行额外的操作，而在像Kubernetes这样的动态环境中，这并不理想。
- en: Before we look at Deployments, which are advanced resources for deploying ReplicaSets,
    we will quickly review ReplicationControllers, which are quite similar to ReplicaSets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论Deployments（用于部署ReplicaSets的高级资源）之前，我们将快速回顾一下ReplicationControllers，它们与ReplicaSets非常相似。
- en: ReplicationControllers
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReplicationControllers
- en: The ReplicationController was the original method for Pod replication in Kubernetes,
    but now, it has been almost completely replaced by ReplicaSet resources. We can
    consider a ReplicationController as a less configurable ReplicaSet. Nowadays,
    we don’t directly create ReplicationControllers as we usually create Deployments
    for deploying application’s components running on Kubernetes. We learned that
    ReplicaSets have two options for selecting associated labels. The `labelSelector`
    key can be either a simple label search (`matchLabels`) or a more advanced rule
    that uses `matchExpressions`. ReplicationController manifests can only look for
    specific labels in Pods, which makes them simpler to use. The Pod template section
    looks similar in both ReplicaSets and ReplicaControllers. However, there is also
    a fundamental difference between ReplicationControllers and ReplicaSets. We can
    execute application upgrades by using rolling-update actions. These are not available
    for ReplicaSets but upgrades are provided in such resources thanks to the use
    of Deployments.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController 是 Kubernetes 中用于 Pod 复制的原始方法，但现在它已几乎完全被 ReplicaSet 资源所取代。我们可以将
    ReplicationController 看作是一个可配置性较低的 ReplicaSet。如今，我们不再直接创建 ReplicationController，因为我们通常会创建
    Deployments 来部署运行在 Kubernetes 上的应用组件。我们了解到，ReplicaSets 有两种选择关联标签的方式。`labelSelector`
    键可以是简单的标签查询（`matchLabels`），也可以是使用 `matchExpressions` 的更高级规则。ReplicationController
    清单只能查找 Pods 中的特定标签，这使得它们更易于使用。Pod 模板部分在 ReplicaSets 和 ReplicationControllers 中看起来相似。然而，ReplicationControllers
    和 ReplicaSets 之间也有一个根本性的区别。我们可以通过使用滚动更新操作来执行应用程序的升级。这些操作对于 ReplicaSets 不可用，但通过使用
    Deployments，升级功能在这些资源中得以提供。
- en: Deployments
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Deployments
- en: We can say that a Deployment is an advanced ReplicaSet. It adds the life cycle
    management part we missed by allowing us to upgrade the specifications of the
    Pods by creating a new ReplicaSet resource. This is the most used workload management
    resource in production. A Deployment resource creates and manages different ReplicaSets
    resources. When a Deployment resource is created, an associated ReplicaSet is
    also dynamically created, following the `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>`
    nomenclature. This dynamically created ReplicaSet will create associated Pods
    that follow the described nomenclature, so we will see Pod names such as `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>-<POD_
    RANDOM_UNIQUE_GENERATED_ID>` in the defined namespace. This will help us follow
    which Deployment generates which Pod resources. Deployment resources manage the
    complete ReplicaSet life cycle. To do this, whenever we change any Deployment
    template specification key, a new ReplicaSet resource is created and this triggers
    the creation of new associated Pods. The Deployment resource keeps track of all
    associated ReplicaSets, which makes it easy to roll back to a previous release,
    without the latest resource modifications. This is very useful for releasing new
    application updates. Whenever an issue occurs with the updated resource, we can
    go back to any previous version in a few seconds thanks to Deployment resources
    – in fact, we can go back to any previous existing ReplicaSet resource. We will
    deep dive into rolling updates in [*Chapter 13*](B19845_13.xhtml#_idTextAnchor287),
    *Managing the Application* *Life Cycle*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，Deployment 是一个高级版的 ReplicaSet。它通过允许我们通过创建新的 ReplicaSet 资源来升级 Pod 的规格，从而增加了我们之前错过的生命周期管理部分。这是生产中使用最广泛的工作负载管理资源。Deployment
    资源创建并管理不同的 ReplicaSet 资源。当创建一个 Deployment 资源时，一个关联的 ReplicaSet 也会动态创建，遵循 `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>`
    命名法。这个动态创建的 ReplicaSet 将创建与之关联的 Pods，遵循描述的命名规则，因此我们会在定义的命名空间中看到类似 `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>-<POD_RANDOM_UNIQUE_GENERATED_ID>`
    的 Pod 名称。这将帮助我们跟踪哪个 Deployment 生成了哪个 Pod 资源。Deployment 资源管理着完整的 ReplicaSet 生命周期。为此，每当我们更改任何
    Deployment 模板的规格键时，会创建一个新的 ReplicaSet 资源，并触发新的关联 Pods 的创建。Deployment 资源会跟踪所有关联的
    ReplicaSets，这使得我们能够轻松地回滚到先前的版本，而不必包含最新的资源修改。这对于发布新的应用更新非常有用。每当更新的资源出现问题时，我们可以在几秒钟内回滚到任何先前的版本，这得益于
    Deployment 资源——事实上，我们可以回滚到任何先前存在的 ReplicaSet 资源。我们将在[*第 13 章*](B19845_13.xhtml#_idTextAnchor287)，*管理应用生命周期*中深入探讨滚动更新。
- en: 'The following code snippet shows the syntax for these resources:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了这些资源的语法：
- en: '![Figure 9.3 – Deployment manifest](img/B19845_09_3.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 部署清单](img/B19845_09_3.jpg)'
- en: Figure 9.3 – Deployment manifest
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 部署清单
- en: The `strategy` key allows us to decide whether our new containers try to start
    before the old ones die (the `RollingUpdate` value, which is used by default)
    or completely recreate the associated ReplicaSet (the `Recreate` value), which
    is needed when only one container can access attached volumes in write mode at
    the time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`strategy`键允许我们决定新容器是否在旧容器死亡之前尝试启动（`RollingUpdate`值，默认使用此值），或者完全重新创建相关的ReplicaSet（`Recreate`值），当只有一个容器能够在特定时刻以写模式访问附加卷时需要使用这种方法。'
- en: We will use Deployments to deploy stateless or stateful application workloads
    in which we don’t require any special storage attachment and all replicas can
    be treated in the same way (all replicas are the same). Deployments work very
    well for deploying web Services with static content and dynamic ones when session
    persistence is managed in a different application component. We can’t use Deployment
    resources to deploy our application containers when each replica has to attach
    its own specific data volume or when we need to execute processes in order.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Deployments来部署无状态或有状态的应用程序工作负载，其中不需要任何特殊的存储附加，并且所有副本可以按相同的方式处理（所有副本都是一样的）。Deployments非常适合用于部署具有静态内容的Web服务和动态Web服务，当会话持久性由不同的应用组件管理时。我们不能使用Deployment资源来部署我们的应用容器，特别是在每个副本必须附加其特定数据卷，或需要按顺序执行进程的情况下。
- en: We will now learn how StatefulSet resources help us solve these specific situations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习StatefulSet资源如何帮助我们解决这些特定情况。
- en: StatefulSets
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StatefulSets
- en: StatefulSet resources are designed to manage stateful application components
    – those where persistent data must be unique between replicas. These resources
    also allow us to provide an order to different replicas when processes are executed.
    Each replica will receive a unique ordered identifier (an ordinal number starting
    from 0) and it will be used to scale the number of replicas up or down.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet资源旨在管理有状态的应用程序组件——这些组件的持久化数据必须在副本之间保持唯一。这些资源还允许我们在执行进程时为不同的副本提供顺序。每个副本将获得一个唯一的有序标识符（从0开始的序号），并且它将用于增加或减少副本的数量。
- en: 'The following code snippet shows an example of a StatefulSet resource:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了一个StatefulSet资源的示例：
- en: '![Figure 9.4 – StatefulSet manifest](img/B19845_09_4.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – StatefulSet清单](img/B19845_09_4.jpg)'
- en: Figure 9.4 – StatefulSet manifest
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – StatefulSet清单
- en: The preceding code snippet shows `template` sections for both the Pod resources
    and the volume resources.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码片段展示了`template`部分，包括了Pod资源和卷资源。
- en: The names for each Pod will follow `<STATEFULSET_NAME>-<REPLICA_NUMBER>`. For
    example, if we create a `database` StatefulSet resource with three replicas, the
    associated Pods will be `database-0`, `database-1`, and `database-2`. This name
    structure is also applied to the volumes defined in the StatefulSet’s `volumeClaimTemplates`
    template section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Pod的名称将遵循`<STATEFULSET_NAME>-<REPLICA_NUMBER>`的格式。例如，如果我们创建一个名为`database`的StatefulSet资源，且有三个副本，那么相关的Pods将为`database-0`，`database-1`和`database-2`。这种命名结构同样适用于StatefulSet的`volumeClaimTemplates`模板部分中定义的卷。
- en: Notice that we also included the `serviceName` key in the previous code snippet.
    A headless Service (without `ClusterIP`) should be created to reference the ReplicaSet’s
    Pods in the Kubernetes internal DNS, but this key tells Kubernetes to create the
    required DNS entries. For the example presented, the first replica will be announced
    to the cluster DNS as `database-0.database.NAMESPACE.svc.<CLUSTER_NAME>`, and
    all other replicas will follow the same name schema. These names can be integrated
    into our application to create an application cluster or even configure advanced
    load-balancing mechanisms other than the default (used for ReplicaSets and Deployments).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还在之前的代码片段中包含了`serviceName`键。应创建一个无头服务（没有`ClusterIP`）以在Kubernetes内部DNS中引用ReplicaSet的Pod，但这个键告诉Kubernetes创建所需的DNS条目。在所示的示例中，第一个副本将作为`database-0.database.NAMESPACE.svc.<CLUSTER_NAME>`发布到集群DNS，所有其他副本将遵循相同的命名规则。这些名称可以集成到我们的应用程序中，以创建应用程序集群，甚至配置默认负载均衡机制以外的高级负载均衡机制（用于ReplicaSets和Deployments）。
- en: When we use StatefulSet resources, Pods will be created in order, which may
    introduce extra complexity when we need to remove some replicas. We will need
    to guarantee the correct execution of processes that may resolve dependencies
    between replicas; therefore, if we need to remove a StatefulSet replica, it will
    be safer to scale down the number of replicas instead of directly removing it.
    Remember, we have to prepare our application to manage unique replicas completely,
    and this may need some application process to remove an application’s cluster
    component, for example. This situation is typical when you run distributed databases
    with multiple instances and decommissioning one instance requires database changes,
    but this also applies to any ReplicaSet manifest updates. You have to ensure that
    the changes are applied in the right order and, usually, it is preferred to scale
    down to zero and then scale up to the required number of replicas.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用StatefulSet资源时，Pods会按顺序创建，这可能会在需要删除某些副本时引入额外的复杂性。我们需要确保正确执行可能解决副本之间依赖关系的进程；因此，如果我们需要删除一个StatefulSet副本，缩减副本数量比直接删除副本更安全。记住，我们必须准备好让应用程序完全管理唯一副本，这可能需要一些应用程序进程来删除某个应用程序的集群组件。例如，在运行多个实例的分布式数据库时，这种情况是典型的，去除一个实例需要数据库更改，但这同样适用于任何ReplicaSet清单的更新。你必须确保更改按正确的顺序应用，通常，最好是先缩减到零副本，然后再扩展到所需的副本数。
- en: In the StatefulSet example presented in the preceding code snippet, we specified
    a `volumeClaimTemplate` section, which defines the properties that are required
    for a dynamically provisioned volume. We will learn how dynamic storage provisioning
    works in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231), *Leveraging Application
    Data Management in Kubernetes*, but it is important to understand that this `template`
    section will inform the Kubernetes API that every replica requires its own ordered
    volume. This requirement for dynamic provisioning will usually be associated with
    the use of `StorageClass` resources.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码片段中呈现的StatefulSet示例中，我们指定了`volumeClaimTemplate`部分，它定义了动态配置卷所需的属性。我们将在[*第10章*](B19845_10.xhtml#_idTextAnchor231)，*在Kubernetes中利用应用程序数据管理*中学习动态存储配置的工作原理，但理解这一点很重要：此`template`部分会通知Kubernetes
    API，每个副本都需要其自己的有序卷。动态配置的这一要求通常会与`StorageClass`资源的使用相关联。
- en: Once these volumes (associated with each replica) are provisioned and used,
    deleting a replica (either directly by using `kubectl delete pod` or by scaling
    down the number of replicas) will never remove the associated volume. You can
    be sure that a database deployed via a ReplicaSet will never lose its data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些卷（与每个副本相关联）被配置并使用，删除一个副本（无论是通过使用`kubectl delete pod`命令直接删除，还是通过缩减副本数量）都不会删除相关联的卷。你可以确信，通过ReplicaSet部署的数据库永远不会丢失数据。
- en: Important note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The ReplicaSet’s associated volumes will not be automatically removed, which
    makes these resources interesting for any workload if you need to ensure that
    data will not be deleted if you remove the resource.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet相关的卷不会被自动删除，这使得这些资源对于任何工作负载都很有吸引力，特别是在你需要确保删除资源时不会删除数据的情况下。
- en: We can use StatefulSet to ensure that a replicated Service is managed uniquely.
    Software such as Hashicorp’s Consul runs clusterized on several predefined Nodes;
    we can deploy it on top of Kubernetes using containers, but Pods will need to
    be deployed in order and with their specific storage as if they were completely
    different hosts. A similar approach has to be applied in database Services because
    the replication of their processes may lead to data corruption. In these cases,
    we can use StatefulSet replicated resources, but the application should manage
    the integration between the different deployed replicas and the scaling up and
    down procedure. Kubernetes just provides the underlying architecture that guarantees
    the data’s uniqueness and the replica execution order.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用StatefulSet来确保一个复制的服务被唯一管理。像Hashicorp的Consul这样的软件在多个预定义节点上以集群方式运行；我们可以通过容器将其部署在Kubernetes之上，但Pod需要按顺序部署，并且每个Pod都需要特定的存储，就好像它们是完全不同的主机一样。在数据库服务中也必须采用类似的方法，因为它们进程的复制可能会导致数据损坏。在这些情况下，我们可以使用StatefulSet复制的资源，但应用程序应该管理不同部署副本之间的集成，以及扩缩容过程。Kubernetes仅提供了底层架构，确保数据的唯一性和副本的执行顺序。
- en: DaemonSets
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DaemonSets
- en: A DaemonSet resource will execute exactly one associated Pod in each Kubernetes
    cluster Node. This ensures that any newly joined Node will get its own replica
    automatically.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 资源将在每个 Kubernetes 集群节点上执行一个关联的 Pod。这确保了任何新加入的节点将自动获得自己的副本。
- en: 'The following code snippet shows a DaemonSet manifest example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了一个 DaemonSet 清单示例：
- en: '![Figure 9.5 – DaemonSet manifest](img/B19845_09_5.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – DaemonSet 清单](img/B19845_09_5.jpg)'
- en: Figure 9.5 – DaemonSet manifest
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – DaemonSet 清单
- en: As you may have noticed, we use label selectors to match and associate Pods.
    In the preceding example, we also introduced the `tolerations` key. Let’s quickly
    introduce how `NoSchedule` (only Pods with appropriate tolerations for the Node
    are allowed), `PreferNoSchedule` (Pods will not run on the Node unless no other
    one is available), or `NoExecute` (Pods will be evicted from the Node if they
    don’t have the appropriate tolerations). Taints and tolerations must match, and
    this allows us to dedicate Nodes for certain tasks and avoid the execution of
    any other workloads on them. The kubelet will use dynamic taints to evict Pods
    when issues are found on a cluster Node – for example, when too much memory is
    in use or the disk is getting full. In our example, we add a toleration to execute
    the DaemonSet Pods on Nodes with the `node-role.kubernetes.io/control-plane=NoSchedule`
    taint.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经注意到的，我们使用标签选择器来匹配和关联 Pods。在前面的示例中，我们还介绍了 `tolerations` 键。让我们快速介绍一下 `NoSchedule`（只有具有适当容忍度的
    Pods 才能在该节点上运行）、`PreferNoSchedule`（Pods 除非没有其他节点可用，否则不会在该节点上运行）和 `NoExecute`（如果
    Pods 没有适当的容忍度，它们将从节点上驱逐）。污点和容忍度必须匹配，这使我们能够为特定任务专门分配节点，避免在其上执行其他工作负载。kubelet 将使用动态污点来驱逐
    Pods，当集群节点出现问题时—例如，当内存使用过多或磁盘空间不足时。在我们的示例中，我们添加了一个容忍度，以便在具有 `node-role.kubernetes.io/control-plane=NoSchedule`
    污点的节点上执行 DaemonSet Pods。
- en: DaemonSets are often used to deploy applications that should run on all Nodes,
    such as those running as software agents for monitoring or logging purposes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSets 通常用于部署应该在所有节点上运行的应用程序，例如那些作为软件代理运行的应用程序，用于监控或日志记录目的。
- en: Important note
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Although it isn’t too common, it is possible to use static Pods to run Node-specific
    processes. This is the mechanism that’s used by Kubernetes kubeadm-based Deployments.
    Static Pods are Pods associated with a Node, executed directly by the kubelet,
    and thus, they are not managed by Kubernetes. You can identify these Pods by their
    name because they include the host’s name. Manifests for executing static Pods
    are located in the `/etc/kubernetes/manifests` directory in kubeadm clusters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不常见，但确实可以使用静态 Pods 来运行节点特定的进程。这是 Kubernetes 基于 kubeadm 的部署使用的机制。静态 Pods 是与节点关联的
    Pods，由 kubelet 直接执行，因此它们不受 Kubernetes 管理。您可以通过它们的名称来识别这些 Pods，因为它们包含主机的名称。执行静态
    Pods 的清单位于 kubeadm 集群的 `/etc/kubernetes/manifests` 目录中。
- en: At this point, we have to mention that none of the workload management resources
    presented so far provide a mechanism to run a task that shouldn’t be maintained
    during its execution time. We will now review Job resources, which are specifically
    created for this purpose.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们必须提到，迄今为止呈现的所有工作负载管理资源都没有提供一个机制来运行不应该在执行期间维持的任务。接下来我们将回顾 Job 资源，这些资源专门为此目的而创建。
- en: Jobs
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jobs
- en: A Job resource is in charge of executing a Pod until we get a successful termination.
    The Job resource also tracks the execution of a set of Pods using template selectors.
    We configure a required number of successful executions and the Job resource is
    considered *Completed* when all the required Pod executions are successfully finished.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Job 资源负责执行一个 Pod，直到我们获得成功的终止。Job 资源还通过使用模板选择器跟踪一组 Pods 的执行。我们配置所需的成功执行次数，当所有所需的
    Pod 执行成功完成时，Job 资源将被视为*完成*。
- en: In a Job resource, we can configure parallelism for executing more than one
    Pod at a time and being able to reach the required number of successful executions
    faster. Pods related to a Job will remain in our Kubernetes cluster until we delete
    the associated Job or remove them manually.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Job 资源中，我们可以配置并行性，以便同时执行多个 Pod，并能够更快地达到所需的成功执行次数。与 Job 相关的 Pods 将一直保留在我们的
    Kubernetes 集群中，直到我们删除相关的 Job 或手动移除它们。
- en: A Job can be suspended, which will delete currently active Pods (in execution)
    until we resume it again.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Job 可以被挂起，这将删除当前活动的 Pods（正在执行的），直到我们重新恢复它。
- en: We can use Jobs to execute one-time tasks, but they are usually associated with
    periodic executions thanks to `CronJob` resources. Another common use case is
    the execution of certain one-time tasks from applications directly in the Kubernetes
    cluster. In these cases, your application needs to be able to reach the Kubernetes
    API internally (the `kubernetes` Service in the `default` namespace) and the appropriate
    permissions for creating Jobs. This is usually achieved by associating a namespaced
    `Role`, which allows such actions, with the `ServiceAccount` resource that executes
    your application’s Pod. This association is established using a namespaced `RoleBinding`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Jobs 执行一次性任务，但它们通常与周期性执行相关联，感谢 `CronJob` 资源。另一个常见的使用场景是在 Kubernetes 集群中直接执行应用程序的一些一次性任务。在这种情况下，您的应用程序需要能够内部访问
    Kubernetes API（`default` 命名空间中的 `kubernetes` 服务），并且具有创建 Jobs 的适当权限。通常通过将允许此类操作的命名空间
    `Role` 与执行应用程序 Pod 的 `ServiceAccount` 资源关联来实现。这种关联是通过使用命名空间 `RoleBinding` 来建立的。
- en: 'The following code snippet shows a `Job` manifest example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了一个 `Job` 清单示例：
- en: '![Figure 9.6 – Job manifest](img/B19845_09_6.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – Job 清单](img/B19845_09_6.jpg)'
- en: Figure 9.6 – Job manifest
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – Job 清单
- en: Here, we defined the number of successful completitions and the number of failures
    that will set the Job as failed by setting the `completions` and `backoffLimit`
    keys. At least three Pods must exit successfully before the limit of four failures
    is reached. Multiple Pods can be executed in parallel to speed up the completion
    by setting the `parallelism` key, which defaults to `1`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过设置 `completions` 和 `backoffLimit` 键，定义了成功完成的次数和将 Job 标记为失败的失败次数。至少需要三个
    Pods 成功退出，才能达到四次失败的限制。可以通过设置 `parallelism` 键（默认值为 `1`）来并行执行多个 Pods，以加快完成速度。
- en: The *TTL-after-finished* controller provides a `ttlSecondsAfterFinished` key.
    Since this key is based on a date-time reference, it is key to maintain our clusters’
    time according to our time zone.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*TTL-after-finished* 控制器提供了一个 `ttlSecondsAfterFinished` 键。由于此键基于日期时间参考，因此维护集群的时间并确保与我们所在时区一致非常重要。'
- en: Jobs are commonly used within CronJobs to define tasks that should be executed
    at certain periods – for example, for executing backups. Let’s learn how to implement
    CronJobs so that we can schedule Jobs periodically.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Jobs 通常在 CronJobs 中使用，用于定义应该在某些时间段内执行的任务——例如，执行备份。让我们学习如何实现 CronJobs，以便我们能够定期安排
    Jobs 执行。
- en: CronJobs
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CronJobs
- en: 'CronJob resources are used to schedule Jobs at specific times. The following
    code snippet shows a `CronJob` manifest example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob 资源用于在特定时间安排 Jobs。以下代码片段展示了一个 `CronJob` 清单示例：
- en: '![Figure 9.7 – CronJob manifest](img/B19845_09_7.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – CronJob 清单](img/B19845_09_7.jpg)'
- en: Figure 9.7 – CronJob manifest
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – CronJob 清单
- en: 'To be able to review logs from executed Pods (associated with the Jobs created),
    we can set `failedJobsHistoryLimit` and `successfulJobsHistoryLimit` to the desired
    number of Jobs to keep to be able to review the Pods’ logs. Notice that we planned
    the example Job daily, at 00:00, using the common *Unix Crontab* format, as shown
    in the following schema:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够查看已执行的 Pods（与创建的 Jobs 相关联）的日志，我们可以设置 `failedJobsHistoryLimit` 和 `successfulJobsHistoryLimit`，以指定要保留的
    Jobs 数量，从而能够查看 Pods 的日志。请注意，我们将示例 Job 设置为每日执行，时间为 00:00，使用常见的*Unix Crontab*格式，如下所示：
- en: '![Figure 9.8 – Unix Crontab format](img/B19845_09_8.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – Unix Crontab 格式](img/B19845_09_8.jpg)'
- en: Figure 9.8 – Unix Crontab format
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – Unix Crontab 格式
- en: The `schedule` key defines when the Job will be created and associated Pods
    will run. Remember to always quote your value to avoid problems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`schedule` 键定义了何时创建 Job，以及关联的 Pods 何时运行。请记得始终对您的值加上引号，以避免出现问题。'
- en: Important note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: CronJob resources use the Unix Crontab format, hence values such as `@hourly`,
    `@daily`, `@monthly`, or `@yearly` can be used.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob 资源使用 Unix Crontab 格式，因此可以使用诸如 `@hourly`、`@daily`、`@monthly` 或 `@yearly`
    等值。
- en: CronJobs can be suspended, which will affect any new Job creation if we change
    the value of the `suspend` key to `true`. To enable the CronJob again, we need
    to change this key to `false`, which will continue with the normal scheduling
    for creating new Jobs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: CronJobs 可以被暂停，如果我们将 `suspend` 键的值更改为 `true`，将会影响任何新的 Job 创建。要重新启用 CronJob，我们需要将此键更改为
    `false`，这将继续按照正常的时间表创建新的 Jobs。
- en: A common use case for CronJobs is the execution of backup tasks for applications
    deployed on Kubernetes. With this solution, we avoid opening internal applications
    externally if user access isn’t required.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: CronJobs 的一个常见用例是执行部署在 Kubernetes 上的应用程序的备份任务。通过这个解决方案，我们可以避免将内部应用程序暴露到外部，如果用户访问不是必需的。
- en: Now that we understand the different resources we can use to deploy our workloads,
    let’s quickly review how they will help us provide resilience and high availability
    to our applications.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了可以用来部署工作负载的不同资源，让我们快速回顾一下它们将如何帮助我们为应用程序提供弹性和高可用性。
- en: Ensuring resilience and high availability with Kubernetes resources
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 资源确保弹性和高可用性
- en: Pod resources provide resilience out of the box as we can configure them to
    always restart if their processes fail. We can use the `spec.restartPolicy` key
    to define when they should restart. It is important to understand that this option
    is limited to the host’s scope, so a Pod will just try to restart on the host
    on which it was previously running. Pod resources do not provide high availability
    or resilience cluster-wide.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 资源开箱即用提供弹性，因为我们可以配置它们在进程失败时总是重新启动。我们可以使用 `spec.restartPolicy` 键来定义它们应何时重新启动。需要理解的是，这个选项仅限于主机范围，因此
    Pod 只会尝试在其先前运行的主机上重新启动。Pod 资源并不提供集群范围的高可用性或弹性。
- en: 'Deployments, and therefore ReplicaSets, and StatefulSets are prepared for applying
    resilience cluster-wide because resilience doesn’t depend on hosts. A Pod will
    still try to restart on the Node where it was previously running, but if it is
    not possible to run it, it will be scheduled to a new available one. This will
    allow Kubernetes administrators to perform maintenance tasks on Nodes moving workloads
    from one host to another, but this may impact your applications if they are not
    ready for such movements. In other words, if you only have one replica of your
    processes, they will go down for seconds (or minutes, depending on the size of
    your image and the time required by your processes to start), and this will impact
    your application. The solution is simple: deploy more than one replica of your
    application’s Pods. However, it is important to understand that your application
    needs to be prepared for multiple replicated processes working in parallel.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Deployments，因此 ReplicaSets 和 StatefulSets 都是为实现全局弹性而设计的，因为弹性不依赖于主机。Pod 仍然会尝试在它之前运行的节点上重新启动，但如果无法运行，它将被调度到一个新的可用节点。这将允许
    Kubernetes 管理员在节点上执行维护任务，将工作负载从一个主机迁移到另一个主机，但如果应用程序没有为这种迁移做好准备，可能会对其产生影响。换句话说，如果你的进程只有一个副本，它们会在几秒钟（或几分钟，具体取决于镜像的大小和进程启动所需的时间）内停止，这将影响你的应用程序。解决方案很简单：部署多个副本的应用程序
    Pod。然而，重要的是要理解，你的应用程序需要为多个副本进程并行工作做好准备。
- en: StatefulSets’ replicas will never use the same volume, but this isn’t true for
    Deployments. All the replicas will share the volumes, and you must be aware of
    that. Sharing static content will work like a charm, but if multiple processes
    are trying to write the same file at the same time, you may encounter problems
    if your code doesn’t manage concurrency.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets 的副本永远不会使用相同的存储卷，但 Deployments 并非如此。所有副本将共享存储卷，你必须意识到这一点。共享静态内容将非常顺利，但如果多个进程试图同时写入同一个文件，如果你的代码没有处理并发问题，可能会遇到问题。
- en: DaemonSets work differently and we don’t have to manage any replication; just
    one Pod will run on each Node, but they will share volumes too. Because of the
    nature of such resources, it is not common to include shared volumes in these
    cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSets 的工作方式不同，我们不需要管理任何复制；每个节点上只会运行一个 Pod，但它们也会共享存储卷。由于这种资源的特性，在这种情况下不常见包含共享存储卷。
- en: But even if our application runs in a replicated manner, we can’t ensure that
    all the replicas die at the same time without configuring a **Pod disruption policy**.
    We can configure a minimum number of Pods to be available at the same time, ensuring
    not only resilience but also high availability. Our application will have some
    impact, but it will continue serving requests (high availability).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使我们的应用程序以复制方式运行，我们也不能确保所有副本同时停止，除非配置了 **Pod 中断策略**。我们可以配置一个最小可用 Pod 数量，确保不仅提供弹性，还能实现高可用性。我们的应用程序会受到一定影响，但它将继续提供服务（高可用性）。
- en: 'To configure a disruption policy, we must use `PodDisruptionBudget` resources
    to provide the logic we need for our application. We will be able to set up the
    number of Pods that are required for our application workload under all circumstances
    by configuring the `minAvailable` or `maxUnavailable` keys. We can use integers
    (the number of Pods) or a percentage of the configured replicas. `PodDisruptionBudget`
    resources use selectors to choose between the Pods in the namespace (which we
    already use to create Deployments, ReplicaSets, and more). The following code
    snippet shows an example:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置中断策略，我们必须使用`PodDisruptionBudget`资源为我们的应用程序提供所需的逻辑。通过配置`minAvailable`或`maxUnavailable`键，我们可以设置在任何情况下我们的应用程序工作负载所需的Pod数量。我们可以使用整数（Pod的数量）或配置的副本百分比。`PodDisruptionBudget`资源使用选择器在命名空间中的Pod之间进行选择（这些选择器我们已经用于创建Deployments、ReplicaSets等）。以下代码片段展示了一个示例：
- en: '[PRE0]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, a minimum of two Pods with the `app=webserver` label are being
    monitored. We will define the number of replicas in our Deployment, but the `PodDisruptionBudget`
    resource will not allow us to scale down below two replicas. Therefore, two replicas
    will be running even if we decide to execute `kubectl drain node1` (assuming,
    in this example, that the `webserver` Deployment matches the `app=webserver` Pod’s
    labels and `node1` and `node2` have one replica each). `PodDisruptionBudget` resources
    are namespaced, so we can show all these resources in the namespace by executing
    `kubectl` `get poddisruptionbudgets`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，正在监控至少两个带有`app=webserver`标签的Pod。我们将在Deployment中定义副本数，但`PodDisruptionBudget`资源不会允许我们将副本缩减到少于两个副本。因此，即使我们决定执行`kubectl
    drain node1`（假设在此示例中，`webserver` Deployment与`app=webserver` Pod的标签匹配，且`node1`和`node2`各有一个副本），仍将运行两个副本。`PodDisruptionBudget`资源是命名空间级的，因此我们可以通过执行`kubectl
    get poddisruptionbudgets`来查看命名空间中的所有这些资源。
- en: In the following section, we will review some interesting ideas for solving
    common application architecture patterns using Pod features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将回顾一些有趣的想法，使用Pod特性来解决常见的应用架构模式。
- en: Understanding advanced Pod application patterns
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解高级Pod应用模式
- en: In this section, we are going to discuss some interesting patterns using simple
    Pods. All the patterns we are going to review are based on the special mechanisms
    offered by Kubernetes for sharing kernel namespaces in a Pod, which allow containers
    running inside to mount the same volumes and interconnect via localhost.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些使用简单Pod的有趣模式。我们将要回顾的所有模式都是基于Kubernetes为Pod提供的特殊机制，这些机制允许容器在Pod内运行时共享内核命名空间，从而使容器能够挂载相同的卷，并通过localhost进行互联。
- en: Init containers
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化容器
- en: More than one Pod can run inside a container. Pods allow us to isolate different
    application processes that we want to maintain separately in different containers.
    This helps us, for example, to maintain different images that can be represented
    by separated code repositories and build workflows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一个容器内可以运行多个Pod。Pod允许我们隔离不同的应用程序进程，将它们分别维护在不同的容器中。这有助于我们，例如，维护可以由不同的代码库和构建工作流表示的不同镜像。
- en: Init containers run before the main application container (or containers, if
    we run more in parallel). These init containers can be used to set permissions
    on shared filesystems presented as volumes, create database schemas, or any other
    procedure that helps initialize our application. We can even use them to check
    dependencies before a process starts or even provision required files by retrieving
    them from an external source.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化容器在主应用容器（如果我们并行运行多个容器）之前运行。这些初始化容器可以用于设置作为卷呈现的共享文件系统上的权限、创建数据库模式或任何有助于初始化应用程序的过程。我们甚至可以使用它们在进程开始之前检查依赖关系，或通过从外部源检索文件来预置所需的文件。
- en: We can define many init containers, and they will be executed in order, one
    by one, and all of them must end successfully before the actual application containers
    start. If any of the init containers fails, the Pod fails, although these containers
    don’t have associated probes for verifying their state. The processes executed
    by them must include verification if something goes wrong.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义多个初始化容器，它们将按顺序逐个执行，所有容器必须成功完成，才能启动实际的应用容器。如果任何一个初始化容器失败，Pod也会失败，尽管这些容器没有与之关联的探针来验证它们的状态。它们执行的过程必须包含故障检查，以防出现问题。
- en: It is important to understand that the total CPU and memory resources consumed
    by a Pod are calculated from the initialization of the Pod, hence init containers
    are checked. You keep the resource usage between the defined limits for your Pod
    (which includes the usage of all the containers running in parallel).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Pod 所消耗的总 CPU 和内存资源是从 Pod 初始化开始计算的，因此需要检查初始化容器。你需要确保 Pod 的资源使用保持在定义的限制范围内（包括所有并行运行容器的资源使用）。
- en: Sidecar containers
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sidecar 容器
- en: '`kubectl patch`) to modify the running Deployment resource manifest.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl patch`命令来修改正在运行的 Deployment 资源清单。
- en: Some modern monitoring applications, designed to integrate into Kubernetes,
    also use sidecar containers to deploy an application-specific monitoring component,
    which retrieves application metrics and exposes them as a new Service.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一些现代监控应用程序设计为与 Kubernetes 集成，也使用 sidecar 容器来部署应用程序特定的监控组件，从应用程序获取指标并将其暴露为一个新的服务。
- en: The next few patterns we are going to review are based on this sidecar container
    concept.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要审查的几个模式基于这个 sidecar 容器的概念。
- en: Ambassador containers
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大使容器
- en: The **Ambassador** applications pattern is designed to offload common client
    connectivity tasks, helping legacy applications implement more advanced features
    without changing any of their old code. With this design, we can improve the application’s
    routing, communications security, and resilience by adding additional load balancing,
    API gateways, and SSL encryption.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**大使**应用程序模式旨在卸载常见的客户端连接任务，帮助遗留应用程序在不改变旧代码的情况下实现更先进的功能。通过这种设计，我们可以通过增加负载均衡、API
    网关和 SSL 加密来改善应用程序的路由、通信安全性和弹性。'
- en: We can deploy this pattern within Pods by adding special containers, designed
    for delivering light reverse-proxy features. In this way, Ambassador containers
    are used for deploying service mesh solutions, intercepting application process
    communications, and securing the interconnection with other application components
    by enforcing encrypted communications and managing application routes, among other
    features.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在 Pod 中添加特殊容器来部署这个模式，这些容器设计用于提供轻量的反向代理功能。通过这种方式，大使容器可用于部署服务网格解决方案，拦截应用程序进程通信，确保与其他应用组件的互联互通，强制加密通信，并管理应用程序路由等功能。
- en: Adaptor containers
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适配器容器
- en: The **Adaptor** container pattern is used, for example, when we want to include
    monitoring or retrieve logs from legacy applications without changing their code.
    To avoid this circumstance, we can include a second container in our application’s
    Pod to get the metrics or the logs from our application without modifying any
    of its original code. This also allows us to homogenize the content of a log or
    send it to a remote server. Well-prepared containers will redirect processes’
    standard and error output to the foreground, and this allows us to review their
    log, but sometimes, the application can’t redirect the log or more than one log
    is created. We can unify them in one log or redirect their content by adding a
    second process (the Adaptor container), which formats (adding some custom columns,
    date format, and so on) and redirects the result to the standard output or a remote
    logging component. This method does not require special access to the host’s resources
    and it may be transparent for the application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**适配器**容器模式的使用场景之一是当我们希望在不改变遗留应用程序代码的情况下进行监控或获取日志。为了避免这种情况，我们可以在应用程序的 Pod 中添加第二个容器，从应用程序获取指标或日志，而无需修改任何原始代码。这还允许我们统一日志内容或将其发送到远程服务器。经过精心设计的容器会将进程的标准输出和错误输出重定向到前台，这样我们可以查看它们的日志，但有时应用程序无法重定向日志或会生成多个日志。我们可以将它们合并为一个日志，或通过添加第二个进程（适配器容器）来重定向其内容，该容器会格式化日志（添加一些自定义列、日期格式等），并将结果重定向到标准输出或远程日志组件。此方法不需要对主机资源的特殊访问，并且对于应用程序来说可能是透明的。'
- en: '**Prometheus** is a very popular open source monitoring solution and is extended
    in Kubernetes environments. Its main component will poll agent-like components
    and retrieve metrics from them, and it’s very common to use this Adaptor container
    pattern to present the application’s metrics without modifying its standard behavior.
    These metrics will be exposed in the application Pod in a different port, and
    the Prometheus server will connect to it to obtain its metrics.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**Prometheus** 是一个非常流行的开源监控解决方案，并在 Kubernetes 环境中扩展。它的主要组件会轮询代理组件并从中获取指标，常常使用这种适配器容器模式来展示应用程序的指标，而无需修改其标准行为。这些指标会在应用程序
    Pod 的不同端口暴露，Prometheus 服务器会连接到该端口以获取指标。'
- en: Let’s learn how containers’ health is verified by Kubernetes to decide the Pod’s
    status.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解 Kubernetes 如何验证容器的健康状况，以决定 Pod 的状态。
- en: Verifying application health
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证应用程序健康
- en: In this section, we are going to review how application Pods are considered
    healthy. Pods always start in the `Pending` state and continue to the `Running`
    state once the main container is considered healthy. If the Pod executes a Service
    process, it will stay in this `Running` state. If the Pod is associated with a
    Job resource, it may end successfully (the `Succeeded` state) or fail (the `Failed`
    state).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将回顾应用程序 Pods 如何被认为是健康的。Pods 总是从 `Pending` 状态开始，一旦主容器被认为健康，它们会继续进入 `Running`
    状态。如果 Pod 执行一个服务进程，它将保持在 `Running` 状态。如果 Pod 与一个 Job 资源关联，它可能会成功结束（`Succeeded`
    状态）或失败（`Failed` 状态）。
- en: If we remove a Pod resource, it will go to `Terminating` until it is completely
    removed from Kubernetes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们移除一个 Pod 资源，它将进入 `Terminating` 状态，直到从 Kubernetes 中完全移除。
- en: Important note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: If Kubernetes cannot retrieve the Pod’s status, its state will be `Unknown`.
    This is usually due to communication issues between the hosts’ kubelet and the
    API server.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 无法获取 Pod 的状态，它的状态将为 `Unknown`。这通常是由于主机的 kubelet 和 API 服务器之间的通信问题。
- en: 'Kubernetes reviews the state of the containers to set the Pod’s state, and
    containers can be either `Waiting`, `Running`, or `Terminated`. We can use `kubectl
    describe pod <POD_NAME>` to review the details of these phases. Let’s quickly
    review these states:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 会审查容器的状态来设置 Pod 的状态，容器可以是 `Waiting`、`Running` 或 `Terminated`。我们可以使用
    `kubectl describe pod <POD_NAME>` 来查看这些阶段的详细信息。让我们快速回顾一下这些状态：
- en: '`Waiting` represents the state before `Running`, where all the pre-container-execution
    processes appear. In this phase, the container image is pulled from the registry
    and different volume mounts are prepared. If the Pod can’t run, we can have a
    `Pending` state, which will indicate a problem with deploying the workload.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Waiting` 表示 `Running` 之前的状态，在这个阶段会出现所有的容器执行前的流程。此阶段，容器镜像会从镜像仓库拉取，并准备不同的卷挂载。如果
    Pod 无法运行，它将进入 `Pending` 状态，这表明在部署工作负载时出现了问题。'
- en: '`Running` indicates that the containers are running correctly, without any
    issues.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Running` 表示容器运行正常，没有任何问题。'
- en: The `Terminated` state is considered when the containers are stopped.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Terminated` 状态是在容器停止时被考虑的状态。'
- en: If the Pod was configured with a `restartPolicy` property of the `Always` or
    `OnFailure` type, all the containers will be restarted on the node where they
    stopped. That’s why a Pod resource does not provide either high availability or
    resilience if the node goes down.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Pod 配置了 `restartPolicy` 属性，类型为 `Always` 或 `OnFailure`，那么所有容器将在停止的节点上重新启动。这就是为什么如果节点宕机，Pod
    资源既不提供高可用性也不提供弹性。
- en: Let’s review how the Pod’s status is evaluated in these phases thanks to the
    execution of **probes**.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过**探针**的执行来回顾一下 Pod 状态是如何在这些阶段中被评估的。
- en: Understanding the execution of probes
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解探针的执行
- en: 'The kubelet will execute probes periodically by executing some code inside
    the containers or by directly executing network requests. Different probe types
    are available depending on the type of check we need for our application’s components:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet 会定期执行探针，通过在容器内执行某些代码或直接执行网络请求。根据我们需要检查的应用组件类型，提供了不同类型的探针：
- en: '`exec`: This executes a command inside the container and the kubelet verifies
    whether this command exits correctly.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exec`：该方法在容器内部执行一个命令，kubelet 会验证该命令是否正确退出。'
- en: '`httpGet`: This method is probably the most common as modern applications expose
    Services via the REST API. This check’s response must return 2XX or 3XX (redirects)
    codes.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`httpGet`：这种方法可能是最常见的，因为现代应用程序通过 REST API 暴露服务。此检查的响应必须返回 2XX 或 3XX（重定向）代码。'
- en: '`tcpSocket`: This probe is used to check whether the application’s port is
    available.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tcpSocket`：此探针用于检查应用程序的端口是否可用。'
- en: '`grpc`: If our application is consumed via modern **Google Remote Procedure
    Calls** (**gRPCs**), we can use this method to verify the container’s state.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grpc`：如果我们的应用程序通过现代**谷歌远程过程调用**（**gRPCs**）被使用，我们可以通过这种方式验证容器的状态。'
- en: Probes must return a valid value to consider the container healthy. Different
    probes can be executed one after another through the different phases of their
    lives. Let’s consider the different options available to verify whether the container’s
    processes are starting or serving the application itself.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 探针必须返回有效值才能认为容器是健康的。不同的探针可以在它们的生命周期中依次执行。我们来看看可用的不同选项，以验证容器的进程是否正在启动或服务应用程序本身。
- en: Startup probes
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动探针
- en: '`Always` or `OnFailure` in its `restartPolicy` key.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`Always` 或 `OnFailure` 在其`restartPolicy`键中。'
- en: We will set up these probes if our processes take a lot of time before they
    are ready – for example, when we start a database server and it must manage previous
    transactions in its data before it is ready, or when our processes already integrate
    some sequenced checks before the final execution of the main process.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的进程在准备好之前需要很长时间，我们将设置这些探针——例如，当我们启动一个数据库服务器并且它必须处理之前的数据事务才能准备好，或者当我们的进程已经集成了一些顺序检查，最终执行主进程之前。
- en: Liveness probes
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存活探针
- en: '`restartPolicy` value. They are used when it’s hard to manage the failure of
    your main process within the process itself. It may be easier to integrate an
    external check via the `livenessProbe` key, which verifies whether or not the
    main process is healthy.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`restartPolicy`值。当在进程内部管理主进程的失败很困难时，它们将被使用。通过`livenessProbe`键集成一个外部检查，来验证主进程是否健康，可能会更为简单。'
- en: Readiness probes
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 就绪探针
- en: '`selector` section will not mark this Pod as ready for requests until this
    probe ends successfully. The same happens when the probe fails; it will be removed
    from the list of available endpoints for the Service resource.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`selector`部分在此探针成功结束之前不会将此Pod标记为准备好接收请求。当探针失败时，同样会将其从可用端点列表中移除。'
- en: Readiness probes are key to managing traffic to the Pods because we can ensure
    that the application component will correctly manage requests. This probe should
    always be set up to improve our microservices’ interactions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探针对于管理Pod的流量至关重要，因为我们可以确保应用程序组件能正确处理请求。此探针应始终设置，以改善我们的微服务交互。
- en: There are common keys that can be used at the `spec.containers` level that will
    help us customize the behavior of the different probe types presented. For example,
    we can configure the number of failed checks required to consider the probe as
    failed (`failureThreshold`) or the period between the execution of a probe type
    (`periodSeconds`). We can also configure some delay before any of these probes
    start by setting the `initialDelaySeconds` key, although it is recommended to
    understand how the application works and adjust the probes to fit our initial
    sequence. In the *Labs* section of this chapter, we will review some of the probes
    we’ve just discussed.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在`spec.containers`级别，有一些常用的键可以帮助我们自定义不同探针类型的行为。例如，我们可以配置需要多少次失败检查才能将探针视为失败（`failureThreshold`）或探针类型执行之间的时间间隔（`periodSeconds`）。我们还可以通过设置`initialDelaySeconds`键来配置这些探针启动前的延迟，尽管推荐先了解应用程序的工作方式，并调整探针以适应我们的初始顺序。在本章的*实验室*部分，我们将回顾一些刚才讨论过的探针。
- en: Now that we know how Kubernetes (the kubelet component) verifies the health
    of the Pods starting or running in the cluster, we must understand the *stop*
    sequence when they are considered `Completed` or `Failed`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道Kubernetes（kubelet组件）如何验证集群中启动或运行的Pods的健康状态，我们还必须理解它们在被认为`Completed`或`Failed`时的*停止*顺序。
- en: Termination of Pods
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pods的终止
- en: We can use the `terminationGracePeriodSeconds` key to set how much time the
    kubelet will wait if the Pod’s processes take a long time to end. When a Pod is
    deleted, the kubelet sends it a `SIGTERM` signal, but if it takes too long, the
    kubelet will send a `SIGKILL` signal to all container processes that are still
    alive when the `terminationGracePeriodSeconds` configured time is reached. This
    time threshold can also be configured at the probe level.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`terminationGracePeriodSeconds`键来设置如果Pod的进程需要很长时间才能结束，kubelet会等待多久。当Pod被删除时，kubelet会向它发送`SIGTERM`信号，但如果花费的时间太长，kubelet将在`terminationGracePeriodSeconds`配置的时间达到时，向所有仍在运行的容器进程发送`SIGKILL`信号。这个时间阈值也可以在探针级别进行配置。
- en: To remove a Pod immediately, we can force and change this Pod-level defined
    grace period by using `kubelet delete pod <POD_NAME> --force` along with `--grace-period=0`.
    Forcing the deletion of a Pod may result in unexpected consequences for your applications
    if you don’t understand how it works. The kubectl client sends the `SIGKILL` signal
    and doesn’t wait for confirmation, informing the API server that the Pod is already
    terminated. When the Pods are part of a StatefulSet, this may be dangerous as
    the Kubernetes cluster will try to execute a new Pod without confirming whether
    it has already been terminated. To avoid these situations, it is better to scale
    down to the replicas and scale up to do a full restart.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要立即删除一个Pod，我们可以通过使用`kubelet delete pod <POD_NAME> --force`并结合`--grace-period=0`来强制并更改此Pod级别定义的宽限期。如果你不了解其工作原理，强制删除Pod可能会对你的应用程序造成意想不到的后果。kubectl客户端发送`SIGKILL`信号，并且不等待确认，通知API服务器Pod已经终止。当Pod属于StatefulSet时，这可能是危险的，因为Kubernetes集群将尝试执行一个新的Pod，而不确认它是否已经被终止。为了避免这些情况，最好是先缩减副本数，然后再扩展副本以进行完全重启。
- en: Our applications may need to execute some specific processes to manage the interactions
    between different components when we update some of them, or even if they fail
    with an error. We can include some triggers when our containers start or stop
    – for example, to reconfigure a new master process in a clusterized application.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们更新应用的某些组件，甚至当它们因错误失败时，我们的应用可能需要执行一些特定的进程来管理不同组件之间的交互。我们可以在容器启动或停止时包含一些触发器—例如，在集群化应用中重新配置新的主进程。
- en: Container life cycle hooks
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器生命周期钩子
- en: 'Containers within a Pod can include a **life cycle hook** in their specifications.
    Two types are available:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Pod中的容器可以在其规格中包含**生命周期钩子**。有两种类型可用：
- en: '**PostStart** hooks can be used to execute a process *after* a container is
    created.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PostStart**钩子可以用来在容器创建*之后*执行一个进程。'
- en: '**PreStop** hooks are executed *before* the container is terminated. The grace
    period starts when the kubelet receives a stop action, so this hook may be affected
    if the defined process takes too long.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PreStop**钩子在容器被终止*之前*执行。宽限期从kubelet接收到停止操作时开始，因此，如果定义的进程花费时间过长，这个钩子可能会受到影响。'
- en: Pods can be scaled up or down manually whenever our application needs it and
    it’s supported, but we can go further and manage replicas automatically. The following
    section will show us how to make it possible.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的应用需要时，Pods可以手动进行扩展或缩减，只要得到支持，但我们还可以进一步管理副本的自动化。接下来的部分将展示如何实现这一点。
- en: Resource management and scalability
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源管理和可扩展性
- en: By default, Pods run without compute resource limits. This is fine for learning
    how your application behaves, and it can help you define its requirements and
    limits.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Pods在没有计算资源限制的情况下运行。这对于了解应用程序的行为是可以的，而且它有助于你定义其需求和限制。
- en: 'Kubernetes cluster administrators can also define quotas that can be configured
    at different levels. It is usual to define them at the namespace level and your
    applications will be confined with limits for CPU and memory. But these quotas
    can also identify some special resources, such as GPUs, storage, or even the number
    of resources that can be deployed in a namespace. In this section, we will learn
    how to limit resources in our Pods and containers, but you should always ask your
    Kubernetes administrators if any quota is applied at the namespace level to prepare
    your deployments for such compliance. More information about resource quota configurations
    can be found in the Kubernetes official documentation: [https://kubernetes.io/docs/concepts/policy/resource-quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群管理员还可以定义配额，这些配额可以在不同级别进行配置。通常会在命名空间级别定义这些配额，您的应用将受到CPU和内存限制。但这些配额也可以识别一些特殊资源，如GPU、存储，甚至可以在命名空间中部署的资源数量。在这一部分，我们将学习如何在Pod和容器中限制资源，但在部署之前，您应始终询问Kubernetes管理员是否在命名空间级别应用了任何配额，以准备好遵守这些配额。有关资源配额配置的更多信息，可以在Kubernetes官方文档中找到：[https://kubernetes.io/docs/concepts/policy/resource-quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas)。
- en: 'We will use the `spec.resources` section to define the limits and requests
    associated with a Pod. Let’s look at how they work:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`spec.resources`部分来定义与Pod相关的限制和请求。让我们来看看它们如何工作：
- en: '`spec.resources.requests.memory` and `spec.resources.requests.cpu`, respectively),
    we can define the minimum resources required in any cluster host to run our Pod.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.resources.requests.memory`和`spec.resources.requests.cpu`分别允许我们定义在任何集群主机上运行Pod所需的最小资源。'
- en: '`spec.resources.limits.memory` and `spec.resources.limits.cpu` to configure
    the maximum memory and number of CPUs allocable, respectively.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.resources.limits.memory`和`spec.resources.limits.cpu`分别配置最大内存和可分配的CPU数量。'
- en: Resources can be defined either at the Pod or container level and they must
    be compliant with each other. The sum of all the container resource limits must
    not exceed the Pod values. If we omit the Pod resources, the sum of the defined
    container resources will be used. If any of the containers do not contain a resource
    definition, the Pod limits and requests will be used. The container’s equivalent
    key is `spec.containers[].resources`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 资源可以在Pod或容器级别定义，并且它们必须相互兼容。所有容器资源限制的总和不得超过Pod的资源值。如果我们省略Pod资源，则将使用定义的容器资源的总和。如果任何容器没有资源定义，则将使用Pod的限制和请求。容器的等效键为`spec.containers[].resources`。
- en: Memory limits and requests will be configured in bytes and we can use suffixes
    such as `ki`, `Mi`, `Gi`, and `Ti` for multiples of 1,000, or `k`, `M`, and `T`
    for multiples of 1,024\. For example, to specify a limit of 100 MB of memory,
    we will use `100M`. When the limited memory allowed is reached, `OOMKiller` will
    be triggered in the execution host and the Pod or container will be terminated.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 内存限制和请求将以字节为单位进行配置，我们可以使用后缀如`ki`、`Mi`、`Gi`和`Ti`表示以1,000为倍数的内存，或使用`k`、`M`和`T`表示以1,024为倍数的内存。例如，要指定100
    MB的内存限制，我们将使用`100M`。当达到允许的内存限制时，`OOMKiller`将在执行主机中触发，Pod或容器将被终止。
- en: For the CPU, we will define the number of CPUs (it doesn’t matter whether they
    are physical or virtual) to be allowed or requested, if we are defining a request
    limit. When the CPU limit is reached, the container or Pod will not get more CPU
    resources, which will probably make your Pod to be considered unhealthy because
    checks will fail. CPU resources must be configured in either integers or fractionals,
    and we can add `m` as a suffix to represent millicores; hence, 0.5 CPUs can also
    be written as `500m`, and 0.001 CPUs will be represented as `1m`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CPU，我们将定义允许或请求的CPU数量（无论是物理CPU还是虚拟CPU），如果我们正在定义请求限制。当CPU限制达到时，容器或Pod将无法获得更多的CPU资源，这可能会使您的Pod被认为是不健康的，因为检查会失败。CPU资源必须以整数或小数形式进行配置，并且我们可以添加`m`作为后缀来表示毫核；因此，0.5个CPU也可以写作`500m`，而0.001个CPU将表示为`1m`。
- en: Important note
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When we are using Linux nodes, we can request and limit huge page resources,
    which allows us to define the page size for memory blocks allocated by the kernel.
    Specific key names must be used; for example, `spec.resources.limits.hugepages-2Mi`
    allows us to define the limit of memory blocks allocated for 2 MiB huge pages.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用Linux节点时，可以请求并限制巨大页资源，这使我们能够定义内核为内存块分配的页大小。必须使用特定的键名；例如，`spec.resources.limits.hugepages-2Mi`使我们能够定义为2
    MiB巨大页分配的内存块限制。
- en: Your administrators can prepare for some `LimitRange` resources, which will
    define constraints for the limits and requests associated with your Pod resources.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 你的管理员可以为某些 `LimitRange` 资源做准备，这些资源将定义与 Pod 资源相关的限制和请求的约束。
- en: Now that we know how we can limit and ask for resources, we can vertically scale
    a workload by increasing its limits. Horizontal scaling, on the other hand, will
    require the replication of Pods. We can now continue and learn how to dynamically
    and horizontally scale Pods related to a running workload.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何限制和请求资源，我们可以通过增加限制来垂直扩展工作负载。而水平扩展则需要复制 Pods。接下来我们将继续学习如何动态地水平扩展与运行中工作负载相关的
    Pods。
- en: Important note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Vertical Pod autoscaling** is also available as a project inside Kubernetes.
    It is less popular because vertical scaling impacts your current Deployments or
    StatefulSets as it requires scaling the number of resources on your running replicas
    up or down. This makes them hard to apply and it is better to fine-grain resources
    in your applications and use horizontal Pod autoscaling, which does not modify
    current replica specifications.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**垂直 Pod 自动扩展**也是 Kubernetes 内部的一个项目。它不太流行，因为垂直扩展会影响当前的 Deployments 或 StatefulSets，因为它需要扩展运行中副本的资源数量。这使得垂直扩展应用起来更为复杂，通常更好的做法是精细化管理应用中的资源，使用水平
    Pod 自动扩展，它不会修改当前的副本规格。'
- en: Horizontal Pod autoscaling
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水平 Pod 自动扩展
- en: '**HorizontalPodAutoscaler** works as a controller. It scales Pods up or down
    when the load associated with the workload is increased or decreased. Autoscaling
    is only available for Deployments (by scaling and modifying their ReplicaSets)
    and StatefulSets. To measure the consumption of resources associated with a specific
    workload, we have to include a tool such as **Kubernetes Metrics Server** in our
    cluster. This server will be used to manage the standard metrics. This can be
    easily deployed using its manifests at [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server).
    It can also be executed as a pluggable add-on if you are using Minikube on your
    laptop or desktop computer.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**HorizontalPodAutoscaler** 作为控制器工作。当工作负载的负载增加或减少时，它会增加或减少 Pod 的副本数量。自动扩展仅适用于
    Deployments（通过扩展和修改其 ReplicaSets）和 StatefulSets。为了衡量与特定工作负载相关的资源消耗，我们需要在集群中加入一个工具，例如
    **Kubernetes Metrics Server**。这个服务器将用于管理标准度量指标。可以通过其清单文件轻松部署，地址为 [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)。如果你在笔记本电脑或台式机上使用
    Minikube，它也可以作为可插拔的附加组件运行。'
- en: We will define a `HorizontalPodAutoscaler` (`hpa`) resource; the controller
    will retrieve and analyze the metrics for a workload resource specified in the
    `hpa` definition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个 `HorizontalPodAutoscaler`（`hpa`）资源；控制器将检索并分析 `hpa` 定义中指定的工作负载资源的度量。
- en: Different types of metrics can be used for the `hpa` resource, although the
    most common is the Pod’s CPU consumption.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 可以为 `hpa` 资源使用不同类型的度量指标，虽然最常用的是 Pod 的 CPU 消耗。
- en: Metrics related to Pods can be defined and thus the controller checks their
    metrics and analyzes them using an algorithm that combines these metrics with
    cluster available resources and Pod states ([https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details))
    and then decides whether or not the associated resource (Deployment or StatefulSet)
    should be scaled.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 可以定义与 Pod 相关的度量指标，控制器会检查这些度量指标并通过结合这些度量与集群可用资源和 Pod 状态的算法进行分析（[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details)），然后决定是否需要扩展关联的资源（Deployment
    或 StatefulSet）。
- en: 'To define an `hpa` resource, we will set up a metric to analyze and a range
    of replicas to use (max and min replicas). When this value is reached, the controller
    reviews the current replicas, and if there’s still room for a new one, it will
    be created. `hpa` resources can be defined in either imperative or declarative
    format. For example, to manage a minimum of two Pods and a maximum of 10 when
    more than 50% of the CPU consumption is reached for the current Pods, we can use
    the following syntax:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一个 `hpa` 资源，我们需要设置一个度量指标来进行分析，并指定一个副本的范围（最大和最小副本数）。当该值达到时，控制器会检查当前的副本数，如果还有空间创建新的副本，则会创建新的副本。`hpa`
    资源可以以命令式或声明式格式定义。例如，要管理最少两个 Pods 和最多十个 Pods，当当前 Pods 的 CPU 使用率超过 50% 时，可以使用以下语法：
- en: '[PRE1]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When a resource’s CPU consumption is more than 50%, then a replica is created,
    while one replica is decreased when this metric is below that value; however,
    we will never execute more than 10 replicas or less than two.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当资源的 CPU 消耗超过 50% 时，会创建一个副本；当该指标低于此值时，会减少一个副本；但是，我们永远不会执行超过 10 个副本或少于两个副本。
- en: Important note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: We can review the manifest that’s used for creating any resource by adding `–o
    yaml`. The manifest will be presented and we will be able to verify its values.
    As an example, we can use `kubectl autoscale deploy webserver --cpu-percent=50
    --min=2 --max=10 -``o yaml`.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加 `–o yaml` 来查看用于创建任何资源的清单。清单将显示出来，我们可以验证其值。例如，我们可以使用 `kubectl autoscale
    deploy webserver --cpu-percent=50 --min=2 --max=10 -o yaml`。
- en: If we want to review values before creating the resource, we can add the `--dry-run=client`
    argument to only show the manifest, without actually creating the resource.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在创建资源之前查看值，可以添加 `--dry-run=client` 参数，只显示清单，而不实际创建资源。
- en: As `hpa` resources are namespaced, we can get all the already deployed `hpa`
    resources by executing `kubectl get` `hpa -A`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `hpa` 资源是命名空间的，我们可以通过执行 `kubectl get hpa -A` 获取所有已部署的 `hpa` 资源。
- en: With that, we have seen how Kubernetes provides resilience, high availability,
    and autoscaling facilities out of the box by using specific resources. In the
    next section, we will learn how it also provides some interesting security features
    that will help us improve our application security.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们已经看到 Kubernetes 如何通过使用特定的资源提供开箱即用的弹性、高可用性和自动扩展功能。在下一节中，我们将学习它如何提供一些有趣的安全功能，帮助我们提升应用程序的安全性。
- en: Improving application security with Pods
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升应用程序安全性与 Pods
- en: In a Kubernetes cluster, we can categorize the applications’ workloads distributed
    cluster-wide as either privileged or unprivileged. Privileged workloads should
    always be avoided for normal applications unless they are strictly necessary.
    In this section, we will help you define the security of your applications by
    declaring your requirements in your workload manifests.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群中，我们可以将分布在集群中的应用程序工作负载分类为特权或非特权。除非严格必要，否则应始终避免为普通应用程序使用特权工作负载。在本节中，我们将帮助你通过在工作负载清单中声明需求，定义应用程序的安全性。
- en: Security contexts
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全上下文
- en: 'In a security context, we define the privileges and security configuration
    required for a Pod or the containers included in it. Security contexts allow us
    to configure the following security features:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全上下文中，我们定义了 Pod 或其中包含的容器所需的特权和安全配置。安全上下文允许我们配置以下安全功能：
- en: '`runAsUser`/`runAsGroup`: These options manage the `userID` and `groupID` properties
    that run the main process with containers. We can add more groups by using the
    `supplementalGroups` key.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runAsUser`/`runAsGroup`：这些选项管理运行容器中主进程的 `userID` 和 `groupID` 属性。我们可以通过使用 `supplementalGroups`
    键添加更多的组。'
- en: '`runAsNonRoot`: This key can control whether we allow the process to run as
    `root`.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runAsNonRoot`：该键可以控制是否允许进程以 `root` 身份运行。'
- en: '`fsGroup`/`fsGroupChangePolicy`: These options manage the permissions of the
    volumes included within a Pod. The `fsGroup` key will set the owner of the filesystems
    mounted as volumes and the owner of any new file. We can use `fsGroupChangePolicy`
    to only apply the ownership change if the permissions don’t match the configured
    `fsGroup`.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsGroup`/`fsGroupChangePolicy`：这些选项管理 Pod 中包含的卷的权限。`fsGroup` 键将设置作为卷挂载的文件系统的所有者，以及任何新文件的所有者。我们可以使用
    `fsGroupChangePolicy` 仅在权限与配置的 `fsGroup` 不匹配时应用所有权更改。'
- en: '`seLinuxOptions`/`seccompProfile`: These options allow us to overwrite default
    SELinux and `seccomp` settings by configuring special SELinux labels and a special
    `seccomp` profile.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seLinuxOptions`/`seccompProfile`：这些选项允许我们通过配置特定的 SELinux 标签和 `seccomp` 配置文件来覆盖默认的
    SELinux 和 `seccomp` 设置。'
- en: '`capabilities`: Kernel capabilities can be added or removed (`drop`) to only
    allow specific kernel interactions (containers share the host’s kernel). You should
    avoid unnecessary capabilities in your applications.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`capabilities`：内核能力可以添加或移除（`drop`），以仅允许特定的内核交互（容器共享主机的内核）。你应避免在应用程序中使用不必要的能力。'
- en: '`privileged`/`AllowPrivilegeEscalation`: We can allow processes inside a container
    to be executed as `privileged` (with all the capabilities) by setting the `privileged`
    key to `true` or to be able to gain privileges, even if this key was set to `false`,
    by setting `AllowPrivilegeEscalation` to `true`. In this case, container processes
    do not have all capabilities but they will allow internal processes to run as
    if they had the `CAP_SYS_ADMIN` capability.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`privileged`/`AllowPrivilegeEscalation`: 我们可以通过将`privileged`键设置为`true`来允许容器内部的进程以`privileged`身份（拥有所有能力）执行，或者通过将`AllowPrivilegeEscalation`设置为`true`来允许进程获得权限，即使`privileged`键被设置为`false`。在这种情况下，容器进程并不具备所有能力，但它们会允许内部进程像拥有`CAP_SYS_ADMIN`能力一样运行。'
- en: '`readOnlyRootFilesystem`: It is always a very good idea to run your containers
    with their root filesystem in read-only mode. This won’t allow processes to make
    any changes in the container. If you understand the requirements of your application,
    you will be able to identify any directory that may be changed and add an appropriate
    volume to run your processes correctly. It is quite usual, for example, to add
    `/tmp` as a separate temporal filesystem (`emptyDir`).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readOnlyRootFilesystem`: 总是非常好的做法是将容器的根文件系统设置为只读模式运行。这将不允许进程对容器内的任何内容进行更改。如果你理解应用程序的需求，你将能够识别出可能需要更改的任何目录，并添加适当的卷以正确运行你的进程。例如，通常会将`/tmp`添加为单独的临时文件系统（`emptyDir`）。'
- en: Some of these keys are available at the container or Pod level or both. Use
    `kubectl explain pod.spec.securityContext` or `kubectl explain pod.spec.containers.securityContext`
    to retrieve a detailed list of the options available in each scope. You have to
    be aware of the scope that’s used because Pod specifications apply to all containers
    unless the same key exists under the container scope – in which case, its value
    will be used.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些键可用于容器或Pod级别，或两者都有。使用`kubectl explain pod.spec.securityContext`或`kubectl
    explain pod.spec.containers.securityContext`来获取每个作用域中可用选项的详细列表。你必须了解所使用的作用域，因为Pod规范适用于所有容器，除非在容器作用域下存在相同的键——在这种情况下，将使用容器作用域中的值。
- en: Let’s review the best settings we can prepare to improve our application security.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下可以准备的最佳设置，以提升我们的应用程序安全性。
- en: Security best practices
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全最佳实践
- en: 'The following list shows some of the most used settings for improving security.
    You, as a developer, can improve your application security if you ensure the following
    security measures can be enabled for your Pods:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了一些常用的安全设置。如果你是开发者，可以通过确保为你的Pods启用以下安全措施来提升应用程序的安全性：
- en: '`runAsNonRoot` must always be set to `true` to avoid the use of `root` on your
    containers. Ensure you also configure `runAsUser` and `runAsGroup` to IDs greater
    than `1000`. Your Kubernetes administrators can suggest some IDs for your application.
    This will help control application IDs cluster-wide.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runAsNonRoot`必须始终设置为`true`，以避免在容器中使用`root`。确保还配置`runAsUser`和`runAsGroup`，并将它们设置为大于`1000`的ID。你的Kubernetes管理员可以为你的应用程序推荐一些ID。这将有助于在集群范围内控制应用程序ID。'
- en: Always drop all capabilities and enable only those required by your application.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 始终禁用所有能力，只启用应用程序所需的能力。
- en: Never use privileged containers for your applications unless it is strictly
    necessary. Usually, only monitoring- or kernel-related applications require special
    privileges.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非绝对必要，否则不要为应用程序使用特权容器。通常，只有与监控或内核相关的应用程序需要特殊权限。
- en: Identify the filesystem’s requirement for your application and always set `readOnlyRootFilesystem`
    to `true`. This simple setting improves security, disabling any unexpected changes.
    Required filesystems can be mounted as volumes (many options are available, as
    we will learn in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231), *Leveraging
    Application Data Management* *in Kubernetes*).
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别应用程序所需的文件系统，并始终将`readOnlyRootFilesystem`设置为`true`。这个简单的设置提高了安全性，禁止了任何意外的更改。所需的文件系统可以作为卷挂载（有很多选项可用，我们将在[*第10章*](B19845_10.xhtml#_idTextAnchor231)，“在Kubernetes中利用应用数据管理”中学习）。
- en: Ask your Kubernetes administrators whether there are some SELinux settings you
    should consider to apply them on your Pods. This also applies to `seccomp` profiles.
    Your administrators may have configured a default profile. Ask your administrators
    about this situation to avoid any system call issues.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向你的Kubernetes管理员询问是否有一些SELinux设置需要考虑并应用于你的Pods。这同样适用于`seccomp`配置文件。管理员可能已经配置了默认配置文件。请向管理员了解这种情况，以避免任何系统调用问题。
- en: Your administrators may have been using tools such as Kyverno or OPA Gatekeeper
    to improve cluster security. In these cases, they can enforce security context
    settings by using **admission controllers** in the Kubernetes cluster. The use
    of these features is outside the scope of this book but you may ask your administrators
    about the compliance rules required to execute applications in your Kubernetes
    platform.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理员可能已经在使用诸如Kyverno或OPA Gatekeeper之类的工具来提高集群安全性。在这些情况下，他们可以通过在Kubernetes集群中使用**准入控制器**来强制执行安全上下文设置。这些功能的使用超出了本书的范围，但您可以向管理员询问在您的Kubernetes平台中执行应用程序所需的合规规则。
- en: In the next section, we will review how to implement some of the Kubernetes
    features we learned about in this chapter by preparing the multi-component application
    we used in previous chapters ([*Chapter 5*](B19845_05.xhtml#_idTextAnchor118),
    *Creating Multi-Container Applications*, and [*Chapter 7*](B19845_07.xhtml#_idTextAnchor147),
    *Orchestrating with Swarm*) to run on Kubernetes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将回顾如何通过准备多组件应用程序来实现本章中学习的一些Kubernetes特性，该应用程序在之前的章节中已经使用过（[*Chapter
    5*](B19845_05.xhtml#_idTextAnchor118)，*创建多容器应用程序*，以及[*Chapter 7*](B19845_07.xhtml#_idTextAnchor147)，*使用Swarm进行编排*），在Kubernetes上运行。
- en: Labs
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: This section will show you how to deploy the `simplestlab` three-tier application
    in Kubernetes. Manifests for all its components have been prepared for you while
    following the techniques and Kubernetes resources explained in this chapter. You
    will be able to verify the usage of the different options and you will able to
    play with them to review the content and best practices described in this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向您展示如何在Kubernetes中部署`simplestlab`三层应用程序。我们已经为您准备了所有组件的清单，这些清单遵循了本章中解释的技术和Kubernetes资源。您将能够验证不同选项的使用，并能够根据本章描述的内容和最佳实践进行操作。
- en: The code for these labs is available in this book’s GitHub repository at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git).
    Ensure you have the latest revision available by simply executing `git clone`
    [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git)
    to download all its content, or `git pull` if you’ve already downloaded the repository
    before. All the manifests and the steps required for running `simplestlab` are
    located inside the `Containers-for-Developers-Handbook/Chapter9` directory.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验的代码可以在本书的GitHub存储库中找到：[https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git)。通过简单执行
    `git clone` [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git)
    来下载其所有内容，或者如果您之前已经下载了存储库，则执行 `git pull` 更新至最新版本。所有清单和运行`simplestlab`所需的步骤都位于`Containers-for-Developers-Handbook/Chapter9`目录中。
- en: 'In the labs in GitHub, we will deploy the `simplestlab` application, which
    is used in previous chapters, on Kubernetes by defining appropriate resource manifests:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上的实验中，我们将通过定义适当的资源清单在Kubernetes上部署`simplestlab`应用程序，该应用程序在之前的章节中已经使用过。
- en: The **database** component will be deployed using a StatefulSet resource
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库**组件将使用StatefulSet资源部署。'
- en: The **application backend** component will be deployed using a Deployment resource
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序后端**组件将使用Deployment资源部署。'
- en: The **load balancer** (or **presenter**) component will be deployed using a
    DaemonSet resource
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡器**（或**展示器**）组件将使用DaemonSet资源部署。'
- en: 'In their manifests, we have included some of the mechanisms we learned about
    in this chapter for checking the component’s health, replicating their processes,
    and improving their security by disallowing their execution as the root user,
    among other features. Let’s start by reviewing and deploying the database component:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在其清单中，我们已经包含了本章中学习的一些机制，用于检查组件的健康状况、复制其进程，并通过不允许其作为root用户执行等功能来提高其安全性。让我们从回顾和部署数据库组件开始：
- en: 'We will use a StatefulSet to ensure that replicating its processes (scaling
    up) will never represent a problem to our data. It is important to understand
    that a new replica starts empty, without data, and joins the pool of available
    endpoints for the Service, which will probably be a problem. This means that in
    these conditions, the Postgres database isn’t scalable, so this component is deployed
    as a StatefulSet to preserve its data even in the case of a manual replication.
    This example only provides resilience, so do not scale this component. If you
    need to deploy a database with high availability, you will need a distributed
    database such as MongoDB. The full manifest for the database manifest can be found
    in `Chapter9/db.satatefulset.yaml`. Here is a small extract from this file:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个 StatefulSet 来确保复制其进程（扩展）永远不会成为我们数据的问题。重要的是要理解，新的副本将空白地开始，没有数据，并加入到可用端点池中，这可能会成为一个问题。这意味着在这些条件下，Postgres
    数据库不可扩展，因此此组件部署为 StatefulSet，以保持其数据，即使在手动复制的情况下也是如此。此示例仅提供了弹性，因此不要扩展此组件。如果您需要部署具有高可用性的数据库，则需要像
    MongoDB 这样的分布式数据库。完整的数据库清单可以在 `Chapter9/db.satatefulset.yaml` 中找到。这里是来自此文件的一个小节选：
- en: '[PRE2]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, we defined a template for the Pods to create and a separate template
    for the VolumeClaims (we will talk about them in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231)).
    This ensures that each Pod will get its own volume. The volume that’s created
    will be mounted in the database container as the `/data` filesystem and its size
    will be 1,000 MB (1 Gi). No other container is created. The `POSTGRES_PASSWORD`
    and `PGDATA` environment variables are set and passed to the container. They will
    be used to create the password for the Postgres user and the patch for the database
    data. The image that’s used for the container is `docker.io/frjaraur/simplestdb:1.0`
    and port `5432` will be used to expose its Service. Pods only expose their Services
    internally, in the Kubernetes network, so you will never be able to reach these
    Services from remote clients. We specified one replica and the controller will
    associate the pods with this StatefulSet by searching for Pods with `component=db`
    and `app=simplestlab` labels. We simplified the database’s probes by just checking
    a TCP connection to port `5432`. We defined a security context at the Pod’s level,
    which will apply to all the containers by default:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们定义了用于创建 Pod 的模板以及用于 VolumeClaims 的单独模板（我们将在 [*第 10 章*](B19845_10.xhtml#_idTextAnchor231)
    中讨论它们）。这确保每个 Pod 都将获得自己的卷。创建的卷将被挂载到数据库容器中作为 `/data` 文件系统，其大小将为 1,000 MB（1 GiB）。不会创建其他容器。设置并传递了
    `POSTGRES_PASSWORD` 和 `PGDATA` 环境变量到容器中。它们将用于为 Postgres 用户创建密码和数据库数据的补丁。用于容器的镜像是
    `docker.io/frjaraur/simplestdb:1.0`，并将使用端口 `5432` 来公开其服务。Pod 仅在 Kubernetes 网络内部公开其服务，因此您永远无法从远程客户端访问这些服务。我们指定了一个副本，并且控制器将通过搜索带有
    `component=db` 和 `app=simplestlab` 标签的 Pod 将这些 Pod 与此 StatefulSet 关联起来。我们通过仅检查到端口
    `5432` 的 TCP 连接来简化数据库的探测。我们在 Pod 级别定义了安全上下文，这将默认应用于所有容器：
- en: '[PRE3]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The database processes will run as `10000:10000` `user:group`, hence they are
    secure (no root is required). We could have gone further if we set the container
    as read-only but in this case, we didn’t as Docker’s official Postgres image;
    however, it would have been better to use a full read-only filesystem.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据库进程将作为 `10000:10000` 的 `user:group` 运行，因此它们是安全的（不需要 root）。如果将容器设置为只读，我们本可以更进一步，但在这种情况下我们没有这样做，因为
    Docker 的官方 Postgres 镜像；然而，使用完全只读文件系统会更好。
- en: The Pod will get an IP address, though this may change if the Pod is recreated
    for any reason, which makes Pods’ IP addresses impossible to use in such dynamic
    environments. We will use a Service to associate a *fixed* IP address with a Service
    and then with the endpoints of the Pods related to the Service.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pod 将获得一个 IP 地址，但如果 Pod 由于任何原因重新创建，此 IP 地址可能会更改，这使得 Pod 的 IP 地址在这种动态环境下无法使用。我们将使用一个
    Service 来将一个 *固定的* IP 地址与 Service 关联，然后与与 Service 相关的 Pod 的端点关联起来。
- en: 'The following is an extract from the Service manifest (you will find it as
    `Chapter9/db.service.yaml`):'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是从 Service 清单中提取的内容（您可以在 `Chapter9/db.service.yaml` 中找到）：
- en: '[PRE4]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This Service is associated with the Pods by using a selector (the `components=db`
    and `app=simplestlab` labels) and Kubernetes will route the traffic to the appropriate
    Pods. When a TCP packet reaches the Service’s port, `5432`, it is load balanced
    to all the available Pod’s endpoints (in this case, we will just have one replica)
    in port `5432`. In both cases, we used port `5432`, but you must understand that
    `targetPort` refers to the container port, while the port key refers to the Service’s
    port, and they can be completely different. We are using a headless Service because
    it works very well with StatefulSets and their resolution in round-robin mode.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该Service通过使用选择器（`components=db`和`app=simplestlab`标签）与Pods关联，Kubernetes将流量路由到适当的Pods。当TCP数据包到达Service的端口`5432`时，它会负载均衡到所有可用Pod的端点（在这种情况下，我们只有一个副本），端口为`5432`。在这两种情况下，我们使用了端口`5432`，但你必须理解，`targetPort`指的是容器端口，而端口键指的是Service的端口，它们可以是完全不同的。我们使用的是无头Service，因为它与StatefulSets及其轮询模式解析非常兼容。
- en: 'With the StatefulSet definition and the Service, we can deploy the database
    component:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用StatefulSet定义和Service，我们可以部署数据库组件：
- en: '[PRE5]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The application (backend component) is deployed as a `Deployment` workload.
    Let’s see an extract of its manifest:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该应用程序（后端组件）作为`Deployment`工作负载进行部署。让我们来看一下其清单的一个片段：
- en: '[PRE6]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can find the complete manifest in the `Chapter9/app.deployment.yaml` file.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在`Chapter9/app.deployment.yaml`文件中找到完整的清单。
- en: 'For this component, we defined three replicas, so three Pods will be deployed
    cluster-wide. In this component, we are using the `docker.io/frjaraur/simplestapp:1.0`
    image. We’ve configured two security contexts, one at the Pod’s level:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于该组件，我们定义了三个副本，因此会在集群中部署三个Pods。此组件使用`docker.io/frjaraur/simplestapp:1.0`镜像。我们已配置了两个安全上下文，其中一个是在Pod级别：
- en: '[PRE7]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The second is for enforcing the use of a read-only filesystem for the container:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个用于强制容器使用只读文件系统：
- en: '[PRE8]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we prepared `readinessProbe` using `httpGet` but we still keep `tcpSocket`
    for `livenessProbe`. We coded `/healthz` as the application’s health endpoint
    for checking its healthiness.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们为`readinessProbe`使用了`httpGet`，但仍然保留`tcpSocket`用于`livenessProbe`。我们将`/healthz`作为应用程序的健康检查端点，用于检查其健康状态。
- en: 'In this component, we added a resource section for the app container:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此组件中，我们为应用容器添加了资源部分：
- en: '[PRE9]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, we asked Kubernetes for at least 10 millicores of CPU and 20M
    of memory. The `limits` section describes the maximum CPU (20 millicores) and
    memory (30Mi). If the memory limit is reached, Kubelet will trigger the OOM-Killer
    procedure and it will kill the container. When the CPU limit is reached, the kernel
    does not provide more CPU cycles to the container, which may lead the probes to
    fail and hence the container will die. This component is stateless and it is running
    completely in read-only mode.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，我们要求Kubernetes至少分配10毫核心CPU和20M内存。`limits`部分描述了最大CPU（20毫核心）和内存（30Mi）。如果内存限制被达到，Kubelet将触发OOM-Killer过程并杀死容器。当CPU限制达到时，内核不会再为容器提供更多的CPU周期，这可能导致探针失败，进而导致容器崩溃。这个组件是无状态的，并且完全以只读模式运行。
- en: Important note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In the full YAML file manifest, you will see that we are using the environment
    variables for passing sensitive data. Always avoid passing sensitive data in environment
    variables as anyone with access to your manifest files will be able to read it.
    We will learn how to include sensitive data in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231),
    *Leveraging Application Data Management* *in Kubernetes*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整的YAML文件清单中，你将看到我们使用了环境变量来传递敏感数据。始终避免通过环境变量传递敏感数据，因为任何能够访问你的清单文件的人都能读取这些数据。我们将学习如何在[*第10章*](B19845_10.xhtml#_idTextAnchor231)中包含敏感数据，*在Kubernetes中利用应用数据管理*。
- en: 'We will also add a Service for accessing the `app` `Deployment` workload:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将为访问`app` `Deployment`工作负载添加一个Service：
- en: '[PRE10]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We create both Kubernetes resources:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了这两个Kubernetes资源：
- en: '[PRE11]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s see an extract from the DaemonSet manifest:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看一下DaemonSet清单的一个片段：
- en: '[PRE12]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that we added `/var/cache/nginx` and `/tmp` as `emptyDir` volumes, as
    mentioned previously. This component will be also stateless and run in read-only
    mode, but some temporal directories must be created as `emptyDir` volumes so that
    they can be written to without allowing the full container’s filesystem.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们添加了`/var/cache/nginx`和`/tmp`作为`emptyDir`卷，如前所述。该组件也将是无状态的，并且以只读模式运行，但必须创建一些临时目录作为`emptyDir`卷，这样可以在不允许整个容器文件系统的情况下写入数据。
- en: 'The following security contexts are created:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了以下安全上下文：
- en: 'At the Pod level:'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Pod级别：
- en: '[PRE13]'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At the container level:'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在容器级别：
- en: '[PRE14]'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we have the `Service` definition, where we will use a `NodePort` type
    to quickly expose our application:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有了 `Service` 定义，在这里我们将使用 `NodePort` 类型来快速暴露我们的应用程序：
- en: '[PRE15]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s deploy all the `lb` component (frontend) manifests:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们部署所有的 `lb` 组件（前端）清单：
- en: '[PRE16]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Figure 9.9 – simplestlab application web GUI](img/B19845_09_9.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – simplestlab 应用程序 Web 图形用户界面](img/B19845_09_9.jpg)'
- en: Figure 9.9 – simplestlab application web GUI
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – simplestlab 应用程序 Web 图形用户界面
- en: You can find additional steps for scaling up and down the application backend
    component in the `Chapter9` code repository. The labs included in this chapter
    will help you understand how to deploy an application using different Kubernetes
    resources.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 `Chapter9` 代码库中找到关于扩展和缩减应用程序后端组件的额外步骤。本章中包含的实验将帮助你理解如何使用不同的 Kubernetes
    资源来部署应用程序。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the resources that can help us deploy application
    workloads in Kubernetes. We took a look at the different options for running replicated
    processes and verifying their health to provide resilience, high availability,
    and auto-scalability. We also learned about some of the Pod features that can
    help us implement advanced patterns and improve the overall application security.
    We are now ready to deploy our application using the best patterns and apply and
    customize the resources provided by Kubernetes, and we know how to implement appropriate
    health checks while limiting resource consumption in our platform.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了可以帮助我们在 Kubernetes 中部署应用工作负载的资源。我们查看了运行复制进程的不同选项，并验证其健康状态，以提供弹性、高可用性和自动扩展。我们还学习了一些
    Pod 特性，这些特性可以帮助我们实现高级模式并提高整体应用程序的安全性。现在，我们已经准备好使用最佳模式部署我们的应用程序，并应用和自定义 Kubernetes
    提供的资源，同时知道如何实施适当的健康检查，并在平台中限制资源消耗。
- en: In the next chapter, we will deep dive into the options we have for managing
    data within Kubernetes and presenting it to our applications.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将深入探讨管理 Kubernetes 中的数据以及如何将其呈现给我们的应用程序的选项。
