- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Implementing Architecture Patterns
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现架构模式
- en: 'Kubernetes is the most popular container orchestrator in production. This platform
    provides different resources that allow us to deploy our applications with high
    resilience and with their components distributed cluster-wide while the platform
    itself runs with high availability. In this chapter, we will learn how these resources
    can provide different application architecture patterns, along with use cases
    and best practices to implement them. We will also review different options for
    managing application data and learn how to manage the health of our applications
    to make them respond to possible health and performance issues in the most effective
    way. At the end of this chapter, we will review how Kubernetes provides security
    patterns that improve application security. This chapter will give you a good
    overview of which Kubernetes resources will fit your applications’ requirements
    most accurately. The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是生产环境中最流行的容器调度器。该平台提供了不同的资源，允许我们以高韧性和分布式的方式部署应用程序，同时平台本身具有高可用性。在本章中，我们将学习这些资源如何提供不同的应用架构模式，并结合用例和最佳实践来实施它们。我们还将回顾不同的应用数据管理选项，并学习如何管理应用程序的健康，以最有效的方式应对可能出现的健康和性能问题。在本章的最后，我们将回顾
    Kubernetes 提供的安全模式，以提高应用程序的安全性。本章将为您提供一个关于哪些 Kubernetes 资源最适合您应用需求的良好概览。本章将涵盖以下主题：
- en: Applying Kubernetes resources to common application patterns
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Kubernetes 资源应用于常见应用模式
- en: Understanding advanced Pod application patterns
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解高级 Pod 应用模式
- en: Verifying application health
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证应用健康状况
- en: Resource management and scalability
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源管理与可扩展性
- en: Improving application security with Pods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pods 提升应用安全性
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the labs for this chapter at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9),
    where you will find some extended explanations that have been omitted in this
    chapter’s content to make it easier to follow. The *Code In Action* video for
    this chapter can be found at [https://packt.link/JdOIY](https://packt.link/JdOIY).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的实验可以在 [https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9)
    找到，您将在那里找到一些扩展的解释，这些内容在本章中被省略，以便更容易理解。本章的 *Code In Action* 视频可以在 [https://packt.link/JdOIY](https://packt.link/JdOIY)
    找到。
- en: We will start this chapter by reviewing some of the resource types that were
    presented in [*Chapter 8*](B19845_08.xhtml#_idTextAnchor170), *Deploying Applications
    with the Kubernetes Orchestrator*, and present some common use cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从回顾 [*第 8 章*](B19845_08.xhtml#_idTextAnchor170)，《使用 Kubernetes 调度器部署应用》，开始，并介绍一些常见的用例。
- en: Applying Kubernetes resources to common application patterns
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Kubernetes 资源应用于常见应用模式
- en: 'The Kubernetes container orchestrator is based on resources that are managed
    by different controllers. By default, our applications can use one of the following
    to run our processes as containers:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 容器调度器基于由不同控制器管理的资源。默认情况下，我们的应用程序可以使用以下其中之一来运行容器中的进程：
- en: Pods
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods
- en: ReplicaSets
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaSets
- en: ReplicaControllers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaControllers
- en: Deployments
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deployments
- en: StatefulSets
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSets
- en: DaemonSets
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSets
- en: Jobs
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jobs
- en: CronJobs
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CronJobs
- en: This list shows the standard or default resources that are allowed on every
    Kubernetes installation, but we can create custom resources to implement any non-standard
    or more specific application behavior. In this section, we are going to learn
    about these standard resources so that we can decide which one will fit our application’s
    needs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表显示了每个 Kubernetes 安装中允许的标准或默认资源，但我们可以创建自定义资源来实现任何非标准或更具体的应用行为。在本节中，我们将学习这些标准资源，以便我们能够决定哪个最适合我们的应用需求。
- en: Pods
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pods
- en: Pods are the minimal unit for deploying workloads on a Kubernetes cluster. A
    Pod can contain multiple containers, and we have different mechanisms for doing
    this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是在 Kubernetes 集群中部署工作负载的最小单元。一个 Pod 可以包含多个容器，我们有不同的机制来实现这一点。
- en: 'The following applies to all the containers running inside a Pod by default:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，以下内容适用于 Pod 内运行的所有容器：
- en: They all share the network namespace, so they all refer to the same localhost
    and run with the same IP address.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们共享网络命名空间，因此它们都指向相同的本地主机，并使用相同的 IP 地址运行。
- en: They are all scheduled together in the same host.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都在同一主机上调度。
- en: They all share the namespaces that can be defined at the container level. We
    will define resource limits (using cgroups) for each container, although we can
    also define resource limits at the Pod level. Pod resource limits will be applied
    for all the containers.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们共享可以在容器级别定义的命名空间。我们将为每个容器定义资源限制（使用 cgroups），尽管我们也可以在 Pod 级别定义资源限制。Pod 资源限制将应用于所有容器。
- en: Volumes attached to a Pod are available to all containers running inside.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加到 Pod 的卷对所有在 Pod 内运行的容器都是可用的。
- en: So, we can see that Pods are groups of containers running together that share
    kernel namespaces and compute resources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以看到 Pods 是一组一起运行的容器，它们共享内核命名空间和计算资源。
- en: You will find a lot of Pods running a single container, and this is completely
    fine. The distribution of containers inside different Pods depends on your application’s
    components distribution. You need to ask yourself whether or not some processes
    must run together. For example, you can put together two containers that need
    to communicate fast, or you may need to keep track of files created by one of
    them without sharing a remote data volume. But it is very important to understand
    that all the containers running in a Pod scale and replicate together. This means
    that multiple replicas of a Pod will execute the same number of replicas of their
    containers, and thus your application’s behavior can be impacted because the same
    type of process will run multiple times, and it will also access your files at
    the same time. This, for example, will break the data in your database or may
    lead to data inconsistency. Therefore, you need to decide wisely whether or not
    your application should distribute your application’s containers into different
    Pods or run them together.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现许多 Pods 只运行一个容器，这完全是可以接受的。容器在不同 Pods 之间的分布取决于你的应用程序组件的分布。你需要问自己，某些进程是否必须一起运行。例如，你可以将需要快速通信的两个容器放在一起，或者你可能需要在不共享远程数据卷的情况下跟踪其中一个容器创建的文件。但非常重要的是要理解，所有在
    Pod 中运行的容器是一起扩展和复制的。这意味着，Pod 的多个副本将执行相同数量的容器副本，因此应用程序的行为可能会受到影响，因为相同类型的进程会多次运行，并且它们也会同时访问你的文件。例如，这会破坏你的数据库中的数据，或者可能导致数据不一致。因此，你需要明智地决定是否将应用程序的容器分布到不同的
    Pods 中，或者将它们一起运行。
- en: Kubernetes will keep track of the status of each container running inside a
    Pod by executing the Pod’s defined probes. Each container should have its own
    probes (different types of probes exist, as we will learn in the *Verifying application
    health* section). But at this point, we have to understand that the health of
    all the containers inside a Pod controls the overall behavior of the Pod. If one
    of the containers dies, the entire Pod is set as unhealthy, triggering the defined
    Kubernetes event and resource’s behavior. Thus, we can execute multiple Service
    containers in parallel or prepare our application by executing some pre-processes
    that can, for example, populate some minimal filesystem resources, binaries, permissions,
    and so on. These types of containers, which run before the actual application
    processes, are called `initContainers`, which is the key that’s used to define
    them) sequentially before any other container; therefore, if any of these initial
    containers fail, the Pod will not run as expected.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 会通过执行 Pod 定义的探针来跟踪每个容器在 Pod 中的状态。每个容器应该有自己的探针（存在不同类型的探针，我们将在*验证应用程序健康*部分学习）。但是此时，我们必须理解，Pod
    内所有容器的健康状况会控制整个 Pod 的行为。如果其中一个容器崩溃，整个 Pod 会被标记为不健康，从而触发定义的 Kubernetes 事件和资源行为。因此，我们可以并行执行多个服务容器，或者通过执行一些预处理来准备我们的应用程序，例如填充一些最小的文件系统资源、二进制文件、权限等等。这些在实际应用程序进程之前运行的容器称为
    `initContainers`（它是定义它们的关键字），并且会在其他容器之前按顺序运行；因此，如果这些初始容器中的任何一个失败，Pod 将无法按预期运行。
- en: Important note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The Kubernetes 1.25 release introduced `kubectl debug` action followed by the
    name of the Pod to which your terminal should attach (shared kernel namespaces).
    We will provide a quick example of this in the *Labs* section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.25 版本引入了 `kubectl debug` 操作，后跟 Pod 的名称，你的终端应该连接到该 Pod（共享内核命名空间）。我们将在
    *实验室* 部分提供一个快速示例。
- en: 'Let’s review how we write the manifest required for creating a Pod:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下如何编写创建 Pod 所需的清单：
- en: '![Figure 9.1 – Pod manifest](img/B19845_09_1.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – Pod 清单](img/B19845_09_1.jpg)'
- en: Figure 9.1 – Pod manifest
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – Pod 清单
- en: All the manifests for Kubernetes have at least the `apiVersion`, `kind`, and
    `metadata` keys, and these are used to define which API version will be used to
    reach the associated API server path, which type of resource we are defining,
    and information that uniquely describes the resource within the Kubernetes cluster,
    respectively. We can access all the resource manifest information via the Kubernetes
    API by using the JSON or YAML keys hierarchy; for example, to retrieve a Pod’s
    name, we can use `.metadata.name` to access its key. The properties of the resource
    should usually be written in the `spec` or `data` section. Kubernetes roles, role
    bindings (in cluster and namespaces scopes), Service accounts, and other resources
    do not include either `data` or `spec` keys for declaring their functionality.
    And we can even create custom resources, with custom definitions for declaring
    their properties. In the default workload resources, we will always use the `spec`
    section to define the behavior of our resource.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Kubernetes 的清单文件至少包含 `apiVersion`、`kind` 和 `metadata` 这三个键，它们分别用于定义将使用哪个
    API 版本来访问关联的 API 服务器路径，定义我们正在定义的资源类型，以及描述资源在 Kubernetes 集群内唯一标识的信息。我们可以通过 Kubernetes
    API 使用 JSON 或 YAML 键层次结构访问所有资源清单信息；例如，要获取 Pod 的名称，我们可以使用 `.metadata.name` 来访问其键。资源的属性通常应写入
    `spec` 或 `data` 部分。Kubernetes 角色、角色绑定（在集群和命名空间范围内）、服务帐户以及其他资源没有包含 `data` 或 `spec`
    键来声明其功能。我们甚至可以创建自定义资源，使用自定义定义来声明其属性。在默认的工作负载资源中，我们总是会使用 `spec` 部分来定义资源的行为。
- en: Notice that in the previous code snippet, the `containers` key is an array.
    This allows us to define multiple containers, as we already mentioned, and the
    same happens with initial containers; we will define a list of containers in both
    cases and we will need at least the image that the container runtime must use
    and the name for the container.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码片段中，`containers` 键是一个数组。这允许我们定义多个容器，正如我们之前提到的，初始容器也是如此；在两种情况下，我们都将定义一个容器列表，并且我们至少需要容器运行时必须使用的镜像和容器的名称。
- en: Important note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We can use `kubectl explain pod.spec.containers --recursive` to retrieve all
    the existent keys under the `spec` section for a defined resource. The `explain`
    action allows you to retrieve all the keys for each resource directly from the
    Kubernetes cluster; this is important as it doesn’t depend on your `kubectl` binary
    version. The output of this action also shows which keys can be changed at runtime,
    once the resource has been created in the cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `kubectl explain pod.spec.containers --recursive` 来检索定义资源下 `spec` 部分的所有现有键。`explain`
    操作允许你直接从 Kubernetes 集群中检索每个资源的所有键；这很重要，因为它不依赖于你 `kubectl` 二进制文件的版本。该操作的输出还显示哪些键可以在运行时更改，一旦资源在集群中创建。
- en: It is important to mention here that Pods by themselves don’t have cluster-wide
    auto-healing. This means that when you run a Pod and it is considered unhealthy
    for whatever reason (any of the containers is considered unhealthy) in a host
    within the cluster, it will not execute on another host. Pods include the `restartPolicy`
    property to manage the behavior of Pods when they die. We can set this property
    to `Always` (always restart the container’s Pods), `OnFailure` (only restart containers
    when they fail), or `Never`. A new Pod will never be recreated on another cluster
    host. We will need more advanced resources for managing the containers’ life cycle
    cluster-wide; these will be discussed in the subsequent sections.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要特别提到的是，Pod 本身没有集群范围的自动修复功能。这意味着，当你运行一个 Pod，并且它由于某种原因（任何容器被认为不健康）被认为不健康时，它不会在集群中的其他主机上执行。Pod
    包含 `restartPolicy` 属性来管理 Pod 死亡后的行为。我们可以将该属性设置为 `Always`（始终重启容器的 Pod）、`OnFailure`（仅在容器失败时重启）、或
    `Never`。新的 Pod 永远不会在其他集群主机上重新创建。我们需要更高级的资源来管理集群范围内容器的生命周期，这些将在后续章节中讨论。
- en: Pods are used to run a test for an application or one of its components, but
    we never use them to run actual Services because Kubernetes just keeps them running;
    it doesn’t manage their updates or reconfiguration. Let’s review how ReplicaSets
    solve these situations when we need to keep our application’s containers up and
    running.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 用于运行应用程序或其组件的测试，但我们从不使用它们来运行实际的服务，因为 Kubernetes 只是保持它们运行；它不管理它们的更新或重新配置。让我们回顾一下副本集是如何解决在需要保持应用程序容器持续运行时的这些问题。
- en: ReplicaSets
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 副本集（ReplicaSets）
- en: 'A ReplicaSet is a set of Pods that should be running at the same time for an
    application’s components (or for the application itself if it just has one component).
    To define a ReplicaSet resource, we need to write the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 是一组应该同时运行的 Pods，用于应用的组件（或者如果应用只有一个组件，则用于应用本身）。为了定义一个 ReplicaSet 资源，我们需要编写以下内容：
- en: A `selector` section in which we define which Pods are part of the resource
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `selector` 部分，用于定义哪些 Pods 是资源的一部分
- en: The number of replicas required to keep the resource healthy
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维持资源健康所需的副本数量
- en: A Pod template in which we define how new Pods should be created when one of
    the Pods in the set dies
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Pod 模板，用于定义当集合中的某个 Pod 死亡时，如何创建新的 Pods
- en: 'Let’s review the syntax of these resources:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这些资源的语法：
- en: '![Figure 9.2 – ReplicaSet manifest](img/B19845_09_2.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – ReplicaSet 清单](img/B19845_09_2.jpg)'
- en: Figure 9.2 – ReplicaSet manifest
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – ReplicaSet 清单
- en: As you can see, the `template` section describes a Pod definition inside the
    ReplicaSet’s `spec` section. This `spec` section also includes the `selector`
    section, which defines what Pods will be included. We can use `matchLabels` to
    include exact label-key pairs from Pods, and `matchExpressions` to include advanced
    rules such as the existence of a defined label or its value included in a list
    of strings.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`template` 部分描述了 ReplicaSet 中 `spec` 部分的 Pod 定义。这个 `spec` 部分还包括 `selector`
    部分，用于定义哪些 Pods 会被包含。我们可以使用 `matchLabels` 来包含来自 Pods 的精确标签键值对，使用 `matchExpressions`
    来包括一些高级规则，如定义的标签是否存在，或者其值是否包含在字符串列表中。
- en: Important note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Selectors from ReplicaSet resources apply to running Pods too. This means that
    you have to take care of the labels that uniquely identify your application’s
    components. ReplicaSet resources are namespaced, so we can use `kubectl get pods
    -–show-labels` before actually creating a ReplicaSet to ensure that the right
    Pods will be included in the set.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 资源的选择器也适用于正在运行的 Pods。这意味着你需要注意那些唯一标识你应用组件的标签。ReplicaSet 资源是有命名空间的，因此我们可以在实际创建
    ReplicaSet 之前使用 `kubectl get pods --show-labels` 来确保正确的 Pods 会被包含在集合中。
- en: In the Pod template, we will define the volumes to be attached to the different
    containers created by the ReplicaSet, but it is important to understand that these
    volumes are common to all the replicas. Therefore, all container replicas will
    attach the same volumes (in fact, the hosts where they run mount the volumes and
    the kubelet makes them available to the Pods’ containers), which may generate
    issues if your application does not allow such a situation. For example, if you
    are deploying a database, running more than one replica that’s attaching the same
    volume will probably break your data files. We should ensure that our application
    can run more than one replicated process at a time, and if not, ensure we apply
    the appropriate `ReadWriteOnce` mode flag in the `accessMode` key. We will deep
    dive into this key, its importance, and its meaning for our workloads in *Chapter
    10*, *Leveraging Application Data Management* *in Kubernetes*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pod 模板中，我们将定义要附加到 ReplicaSet 创建的不同容器上的卷，但需要理解的是，这些卷对于所有副本都是共享的。因此，所有容器副本都会附加相同的卷（实际上，运行它们的主机挂载这些卷，kubelet
    会将它们提供给 Pods 的容器），如果你的应用不允许这种情况，可能会产生问题。例如，如果你正在部署数据库，运行多个副本并附加相同的卷可能会破坏你的数据文件。我们应该确保我们的应用可以同时运行多个复制进程，如果不能，请确保在
    `accessMode` 键中应用适当的 `ReadWriteOnce` 模式标志。我们将在 *第 10 章*，*在 Kubernetes 中利用应用数据管理*
    中深入探讨这个键、它的重要性以及它对我们工作负载的意义。
- en: The most important key in ReplicaSets is the `replicas` key, which defines the
    number of active healthy Pods that should be running. This allows us to scale
    up or down the number of instances for our application’s processes. The names
    of the Pods associated with a ReplicaSet will follow `<REPLICASET_NAME>-<POD_RANDOM_UNIQUE_GENERATED_ID>`.
    This helps us understand which ReplicaSet generated them. We also can review the
    ReplicaSet creator by using `kubectl get pod –o yaml`. The `metadata.OwnerReferences`
    key shows the ReplicaSet that finally created each Pod resource.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSets 中最重要的键是 `replicas` 键，它定义了应该运行的活动健康 Pods 数量。这允许我们对应用的进程数量进行扩展或缩减。与
    ReplicaSet 关联的 Pods 的名称将遵循 `<REPLICASET_NAME>-<POD_RANDOM_UNIQUE_GENERATED_ID>`
    格式。这有助于我们理解哪些 ReplicaSet 创建了它们。我们还可以通过使用 `kubectl get pod –o yaml` 来查看 ReplicaSet
    的创建者。`metadata.OwnerReferences` 键显示了最终创建每个 Pod 资源的 ReplicaSet。
- en: 'We can modify the number of replicas of a running ReplicaSet resource using
    any of the following methods:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下任一方法修改正在运行的ReplicaSet资源的副本数：
- en: Editing the running ReplicaSet resource directly in Kubernetes using `kubectl`
    `edit <REPLICASET_NAME>`
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl`直接编辑Kubernetes中正在运行的ReplicaSet资源：`edit <REPLICASET_NAME>`
- en: Patching the current ReplicaSet resource using `kubectl patch`
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl patch`修补当前的ReplicaSet资源
- en: 'Using the `scale` action with `kubectl`, setting the number of replicas: `kubectl
    scale rs --replicas <``NUMBER_OF_REPLICAS> <REPLICASET_NAME>`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl`的`scale`命令，设置副本数：`kubectl scale rs --replicas <``NUMBER_OF_REPLICAS>
    <REPLICASET_NAME>`
- en: Although changing the number of replicas works automatically, other changes
    don’t work so well. In the Pod template, if we change the image to use for creating
    containers, the resource will show this change, but the current associated Pods
    will not change. This is because ReplicaSets do not manage their changes; we need
    to work with Deployment resources, which are more advanced. To make any change
    available in a ReplicaSet, we need to recreate the Pods manually by just removing
    the current Pods (using `kubectl delete pod <REPLICASET_POD_NAMES>`) or scaling
    the replicas down to zero and scaling up after all are deleted. Any of these methods
    will create fresh new replicas, using the new ReplicaSet definition.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然更改副本数会自动生效，但其他更改则效果不佳。在Pod模板中，如果我们更改用于创建容器的镜像，资源会显示此更改，但当前关联的Pods不会改变。这是因为ReplicaSets并不管理这些更改；我们需要使用更高级的Deployment资源来进行操作。要在ReplicaSet中使任何更改生效，我们需要手动重新创建Pods，可以通过删除当前Pods（使用`kubectl
    delete pod <REPLICASET_POD_NAMES>`）或将副本数缩减为零，待所有Pods删除后再扩容。任何一种方法都会创建新的副本，使用新的ReplicaSet定义。
- en: Important note
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can use `kubectl delete pod --selector <LABEL_SELECTOR>`, with the current
    ReplicaSet selectors that were used to create them, to delete all associated Pod
    resources.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kubectl delete pod --selector <LABEL_SELECTOR>`，结合创建当前ReplicaSet时使用的标签选择器，删除所有关联的Pod资源。
- en: ReplicaSets, by default, don’t publish any Service; we need to create a Service
    resource to consume the deployed containers. When we create a Service associated
    with a ReplicaSet (using the Service’s label selector with the appropriate ReplicaSet’s
    labels), all the ReplicaSet instances will be accessible by using the Service’s
    `ClusterIP` address (default Service mode). All replicas get the same number of
    requests because the internal load balancing provides round-robin access.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，ReplicaSets不会发布任何Service；我们需要创建一个Service资源来访问部署的容器。当我们创建与ReplicaSet关联的Service（使用Service的标签选择器与相应ReplicaSet的标签），所有ReplicaSet实例都可以通过Service的`ClusterIP`地址访问（默认的Service模式）。所有副本会接收相同数量的请求，因为内部负载均衡提供了轮询访问。
- en: We will probably not use ReplicaSets as standalone resources in production as
    we have seen that any change in their definition requires additional interaction
    from our side, and that’s not ideal in dynamic environments such as Kubernetes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在生产环境中可能不会单独使用ReplicaSets，因为我们已经看到，任何对其定义的更改都需要我们进行额外的操作，而在像Kubernetes这样的动态环境中，这并不理想。
- en: Before we look at Deployments, which are advanced resources for deploying ReplicaSets,
    we will quickly review ReplicationControllers, which are quite similar to ReplicaSets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论Deployments（用于部署ReplicaSets的高级资源）之前，我们将快速回顾一下ReplicationControllers，它们与ReplicaSets非常相似。
- en: ReplicationControllers
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReplicationControllers
- en: The ReplicationController was the original method for Pod replication in Kubernetes,
    but now, it has been almost completely replaced by ReplicaSet resources. We can
    consider a ReplicationController as a less configurable ReplicaSet. Nowadays,
    we don’t directly create ReplicationControllers as we usually create Deployments
    for deploying application’s components running on Kubernetes. We learned that
    ReplicaSets have two options for selecting associated labels. The `labelSelector`
    key can be either a simple label search (`matchLabels`) or a more advanced rule
    that uses `matchExpressions`. ReplicationController manifests can only look for
    specific labels in Pods, which makes them simpler to use. The Pod template section
    looks similar in both ReplicaSets and ReplicaControllers. However, there is also
    a fundamental difference between ReplicationControllers and ReplicaSets. We can
    execute application upgrades by using rolling-update actions. These are not available
    for ReplicaSets but upgrades are provided in such resources thanks to the use
    of Deployments.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicationController 是 Kubernetes 中用于 Pod 复制的原始方法，但现在它已几乎完全被 ReplicaSet 资源所取代。我们可以将
    ReplicationController 看作是一个可配置性较低的 ReplicaSet。如今，我们不再直接创建 ReplicationController，因为我们通常会创建
    Deployments 来部署运行在 Kubernetes 上的应用组件。我们了解到，ReplicaSets 有两种选择关联标签的方式。`labelSelector`
    键可以是简单的标签查询（`matchLabels`），也可以是使用 `matchExpressions` 的更高级规则。ReplicationController
    清单只能查找 Pods 中的特定标签，这使得它们更易于使用。Pod 模板部分在 ReplicaSets 和 ReplicationControllers 中看起来相似。然而，ReplicationControllers
    和 ReplicaSets 之间也有一个根本性的区别。我们可以通过使用滚动更新操作来执行应用程序的升级。这些操作对于 ReplicaSets 不可用，但通过使用
    Deployments，升级功能在这些资源中得以提供。
- en: Deployments
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Deployments
- en: We can say that a Deployment is an advanced ReplicaSet. It adds the life cycle
    management part we missed by allowing us to upgrade the specifications of the
    Pods by creating a new ReplicaSet resource. This is the most used workload management
    resource in production. A Deployment resource creates and manages different ReplicaSets
    resources. When a Deployment resource is created, an associated ReplicaSet is
    also dynamically created, following the `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>`
    nomenclature. This dynamically created ReplicaSet will create associated Pods
    that follow the described nomenclature, so we will see Pod names such as `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>-<POD_
    RANDOM_UNIQUE_GENERATED_ID>` in the defined namespace. This will help us follow
    which Deployment generates which Pod resources. Deployment resources manage the
    complete ReplicaSet life cycle. To do this, whenever we change any Deployment
    template specification key, a new ReplicaSet resource is created and this triggers
    the creation of new associated Pods. The Deployment resource keeps track of all
    associated ReplicaSets, which makes it easy to roll back to a previous release,
    without the latest resource modifications. This is very useful for releasing new
    application updates. Whenever an issue occurs with the updated resource, we can
    go back to any previous version in a few seconds thanks to Deployment resources
    – in fact, we can go back to any previous existing ReplicaSet resource. We will
    deep dive into rolling updates in [*Chapter 13*](B19845_13.xhtml#_idTextAnchor287),
    *Managing the Application* *Life Cycle*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，Deployment 是一个高级版的 ReplicaSet。它通过允许我们通过创建新的 ReplicaSet 资源来升级 Pod 的规格，从而增加了我们之前错过的生命周期管理部分。这是生产中使用最广泛的工作负载管理资源。Deployment
    资源创建并管理不同的 ReplicaSet 资源。当创建一个 Deployment 资源时，一个关联的 ReplicaSet 也会动态创建，遵循 `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>`
    命名法。这个动态创建的 ReplicaSet 将创建与之关联的 Pods，遵循描述的命名规则，因此我们会在定义的命名空间中看到类似 `<DEPLOYMENT_NAME>-<RS_RANDOM_UNIQUE_GENERATED_ID>-<POD_RANDOM_UNIQUE_GENERATED_ID>`
    的 Pod 名称。这将帮助我们跟踪哪个 Deployment 生成了哪个 Pod 资源。Deployment 资源管理着完整的 ReplicaSet 生命周期。为此，每当我们更改任何
    Deployment 模板的规格键时，会创建一个新的 ReplicaSet 资源，并触发新的关联 Pods 的创建。Deployment 资源会跟踪所有关联的
    ReplicaSets，这使得我们能够轻松地回滚到先前的版本，而不必包含最新的资源修改。这对于发布新的应用更新非常有用。每当更新的资源出现问题时，我们可以在几秒钟内回滚到任何先前的版本，这得益于
    Deployment 资源——事实上，我们可以回滚到任何先前存在的 ReplicaSet 资源。我们将在[*第 13 章*](B19845_13.xhtml#_idTextAnchor287)，*管理应用生命周期*中深入探讨滚动更新。
- en: 'The following code snippet shows the syntax for these resources:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了这些资源的语法：
- en: '![Figure 9.3 – Deployment manifest](img/B19845_09_3.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 部署清单](img/B19845_09_3.jpg)'
- en: Figure 9.3 – Deployment manifest
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 部署清单
- en: The `strategy` key allows us to decide whether our new containers try to start
    before the old ones die (the `RollingUpdate` value, which is used by default)
    or completely recreate the associated ReplicaSet (the `Recreate` value), which
    is needed when only one container can access attached volumes in write mode at
    the time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`strategy`键允许我们决定新容器是否在旧容器死亡之前尝试启动（`RollingUpdate`值，默认使用此值），或者完全重新创建相关的ReplicaSet（`Recreate`值），当只有一个容器能够在特定时刻以写模式访问附加卷时需要使用这种方法。'
- en: We will use Deployments to deploy stateless or stateful application workloads
    in which we don’t require any special storage attachment and all replicas can
    be treated in the same way (all replicas are the same). Deployments work very
    well for deploying web Services with static content and dynamic ones when session
    persistence is managed in a different application component. We can’t use Deployment
    resources to deploy our application containers when each replica has to attach
    its own specific data volume or when we need to execute processes in order.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Deployments来部署无状态或有状态的应用程序工作负载，其中不需要任何特殊的存储附加，并且所有副本可以按相同的方式处理（所有副本都是一样的）。Deployments非常适合用于部署具有静态内容的Web服务和动态Web服务，当会话持久性由不同的应用组件管理时。我们不能使用Deployment资源来部署我们的应用容器，特别是在每个副本必须附加其特定数据卷，或需要按顺序执行进程的情况下。
- en: We will now learn how StatefulSet resources help us solve these specific situations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习StatefulSet资源如何帮助我们解决这些特定情况。
- en: StatefulSets
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StatefulSets
- en: StatefulSet resources are designed to manage stateful application components
    – those where persistent data must be unique between replicas. These resources
    also allow us to provide an order to different replicas when processes are executed.
    Each replica will receive a unique ordered identifier (an ordinal number starting
    from 0) and it will be used to scale the number of replicas up or down.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet资源旨在管理有状态的应用程序组件——这些组件的持久化数据必须在副本之间保持唯一。这些资源还允许我们在执行进程时为不同的副本提供顺序。每个副本将获得一个唯一的有序标识符（从0开始的序号），并且它将用于增加或减少副本的数量。
- en: 'The following code snippet shows an example of a StatefulSet resource:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了一个StatefulSet资源的示例：
- en: '![Figure 9.4 – StatefulSet manifest](img/B19845_09_4.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – StatefulSet清单](img/B19845_09_4.jpg)'
- en: Figure 9.4 – StatefulSet manifest
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – StatefulSet清单
- en: The preceding code snippet shows `template` sections for both the Pod resources
    and the volume resources.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码片段展示了`template`部分，包括了Pod资源和卷资源。
- en: The names for each Pod will follow `<STATEFULSET_NAME>-<REPLICA_NUMBER>`. For
    example, if we create a `database` StatefulSet resource with three replicas, the
    associated Pods will be `database-0`, `database-1`, and `database-2`. This name
    structure is also applied to the volumes defined in the StatefulSet’s `volumeClaimTemplates`
    template section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Pod的名称将遵循`<STATEFULSET_NAME>-<REPLICA_NUMBER>`的格式。例如，如果我们创建一个名为`database`的StatefulSet资源，且有三个副本，那么相关的Pods将为`database-0`，`database-1`和`database-2`。这种命名结构同样适用于StatefulSet的`volumeClaimTemplates`模板部分中定义的卷。
- en: Notice that we also included the `serviceName` key in the previous code snippet.
    A headless Service (without `ClusterIP`) should be created to reference the ReplicaSet’s
    Pods in the Kubernetes internal DNS, but this key tells Kubernetes to create the
    required DNS entries. For the example presented, the first replica will be announced
    to the cluster DNS as `database-0.database.NAMESPACE.svc.<CLUSTER_NAME>`, and
    all other replicas will follow the same name schema. These names can be integrated
    into our application to create an application cluster or even configure advanced
    load-balancing mechanisms other than the default (used for ReplicaSets and Deployments).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还在之前的代码片段中包含了`serviceName`键。应创建一个无头服务（没有`ClusterIP`）以在Kubernetes内部DNS中引用ReplicaSet的Pod，但这个键告诉Kubernetes创建所需的DNS条目。在所示的示例中，第一个副本将作为`database-0.database.NAMESPACE.svc.<CLUSTER_NAME>`发布到集群DNS，所有其他副本将遵循相同的命名规则。这些名称可以集成到我们的应用程序中，以创建应用程序集群，甚至配置默认负载均衡机制以外的高级负载均衡机制（用于ReplicaSets和Deployments）。
- en: When we use StatefulSet resources, Pods will be created in order, which may
    introduce extra complexity when we need to remove some replicas. We will need
    to guarantee the correct execution of processes that may resolve dependencies
    between replicas; therefore, if we need to remove a StatefulSet replica, it will
    be safer to scale down the number of replicas instead of directly removing it.
    Remember, we have to prepare our application to manage unique replicas completely,
    and this may need some application process to remove an application’s cluster
    component, for example. This situation is typical when you run distributed databases
    with multiple instances and decommissioning one instance requires database changes,
    but this also applies to any ReplicaSet manifest updates. You have to ensure that
    the changes are applied in the right order and, usually, it is preferred to scale
    down to zero and then scale up to the required number of replicas.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用StatefulSet资源时，Pods会按顺序创建，这可能会在需要删除某些副本时引入额外的复杂性。我们需要确保正确执行可能解决副本之间依赖关系的进程；因此，如果我们需要删除一个StatefulSet副本，缩减副本数量比直接删除副本更安全。记住，我们必须准备好让应用程序完全管理唯一副本，这可能需要一些应用程序进程来删除某个应用程序的集群组件。例如，在运行多个实例的分布式数据库时，这种情况是典型的，去除一个实例需要数据库更改，但这同样适用于任何ReplicaSet清单的更新。你必须确保更改按正确的顺序应用，通常，最好是先缩减到零副本，然后再扩展到所需的副本数。
- en: In the StatefulSet example presented in the preceding code snippet, we specified
    a `volumeClaimTemplate` section, which defines the properties that are required
    for a dynamically provisioned volume. We will learn how dynamic storage provisioning
    works in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231), *Leveraging Application
    Data Management in Kubernetes*, but it is important to understand that this `template`
    section will inform the Kubernetes API that every replica requires its own ordered
    volume. This requirement for dynamic provisioning will usually be associated with
    the use of `StorageClass` resources.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码片段中呈现的StatefulSet示例中，我们指定了`volumeClaimTemplate`部分，它定义了动态配置卷所需的属性。我们将在[*第10章*](B19845_10.xhtml#_idTextAnchor231)，*在Kubernetes中利用应用程序数据管理*中学习动态存储配置的工作原理，但理解这一点很重要：此`template`部分会通知Kubernetes
    API，每个副本都需要其自己的有序卷。动态配置的这一要求通常会与`StorageClass`资源的使用相关联。
- en: Once these volumes (associated with each replica) are provisioned and used,
    deleting a replica (either directly by using `kubectl delete pod` or by scaling
    down the number of replicas) will never remove the associated volume. You can
    be sure that a database deployed via a ReplicaSet will never lose its data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些卷（与每个副本相关联）被配置并使用，删除一个副本（无论是通过使用`kubectl delete pod`命令直接删除，还是通过缩减副本数量）都不会删除相关联的卷。你可以确信，通过ReplicaSet部署的数据库永远不会丢失数据。
- en: Important note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The ReplicaSet’s associated volumes will not be automatically removed, which
    makes these resources interesting for any workload if you need to ensure that
    data will not be deleted if you remove the resource.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet相关的卷不会被自动删除，这使得这些资源对于任何工作负载都很有吸引力，特别是在你需要确保删除资源时不会删除数据的情况下。
- en: We can use StatefulSet to ensure that a replicated Service is managed uniquely.
    Software such as Hashicorp’s Consul runs clusterized on several predefined Nodes;
    we can deploy it on top of Kubernetes using containers, but Pods will need to
    be deployed in order and with their specific storage as if they were completely
    different hosts. A similar approach has to be applied in database Services because
    the replication of their processes may lead to data corruption. In these cases,
    we can use StatefulSet replicated resources, but the application should manage
    the integration between the different deployed replicas and the scaling up and
    down procedure. Kubernetes just provides the underlying architecture that guarantees
    the data’s uniqueness and the replica execution order.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用StatefulSet来确保一个复制的服务被唯一管理。像Hashicorp的Consul这样的软件在多个预定义节点上以集群方式运行；我们可以通过容器将其部署在Kubernetes之上，但Pod需要按顺序部署，并且每个Pod都需要特定的存储，就好像它们是完全不同的主机一样。在数据库服务中也必须采用类似的方法，因为它们进程的复制可能会导致数据损坏。在这些情况下，我们可以使用StatefulSet复制的资源，但应用程序应该管理不同部署副本之间的集成，以及扩缩容过程。Kubernetes仅提供了底层架构，确保数据的唯一性和副本的执行顺序。
- en: DaemonSets
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DaemonSets
- en: A DaemonSet resource will execute exactly one associated Pod in each Kubernetes
    cluster Node. This ensures that any newly joined Node will get its own replica
    automatically.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows a DaemonSet manifest example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – DaemonSet manifest](img/B19845_09_5.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – DaemonSet manifest
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed, we use label selectors to match and associate Pods.
    In the preceding example, we also introduced the `tolerations` key. Let’s quickly
    introduce how `NoSchedule` (only Pods with appropriate tolerations for the Node
    are allowed), `PreferNoSchedule` (Pods will not run on the Node unless no other
    one is available), or `NoExecute` (Pods will be evicted from the Node if they
    don’t have the appropriate tolerations). Taints and tolerations must match, and
    this allows us to dedicate Nodes for certain tasks and avoid the execution of
    any other workloads on them. The kubelet will use dynamic taints to evict Pods
    when issues are found on a cluster Node – for example, when too much memory is
    in use or the disk is getting full. In our example, we add a toleration to execute
    the DaemonSet Pods on Nodes with the `node-role.kubernetes.io/control-plane=NoSchedule`
    taint.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSets are often used to deploy applications that should run on all Nodes,
    such as those running as software agents for monitoring or logging purposes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Although it isn’t too common, it is possible to use static Pods to run Node-specific
    processes. This is the mechanism that’s used by Kubernetes kubeadm-based Deployments.
    Static Pods are Pods associated with a Node, executed directly by the kubelet,
    and thus, they are not managed by Kubernetes. You can identify these Pods by their
    name because they include the host’s name. Manifests for executing static Pods
    are located in the `/etc/kubernetes/manifests` directory in kubeadm clusters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have to mention that none of the workload management resources
    presented so far provide a mechanism to run a task that shouldn’t be maintained
    during its execution time. We will now review Job resources, which are specifically
    created for this purpose.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Job resource is in charge of executing a Pod until we get a successful termination.
    The Job resource also tracks the execution of a set of Pods using template selectors.
    We configure a required number of successful executions and the Job resource is
    considered *Completed* when all the required Pod executions are successfully finished.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: In a Job resource, we can configure parallelism for executing more than one
    Pod at a time and being able to reach the required number of successful executions
    faster. Pods related to a Job will remain in our Kubernetes cluster until we delete
    the associated Job or remove them manually.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: A Job can be suspended, which will delete currently active Pods (in execution)
    until we resume it again.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: We can use Jobs to execute one-time tasks, but they are usually associated with
    periodic executions thanks to `CronJob` resources. Another common use case is
    the execution of certain one-time tasks from applications directly in the Kubernetes
    cluster. In these cases, your application needs to be able to reach the Kubernetes
    API internally (the `kubernetes` Service in the `default` namespace) and the appropriate
    permissions for creating Jobs. This is usually achieved by associating a namespaced
    `Role`, which allows such actions, with the `ServiceAccount` resource that executes
    your application’s Pod. This association is established using a namespaced `RoleBinding`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows a `Job` manifest example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Job manifest](img/B19845_09_6.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Job manifest
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Here, we defined the number of successful completitions and the number of failures
    that will set the Job as failed by setting the `completions` and `backoffLimit`
    keys. At least three Pods must exit successfully before the limit of four failures
    is reached. Multiple Pods can be executed in parallel to speed up the completion
    by setting the `parallelism` key, which defaults to `1`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The *TTL-after-finished* controller provides a `ttlSecondsAfterFinished` key.
    Since this key is based on a date-time reference, it is key to maintain our clusters’
    time according to our time zone.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Jobs are commonly used within CronJobs to define tasks that should be executed
    at certain periods – for example, for executing backups. Let’s learn how to implement
    CronJobs so that we can schedule Jobs periodically.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: CronJobs
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CronJob resources are used to schedule Jobs at specific times. The following
    code snippet shows a `CronJob` manifest example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – CronJob manifest](img/B19845_09_7.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – CronJob manifest
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to review logs from executed Pods (associated with the Jobs created),
    we can set `failedJobsHistoryLimit` and `successfulJobsHistoryLimit` to the desired
    number of Jobs to keep to be able to review the Pods’ logs. Notice that we planned
    the example Job daily, at 00:00, using the common *Unix Crontab* format, as shown
    in the following schema:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Unix Crontab format](img/B19845_09_8.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Unix Crontab format
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The `schedule` key defines when the Job will be created and associated Pods
    will run. Remember to always quote your value to avoid problems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: CronJob resources use the Unix Crontab format, hence values such as `@hourly`,
    `@daily`, `@monthly`, or `@yearly` can be used.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: CronJobs can be suspended, which will affect any new Job creation if we change
    the value of the `suspend` key to `true`. To enable the CronJob again, we need
    to change this key to `false`, which will continue with the normal scheduling
    for creating new Jobs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: A common use case for CronJobs is the execution of backup tasks for applications
    deployed on Kubernetes. With this solution, we avoid opening internal applications
    externally if user access isn’t required.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the different resources we can use to deploy our workloads,
    let’s quickly review how they will help us provide resilience and high availability
    to our applications.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring resilience and high availability with Kubernetes resources
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pod resources provide resilience out of the box as we can configure them to
    always restart if their processes fail. We can use the `spec.restartPolicy` key
    to define when they should restart. It is important to understand that this option
    is limited to the host’s scope, so a Pod will just try to restart on the host
    on which it was previously running. Pod resources do not provide high availability
    or resilience cluster-wide.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployments, and therefore ReplicaSets, and StatefulSets are prepared for applying
    resilience cluster-wide because resilience doesn’t depend on hosts. A Pod will
    still try to restart on the Node where it was previously running, but if it is
    not possible to run it, it will be scheduled to a new available one. This will
    allow Kubernetes administrators to perform maintenance tasks on Nodes moving workloads
    from one host to another, but this may impact your applications if they are not
    ready for such movements. In other words, if you only have one replica of your
    processes, they will go down for seconds (or minutes, depending on the size of
    your image and the time required by your processes to start), and this will impact
    your application. The solution is simple: deploy more than one replica of your
    application’s Pods. However, it is important to understand that your application
    needs to be prepared for multiple replicated processes working in parallel.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets’ replicas will never use the same volume, but this isn’t true for
    Deployments. All the replicas will share the volumes, and you must be aware of
    that. Sharing static content will work like a charm, but if multiple processes
    are trying to write the same file at the same time, you may encounter problems
    if your code doesn’t manage concurrency.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSets work differently and we don’t have to manage any replication; just
    one Pod will run on each Node, but they will share volumes too. Because of the
    nature of such resources, it is not common to include shared volumes in these
    cases.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: But even if our application runs in a replicated manner, we can’t ensure that
    all the replicas die at the same time without configuring a **Pod disruption policy**.
    We can configure a minimum number of Pods to be available at the same time, ensuring
    not only resilience but also high availability. Our application will have some
    impact, but it will continue serving requests (high availability).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure a disruption policy, we must use `PodDisruptionBudget` resources
    to provide the logic we need for our application. We will be able to set up the
    number of Pods that are required for our application workload under all circumstances
    by configuring the `minAvailable` or `maxUnavailable` keys. We can use integers
    (the number of Pods) or a percentage of the configured replicas. `PodDisruptionBudget`
    resources use selectors to choose between the Pods in the namespace (which we
    already use to create Deployments, ReplicaSets, and more). The following code
    snippet shows an example:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, a minimum of two Pods with the `app=webserver` label are being
    monitored. We will define the number of replicas in our Deployment, but the `PodDisruptionBudget`
    resource will not allow us to scale down below two replicas. Therefore, two replicas
    will be running even if we decide to execute `kubectl drain node1` (assuming,
    in this example, that the `webserver` Deployment matches the `app=webserver` Pod’s
    labels and `node1` and `node2` have one replica each). `PodDisruptionBudget` resources
    are namespaced, so we can show all these resources in the namespace by executing
    `kubectl` `get poddisruptionbudgets`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will review some interesting ideas for solving
    common application architecture patterns using Pod features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Understanding advanced Pod application patterns
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to discuss some interesting patterns using simple
    Pods. All the patterns we are going to review are based on the special mechanisms
    offered by Kubernetes for sharing kernel namespaces in a Pod, which allow containers
    running inside to mount the same volumes and interconnect via localhost.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Init containers
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More than one Pod can run inside a container. Pods allow us to isolate different
    application processes that we want to maintain separately in different containers.
    This helps us, for example, to maintain different images that can be represented
    by separated code repositories and build workflows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Init containers run before the main application container (or containers, if
    we run more in parallel). These init containers can be used to set permissions
    on shared filesystems presented as volumes, create database schemas, or any other
    procedure that helps initialize our application. We can even use them to check
    dependencies before a process starts or even provision required files by retrieving
    them from an external source.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We can define many init containers, and they will be executed in order, one
    by one, and all of them must end successfully before the actual application containers
    start. If any of the init containers fails, the Pod fails, although these containers
    don’t have associated probes for verifying their state. The processes executed
    by them must include verification if something goes wrong.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that the total CPU and memory resources consumed
    by a Pod are calculated from the initialization of the Pod, hence init containers
    are checked. You keep the resource usage between the defined limits for your Pod
    (which includes the usage of all the containers running in parallel).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Sidecar containers
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`kubectl patch`) to modify the running Deployment resource manifest.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Some modern monitoring applications, designed to integrate into Kubernetes,
    also use sidecar containers to deploy an application-specific monitoring component,
    which retrieves application metrics and exposes them as a new Service.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The next few patterns we are going to review are based on this sidecar container
    concept.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Ambassador containers
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Ambassador** applications pattern is designed to offload common client
    connectivity tasks, helping legacy applications implement more advanced features
    without changing any of their old code. With this design, we can improve the application’s
    routing, communications security, and resilience by adding additional load balancing,
    API gateways, and SSL encryption.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: We can deploy this pattern within Pods by adding special containers, designed
    for delivering light reverse-proxy features. In this way, Ambassador containers
    are used for deploying service mesh solutions, intercepting application process
    communications, and securing the interconnection with other application components
    by enforcing encrypted communications and managing application routes, among other
    features.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Adaptor containers
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Adaptor** container pattern is used, for example, when we want to include
    monitoring or retrieve logs from legacy applications without changing their code.
    To avoid this circumstance, we can include a second container in our application’s
    Pod to get the metrics or the logs from our application without modifying any
    of its original code. This also allows us to homogenize the content of a log or
    send it to a remote server. Well-prepared containers will redirect processes’
    standard and error output to the foreground, and this allows us to review their
    log, but sometimes, the application can’t redirect the log or more than one log
    is created. We can unify them in one log or redirect their content by adding a
    second process (the Adaptor container), which formats (adding some custom columns,
    date format, and so on) and redirects the result to the standard output or a remote
    logging component. This method does not require special access to the host’s resources
    and it may be transparent for the application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus** is a very popular open source monitoring solution and is extended
    in Kubernetes environments. Its main component will poll agent-like components
    and retrieve metrics from them, and it’s very common to use this Adaptor container
    pattern to present the application’s metrics without modifying its standard behavior.
    These metrics will be exposed in the application Pod in a different port, and
    the Prometheus server will connect to it to obtain its metrics.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn how containers’ health is verified by Kubernetes to decide the Pod’s
    status.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Verifying application health
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to review how application Pods are considered
    healthy. Pods always start in the `Pending` state and continue to the `Running`
    state once the main container is considered healthy. If the Pod executes a Service
    process, it will stay in this `Running` state. If the Pod is associated with a
    Job resource, it may end successfully (the `Succeeded` state) or fail (the `Failed`
    state).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: If we remove a Pod resource, it will go to `Terminating` until it is completely
    removed from Kubernetes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: If Kubernetes cannot retrieve the Pod’s status, its state will be `Unknown`.
    This is usually due to communication issues between the hosts’ kubelet and the
    API server.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes reviews the state of the containers to set the Pod’s state, and
    containers can be either `Waiting`, `Running`, or `Terminated`. We can use `kubectl
    describe pod <POD_NAME>` to review the details of these phases. Let’s quickly
    review these states:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '`Waiting` represents the state before `Running`, where all the pre-container-execution
    processes appear. In this phase, the container image is pulled from the registry
    and different volume mounts are prepared. If the Pod can’t run, we can have a
    `Pending` state, which will indicate a problem with deploying the workload.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Running` indicates that the containers are running correctly, without any
    issues.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Terminated` state is considered when the containers are stopped.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the Pod was configured with a `restartPolicy` property of the `Always` or
    `OnFailure` type, all the containers will be restarted on the node where they
    stopped. That’s why a Pod resource does not provide either high availability or
    resilience if the node goes down.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review how the Pod’s status is evaluated in these phases thanks to the
    execution of **probes**.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the execution of probes
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The kubelet will execute probes periodically by executing some code inside
    the containers or by directly executing network requests. Different probe types
    are available depending on the type of check we need for our application’s components:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '`exec`: This executes a command inside the container and the kubelet verifies
    whether this command exits correctly.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`httpGet`: This method is probably the most common as modern applications expose
    Services via the REST API. This check’s response must return 2XX or 3XX (redirects)
    codes.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcpSocket`: This probe is used to check whether the application’s port is
    available.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grpc`: If our application is consumed via modern **Google Remote Procedure
    Calls** (**gRPCs**), we can use this method to verify the container’s state.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probes must return a valid value to consider the container healthy. Different
    probes can be executed one after another through the different phases of their
    lives. Let’s consider the different options available to verify whether the container’s
    processes are starting or serving the application itself.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Startup probes
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Always` or `OnFailure` in its `restartPolicy` key.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: We will set up these probes if our processes take a lot of time before they
    are ready – for example, when we start a database server and it must manage previous
    transactions in its data before it is ready, or when our processes already integrate
    some sequenced checks before the final execution of the main process.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Liveness probes
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`restartPolicy` value. They are used when it’s hard to manage the failure of
    your main process within the process itself. It may be easier to integrate an
    external check via the `livenessProbe` key, which verifies whether or not the
    main process is healthy.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Readiness probes
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`selector` section will not mark this Pod as ready for requests until this
    probe ends successfully. The same happens when the probe fails; it will be removed
    from the list of available endpoints for the Service resource.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Readiness probes are key to managing traffic to the Pods because we can ensure
    that the application component will correctly manage requests. This probe should
    always be set up to improve our microservices’ interactions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: There are common keys that can be used at the `spec.containers` level that will
    help us customize the behavior of the different probe types presented. For example,
    we can configure the number of failed checks required to consider the probe as
    failed (`failureThreshold`) or the period between the execution of a probe type
    (`periodSeconds`). We can also configure some delay before any of these probes
    start by setting the `initialDelaySeconds` key, although it is recommended to
    understand how the application works and adjust the probes to fit our initial
    sequence. In the *Labs* section of this chapter, we will review some of the probes
    we’ve just discussed.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how Kubernetes (the kubelet component) verifies the health
    of the Pods starting or running in the cluster, we must understand the *stop*
    sequence when they are considered `Completed` or `Failed`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Termination of Pods
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use the `terminationGracePeriodSeconds` key to set how much time the
    kubelet will wait if the Pod’s processes take a long time to end. When a Pod is
    deleted, the kubelet sends it a `SIGTERM` signal, but if it takes too long, the
    kubelet will send a `SIGKILL` signal to all container processes that are still
    alive when the `terminationGracePeriodSeconds` configured time is reached. This
    time threshold can also be configured at the probe level.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: To remove a Pod immediately, we can force and change this Pod-level defined
    grace period by using `kubelet delete pod <POD_NAME> --force` along with `--grace-period=0`.
    Forcing the deletion of a Pod may result in unexpected consequences for your applications
    if you don’t understand how it works. The kubectl client sends the `SIGKILL` signal
    and doesn’t wait for confirmation, informing the API server that the Pod is already
    terminated. When the Pods are part of a StatefulSet, this may be dangerous as
    the Kubernetes cluster will try to execute a new Pod without confirming whether
    it has already been terminated. To avoid these situations, it is better to scale
    down to the replicas and scale up to do a full restart.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Our applications may need to execute some specific processes to manage the interactions
    between different components when we update some of them, or even if they fail
    with an error. We can include some triggers when our containers start or stop
    – for example, to reconfigure a new master process in a clusterized application.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Container life cycle hooks
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Containers within a Pod can include a **life cycle hook** in their specifications.
    Two types are available:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '**PostStart** hooks can be used to execute a process *after* a container is
    created.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PreStop** hooks are executed *before* the container is terminated. The grace
    period starts when the kubelet receives a stop action, so this hook may be affected
    if the defined process takes too long.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods can be scaled up or down manually whenever our application needs it and
    it’s supported, but we can go further and manage replicas automatically. The following
    section will show us how to make it possible.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Resource management and scalability
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Pods run without compute resource limits. This is fine for learning
    how your application behaves, and it can help you define its requirements and
    limits.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes cluster administrators can also define quotas that can be configured
    at different levels. It is usual to define them at the namespace level and your
    applications will be confined with limits for CPU and memory. But these quotas
    can also identify some special resources, such as GPUs, storage, or even the number
    of resources that can be deployed in a namespace. In this section, we will learn
    how to limit resources in our Pods and containers, but you should always ask your
    Kubernetes administrators if any quota is applied at the namespace level to prepare
    your deployments for such compliance. More information about resource quota configurations
    can be found in the Kubernetes official documentation: [https://kubernetes.io/docs/concepts/policy/resource-quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `spec.resources` section to define the limits and requests
    associated with a Pod. Let’s look at how they work:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '`spec.resources.requests.memory` and `spec.resources.requests.cpu`, respectively),
    we can define the minimum resources required in any cluster host to run our Pod.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec.resources.limits.memory` and `spec.resources.limits.cpu` to configure
    the maximum memory and number of CPUs allocable, respectively.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resources can be defined either at the Pod or container level and they must
    be compliant with each other. The sum of all the container resource limits must
    not exceed the Pod values. If we omit the Pod resources, the sum of the defined
    container resources will be used. If any of the containers do not contain a resource
    definition, the Pod limits and requests will be used. The container’s equivalent
    key is `spec.containers[].resources`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Memory limits and requests will be configured in bytes and we can use suffixes
    such as `ki`, `Mi`, `Gi`, and `Ti` for multiples of 1,000, or `k`, `M`, and `T`
    for multiples of 1,024\. For example, to specify a limit of 100 MB of memory,
    we will use `100M`. When the limited memory allowed is reached, `OOMKiller` will
    be triggered in the execution host and the Pod or container will be terminated.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: For the CPU, we will define the number of CPUs (it doesn’t matter whether they
    are physical or virtual) to be allowed or requested, if we are defining a request
    limit. When the CPU limit is reached, the container or Pod will not get more CPU
    resources, which will probably make your Pod to be considered unhealthy because
    checks will fail. CPU resources must be configured in either integers or fractionals,
    and we can add `m` as a suffix to represent millicores; hence, 0.5 CPUs can also
    be written as `500m`, and 0.001 CPUs will be represented as `1m`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: When we are using Linux nodes, we can request and limit huge page resources,
    which allows us to define the page size for memory blocks allocated by the kernel.
    Specific key names must be used; for example, `spec.resources.limits.hugepages-2Mi`
    allows us to define the limit of memory blocks allocated for 2 MiB huge pages.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Your administrators can prepare for some `LimitRange` resources, which will
    define constraints for the limits and requests associated with your Pod resources.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how we can limit and ask for resources, we can vertically scale
    a workload by increasing its limits. Horizontal scaling, on the other hand, will
    require the replication of Pods. We can now continue and learn how to dynamically
    and horizontally scale Pods related to a running workload.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical Pod autoscaling** is also available as a project inside Kubernetes.
    It is less popular because vertical scaling impacts your current Deployments or
    StatefulSets as it requires scaling the number of resources on your running replicas
    up or down. This makes them hard to apply and it is better to fine-grain resources
    in your applications and use horizontal Pod autoscaling, which does not modify
    current replica specifications.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal Pod autoscaling
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**HorizontalPodAutoscaler** works as a controller. It scales Pods up or down
    when the load associated with the workload is increased or decreased. Autoscaling
    is only available for Deployments (by scaling and modifying their ReplicaSets)
    and StatefulSets. To measure the consumption of resources associated with a specific
    workload, we have to include a tool such as **Kubernetes Metrics Server** in our
    cluster. This server will be used to manage the standard metrics. This can be
    easily deployed using its manifests at [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server).
    It can also be executed as a pluggable add-on if you are using Minikube on your
    laptop or desktop computer.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: We will define a `HorizontalPodAutoscaler` (`hpa`) resource; the controller
    will retrieve and analyze the metrics for a workload resource specified in the
    `hpa` definition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Different types of metrics can be used for the `hpa` resource, although the
    most common is the Pod’s CPU consumption.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Metrics related to Pods can be defined and thus the controller checks their
    metrics and analyzes them using an algorithm that combines these metrics with
    cluster available resources and Pod states ([https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details))
    and then decides whether or not the associated resource (Deployment or StatefulSet)
    should be scaled.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'To define an `hpa` resource, we will set up a metric to analyze and a range
    of replicas to use (max and min replicas). When this value is reached, the controller
    reviews the current replicas, and if there’s still room for a new one, it will
    be created. `hpa` resources can be defined in either imperative or declarative
    format. For example, to manage a minimum of two Pods and a maximum of 10 when
    more than 50% of the CPU consumption is reached for the current Pods, we can use
    the following syntax:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When a resource’s CPU consumption is more than 50%, then a replica is created,
    while one replica is decreased when this metric is below that value; however,
    we will never execute more than 10 replicas or less than two.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: We can review the manifest that’s used for creating any resource by adding `–o
    yaml`. The manifest will be presented and we will be able to verify its values.
    As an example, we can use `kubectl autoscale deploy webserver --cpu-percent=50
    --min=2 --max=10 -``o yaml`.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: If we want to review values before creating the resource, we can add the `--dry-run=client`
    argument to only show the manifest, without actually creating the resource.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: As `hpa` resources are namespaced, we can get all the already deployed `hpa`
    resources by executing `kubectl get` `hpa -A`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have seen how Kubernetes provides resilience, high availability,
    and autoscaling facilities out of the box by using specific resources. In the
    next section, we will learn how it also provides some interesting security features
    that will help us improve our application security.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Improving application security with Pods
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a Kubernetes cluster, we can categorize the applications’ workloads distributed
    cluster-wide as either privileged or unprivileged. Privileged workloads should
    always be avoided for normal applications unless they are strictly necessary.
    In this section, we will help you define the security of your applications by
    declaring your requirements in your workload manifests.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Security contexts
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a security context, we define the privileges and security configuration
    required for a Pod or the containers included in it. Security contexts allow us
    to configure the following security features:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '`runAsUser`/`runAsGroup`: These options manage the `userID` and `groupID` properties
    that run the main process with containers. We can add more groups by using the
    `supplementalGroups` key.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runAsNonRoot`: This key can control whether we allow the process to run as
    `root`.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fsGroup`/`fsGroupChangePolicy`: These options manage the permissions of the
    volumes included within a Pod. The `fsGroup` key will set the owner of the filesystems
    mounted as volumes and the owner of any new file. We can use `fsGroupChangePolicy`
    to only apply the ownership change if the permissions don’t match the configured
    `fsGroup`.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seLinuxOptions`/`seccompProfile`: These options allow us to overwrite default
    SELinux and `seccomp` settings by configuring special SELinux labels and a special
    `seccomp` profile.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`capabilities`: Kernel capabilities can be added or removed (`drop`) to only
    allow specific kernel interactions (containers share the host’s kernel). You should
    avoid unnecessary capabilities in your applications.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`privileged`/`AllowPrivilegeEscalation`: We can allow processes inside a container
    to be executed as `privileged` (with all the capabilities) by setting the `privileged`
    key to `true` or to be able to gain privileges, even if this key was set to `false`,
    by setting `AllowPrivilegeEscalation` to `true`. In this case, container processes
    do not have all capabilities but they will allow internal processes to run as
    if they had the `CAP_SYS_ADMIN` capability.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readOnlyRootFilesystem`: It is always a very good idea to run your containers
    with their root filesystem in read-only mode. This won’t allow processes to make
    any changes in the container. If you understand the requirements of your application,
    you will be able to identify any directory that may be changed and add an appropriate
    volume to run your processes correctly. It is quite usual, for example, to add
    `/tmp` as a separate temporal filesystem (`emptyDir`).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these keys are available at the container or Pod level or both. Use
    `kubectl explain pod.spec.securityContext` or `kubectl explain pod.spec.containers.securityContext`
    to retrieve a detailed list of the options available in each scope. You have to
    be aware of the scope that’s used because Pod specifications apply to all containers
    unless the same key exists under the container scope – in which case, its value
    will be used.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review the best settings we can prepare to improve our application security.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Security best practices
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following list shows some of the most used settings for improving security.
    You, as a developer, can improve your application security if you ensure the following
    security measures can be enabled for your Pods:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '`runAsNonRoot` must always be set to `true` to avoid the use of `root` on your
    containers. Ensure you also configure `runAsUser` and `runAsGroup` to IDs greater
    than `1000`. Your Kubernetes administrators can suggest some IDs for your application.
    This will help control application IDs cluster-wide.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always drop all capabilities and enable only those required by your application.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Never use privileged containers for your applications unless it is strictly
    necessary. Usually, only monitoring- or kernel-related applications require special
    privileges.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the filesystem’s requirement for your application and always set `readOnlyRootFilesystem`
    to `true`. This simple setting improves security, disabling any unexpected changes.
    Required filesystems can be mounted as volumes (many options are available, as
    we will learn in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231), *Leveraging
    Application Data Management* *in Kubernetes*).
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask your Kubernetes administrators whether there are some SELinux settings you
    should consider to apply them on your Pods. This also applies to `seccomp` profiles.
    Your administrators may have configured a default profile. Ask your administrators
    about this situation to avoid any system call issues.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your administrators may have been using tools such as Kyverno or OPA Gatekeeper
    to improve cluster security. In these cases, they can enforce security context
    settings by using **admission controllers** in the Kubernetes cluster. The use
    of these features is outside the scope of this book but you may ask your administrators
    about the compliance rules required to execute applications in your Kubernetes
    platform.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will review how to implement some of the Kubernetes
    features we learned about in this chapter by preparing the multi-component application
    we used in previous chapters ([*Chapter 5*](B19845_05.xhtml#_idTextAnchor118),
    *Creating Multi-Container Applications*, and [*Chapter 7*](B19845_07.xhtml#_idTextAnchor147),
    *Orchestrating with Swarm*) to run on Kubernetes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Labs
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will show you how to deploy the `simplestlab` three-tier application
    in Kubernetes. Manifests for all its components have been prepared for you while
    following the techniques and Kubernetes resources explained in this chapter. You
    will be able to verify the usage of the different options and you will able to
    play with them to review the content and best practices described in this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The code for these labs is available in this book’s GitHub repository at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git).
    Ensure you have the latest revision available by simply executing `git clone`
    [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git)
    to download all its content, or `git pull` if you’ve already downloaded the repository
    before. All the manifests and the steps required for running `simplestlab` are
    located inside the `Containers-for-Developers-Handbook/Chapter9` directory.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'In the labs in GitHub, we will deploy the `simplestlab` application, which
    is used in previous chapters, on Kubernetes by defining appropriate resource manifests:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The **database** component will be deployed using a StatefulSet resource
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **application backend** component will be deployed using a Deployment resource
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **load balancer** (or **presenter**) component will be deployed using a
    DaemonSet resource
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In their manifests, we have included some of the mechanisms we learned about
    in this chapter for checking the component’s health, replicating their processes,
    and improving their security by disallowing their execution as the root user,
    among other features. Let’s start by reviewing and deploying the database component:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a StatefulSet to ensure that replicating its processes (scaling
    up) will never represent a problem to our data. It is important to understand
    that a new replica starts empty, without data, and joins the pool of available
    endpoints for the Service, which will probably be a problem. This means that in
    these conditions, the Postgres database isn’t scalable, so this component is deployed
    as a StatefulSet to preserve its data even in the case of a manual replication.
    This example only provides resilience, so do not scale this component. If you
    need to deploy a database with high availability, you will need a distributed
    database such as MongoDB. The full manifest for the database manifest can be found
    in `Chapter9/db.satatefulset.yaml`. Here is a small extract from this file:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, we defined a template for the Pods to create and a separate template
    for the VolumeClaims (we will talk about them in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231)).
    This ensures that each Pod will get its own volume. The volume that’s created
    will be mounted in the database container as the `/data` filesystem and its size
    will be 1,000 MB (1 Gi). No other container is created. The `POSTGRES_PASSWORD`
    and `PGDATA` environment variables are set and passed to the container. They will
    be used to create the password for the Postgres user and the patch for the database
    data. The image that’s used for the container is `docker.io/frjaraur/simplestdb:1.0`
    and port `5432` will be used to expose its Service. Pods only expose their Services
    internally, in the Kubernetes network, so you will never be able to reach these
    Services from remote clients. We specified one replica and the controller will
    associate the pods with this StatefulSet by searching for Pods with `component=db`
    and `app=simplestlab` labels. We simplified the database’s probes by just checking
    a TCP connection to port `5432`. We defined a security context at the Pod’s level,
    which will apply to all the containers by default:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The database processes will run as `10000:10000` `user:group`, hence they are
    secure (no root is required). We could have gone further if we set the container
    as read-only but in this case, we didn’t as Docker’s official Postgres image;
    however, it would have been better to use a full read-only filesystem.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Pod will get an IP address, though this may change if the Pod is recreated
    for any reason, which makes Pods’ IP addresses impossible to use in such dynamic
    environments. We will use a Service to associate a *fixed* IP address with a Service
    and then with the endpoints of the Pods related to the Service.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is an extract from the Service manifest (you will find it as
    `Chapter9/db.service.yaml`):'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This Service is associated with the Pods by using a selector (the `components=db`
    and `app=simplestlab` labels) and Kubernetes will route the traffic to the appropriate
    Pods. When a TCP packet reaches the Service’s port, `5432`, it is load balanced
    to all the available Pod’s endpoints (in this case, we will just have one replica)
    in port `5432`. In both cases, we used port `5432`, but you must understand that
    `targetPort` refers to the container port, while the port key refers to the Service’s
    port, and they can be completely different. We are using a headless Service because
    it works very well with StatefulSets and their resolution in round-robin mode.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the StatefulSet definition and the Service, we can deploy the database
    component:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The application (backend component) is deployed as a `Deployment` workload.
    Let’s see an extract of its manifest:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can find the complete manifest in the `Chapter9/app.deployment.yaml` file.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For this component, we defined three replicas, so three Pods will be deployed
    cluster-wide. In this component, we are using the `docker.io/frjaraur/simplestapp:1.0`
    image. We’ve configured two security contexts, one at the Pod’s level:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The second is for enforcing the use of a read-only filesystem for the container:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we prepared `readinessProbe` using `httpGet` but we still keep `tcpSocket`
    for `livenessProbe`. We coded `/healthz` as the application’s health endpoint
    for checking its healthiness.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this component, we added a resource section for the app container:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, we asked Kubernetes for at least 10 millicores of CPU and 20M
    of memory. The `limits` section describes the maximum CPU (20 millicores) and
    memory (30Mi). If the memory limit is reached, Kubelet will trigger the OOM-Killer
    procedure and it will kill the container. When the CPU limit is reached, the kernel
    does not provide more CPU cycles to the container, which may lead the probes to
    fail and hence the container will die. This component is stateless and it is running
    completely in read-only mode.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: In the full YAML file manifest, you will see that we are using the environment
    variables for passing sensitive data. Always avoid passing sensitive data in environment
    variables as anyone with access to your manifest files will be able to read it.
    We will learn how to include sensitive data in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231),
    *Leveraging Application Data Management* *in Kubernetes*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also add a Service for accessing the `app` `Deployment` workload:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We create both Kubernetes resources:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s see an extract from the DaemonSet manifest:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that we added `/var/cache/nginx` and `/tmp` as `emptyDir` volumes, as
    mentioned previously. This component will be also stateless and run in read-only
    mode, but some temporal directories must be created as `emptyDir` volumes so that
    they can be written to without allowing the full container’s filesystem.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following security contexts are created:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the Pod level:'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At the container level:'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we have the `Service` definition, where we will use a `NodePort` type
    to quickly expose our application:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s deploy all the `lb` component (frontend) manifests:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Figure 9.9 – simplestlab application web GUI](img/B19845_09_9.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – simplestlab application web GUI
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: You can find additional steps for scaling up and down the application backend
    component in the `Chapter9` code repository. The labs included in this chapter
    will help you understand how to deploy an application using different Kubernetes
    resources.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the resources that can help us deploy application
    workloads in Kubernetes. We took a look at the different options for running replicated
    processes and verifying their health to provide resilience, high availability,
    and auto-scalability. We also learned about some of the Pod features that can
    help us implement advanced patterns and improve the overall application security.
    We are now ready to deploy our application using the best patterns and apply and
    customize the resources provided by Kubernetes, and we know how to implement appropriate
    health checks while limiting resource consumption in our platform.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will deep dive into the options we have for managing
    data within Kubernetes and presenting it to our applications.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
