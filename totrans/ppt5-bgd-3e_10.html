<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;Controlling containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Controlling containers</h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>The inside of a computer is as dumb as hell but it goes like mad!</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<span class="attribution"><span class="emphasis"><em>Richard Feynman</em></span></span></td></tr></table></div><p>In this chapter, we'll look at the emerging topic of <span class="strong"><strong>containers</strong></span> and see how it relates to configuration management. We'll see how to use Puppet to manage the Docker daemon, as well as images and containers, and explore some different strategies for managing configuration within containers.</p><div class="mediaobject"><img src="graphics/8880_10_01.jpg" alt="Controlling containers"/></div><div class="section" title="Understanding containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec52"/>Understanding containers</h1></div></div></div><p>Although the technology <a id="id440" class="indexterm"/>behind containers is at least thirty years old, it's only in the last few years that containers have really taken off (to mix a metaphor). This is largely thanks to the rise of Docker, a software platform which makes it easier to create and manage containers.</p><div class="section" title="The deployment problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec154"/>The deployment problem</h2></div></div></div><p>The problem that <a id="id441" class="indexterm"/>Docker solves is principally one of <span class="strong"><strong>software deployment</strong></span>: that is, making it possible to install and run your software in a wide variety of environments with minimal effort. Let's take a typical PHP web application, for example. To run the application you need at least the following to be present on a node:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">PHP source code</li><li class="listitem" style="list-style-type: disc">PHP interpreter</li><li class="listitem" style="list-style-type: disc">Its associated dependencies and libraries</li><li class="listitem" style="list-style-type: disc">PHP modules required by your application </li><li class="listitem" style="list-style-type: disc">Compiler and build tools for building native binaries for PHP modules</li><li class="listitem" style="list-style-type: disc">Web server (for example Apache)</li><li class="listitem" style="list-style-type: disc">Module for serving PHP apps (for example, <code class="literal">mod_php</code>)</li><li class="listitem" style="list-style-type: disc">Config files for your application </li><li class="listitem" style="list-style-type: disc">User to run the application</li><li class="listitem" style="list-style-type: disc">Directories for things such as log files, images, and uploaded data</li></ul></div><p>How do you manage all of this stuff? You can use a system package format, such as RPM or DEB, which uses metadata to describe its dependencies in terms of other packages, and scripts which can do much of the system configuration required.</p><p>However, this packaging is specific to a particular version of a particular operating system, and a package intended for Ubuntu 18.04, for example, will not be installable on Ubuntu 16.04 or on Red Hat Enterprise Linux. Maintaining multiple packages for several popular operating systems is a large workload on top of maintaining the application itself.</p></div><div class="section" title="Options for deployment"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec155"/>Options for deployment</h2></div></div></div><p>One way to address this <a id="id442" class="indexterm"/>problem is for the author to provide <span class="strong"><strong>configuration management</strong></span> <span class="strong"><strong>manifests</strong></span> for the software, such as a Puppet module or a Chef recipe to install the software. However, if the intended user of the software does not use a CM tool, or uses a different tool, then this is  no help. Even if they use exactly the same version of the same tool on the same operating system, they may have problems integrating the third-party module with it, and the module itself will depend on other modules and so on. It's certainly not a turnkey solution.</p><p>Another option is the <span class="strong"><strong>omnibus package</strong></span>, a package which contains everything the software needs to run. An omnibus package for our example PHP application might contain the PHP binaries and all dependencies, plus anything else the application needs. These are necessarily quite large packages, however, and omnibus packages are still specific to a particular operating system and version, and involve a lot of maintenance effort.</p><p>Most package managers do not provide an efficient binary update facility, so even the smallest update requires re-downloading the entire package. Some omnibus packages even include their own config management tool!</p><p>Yet another solution is to provide an entire <span class="strong"><strong>virtual machine image</strong></span>, such as a Vagrant box (the Vagrant box we've been using throughout the book is a good example). This contains not only the application plus dependencies and configuration, but the entire operating system as well. This is a fairly portable solution, since any platform which can run the virtual machine host software (for example, VirtualBox or VMware) can run the VM itself.</p><p>However, there is a performance penalty with virtual machines, and they also consume a lot of resources, such as memory and disk space, and the VM images themselves are large and unwieldy to move around a network.</p><p>While in theory you could deploy your application by building a VM image and pushing it to a production VM host, and some people do this, it's far from an efficient method of distribution.</p></div><div class="section" title="Introducing the container"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec156"/>Introducing the container</h2></div></div></div><p>In recent years many <a id="id443" class="indexterm"/>operating systems have added facilities for self-contained execution environments, more concisely called <span class="strong"><strong>containers</strong></span>, in which programs can run natively on the CPU, but with very limited access to the rest of the machine. A container is like a security sandbox, where anything running inside it can access files and programs inside the container, but nothing else.</p><p>This is similar in principle to a virtual machine, except that the underlying technology is quite different. Instead of running on a virtual processor, via a software emulation layer, programs in a container run directly on the underlying physical hardware. This makes containers a great deal more efficient than VMs. To put it another way, you need much less powerful hardware to run containers than you do for virtual machines of the same performance.</p><p>A single virtual machine consumes a <a id="id444" class="indexterm"/>large amount of its host's resources, which means that running more than one VM on the same host can be quite demanding. By contrast, running a process inside a container uses no more resources than running the same process natively.</p><p>Therefore, you can run a very large number of containers on a single host, and each is completely self-contained and has no access to either the host, or any other container (unless you specifically allow it). A container, at the kernel level, is really just a namespace. Processes running in that namespace cannot access anything outside it, and vice versa. All the containers on a machine use the host operating system's kernel, so although containers are portable across different Linux distributions, for example, a Linux container cannot run directly on a Windows host. Linux containers can, however, run on Windows using a virtualization layer provided by Docker for Windows.</p></div><div class="section" title="What Docker does for containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec157"/>What Docker does for containers</h2></div></div></div><p>So if containers themselves are <a id="id445" class="indexterm"/>provided by the kernel, what is Docker for? It turns out that having an engine is not quite the same thing as having a car. The operating system kernel may provide the basic facilities for containerization, but you also need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A specification for how to build containers</li><li class="listitem" style="list-style-type: disc">A standard file format for container images</li><li class="listitem" style="list-style-type: disc">A protocol for storing, versioning, organizing, and retrieving container images</li><li class="listitem" style="list-style-type: disc">Software to start, run, and manage containers</li><li class="listitem" style="list-style-type: disc">Drivers to allow network traffic to and from containers</li><li class="listitem" style="list-style-type: disc">Ways of communicating between containers</li><li class="listitem" style="list-style-type: disc">Facilities for getting data into containers</li></ul></div><p>These need to be provided by additional software. There are in fact many software frontends which allow you to manage containers: Docker, OCID, CoreOS/rkt, Apache Mesos, LXD, VMware Photon, Windows Server Containers, and so on. However, Docker is by far the market leader, and currently the majority of containers in production are running under <a id="id446" class="indexterm"/>Docker (a recent survey put the proportion at over 90%).</p></div></div></div>
<div class="section" title="Deployment with Docker"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec53"/>Deployment with Docker</h1></div></div></div><p>The principle of <a id="id447" class="indexterm"/>deploying software with containers is very simple: the software, plus everything it needs to run, is inside the container <span class="strong"><strong>image</strong></span>, which is like a package file, but is executable directly by the container runtime.</p><p>To run the software, all you need to do is execute a command like the following (if you have Docker installed, try it!):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run bitfield/hello</strong></span>
Hello, world</pre></div><p>Docker will download the <a id="id448" class="indexterm"/>specified image from your configured <span class="strong"><strong>registry</strong></span> (this could be the public registry, called Docker Hub, or your own private Docker registry) and execute it. There are thousands of Docker images available for you to use, and many software companies are increasingly using Docker images as their primary way to deploy products.</p><div class="section" title="Building Docker containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec158"/>Building Docker containers</h2></div></div></div><p>But where do these Docker <a id="id449" class="indexterm"/>images come from? Docker images are like an archive or a package file, containing the file and directory layout of all the files inside the container, including executable binaries, shared libraries, and config files. To create this image file, you use the <code class="literal">docker build</code> command.</p><p>
<code class="literal">docker build</code> takes a special text file called a <span class="strong"><strong>Dockerfile</strong></span>, which specifies what should be in the container. Usually, a new Docker image is based on an existing image with a few modifications. For example, there is a Docker image for Ubuntu Linux, which contains a fully-installed operating system ready to run.</p><p>Your Dockerfile might specify that you use the Ubuntu Docker image as a starting point, and then install the package <code class="literal">nginx</code>. The resulting Docker container contains everything that was in the stock Ubuntu image, plus the <code class="literal">nginx</code> package. You can now upload this image to a registry and run it anywhere using <code class="literal">docker run</code>.</p><p>If you wanted to package your own software with Docker, you could choose a suitable base image (such as Ubuntu) and write a Dockerfile which installs your software onto that base image. When you build the container image with <code class="literal">docker build</code>, the result will be a container with your software inside it, which anyone can run using <code class="literal">docker run</code>. The only thing they need to install is Docker.</p><p>This makes Docker a great way both for software vendors to package their products in an easy-installable format, and for users to try out different software quickly to see if it meets their needs.</p></div><div class="section" title="The layered filesystem"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec159"/>The layered filesystem</h2></div></div></div><p>The Docker filesystem has a <a id="id450" class="indexterm"/>feature called <span class="strong"><strong>layering</strong></span>. Containers are built up in layers, so that if something changes, only the affected layer and those above it need to be rebuilt. This makes it much more efficient to update container images once they've been built and deployed.</p><p>For example, if you change one line of code in your app and rebuild the container, only the layer that contains your app needs to be rebuilt, along with any layers above it. The base image and other layers <a id="id451" class="indexterm"/>below the affected layer remain the same and can be re-used for the new container.</p></div><div class="section" title="Managing containers with Puppet"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec160"/>Managing containers with Puppet</h2></div></div></div><p>There are a few <a id="id452" class="indexterm"/>things you need to be able to do to package <a id="id453" class="indexterm"/>and run software with Docker:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Install, configure, and manage the Docker service itself</li><li class="listitem" style="list-style-type: disc">Build your images</li><li class="listitem" style="list-style-type: disc">Rebuild images when the Dockerfile changes, or a dependency is updated</li><li class="listitem" style="list-style-type: disc">Manage the running images, their data storage, and their configuration</li></ul></div><p>Unless you want to make your images public, you will also need to host an image registry for your own images.</p><p>These sound like the kinds of problems that configuration management tools can solve, and luckily, we have a great configuration management tool available. Oddly enough, while most people recognize that traditional servers need to be built and managed automatically by a tool such as Puppet, the same does not seem to be true (yet) of containers.</p><p>The trouble is, it's so easy to make a simple container and run it that many people think configuration management for containers is overkill. That may be so when you're first trying out Docker and experimenting with simple containers, but when you're running complex, multi-container services in production, at scale, things get more complicated.</p><p>First, containerizing non-trivial applications is non-trivial. They need dependencies and configuration settings and data, and ways to communicate with other applications and services, and while Docker provides you with tools to do this, it doesn't do the work for you.</p><p>Second, you need an infrastructure to build your containers, update them, store and retrieve the resulting images, and deploy and manage them in production. Configuration management for containers is very much like configuration management for traditional server-based applications, except that it's happening at a slightly higher level.</p><p>Containers are great, but <a id="id454" class="indexterm"/>they don't do away with the need for <a id="id455" class="indexterm"/>configuration management tools (remember the <span class="emphasis"><em>Law of Conservation of Pain</em></span> from <a class="link" href="ch01.html" title="Chapter 1. Getting started with Puppet">Chapter 1</a>, <span class="emphasis"><em>Getting started with Puppet</em></span>?):</p><p>
<span class="emphasis"><em>"If you save yourself pain in one place, it pops up again in another. Whatever cool new technology comes along, it won't solve all our problems; at best, it will replace them with refreshingly different problems."</em></span>
</p></div></div>
<div class="section" title="Managing Docker with Puppet"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec54"/>Managing Docker with Puppet</h1></div></div></div><p>Puppet can certainly <a id="id456" class="indexterm"/>install and manage the Docker service for you, just as it can any other software, but it can also do a lot more. It can download and run Docker images, build images from Dockerfiles, mount files and directories on the container, and manage Docker volumes and networks. We'll see how to do all these things in this chapter.</p><div class="section" title="Installing Docker"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec161"/>Installing Docker</h2></div></div></div><p>Before we do anything <a id="id457" class="indexterm"/>else, we'll need to install Docker on our node (using Puppet, of course). The <code class="literal">puppetlabs/docker_platform</code> module is ideal for this.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">If you've already installed and run the <code class="literal">r10k</code> module management tool, as shown in <a class="link" href="ch07.html" title="Chapter 7. Mastering modules">Chapter 7</a>, <span class="emphasis"><em>Mastering modules</em></span>, in the <span class="emphasis"><em>Using r10k</em></span> section, the required module will already be installed. If not, run the following commands to install it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /etc/puppetlabs/code/environments/pbg</strong></span>
<span class="strong"><strong>sudo r10k puppetfile install</strong></span>
</pre></div></li><li class="listitem">Once the module is installed, all you need to do to install Docker on your node is to apply a manifest like the following (<code class="literal">docker_install.pp</code>):<div class="informalexample"><pre class="programlisting">include docker</pre></div></li><li class="listitem">Run the following command to apply the manifest:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo puppet apply --environment pbg /examples/docker_install.pp</strong></span>
</pre></div></li><li class="listitem">To check that Docker is installed, run the following command (you may see a different version number, but that's OK):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker --version</strong></span>
Docker version 17.05.0-ce, build 89658be</pre></div></li></ol></div></div><div class="section" title="Running a Docker container"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec162"/>Running a Docker container</h2></div></div></div><p>In order to run a Docker <a id="id458" class="indexterm"/>container, we first of all have to download it from a Docker registry, which is a server that stores container images. The default registry is Docker Hub, the official public Docker registry.</p><p>To do this with Puppet, you can use the <code class="literal">docker::image</code> resource (<code class="literal">docker_image.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::image { 'bitfield/hello':
  ensure =&gt; 'latest',
}</pre></div><p>As with the <code class="literal">package</code> resource, if you specify <code class="literal">ensure =&gt; latest</code>, Puppet will check the registry every time it runs and make sure you have the latest available version of the image.</p><p>To run the image you've just downloaded, add a <code class="literal">docker::run</code> resource to your manifest (<code class="literal">docker_run.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::run { 'hello':
  image   =&gt; 'bitfield/hello',
  command =&gt; '/bin/sh -c "while true; do echo Hello, world; sleep 1; done"',
}</pre></div><p>Apply this manifest with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo puppet apply /examples/docker_run.pp</strong></span>
</pre></div><p>The <code class="literal">docker::run</code> resource tells Docker to fetch the image <code class="literal">bitfield/hello</code> from the local image cache and run it with the specified command, which in this case just loops forever printing <code class="literal">Hello, world</code>. (I told you containers were useful.)</p><p>The container is now running on your node, and you can check this with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo docker ps</strong></span>
CONTAINER ID        IMAGE               COMMAND                  CREATED
STATUS              PORTS               NAMES
ba1f4aced778        bitfield/hello      „/bin/sh -c ‚while tr"   4 minutes ago       Up 4 minutes                            hello</pre></div><p>The <code class="literal">docker ps</code> command shows all  currently running containers (<code class="literal">docker ps -a</code> will show stopped containers too), with the following information:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The container ID, Docker's internal identifier for the container</li><li class="listitem" style="list-style-type: disc">The image name (<code class="literal">bitfield/hello</code> in our example)</li><li class="listitem" style="list-style-type: disc">The currently executing command in the container</li><li class="listitem" style="list-style-type: disc">The creation time</li><li class="listitem" style="list-style-type: disc">Current status</li><li class="listitem" style="list-style-type: disc">Any ports mapped by the container</li><li class="listitem" style="list-style-type: disc">The human-readable name of the container (which is the title we gave the <code class="literal">docker::run</code> resource in our manifest)</li></ul></div><p>The container is running as <a id="id459" class="indexterm"/>a service, and we can check that with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>systemctl status docker-hello</strong></span>
* docker-hello.service - Daemon for hello
   Loaded: loaded (/etc/systemd/system/docker-hello.service; enabled; vendor preset: enabled)
   Active: active (running) since Tue 2017-05-16 04:07:23 PDT; 1min 4s ago
 Main PID: 24385 (docker)
   CGroup: /system.slice/docker-hello.service
           `-24385 /usr/bin/docker run --net bridge -m 0b --name hello bitfield/hello...
...</pre></div></div><div class="section" title="Stopping a container"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec163"/>Stopping a container</h2></div></div></div><p>According to the Docker <a id="id460" class="indexterm"/>documentation, you can stop a container by running <code class="literal">sudo docker stop NAME</code>. However, if you try this, and then run <code class="literal">sudo docker ps</code> again, you'll see that the container is still running. What's that about?</p><p>The Puppet module assumes by default that you want to run all containers as services; that is, to configure <code class="literal">systemd</code> to keep the container running, and to start it at boot time.</p><p>Therefore, if you want to stop a container which is running as a service, you will need to do this with Puppet, by setting the <code class="literal">ensure</code> parameter on the <code class="literal">docker::run</code> resource to <code class="literal">absent</code>, as in the following example (<code class="literal">docker_absent.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::run { 'hello':
  ensure =&gt; absent,
  image  =&gt; 'bitfield/hello',
}</pre></div><p>Alternatively, on the command line, you can use the <code class="literal">systemctl</code> command to stop the service:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo systemctl stop docker-hello</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip44"/>Tip</h3><p>If you don't want your container to be managed as a service by <code class="literal">systemd</code>, specify the parameter <code class="literal">restart =&gt; always</code> to the <code class="literal">docker::run</code> resource. This tells Docker to restart the container automatically when it exits; so therefore Puppet does not need to <a id="id461" class="indexterm"/>create a <code class="literal">systemd</code> service to manage it.</p></div></div></div><div class="section" title="Running multiple instances of a container"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec164"/>Running multiple instances of a container</h2></div></div></div><p>Of course, the <a id="id462" class="indexterm"/>true power of automation is the ability to scale. We're not limited to running a single instance of a given container; Puppet will happily start as many as you like.</p><p>Each <code class="literal">docker::run</code> resource must have a unique name, as with any other Puppet resource, so you can create them in an <code class="literal">each</code> loop, as in the following example (<code class="literal">docker_run_many.pp</code>):</p><div class="informalexample"><pre class="programlisting">range(1,20).each | $instance | {
  docker::run { "hello-${instance}":
    image   =&gt; 'bitfield/hello',
    command =&gt; '/bin/sh -c "while true; do echo Hello, world; sleep 1; done"',
  }
}</pre></div><p>The <code class="literal">range()</code> function comes from the <code class="literal">stdlib</code> module, and, as you might expect, <code class="literal">range(1,20)</code> returns the sequence of integers between 1 and 20 inclusive. We iterate over this sequence with the <code class="literal">each</code> function, and each time through the loop <code class="literal">$instance</code> is set to the next integer.</p><p>The <code class="literal">docker::run</code> resource title includes the value of <code class="literal">$instance</code> on each iteration, so each container will be uniquely named: <code class="literal">hello-1</code>, <code class="literal">hello-2</code>,... <code class="literal">hello-20</code>. I've chosen the number 20 at random, just as an example; you could compute the number of instances to run based on the resources available, for example the number of system CPUs or available memory.</p><p>Don't forget to stop these containers afterward (edit the example manifest to add <code class="literal">ensure =&gt; absent</code> to the <code class="literal">docker::run</code> resource and re-apply it).</p></div></div>
<div class="section" title="Managing Docker images"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec55"/>Managing Docker images</h1></div></div></div><p>Of course, it's very <a id="id463" class="indexterm"/>useful to be able to download and run public images from Docker Hub or other registries, but to unlock the real power of Docker we need to be able to build and manage our own images too.</p><div class="section" title="Building images from Dockerfiles"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec165"/>Building images from Dockerfiles</h2></div></div></div><p>As we saw in the <a id="id464" class="indexterm"/>previous examples, if you don't already <a id="id465" class="indexterm"/>have the specified container image on your system, Puppet's <code class="literal">docker::image</code> resource will pull it from Docker Hub for you and save it locally.</p><p>The <code class="literal">docker::image</code> resource is most useful, however, for actually <span class="strong"><strong>building</strong></span> Docker images. This is usually done using a Dockerfile, so here is an example Dockerfile we can use to build an image (<code class="literal">Dockerfile.hello</code>):</p><div class="informalexample"><pre class="programlisting">FROM library/alpine:3.6
CMD /bin/sh -c "while true; do echo Hello, world; sleep 1; done"

LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Hello World" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-guide.git" \
  org.label-schema.docker.schema-version="1.0"</pre></div><p>The <code class="literal">FROM</code> statement tells Docker what base image to start from, of the many public images available. <code class="literal">FROM scratch</code> would start with a completely empty container. <code class="literal">FROM library/ubuntu</code> would use the official Ubuntu Docker image.</p><p>Of course, one of the key advantages of containers is that they can be as small or as large as they need to be, so downloading a 188 MB image containing all of Ubuntu is unnecessary if you simply want to run <code class="literal">/bin/echo</code>.</p><p>Alpine is another Linux distribution designed to be as small and lightweight as possible, which makes it ideal for containers. The <code class="literal">library/alpine</code> image is only 4 MB, forty times smaller than <code class="literal">ubuntu</code>; quite a saving. Also, if you build all your containers from the same base image, Docker's layer system means it only has to download and store the base image once.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip45"/>Tip</h3><p>Dockerfiles can be fairly simple, as in the example, or quite complex. You can find out more about the Dockerfile format and commands from the Docker documentation:</p><p>
<a class="ulink" href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a>.</p></div></div><p>The following code shows how to create a Docker image from this file (<code class="literal">docker_build_hello.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::image { 'pbg-hello':
  docker_file =&gt; '/examples/Dockerfile.hello',
  ensure      =&gt; latest,
}</pre></div><p>Once the <code class="literal">docker::image</code> <a id="id466" class="indexterm"/>resource has been applied, the <a id="id467" class="indexterm"/>resulting <code class="literal">pbg-hello</code> image will be available for you to run as a container (<code class="literal">docker_run_hello.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::run { 'pbg-hello':
  image =&gt; 'pbg-hello',
}</pre></div></div><div class="section" title="Managing Dockerfiles"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec166"/>Managing Dockerfiles</h2></div></div></div><p>When you run your own<a id="id468" class="indexterm"/> apps in containers, or third-party apps in<a id="id469" class="indexterm"/> your own containers, you can manage the associated Dockerfiles with Puppet. Here's an example of a simple Dockerfile which builds a container using Nginx to serve a web page with a friendly greeting message (<code class="literal">Dockerfile.nginx</code>):</p><div class="informalexample"><pre class="programlisting">FROM nginx:1.13.3-alpine
RUN echo "Hello, world" &gt;/usr/share/nginx/html/index.html

LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Nginx Hello World" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-guide.git" \
  org.label-schema.docker.schema-version="1.0"</pre></div><p>Here's the Puppet manifest which manages this Dockerfile, and builds an image from it (<code class="literal">docker_build_nginx.pp</code>):</p><div class="informalexample"><pre class="programlisting">file { '/tmp/Dockerfile.nginx':
  source =&gt; '/examples/Dockerfile.nginx',
  notify =&gt; Docker::Image['pbg-nginx'],
}

docker::image { 'pbg-nginx':
  docker_file =&gt; '/tmp/Dockerfile.nginx',
  ensure      =&gt; latest,
}</pre></div><p>Run the following command to apply this manifest:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo puppet apply /examples/docker_build_nginx.pp</strong></span>
</pre></div><p>Whenever the contents of the Dockerfile change, applying this manifest will cause the image to be rebuilt.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip46"/>Tip</h3><p>For the purposes of this <a id="id470" class="indexterm"/>example we are building and running the container on the same node. In practice, though, you should build your containers on a dedicated build node and upload the resulting images to the registry, so that your production nodes can download and run them.</p></div></div><p>Here's the manifest to run <a id="id471" class="indexterm"/>the container we just built (<code class="literal">docker_run_nginx.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::run { 'pbg-nginx':
  image         =&gt; 'pbg-nginx:latest',
  ports         =&gt; ['80:80'],
  pull_on_start =&gt; true,
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip47"/>Tip</h3><p>Note the <code class="literal">pull_on_start</code> attribute, which tells Puppet to always download the latest available version of the container when starting or restarting it.</p></div></div><p>If you worked through <a class="link" href="ch07.html" title="Chapter 7. Mastering modules">Chapter 7</a>, <span class="emphasis"><em>Mastering modules</em></span>, the Apache web server will be running and listening on port <code class="literal">80</code>, so you will need to run the following commands to remove it before applying this manifest:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get -y --purge remove apache2</strong></span>
<span class="strong"><strong>sudo service docker restart</strong></span>
<span class="strong"><strong>sudo puppet apply --environment pbg /examples/docker_run_nginx.pp</strong></span>
</pre></div><p>You can check that the container is working by browsing to the following URL on your local machine:</p><p>
<code class="literal">http://localhost:8080</code>
</p><p>You should see the text <code class="literal">Hello, world</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip48"/>Tip</h3><p>If you're using the Vagrant box, port <code class="literal">8080</code> on your local machine is automatically mapped to port <code class="literal">80</code> on the VM, which is then mapped by Docker to port <code class="literal">80</code> on the <code class="literal">pbg-nginx</code> container. If for some reason you need to change this port mapping, edit your Vagrantfile (in the Puppet Beginner's Guide repo) and look for the following line:</p><div class="informalexample"><pre class="programlisting">  config.vm.network "forwarded_port", guest: 80, host: 8080</pre></div><p>Change these settings as <a id="id472" class="indexterm"/>required, and run the following command on your local machine in the PBG repo directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant reload</strong></span>
</pre></div><p>If you're not using the Vagrant box, the container's port <code class="literal">80</code> will be exposed at your local port <code class="literal">80</code>, so the URL will simply appear as follows:</p><p>
<code class="literal">http://localhost</code>
</p></div></div></div></div>
<div class="section" title="Building dynamic containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec56"/>Building dynamic containers</h1></div></div></div><p>Although Dockerfiles are <a id="id473" class="indexterm"/>a fairly powerful and flexible way of building containers, they are only static text files, and very often you will need to pass information into the container to tell it what to do. We might call such containers—whose configuration is flexible and based on data available at build time—<span class="strong"><strong>dynamic containers</strong></span>.</p><div class="section" title="Configuring containers with templates"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec167"/>Configuring containers with templates</h2></div></div></div><p>One way to configure <a id="id474" class="indexterm"/>containers dynamically is to use Puppet to manage the Dockerfile as an EPP template (see <a class="link" href="ch09.html" title="Chapter 9. Managing files with templates">Chapter 9</a>, <span class="emphasis"><em>Managing files with templates</em></span>), and interpolate the required data (which could come from Hiera, Facter, or directly from Puppet code).</p><p>Let's upgrade our previous <code class="literal">Hello, world</code> example to have Nginx serve any arbitrary text string, supplied by Puppet at build time.</p><p>Here's the manifest to generate the Dockerfile from a template and run the resulting image (<code class="literal">docker_template.pp</code>):</p><div class="informalexample"><pre class="programlisting">file { '/tmp/Dockerfile.nginx':
  content =&gt; epp('/examples/Dockerfile.nginx.epp',
    {
      'message' =&gt; 'Containers rule!'
    }
  ),
  notify =&gt; Docker::Image['pbg-nginx'],
}

docker::image { 'pbg-nginx':
  docker_file =&gt; '/tmp/Dockerfile.nginx',
  ensure      =&gt; latest,
  notify      =&gt; Docker::Run['pbg-nginx'],
}

docker::run { 'pbg-nginx':
  image         =&gt; 'pbg-nginx:latest',
  ports         =&gt; ['80:80'],
  pull_on_start =&gt; true,
}</pre></div><p>Apply this manifest with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo puppet apply --environment pbg /examples/docker_template.pp</strong></span>
</pre></div><p>When you have applied the manifest and built the container, you will find that if you change the value of <code class="literal">message</code> and reapply, the container will be rebuilt with the updated text. The <code class="literal">docker::image</code> <a id="id475" class="indexterm"/>resource uses <code class="literal">notify</code> to tell the <code class="literal">docker::run</code> resource to restart the container when the image changes.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip51"/>Tip</h3><p>Templating the Dockerfile like this is a powerful technique. Since you can have Puppet put any arbitrary data into a Dockerfile, you can configure anything about the container and its build process: the base image, the list of packages to install, files and data that should be added to the container, and even the command entry point for the container.</p></div></div></div><div class="section" title="Self-configuring containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec168"/>Self-configuring containers</h2></div></div></div><p>Let's take this idea <a id="id476" class="indexterm"/>even further and use Puppet to dynamically configure a container which can fetch its data from Git. Instead of serving static text supplied at build time, we will have the container itself check out a Git repo for the website.</p><p>Most of the code from the previous example remains unchanged, except for the Dockerfile resource (<code class="literal">docker_website.pp</code>):</p><div class="informalexample"><pre class="programlisting">file { '/tmp/Dockerfile.nginx':
  content =&gt; epp('/examples/Dockerfile.website.epp',
    {
      'git_url' =&gt; 'https://github.com/bitfield/pbg-website.git'
    }
  ),
  notify  =&gt; Docker::Image['pbg-nginx'],
}

docker::image { 'pbg-nginx':
  docker_file =&gt; '/tmp/Dockerfile.nginx',
  ensure      =&gt; latest,
  notify      =&gt; Docker::Run['pbg-nginx'],
}

docker::run { 'pbg-nginx':
  image         =&gt; 'pbg-nginx:latest',
  ports         =&gt; ['80:80'],
  pull_on_start =&gt; true,
}</pre></div><p>The Dockerfile itself is a little more complicated, because we need to install Git in the container and use it to <a id="id477" class="indexterm"/>check out the supplied Git repo (<code class="literal">Dockerfile.website.epp</code>):</p><div class="informalexample"><pre class="programlisting">&lt;% | String $git_url | -%&gt;
FROM nginx:1.13.3-alpine
RUN apk update \
  &amp;&amp; apk add git \
  &amp;&amp; cd /usr/share/nginx \
  &amp;&amp; mv html html.orig \
  &amp;&amp; git clone &lt;%= $git_url %&gt; html

LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Nginx Git Website" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-guide.git" \
  org.label-schema.docker.schema-version="1.0"</pre></div><p>When you apply this manifest and browse to <code class="literal">http://localhost:8080</code>, you should see the text:</p><div class="informalexample"><pre class="programlisting">Hello, world!
This is the demo website served by the examples in Chapter 10, 'Controlling containers', from the Puppet Beginner's Guide.</pre></div><p>Although we supplied the <code class="literal">git_url</code> parameter directly to the Dockerfile template, that data could of course come from anywhere, including Hiera. With this technique, you can build a container to serve any website simply by changing the Git URL passed to it.</p><p>Using the iteration pattern we saw in the <code class="literal">docker_run_many</code> example earlier in this chapter, you could build a set of containers like this from an array of <code class="literal">git_url</code> values, each serving a different website. Now we're really starting to exploit the power of Docker-plus-Puppet.</p><p>Run the following command to stop the container before going on to the next example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo docker stop pbg-nginx</strong></span>
</pre></div><p>There's one slight problem with this idea. Although it's good to have the container be able to serve content from a Git repo determined at build time, every time the container is started or restarted, it will have to run the <code class="literal">git clone</code> process again. This takes time, and if the repo or the <a id="id478" class="indexterm"/>network is unavailable for some reason, it can stop the container from working.</p><p>A better solution would be to serve the content from persistent storage, and we'll see how to do that in the next section.</p></div></div>
<div class="section" title="Persistent storage for containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec57"/>Persistent storage for containers</h1></div></div></div><p>Containers are <a id="id479" class="indexterm"/>designed to be transient; they run for a while, and then disappear. Anything inside the container disappears with it, including files and data created during the container's run. This isn't always what we want, of course. If you're running a database inside a container, for example, you usually want that data to persist when the container goes away.</p><p>There are two ways of persisting data in a container: the first is to mount a directory from the host machine inside the container, known as a <span class="strong"><strong>host-mounted volume</strong></span>, and the second is to use what's called a <span class="strong"><strong>Docker volume</strong></span>. We'll look at both of these in the following sections.</p><div class="section" title="Host-mounted volumes"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec169"/>Host-mounted volumes</h2></div></div></div><p>If you want a container to be <a id="id480" class="indexterm"/>able to access files on the host <a id="id481" class="indexterm"/>machine's filesystem (such as application code that you're working on and you want to test, for example), the easiest way to do that is to mount a directory from the host on the container. The following example shows how to do this (<code class="literal">docker_mount.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::run { 'mount_test':
  image   =&gt; 'library/alpine:3.6',
  volumes =&gt; ['/tmp/container_data:/mnt/data'],
  command =&gt; '/bin/sh -c "echo Hello, world &gt;/mnt/data/hello.txt"',
}</pre></div><p>The <code class="literal">volumes</code> attribute specifies an array of volumes to attach to the container. If the volume is of the form <code class="literal">HOST_PATH:CONTAINER_PATH</code>, Docker will assume you want to mount the directory <code class="literal">HOST_PATH</code> on the container. The path inside the container will be <code class="literal">CONTAINER_PATH</code>. Any files which already exist in the mounted directory will be accessible to the container, and anything the container writes to the directory will still be available once the container has stopped.</p><p>If you apply this example manifest, the container will mount the host machine's <code class="literal">/tmp/container_data/</code> directory (this will be created if it doesn't exist) as <code class="literal">/mnt/data/</code> in the container.</p><p>The <code class="literal">command</code> attribute tells the container to write the string <code class="literal">Hello, world</code> to the file <code class="literal">/mnt/data/hello.txt</code>.</p><p>Run the following command to apply this manifest:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo puppet apply /examples/docker_mount.pp</strong></span>
</pre></div><p>The container will start, write <a id="id482" class="indexterm"/>the data, and then exit. If all has <a id="id483" class="indexterm"/>gone well, you'll see that the file <code class="literal">/tmp/container_data/hello.txt</code> is now present and contains the data written by the container:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat /tmp/container_data/hello.txt</strong></span>
Hello, world</pre></div><p>Host-mounted volumes are very useful when a container needs to access or share data with applications running on the host machine. For example, you could use a host-mounted volume with a container which runs syntax checks, lint, or continuous integration tests on your source code directory.</p><p>However, containers using host-mounted volumes are not portable, and they rely on a specific directory being present on the host machine. You can't specify a host-mounted volume in a Dockerfile, so you can't publish a container which relies on one. While host-mounted volumes can be useful for testing and development, a better solution in production is to use Docker volumes.</p></div><div class="section" title="Docker volumes"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec170"/>Docker volumes</h2></div></div></div><p>A more portable way of <a id="id484" class="indexterm"/>adding persistent storage to containers is to use a <a id="id485" class="indexterm"/>
<span class="strong"><strong>Docker volume</strong></span>. This is a persistent data object which lives in Docker's storage area and can be attached to one or more containers.</p><p>The following example shows how to use <code class="literal">docker::run</code> to start a container with a Docker volume (<code class="literal">docker_volume.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::run { 'volume_test':
  image   =&gt; 'library/alpine:3.6',
  volumes =&gt; ['pbg-volume:/mnt/volume'],
  command =&gt; '/bin/sh -c "echo Hello from inside a Docker volume &gt;/mnt/volume/index.html"',
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip52"/>Tip</h3><p>The <code class="literal">volumes</code> attribute is a little different from the previous example. It has the form <code class="literal">VOLUME_NAME:CONTAINER_PATH</code>, which tells Docker that this is not a host-mounted volume, but a Docker volume named <code class="literal">VOLUME_NAME</code>. If the value before the colon is a path, Docker assumes you want to mount that path from the host machine, but otherwise, it assumes you want to mount a Docker volume with the specified name.</p></div></div><p>As in the previous example, the container's <code class="literal">command</code> argument writes a message to a file on the mounted volume.</p><p>If you apply this manifest, once <a id="id486" class="indexterm"/>the container has exited, you can see that the volume is still present by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo docker volume ls</strong></span>
DRIVER              VOLUME NAME
local               pbg-volume</pre></div><p>A Docker volume is a good way to store data that you need to keep even when the container is not running (a database, for example). It's also a good way to make data available to containers without having to load it into each container every time it starts.</p><p>In the website example earlier in the chapter, instead of each container checking out its own copy of the Git repo, you could check out the repo into a Docker volume, and then have each container mount this volume when it starts.</p><p>Let's test that idea with the following manifest (<code class="literal">docker_volume2.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker::run { 'volume_test2':
  image   =&gt; 'nginx:alpine',
  volumes =&gt; ['pbg-volume:/usr/share/nginx/html'],
  ports   =&gt; ['80:80'],
}</pre></div><p>This is the same <code class="literal">nginx</code> container we used earlier in the chapter, which serves whatever is in its <code class="literal">/usr/share/nginx/html</code> directory as a website.</p><p>The <code class="literal">volumes</code> attribute tells the container to mount the <code class="literal">pbg-volume</code> volume on <code class="literal">/usr/share/nginx/html</code>.</p><p>Run the following commands to apply this manifest:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo docker stop pbg-nginx</strong></span>
<span class="strong"><strong>sudo puppet apply /examples/docker_volume2.pp</strong></span>
</pre></div><p>If everything works as we expect, we should able to browse to the following URL on the local machine: <code class="literal">http://localhost:8080/</code>
</p><p>We should see the following text:</p><div class="informalexample"><pre class="programlisting">Hello from inside a Docker volume</pre></div><p>This is a very powerful feature of <a id="id487" class="indexterm"/>containers. They can read, write, and modify data created by other containers, maintain persistent storage of their own, and share data with other running containers, all using volumes.</p><p>A common pattern for running applications in Docker is to use multiple, communicating containers, each providing a single specific service. For example, a web application might use an Nginx container to serve an application to users, while storing its session data in a MySQL container <a id="id488" class="indexterm"/>mounting a persistent volume. It could also use a linked Redis container as an in-memory key-value store.</p><p>Apart from sharing data via volumes, though, how do these containers actually communicate over the network? We'll see the answer to that in the next section.</p></div></div>
<div class="section" title="Networking and orchestration"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec58"/>Networking and orchestration</h1></div></div></div><p>We started off the chapter by saying that containers are completely self-contained, and have no access to each other, even if they're running on the same host. But to run real applications, we need containers to communicate. Fortunately, there is a way to do this: the <span class="strong"><strong>Docker network</strong></span>.</p><div class="section" title="Connecting containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec171"/>Connecting containers</h2></div></div></div><p>A Docker network is like a <a id="id489" class="indexterm"/>private chat room for containers: all the containers inside the network can talk to each other, but they can't talk to containers outside it or in other networks, and vice versa. All you need to do is have Docker create a network, give it a name, and then you can start containers inside that network and they will be able to talk to each other.</p><p>Let's develop an example to try this out. Suppose we want to run the Redis database inside a container, and send data to it from another container. This is a common pattern for many applications.</p><p>In our example, we're going to create a Docker network, and start two containers inside it. The first container is a public Docker Hub image that will run the Redis database server. The second container will install the Redis client tool, and write some data to the Redis server container. Then, to check it worked, we can try to read the data back from the server.</p><p>Run the following command to apply the Docker network example manifest:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo puppet apply /examples/docker_network.pp</strong></span>
</pre></div><p>If everything worked as it should, our Redis database should now contain a piece of data named <code class="literal">message</code> containing a friendly greeting, proving that we've passed data from one container to another over the Docker network.</p><p>Run the following command to <a id="id490" class="indexterm"/>connect to the client container and check that this is the case:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo docker exec -it pbg-redis redis-cli get message</strong></span>
"Hello, world"</pre></div><p>So how does it all work? Let's take a look at the example manifest. First of all, we create the network for the two containers to run in, using the <code class="literal">docker_network</code> resource in Puppet (<code class="literal">docker_network.pp</code>):</p><div class="informalexample"><pre class="programlisting">docker_network { 'pbg-net':
  ensure =&gt; present,
}</pre></div><p>Now, we run the Redis server container, using the public <code class="literal">redis:4.0.1-alpine</code> image.</p><div class="informalexample"><pre class="programlisting">docker::run { 'pbg-redis':
  image =&gt; 'redis:4.0.1-alpine',
  net   =&gt; 'pbg-net',
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>Did you notice that we supplied the <code class="literal">net</code> attribute to the <code class="literal">docker::run</code> resource? This specifies the Docker network that the container should run in.</p></div></div><p>Next, we build a container which has the Redis client (<code class="literal">redis-cli</code>) installed so that we can use it to write some data to the Redis container.</p><p>Here's the Dockerfile for the client container (<code class="literal">Dockerfile.pbg-demo</code>):</p><div class="informalexample"><pre class="programlisting">FROM nginx:1.13.3-alpine
RUN apk update \
  &amp;&amp; apk add redis

LABEL org.label-schema.vendor="Bitfield Consulting" \
  org.label-schema.url="http://bitfieldconsulting.com" \
  org.label-schema.name="Redis Demo" \
  org.label-schema.version="1.0.0" \
  org.label-schema.vcs-url="github.com:bitfield/puppet-beginners-guide.git" \
  org.label-schema.docker.schema-version="1.0"</pre></div><p>We build this container in the usual way using <code class="literal">docker::image</code>:</p><div class="informalexample"><pre class="programlisting">docker::image { 'pbg-demo':
  docker_file =&gt; '/examples/Dockerfile.pbg-demo',
  ensure      =&gt; latest,
}</pre></div><p>Finally, we run an instance of <a id="id491" class="indexterm"/>the client container with <code class="literal">docker::run</code>, passing in a command to <code class="literal">redis-cli</code> to write some data to the other container.</p><div class="informalexample"><pre class="programlisting">docker::run { 'pbg-demo':
  image   =&gt; 'pbg-demo',
  net     =&gt; 'pbg-net',
  command =&gt; '/bin/sh -c "redis-cli -h pbg-redis set message \"Hello, world\""',
}</pre></div><p>As you can see, this container also has the attribute <code class="literal">net =&gt; 'pbg-net'</code>. It will therefore run in the same Docker network as the <code class="literal">pbg-redis</code> container, so the two containers will be able to talk to each other.</p><p>When the container starts, the <code class="literal">command</code> attribute calls <code class="literal">redis-cli</code> with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>redis-cli -h pbg-redis set message "Hello, world"</strong></span>
</pre></div><p>The <code class="literal">-h pbg-redis</code> argument tells Redis to connect to the host <code class="literal">pbg-redis</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>How does using the <code class="literal">pbg-redis</code> name connect to the right container? When you start a container inside a network, Docker automagically configures DNS lookups within the container to find other containers in the network by name. When you reference a container name (the title of the container's <code class="literal">docker::run</code> resource, which in our example is <code class="literal">pbg-redis</code>), Docker will route the network connection to the right place.</p></div></div><p>The command <code class="literal">set message "Hello, world"</code> creates a Redis key named <code class="literal">message</code>, and gives it the value <code class="literal">"Hello, world"</code>.</p><p>We now have all the necessary <a id="id492" class="indexterm"/>techniques to containerize a real application: using Puppet to manage multiple containers, built from dynamic data, pushed to a registry, updated on demand, communicating over the network, listening on ports to the outside world, and persisting and sharing data via volumes.</p></div><div class="section" title="Container orchestration"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec172"/>Container orchestration</h2></div></div></div><p>We've seen a number of <a id="id493" class="indexterm"/>ways to manage individual containers in this chapter, but the question of how to provision and manage containers at scale, and across multiple hosts—what we call container <span class="strong"><strong>orchestration</strong></span>—remains.</p><p>For example, if your app runs in a container, you probably won't be running just one instance of the container: you need to run multiple instances, and route and load-balance traffic to them. You also need to be able to distribute your containers across multiple hosts, so that the application is resilient against the failure of any individual container host.</p></div><div class="section" title="What is orchestration?"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec173"/>What is orchestration?</h2></div></div></div><p>When running containers <a id="id494" class="indexterm"/>across a distributed cluster, you also need to be able to deal with issues such as networking between containers and hosts, failover, health monitoring, rolling out updates, service discovery, and sharing configuration data between containers via a key-value database.</p><p>Although container orchestration is a broad task, and different tools and frameworks focus on different aspects of it, the core requirements of orchestration include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scheduling</strong></span>: Running a container on the cluster and deciding which containers to run on which hosts to provide a given service</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cluster management</strong></span>: Monitoring and marshalling the activity of containers and hosts across the cluster, and adding or removing hosts</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Service discovery</strong></span>: Giving containers the ability to find and connect to the services and data they need to operate</li></ul></div></div><div class="section" title="What orchestration tools are available?"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec174"/>What orchestration tools are available?</h2></div></div></div><p>Google's Kubernetes and <a id="id495" class="indexterm"/>Docker's Swarm are both designed to orchestrate containers. Another product, Apache Mesos, is a cluster management framework which can operate on different kinds of resources, including containers.</p><p>Most containers in production today are running under one of these three orchestration systems. Kubernetes has been around the longest and has the biggest user base, but Swarm, though a relatively new arrival, is part of the official Docker stack, so it is being rapidly adopted.</p><p>Because all these products are necessarily rather complicated to set up and operate, there is also the option of <span class="strong"><strong>Platform-as-a-Service</strong></span> (<span class="strong"><strong>PaaS</strong></span>) orchestration: essentially, running your containers on a managed cloud platform. <span class="strong"><strong>Google Container Engine</strong></span> (<span class="strong"><strong>GKE</strong></span>) is Kubernetes as a service; Amazon's <span class="strong"><strong>EC2 Container Service</strong></span> (<span class="strong"><strong>ECS</strong></span>) is a proprietary, Kubernetes-like system.</p><p>As yet, Puppet integration with <a id="id496" class="indexterm"/>container orchestrators is somewhat limited and at an early stage, though, given the popularity of containers, this is likely to advance rapidly. There is some elementary support for generating Kubernetes configuration from Puppet resources, and some for managing Amazon ECS resources, but it's fair to say that automating container orchestration at scale with Puppet is so far still in its infancy. Watch this space, however.</p></div></div>
<div class="section" title="Running Puppet inside containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec59"/>Running Puppet inside containers</h1></div></div></div><p>If a container can <a id="id497" class="indexterm"/>contain a whole operating system, such as Ubuntu, you might be wondering: "can't I just run Puppet inside the container?"</p><p>You can, and some people do take this approach to managing containers. It also has a number of advantages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">You can use your existing Puppet manifests, or Forge modules; no need to write complex Dockerfiles</li><li class="listitem" style="list-style-type: disc">Puppet will keep the container continuously updated; no need to rebuild when something changes</li></ul></div><p>Of course, there are a few disadvantages too:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing Puppet inflates the image size considerably, and pulls in all sorts of dependencies</li><li class="listitem" style="list-style-type: disc">Running Puppet slows down the build process, and also consumes resources in the running container</li></ul></div><p>There are also some hybrid options, such as running Puppet in the container during the build stage, and then removing <a id="id498" class="indexterm"/>Puppet and its dependencies, plus any intermediate build artifacts, before saving the final image.</p><p>Puppet's <code class="literal">image_build</code> module is a promising new way of building containers directly from Puppet manifests, and I expect to see rapid progress in this space in the near future.</p><div class="section" title="Are containers mini VMs or single processes?"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec175"/>Are containers mini VMs or single processes?</h2></div></div></div><p>Which option you favor <a id="id499" class="indexterm"/>probably depends on your basic approach to containers. Do you see them as mini-virtual machines, not too different from the servers you're already <a id="id500" class="indexterm"/>managing? Or do you see them as transient, lightweight, single-process wrappers?</p><p>If you treat containers as mini-VMs, you'll probably want to run Puppet in your containers, in the same way as you do on your physical and virtual servers. On the other hand, if you think a container should just run a single process, it doesn't seem appropriate to run Puppet in it. With single-process containers there's very little to configure.</p><p>I can see arguments in favor of the mini-VM approach. For one thing, it makes it much easier to transition your existing applications and services to containers; instead of running them in a VM, you just move the whole thing (application, support services, and database) into a container, along with all your current management and monitoring tools.</p><p>However, while this is a valid approach, it doesn't really make the most of the inherent advantages of containers: small image sizes, quick deployment, efficient rebuilding, and portability.</p></div><div class="section" title="Configuring containers with Puppet"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec176"/>Configuring containers with Puppet</h2></div></div></div><p>Personally, I'm a <a id="id501" class="indexterm"/>container minimalist: I think the container should contain only what it needs to do the job. Therefore, I prefer to use Puppet to manage, configure, and build my containers from the outside, rather than from the inside, and that's why I've used that approach in this chapter.</p><p>That means generating Dockerfiles from templates and Hiera data, as we've seen in the examples, as well as templating config files which the container needs. You can have the Dockerfile copy these files into the container during the build, or mount individual files and directories from the host onto the container.</p><p>As we've seen, a good way to handle shared data is to have Puppet write it into a Docker volume or a file on the host which is then mounted (usually read-only) by all running containers.</p><p>The advantage of this is that you don't need to rebuild all your containers following a config change. You can <a id="id502" class="indexterm"/>simply have Puppet write the changes to the config volume, and trigger each container to reload its configuration using a <code class="literal">docker::exec</code> resource, which executes a specified command on a running container.</p></div><div class="section" title="Containers need Puppet too"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec177"/>Containers need Puppet too</h2></div></div></div><p>At the risk of laboring a <a id="id503" class="indexterm"/>point, containerization is not an alternative to using configuration management tools such as Puppet. In fact, the need for configuration management is even greater, because you not only have to build and configure the containers themselves, but also store, deploy, and run them: all of which requires an infrastructure.</p><p>As usual, Puppet makes this <a id="id504" class="indexterm"/>sort of task easier, more pleasant, and—most importantly—more scalable.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec60"/>Summary</h1></div></div></div><p>In this chapter, we've examined some of the problems associated with software deployment, some of the options for solving them, and the advantages of the container solution. We've briefly introduced the basics of container technology and Docker, in particular, and seen that containers are another kind of configuration management problem which Puppet can help solve.</p><p>We've installed the <code class="literal">docker_platform</code> module, and used it to set up Docker on our VM, and build and run simple Docker containers. We've seen how to automatically rebuild the container image when the underlying Dockerfile changes, and how to use Puppet to configure a Dockerfile dynamically at build time.</p><p>We've introduced the topic of persistent storage for containers, including host-mounted volumes and Docker volumes, and how to manage these with Puppet. We've set up a Docker network with two communicating containers exchanging data over network ports.</p><p>We've looked at the advantages and disadvantages of running Puppet inside containers, as opposed to using Puppet to configure and build containers from the outside, and also suggested a hybrid strategy where Puppet manages configuration data on a volume attached to running containers.</p><p>Finally, we've covered some of the issues involved in container orchestration, and introduced some of the most popular platforms and frameworks available.</p><p>In the next chapter, we'll learn how to use Puppet to manage cloud computing resources, with an in-depth example developing a software-defined Amazon EC2 infrastructure.</p></div></body></html>