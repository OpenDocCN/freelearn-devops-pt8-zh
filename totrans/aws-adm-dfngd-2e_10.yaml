- en: Orchestrating Data using AWS Data Pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS Data Pipeline 编排数据
- en: In the previous chapter, we explored the AWS analytics suite of services by
    deep diving into Amazon EMR and Amazon Redshift services.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们深入探讨了 AWS 分析套件服务，详细了解了 Amazon EMR 和 Amazon Redshift 服务。
- en: In this chapter, we will be continuing the trend and learning about an extremely
    versatile and powerful data orchestration and transformation service called AWS
    Data Pipeline.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续学习一个极其灵活且强大的数据编排和转换服务——AWS Data Pipeline。
- en: 'Let''s have a quick look at the various topics that we will be covering in
    this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下本章将涵盖的各个主题：
- en: Introducing AWS Data Pipeline along with a quick look at some of its concepts
    and terminologies
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 AWS Data Pipeline，并快速了解其一些概念和术语
- en: Getting started with Data Pipeline using a simple Hello World example
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用简单的 Hello World 示例开始使用 Data Pipeline
- en: Working with the Data Pipeline definition file
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Data Pipeline 定义文件
- en: Executing scripts and commands on remote EC2 instances using a data pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据管道在远程 EC2 实例上执行脚本和命令
- en: Backing up data from one S3 bucket to another using a simple, parameterized
    data pipeline
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用简单的参数化数据管道将数据从一个 S3 桶备份到另一个
- en: Building pipelines using the AWS CLI
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS CLI 构建管道
- en: So without any further ado, let's get started right away!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，废话不多说，我们马上开始吧！
- en: Introducing AWS Data Pipeline
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 AWS Data Pipeline
- en: 'AWS Data Pipeline is an extremely versatile web service that allows you to
    move data back and forth between various AWS services, as well as on-premise data
    sources. The service is designed specifically to provide you with an in-built
    fault tolerance and highly available platform, using which you can define and
    build your very own custom data migration workflows. AWS Data Pipeline also provides
    add-on features such as scheduling, dependency tracking, and error handling, so
    that you do not have to waste extra time and effort in writing them on your own.
    This easy-to-use and flexible service, accompanied by its low operating costs,
    make the AWS Data Pipeline service ideal for use cases such as:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Data Pipeline 是一个极为灵活的 Web 服务，允许您在各种 AWS 服务和本地数据源之间双向传输数据。该服务专为提供内建的容错性和高可用性平台而设计，您可以利用它来定义并构建您自己的定制数据迁移工作流。AWS
    Data Pipeline 还提供了附加功能，如调度、依赖关系追踪和错误处理，使您无需花费额外的时间和精力自行编写这些功能。这个易于使用且灵活的服务，结合其低运营成本，使
    AWS Data Pipeline 成为以下使用场景的理想选择：
- en: Migrating data on a periodic basis from an Amazon EMR cluster over to Amazon
    Redshift for data warehousing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期将数据从 Amazon EMR 集群迁移到 Amazon Redshift 进行数据仓储
- en: Incrementally loading data from files stored in Amazon S3 directly into an Amazon
    RDS database
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将存储在 Amazon S3 中的文件增量加载到 Amazon RDS 数据库中
- en: Copying data from an Amazon MySQL database into an Amazon Redshift cluster
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据从 Amazon MySQL 数据库复制到 Amazon Redshift 集群
- en: Backing up data from an Amazon DynamoDB table to Amazon S3
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据从 Amazon DynamoDB 表备份到 Amazon S3
- en: Backing up files stored in an Amazon S3 bucket on a periodic basis, and much
    more
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期备份存储在 Amazon S3 桶中的文件，等等
- en: In this section, we will be understanding and learning a bit more about AWS
    Data Pipeline by first getting to know some of its internal components, concepts
    and terminologies.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过先了解一些 AWS Data Pipeline 的内部组件、概念和术语，来进一步理解和学习 AWS Data Pipeline。
- en: 'The core foundation of AWS Data Pipeline is, as the name suggests, a pipeline.
    You can create pipelines to schedule and run your data migration or transformation
    tasks. Each pipeline relies on a pipeline definition that essentially contains
    the business logic required to drive the data migration activities. We will be
    learning more about the data pipeline definition in the upcoming sections. For
    now, let''s dive a bit into a few essential pipeline concepts and components:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Data Pipeline 的核心基础，如其名所示，就是一个管道。您可以创建管道来安排和执行数据迁移或转换任务。每个管道依赖于一个管道定义，该定义本质上包含了驱动数据迁移活动所需的业务逻辑。我们将在接下来的章节中深入学习管道定义。现在，让我们先来了解一些基本的管道概念和组件：
- en: '**Pipeline components**: A single pipeline can comprise of multiple sections,
    each having its own specific place in the overall functioning of the pipeline.
    For example, a pipeline can contain sections for specifying the input data source
    from where the data has to be collected, the activity that needs to be performed
    on this data along with a few necessary conditions, the time at which the activity
    has to be triggered, and so on. Each of these sections, individually, are called
    the pipeline''s components and are used together to build a pipeline definition.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道组件**：单个管道可以包含多个部分，每个部分在整个管道的运行中都有其特定的位置。例如，管道可以包含指定输入数据源的部分，用于收集数据、在数据上执行的活动以及一些必要的条件、活动触发的时间等等。这些部分每个单独被称为管道的组件，并一起使用来构建管道定义。'
- en: '**Task runners**: Task runners are special applications or agents that carry
    out the task assigned in a pipeline. The task runners poll AWS Data Pipeline for
    any active tasks available. If found, the task is assigned to a task runner and
    executed. Once the execution completes, the task runner will report the status
    (either success or failure) back to AWS Data Pipeline. By default, AWS provides
    a default task runner for resources that are launched and managed by AWS Data
    Pipeline. You can also install the task runner on instances or on-premise servers
    that you manage:![](img/d75df4f2-8dfb-4f52-9416-473092ea08f6.png)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务执行器**：任务执行器是执行管道中分配任务的特殊应用程序或代理。任务执行器轮询AWS Data Pipeline，查看是否有可用的活动任务。如果发现，任务会分配给任务执行器并执行。一旦执行完成，任务执行器会将状态（成功或失败）报告回AWS
    Data Pipeline。默认情况下，AWS为通过AWS Data Pipeline启动和管理的资源提供了一个默认的任务执行器。您还可以在自己管理的实例或本地服务器上安装任务执行器：![](img/d75df4f2-8dfb-4f52-9416-473092ea08f6.png)'
- en: '**Data nodes**: Data nodes are used to define the location and the type of
    input as well as output data for the pipeline. As of now, the following data nodes
    are provided by AWS Data Pipeline:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据节点**：数据节点用于定义管道的输入和输出数据的位置及类型。目前，AWS Data Pipeline 提供了以下数据节点：'
- en: '`S3DataNode`: Used to define an Amazon S3 location as an input or output for
    storing data'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S3DataNode`：用于将Amazon S3位置定义为存储数据的输入或输出'
- en: '`SqlDataNode`: Defines a SQL table or a database query for use in the pipeline'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SqlDataNode`：定义SQL表或数据库查询供管道使用'
- en: '`RedshiftDataNode`: Used to define an Amazon Redshift table as input or output
    for the pipeline'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RedshiftDataNode`：用于将Amazon Redshift表定义为管道的输入或输出'
- en: '`DynamoDBDataNode`: Used to specify a DynamoDB table as input or output for
    the pipeline'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DynamoDBDataNode`：用于指定DynamoDB表作为管道的输入或输出'
- en: '**Activities**: With the data''s location and type selected using the data
    nodes, the next component left to define is the type of activity to be performed
    on that data. AWS Data Pipeline provides the following set of pre-packaged activities
    that you can use and extend as per your requirements:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动**：通过使用数据节点选择数据的位置和类型，接下来需要定义的是在该数据上执行的活动类型。AWS Data Pipeline 提供了一组预包装的活动，您可以根据需要使用并扩展：'
- en: '`CopyActivity`: Used to copy data from one data node to another'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CopyActivity`：用于将数据从一个数据节点复制到另一个数据节点'
- en: '`ShellCommandActivity`: Used to run a shell command as an activity'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ShellCommandActivity`：用于将Shell命令作为活动执行'
- en: '`SqlActivity`: Executes a SQL query on a data node such as `SqlDataNode` or
    `RedhsiftDataNode`'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SqlActivity`：在如`SqlDataNode`或`RedhsiftDataNode`等数据节点上执行SQL查询'
- en: '`RedshiftCopyActivity`: A specific activity that leverages the `COPY` command
    to copy data between Redshift tables'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RedshiftCopyActivity`：一种特定的活动，利用 `COPY` 命令在Redshift表之间复制数据'
- en: '`EmrActivity`: Used to run an EMR cluster'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EmrActivity`：用于运行EMR集群'
- en: '`PigActivity`: Used to run a custom Pig script on an EMR cluster'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PigActivity`：用于在EMR集群上运行自定义Pig脚本'
- en: '`HiveActivity`: Runs a Hive query on an EMR cluster'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HiveActivity`：在EMR集群上运行Hive查询'
- en: '`HiveCopyActivity`: Used to run a Hive `COPY` query for copying the data from
    the EMR cluster to an Amazon S3 bucket or an Amazon DynamoDB table'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HiveCopyActivity`：用于运行Hive `COPY` 查询，将数据从EMR集群复制到Amazon S3桶或Amazon DynamoDB表'
- en: '**Resources**: With the data nodes and activities selected, the next step in
    configuring a pipeline is selecting the right resource for executing the activity.
    AWS Data Pipeline supports two types of resources:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源**：选择数据节点和活动之后，配置管道的下一步是选择适当的资源来执行活动。AWS Data Pipeline 支持两种类型的资源：'
- en: '`Ec2Resource`: An EC2 instance is leveraged to execute the activity selected
    in the pipeline. This resource type is common for activities, such as `CopyActivity`,
    `ShellCommandActivity`, and so on.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ec2Resource`：EC2 实例用于执行管道中选定的活动。此资源类型常用于诸如`CopyActivity`、`ShellCommandActivity`等活动。'
- en: '`EmrCluster`: An Amazon EMR cluster is used to execute the activity selected
    in the pipeline. This resource is best suited for activities such as `EmrActivity`,
    `PigActivity`, `HiveActivity`, and so on.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EmrCluster`：Amazon EMR 集群用于执行管道中选定的活动。此资源最适用于诸如`EmrActivity`、`PigActivity`、`HiveActivity`等活动。'
- en: '**Actions**: Actions are certain steps that a pipeline takes whenever a *success*,
    *failure* or *late activity* event occurs. You can use actions as a way to monitor
    and notify the execution status of your pipeline; for example, send an SNS notification
    in case a `CopyActivity` fails, and so on.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：操作是在管道中，当发生*成功*、*失败*或*延迟活动*事件时，管道采取的某些步骤。您可以使用操作作为监控和通知管道执行状态的一种方式；例如，在`CopyActivity`失败时发送
    SNS 通知等。'
- en: With these concepts and terms done and dusted, let's move on to some hands-on
    action where we will be creating our very first simple and minimalistic pipeline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了这些概念和术语后，接下来让我们进行一些实际操作，创建我们第一个简单且简洁的管道。
- en: Getting started with AWS Data Pipeline
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 AWS Data Pipeline
- en: Creating your own pipeline is a fairly simple process, once you get to know
    the intricacies of working with the pipeline dashboard. In this section, we will
    be exploring the AWS Data Pipeline dashboard, its various functions, and editor
    to create a simple Hello World example pipeline. To start off, here are a few
    necessary prerequisite steps that you need to complete first, starting with a
    simple Amazon S3 bucket for storing all our data pipeline logs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 创建您自己的管道是一个相对简单的过程，一旦您了解了与管道仪表板互动的复杂性。在本节中，我们将探索 AWS 数据管道仪表板、它的各种功能和编辑器，以创建一个简单的“Hello
    World”示例管道。首先，以下是您需要首先完成的几个必要的先决步骤，从创建一个简单的 Amazon S3 存储桶来存储我们所有的数据管道日志开始。
- en: AWS Data Pipeline is only available in the EU (Ireland), Asia Pacific (Sydney),
    Asia Pacific (Tokyo), US East (N. Virginia), and the US West (Oregon) regions.
    For the purpose of the scenarios in this chapter, we will be using the US East
    (N. Virginia) region only.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Data Pipeline 仅在 EU（爱尔兰）、亚太地区（悉尼）、亚太地区（东京）、美国东部（北弗吉尼亚）和美国西部（俄勒冈）区域提供。为了本章中的情景，我们只使用美国东部（北弗吉尼亚）区域。
- en: From the AWS Management Console, launch the Amazon S3 console by either filtering
    the service name from the Filter option or navigating to this URL: [https://s3.console.aws.amazon.com/s3/home?region=us-east-1](https://s3.console.aws.amazon.com/s3/home?region=us-east-1).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从 AWS 管理控制台，您可以通过从“筛选”选项中筛选服务名称，或直接访问以下 URL 来启动 Amazon S3 控制台：[https://s3.console.aws.amazon.com/s3/home?region=us-east-1](https://s3.console.aws.amazon.com/s3/home?region=us-east-1)。
- en: Next, select the Create bucket option and provide a suitable value in the Bucket
    name field. Leave the rest of the fields to their default values and select Create to
    complete the process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，选择“创建存储桶”选项，并在“存储桶名称”字段中提供一个合适的值。将其他字段保持默认值，然后选择“创建”以完成该过程。
- en: 'With the log bucket created, the next prerequisite step involves the creation
    of a couple of IAM Roles that are required by AWS Data Pipeline for accessing
    resources, as well as what particular action it can perform over them. Since we
    are going to use the AWS Data Pipeline console for our first pipeline build, Data
    Pipeline provides two default IAM Roles that you can leverage out of the box:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 创建好日志存储桶后，接下来的先决步骤是创建几个 AWS Data Pipeline 所需的 IAM 角色，以便访问资源，以及它可以对这些资源执行的具体操作。由于我们将使用
    AWS Data Pipeline 控制台来构建我们的第一个管道，Data Pipeline 提供了两个默认的 IAM 角色，您可以直接使用：
- en: '`DataPipelineDefaultRole`: An IAM Role that grants AWS Data Pipeline access
    to all your AWS resources, including EC2, IAM, Redshift, S3, SNS, SQS and EMR.
    You can customize it to restrict the AWS services that Data Pipeline can access.
    Here is a snippet of the policy that is created:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataPipelineDefaultRole`：一个 IAM 角色，授予 AWS Data Pipeline 访问所有 AWS 资源的权限，包括
    EC2、IAM、Redshift、S3、SNS、SQS 和 EMR。您可以自定义该角色以限制 Data Pipeline 可以访问的 AWS 服务。以下是创建的策略片段：'
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`DataPipelineDefaultResourceRole`: This Role allows applications, scripts,
    or code executed on the Data Pipeline resources'' (EC2/EMR instances) access to
    your AWS resources:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataPipelineDefaultResourceRole`：此角色允许在数据管道资源（EC2/EMR 实例）上执行的应用程序、脚本或代码访问您的
    AWS 资源：'
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With the prerequisites out of the way, let''s now move on to creating our very
    first pipeline:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前提条件后，现在让我们开始创建我们的第一个管道：
- en: From the AWS Management Console, filter out Data Pipeline using the Filter option
    or alternatively, selecting this URL provided here [https://console.aws.amazon.com/datapipeline/home?region=us-east-1](https://console.aws.amazon.com/datapipeline/home?region=us-east-1).
    Select the Get started now option.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 AWS 管理控制台中，使用“筛选”选项过滤出数据管道，或者直接选择此处提供的URL [https://console.aws.amazon.com/datapipeline/home?region=us-east-1](https://console.aws.amazon.com/datapipeline/home?region=us-east-1)。选择“立即开始”选项。
- en: This will bring up the Create Pipeline wizard as displayed. Start by providing
    a suitable name for the pipeline using the Name field followed by an optional
    Description.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将弹出显示创建管道向导。首先，使用“名称”字段提供管道的合适名称，然后可选地填写描述。
- en: Next, select the Build using Architect option from the Source field.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，从源字段中选择“使用架构师构建”选项。
- en: 'AWS Data Pipeline provides different ways for creating pipelines. You can leverage
    either one of the several pre-built templates using the Build using a template option,
    or opt for a more customized approach by selecting the Import a definition option,
    where you can create and upload your own data pipeline definitions. Finally, you
    can use the data pipeline architect mode to drag-drop and customize your pipeline
    using a simple intuitive dashboard, which is what we are going to do in this use
    case:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 数据管道提供了多种创建管道的方法。你可以使用“通过模板构建”选项利用多个预构建的模板，或者选择更定制化的方式，选择“导入定义”选项，在此可以创建并上传自己的数据管道定义。最后，你可以使用数据管道架构师模式，通过一个简单直观的仪表板拖放并定制你的管道，这也是我们在此用例中要做的：
- en: '![](img/e3c7a76f-81bd-4ead-9a72-b8d59c77cfc5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3c7a76f-81bd-4ead-9a72-b8d59c77cfc5.png)'
- en: Moving on, you can also schedule the run of your pipeline by selecting the correct
    option, provided under the Schedule section. For now, select the On pipeline activation option,
    as we want our pipeline to start its execution only when it is first activated.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你还可以通过选择调度部分提供的正确选项来安排管道的运行。目前，选择“管道激活时运行”选项，因为我们希望管道仅在首次激活时才开始执行。
- en: Next, browse and select the correct *S3 bucket* for logging the data pipelines'
    logs using the S3 location for logs option. This should be the same bucket that
    was created during the prerequisite section of this scenario.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，浏览并选择正确的*S3 桶*，通过“S3日志位置”选项来记录数据管道的日志。此桶应该与在本场景的前提部分中创建的桶相同。
- en: Optionally, you can also provide your custom IAM Roles for Data Pipeline by
    selecting the Custom option provided under the Security/Access section. In this
    case, we have gone ahead and selected the Default IAM Roles themselves.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，你也可以通过在安全/访问部分选择提供的自定义选项，为数据管道提供自定义IAM角色。在这种情况下，我们已经选择了默认的IAM角色。
- en: Once all the required fields are populated, select the Edit in Architect option
    to continue.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦填充了所有必需的字段，选择“在架构师中编辑”选项继续。
- en: With this step completed, you should see the *architect* view of your current
    pipeline as depicted. By default, you will only have a single box called Configuration displayed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此步骤后，你应该能看到当前管道的*架构师*视图。如图所示，默认情况下，你将只看到一个名为“配置”的框。
- en: 'Select the Configuration box to view the various configuration options required
    by your pipeline to run. This information should be visible on the right-hand
    side navigation pane under the Others section, as shown in the following screenshot:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择配置框以查看管道运行所需的各种配置选项。此信息应在右侧导航窗格的“其他”部分中可见，如下图所示：
- en: '![](img/ccd00977-34f2-4d55-8629-601c4a4faebe.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccd00977-34f2-4d55-8629-601c4a4faebe.png)'
- en: You can use this Configuration to edit your pipeline's Resource Role, Pipeline
    Log Uri, Schedule Type, and many other such settings as well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用此配置来编辑管道的资源角色、管道日志 URI、调度类型以及其他设置。
- en: To add Resources and Activities to your pipeline, select the Add drop-down list
    as shown. Here, select `ShellCommandActivity` to get started. We will use this
    activity to echo a simple Hello World message for starters.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将资源和活动添加到管道中，选择所示的添加下拉列表。在此，选择`ShellCommandActivity`开始。我们将使用此活动来回显一个简单的“Hello
    World”消息作为开始。
- en: Once the `ShellCommandActivity` option is selected, you should be able to see
    its corresponding configuration items in the adjoining navigation pane under the
    Activities tab.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦选择了`ShellCommandActivity`选项，你应该能够在相邻的导航窗格的“活动”选项卡下看到相应的配置项。
- en: Type in a suitable Name for your activity. Next, from the Type section, select
    the Add an optional field drop-down list and select the Command option as shown.
    In the new Command field, type `echo "This is just a Hello World message!"`.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入适当的活动名称。接下来，从类型部分，选择“添加可选字段”下拉列表，并选择“命令”选项，如图所示。在新的命令字段中，输入`echo "This is
    just a Hello World message!"`。
- en: With the activity in place, the final step left is to provide and associate
    a resource to the pipeline. The resource will execute the `ShellCommandActivity` on
    either an EC2 instance or an EMR instance.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置活动后，最后一步是为流水线提供并关联资源。该资源将在 EC2 实例或 EMR 实例上执行`ShellCommandActivity`。
- en: To create and associate a resource, from the Activities section, select the
    Add an optional field option once again and from the drop-down list, select the
    Runs On option. Using the Runs On option, you can create and select Resources
    for executing the task for your pipeline.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建和关联资源，从“活动”部分再次选择“添加可选字段”选项，并从下拉列表中选择“运行于”选项。使用“运行于”选项，您可以为流水线执行任务创建和选择资源。
- en: 'Select the Create new: Resource option to get started. This will create a new
    resource named `DefaultResource1,` as depicted in the following screenshot:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择“创建新的：资源”选项以开始。这将创建一个名为`DefaultResource1`的新资源，如下图所示：
- en: '![](img/64117e3f-7ddc-421c-a197-4cfe985937e0.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64117e3f-7ddc-421c-a197-4cfe985937e0.png)'
- en: Select the newly created resource or alternatively, select the Resources option
    from the navigation pane to view and add resource specific configurations.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择新创建的资源，或者从导航窗格中选择“资源”选项以查看并添加特定于资源的配置。
- en: 'Fill in the following information as depicted in the previous screenshot in
    the Resources section of your pipeline:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在流水线的“资源”部分填写以下信息，如之前截屏所示：
- en: 'Name: Provide a suitable name for your new resource.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名称：为您的新资源提供一个合适的名称。
- en: 'Type: Select the Ec2Resource option from the drop-down list.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型：从下拉列表中选择`Ec2Resource`选项。
- en: 'Role/Resource Role: You can choose to provide different IAM Roles, however
    I have opted to go for the default pipeline roles itself.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角色/资源角色：您可以选择不同的 IAM 角色，但我选择了默认的流水线角色。
- en: 'Instance Type: Type in `t1.micro` in the adjoining field. If you do not provide
    or select the instance type field, the resource will launch a **m1.medium** instance
    by default.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例类型：在相邻字段中键入`t1.micro`。如果未提供或选择实例类型字段，则资源将默认启动**m1.medium**实例。
- en: 'Terminate After: Select the appropriate time after which the instance should
    be terminated. In this case, I have selected to terminate after `10` minutes.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终止后：选择适当的时间，在这种情况下，我选择在`10`分钟后终止。
- en: 'Here''s a screenshot of what the final pipeline would look like once the Resources section
    is filled out:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终流水线填写“资源”部分后的屏幕截图：
- en: '![](img/61a83fb0-3221-45c2-95c6-ae3fe57d605d.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61a83fb0-3221-45c2-95c6-ae3fe57d605d.png)'
- en: Once the pipeline is ready, click on Save to save the changes made. Selecting
    the Save option automatically compiles your pipeline and checks for any errors
    as well. If any errors are found, they will be displayed in the Errors/Warnings section.
    If no errors are reported, click on Activate to finally activate your pipeline.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦流水线准备就绪，点击“保存”保存所做的更改。选择“保存”选项将自动编译您的流水线并检查任何错误。如果发现任何错误，它们将显示在“错误/警告”部分。如果没有报告错误，请点击“激活”最终激活您的流水线。
- en: The pipeline takes a few minutes to transition from WAITING_FOR_RUNNER state
    to a FINISHED state. This process involves first spinning up the EC2 instance
    or resource, which we defined in the pipeline. Once the resource is up and running,
    Data Pipeline will automatically install the *task runner* on this particular
    resource, as Data Pipeline itself manages it. With the task runner installed,
    it starts polling the data pipeline for pending activities and executes them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线需要几分钟来从“等待运行程序”状态转换为“完成”状态。这个过程首先涉及启动我们在流水线中定义的 EC2 实例或资源。一旦资源启动并运行，数据管道将自动在这个特定资源上安装*任务运行器*，因为数据管道本身管理它。安装了任务运行器后，它开始轮询数据管道以执行挂起的活动。
- en: Once the pipeline's status turns to FINISHED, expand the pipeline's component
    name and select the Attempts tab, as shown. If not specified, Data Pipeline will
    try and execute your pipeline for a default three attempts before it finally stops
    the execution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当流水线状态变为“完成”时，展开流水线组件名称并选择“尝试”选项卡，如所示。如果未指定，数据管道将尝试在最终停止执行之前默认执行三次您的流水线。
- en: 'For each attempt, you can view the corresponding Activity Logs, Stdout as well
    as the Stderr messages:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次尝试，你可以查看相应的活动日志、Stdout以及Stderr消息：
- en: '![](img/cc482e00-5aad-413c-83fb-61841a2ed776.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc482e00-5aad-413c-83fb-61841a2ed776.png)'
- en: Select the Stdout option to view your Hello World message! Et voila! Your first
    pipeline is up and running!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 选择Stdout选项查看你的Hello World消息！瞧！你的第一个管道已启动并运行！
- en: Feel free to try out a few other options for your pipeline by simply selecting
    the pipeline name and click on the Edit Pipeline option. You can also export your
    pipeline's definition by selecting the pipeline name and from the Actions tab,
    opting for the Export option.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试其他管道选项，只需选择管道名称并点击“编辑管道”选项。你还可以通过选择管道名称并在“操作”选项卡中选择“导出”选项来导出管道的定义。
- en: Pipeline definitions are a far better and easier way of creating pipelines if
    you are a fan of working with JSON and CLI interfaces. They offer better flexibility
    and usability as compared to the standard pipeline dashboard which can take time
    to get used to for beginners. With this in mind, in the next section we will be
    exploring a few basics on how you can get started by creating your very own pipeline
    definition file.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢使用JSON和CLI接口，管道定义是一种比标准管道仪表板更好、更简单的创建管道方式。与可能需要时间适应的标准管道仪表板相比，它们提供了更好的灵活性和可用性。考虑到这一点，在接下来的部分中，我们将探讨一些基本内容，帮助你通过创建自己的管道定义文件入门。
- en: Working with data pipeline definition Files
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据管道定义文件
- en: The AWS Data Pipeline console provides us with three different options to get
    started with creating a new pipeline. You could use the architect mode, which
    is exactly what we ended up working with in the earlier section, or alternatively,
    use any one of the pre-defined templates as a boilerplate and build your pipeline
    fro them. Last but not the least, the console also provides you with an ability
    to upload your very own pipeline definition file, which is basically a collection
    of various pipeline objects and conditions written in a JSON format. In this section,
    we will be learning how to write our very own pipeline definitions and later,
    use the same for building a custom pipeline as well.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Data Pipeline控制台提供了三种不同的选项来开始创建新管道。你可以使用架构模式，这正是我们在前面部分使用的方式，或者选择任何一个预定义的模板作为样板，从中构建你的管道。最后但同样重要的是，控制台还允许你上传自己的管道定义文件，它基本上是一个由多个管道对象和条件组成的JSON格式文件。在本部分中，我们将学习如何编写自己的管道定义，并随后使用它们来构建自定义管道。
- en: 'To start, you will need two components to build up a pipeline definition file:
    objects and fields:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要两个组件来构建管道定义文件：对象和字段：
- en: '**Objects**: An object is an individual component required to build a pipeline.
    These can be data nodes, conditions, activities, resources, schedules, and so
    on.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象**：对象是构建管道所需的单个组件。这些可以是数据节点、条件、活动、资源、计划等。'
- en: '**Fields**: Each object is described by one or more fields. The fields are
    made up of key-value pairs that are enclosed in double quotes and separated by
    a colon.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段**：每个对象由一个或多个字段描述。字段由键值对组成，键值对用双引号括起来，并通过冒号分隔。'
- en: 'Here is a skeleton structure of a pipeline definition file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个管道定义文件的基本结构：
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is a look at the pipeline definition file obtained by exporting the Hello
    World pipeline example that we performed a while back:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们之前执行的Hello World管道示例导出的管道定义文件：
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can find the complete copy of code at [https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2)找到完整的代码副本。
- en: 'Each object generally contains an `id`, `name`, and `type` fields that are
    used to describe it and its functionality. For example, the `Resource` object
    in the Hello World scenario contains the following values:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每个对象通常包含一个`id`、`name`和`type`字段，用于描述对象及其功能。例如，在Hello World场景中，`Resource`对象包含以下值：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can also find the same fields in both the `ShellCommandActivity,` as well
    as the default configurations objects.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在`ShellCommandActivity`和默认配置对象中找到相同的字段。
- en: 'A pipeline object can refer to other objects within the same pipeline using
    the `"ref" : "ID_of_referred_resource"` field. Here is an example of the `ShellCommandActivity` referencing
    to the EC2 resource, using the resource ID:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '一个管道对象可以使用`"ref" : "ID_of_referred_resource"`字段引用同一管道中的其他对象。以下是`ShellCommandActivity`引用EC2资源的示例，使用资源ID：'
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can additionally create custom or user-defined fields and refer them to
    other pipeline components, using the same syntax as described in the previous
    code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以创建自定义或用户定义的字段，并使用与前面代码中描述的相同语法，将它们引用到其他管道组件中：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can find the detailed references for data nodes, resources, activities,
    and other objects at [https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html)找到有关数据节点、资源、活动和其他对象的详细参考资料。
- en: Last but not the least; you can also leverage a parameterized template to customize
    the pipeline definition. Using this method, you can basically have one common
    pipeline definition and pass different values to it at the time of pipeline creation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，您还可以利用参数化模板来定制管道定义。使用这种方法，您基本上可以拥有一个通用的管道定义，并在管道创建时向其传递不同的值。
- en: 'To parametrize a pipeline definition you need to specify a variable using the
    following syntax:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要使管道定义参数化，您需要使用以下语法指定一个变量：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With the variable created, you can define its value in a separate `parameters` object
    which can be stored in the same pipeline definition file, or in a separate JSON
    file altogether as well. Consider the following example where we pass the same
    Hello World message in the `ShellCommandActivity` however, this time using a variable
    definition:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 创建变量后，您可以在单独的`parameters`对象中定义其值，该对象可以存储在相同的管道定义文件中，也可以完全存储在一个单独的JSON文件中。考虑以下示例，我们将相同的Hello
    World消息传递给`ShellCommandActivity`，不过这次使用了变量定义：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once the variable is defined, we pass its corresponding values and expression
    in a separate `parameters` object, as shown in the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦变量定义完成，我们将其对应的值和表达式传递到单独的`parameters`对象中，如下代码所示：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, the variable `myVariable` is a simple string type and we have
    also provided it with a default value, in case a value is not provided to this
    variable at the time of the pipeline's creation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，变量`myVariable`是一个简单的字符串类型，并且我们还为其提供了一个默认值，以防在管道创建时未为此变量提供值。
- en: To know more about how to leverage and use variable and parameters in your pipeline
    definitions, visit [https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多有关如何在管道定义中利用和使用变量及参数的信息，请访问[https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html)。
- en: With this, we come towards the end of this section. In the next section, we
    will look at how you can leverage the AWS Data Pipeline to execute scripts and
    commands on remote EC2 instances using a parameterized pipeline definition.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，我们已经接近本节的结束。在下一节中，我们将探讨如何利用AWS数据管道，在使用参数化管道定义的情况下，在远程EC2实例上执行脚本和命令。
- en: Executing remote commands using AWS Data Pipeline
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS数据管道执行远程命令
- en: One of the best parts of working with Data Pipeline is that versatility of tasks
    that you can achieve by just using this one tool. In this section, we will be
    looking at a relatively simple pipeline definition using which you can execute
    remote scripts and commands on EC2 instances.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据管道的一个最大优点是，您可以仅通过这个工具实现多种任务。在本节中，我们将介绍一个相对简单的管道定义，您可以通过它在EC2实例上执行远程脚本和命令。
- en: 'How does this setup work? Well, to start with, we will be requiring one S3
    bucket (can be present in any AWS region) to be created that will store and act
    as a repository for all our shell scripts. Once the bucket is created, simply
    create and upload the following shell script to the bucket. Note however that
    in this case, the shell script is named `simplescript.sh` and the same name is
    used in the following pipeline definition, as well:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置是如何工作的呢？首先，我们需要创建一个S3桶（可以位于任何AWS区域），用于存储并作为所有Shell脚本的存储库。创建桶后，您只需创建并上传以下Shell脚本到该桶。请注意，在此案例中，Shell脚本名为`simplescript.sh`，并且在以下管道定义中也使用了相同的名称：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The script is pretty self-explanatory. It will print out a series of messages
    based on the EC2 instance it is launched from. You can substitute this script
    with any other shell script that can either be used to take backups of particular
    files, or archive existing files into a `tar.gz` and push it over to an awaiting
    S3 bucket for archiving, and so on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本相当不言自明。它将基于启动它的EC2实例打印一系列消息。您可以用任何其他用于备份特定文件或将现有文件存档为`tar.gz`并推送到等待存档的S3存储桶的Shell脚本替换它，等等。
- en: 'With the script file uploaded to the correct S3 bucket, the final step is to
    copy and paste the following pipeline definition in a file and upload it to Data
    Pipeline for execution:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将脚本文件上传到正确的S3存储桶后，最后一步是复制并粘贴以下流水线定义到一个文件中，并上传到数据管道以执行：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Remember to swap out the values for `<DATAPIPELINE_LOG_BUCKET>` and `<S3_BUCKET_SCRIPT_LOCATION>` with
    their corresponding actual values, and to save the file with a JSON extension.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 记得用对应的实际值替换`<DATAPIPELINE_LOG_BUCKET>`和`<S3_BUCKET_SCRIPT_LOCATION>`的值，并将文件保存为JSON扩展名。
- en: This particular pipeline definition relies on the `ShellCommandActivity` to
    first install the AWS CLI on the remote EC2 instance and then execute the shell
    script by copying it locally from the S3 bucket.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此流水线定义依赖于`ShellCommandActivity`，首先在远程EC2实例上安装AWS CLI，然后通过从S3存储桶本地复制执行Shell脚本。
- en: 'To upload the pipeline definition, use the AWS Data Pipeline console to create
    a new pipeline. In the Create Pipeline wizard, provide a suitable Name and Description for
    the new pipeline. Once done, select the Import a definition option from the Source field,
    as shown in the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要上传流水线定义，请使用AWS数据管道控制台创建新的流水线。在“创建流水线”向导中，为新流水线提供适当的名称和描述。完成后，从“源”字段中选择“导入定义”选项，如下截图所示：
- en: '![](img/d70473cc-a32e-44fe-80cb-00934f1f21b0.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d70473cc-a32e-44fe-80cb-00934f1f21b0.png)'
- en: Once the script loads, you should see the custom AWS CLI command in the Parameters section.
    With the pipeline definition successfully loaded, you can now choose to run the
    pipeline, either on a schedule or on activation. In my case, I have select to
    run the pipeline on activation itself, as this is for demo purposes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦脚本加载完成，您应该在“参数”部分中看到自定义AWS CLI命令。成功加载流水线定义后，您现在可以选择在计划或激活时运行流水线。在我的情况下，我选择在激活时运行流水线，因为这是为演示目的而设。
- en: Ensure that the *logging* is enabled for the new pipeline and the correct S3
    bucket for storing the pipeline's logs is mentioned. With all necessary fields
    filled, click on Activate to start up the pipeline.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 确保为新流水线启用了*日志记录*，并提到正确的S3存储桶来存储流水线的日志。填写所有必要的字段后，点击“激活”以启动流水线。
- en: Once again, the pipeline will transition from WAITING_FOR_RUNNER state to the
    FINISHED state. This usually takes a good minute or two to complete.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线将再次从“等待运行者”状态转换为“已完成”状态。通常需要一两分钟才能完成。
- en: 'From the Data Pipeline console, expand on the existing pipeline and select
    the Attempts tab as shown in the following screenshot. Here, click on Stdout to
    view the output of the script''s execution:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据管道控制台扩展现有流水线，并选择“尝试”选项卡，如下截图所示。在此处，点击“标准输出”以查看脚本执行的输出：
- en: '![](img/9197ccab-74f1-480a-b8ea-1fc1352168be.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9197ccab-74f1-480a-b8ea-1fc1352168be.png)'
- en: Once the output is viewed, you can optionally select the pipeline and click
    on Mark Finished option, as well. This will stop the pipeline from undertaking
    any further attempts on executions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 查看输出后，您可以选择流水线，然后点击“标记为完成”选项。这将阻止流水线进一步尝试执行。
- en: Simple, isn't it! You can use a similar method and approach to back up your
    files and execute some commands over managed instances. In the next section, we
    will be looking at one last pipeline definition example as well, that essentially
    helps us take periodic backups of content stored in one Amazon S3 bucket to another
    using both the Data Pipeline console, as well as the AWS CLI!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 简单，不是吗！您可以使用类似的方法和途径来备份您的文件并在托管实例上执行一些命令。在下一部分中，我们还将查看最后一个流水线定义示例，该示例基本上帮助我们定期备份存储在一个Amazon
    S3存储桶中的内容到另一个存储桶，同时使用数据管道控制台和AWS CLI！
- en: Backing up data using AWS Data Pipeline
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS数据管道备份数据
- en: One of the most widely used use cases for AWS Data Pipeline is its ability to
    synchronize and schedule backup jobs. You can use Data Pipeline to take backups
    of data stored within EC2 instances, EBS volumes, databases and even S3 buckets.
    In this section, we will walk through a simple, parameterized pipeline definition
    using which you can effectively schedule and perform backups of files stored within
    an Amazon S3 bucket.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 数据管道最常见的应用场景之一是其同步和调度备份任务的能力。你可以使用数据管道备份存储在 EC2 实例、EBS 卷、数据库甚至 S3 存储桶中的数据。在本节中，我们将介绍一个简单的、带参数的管道定义，你可以使用它来有效地调度和执行存储在
    Amazon S3 存储桶中的文件的备份。
- en: 'First up, let''s have a look at the pipeline definition file itself:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看看管道定义文件本身：
- en: You can find the complete copy of code at [https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2)
    找到完整的代码副本。
- en: 'To start with, we once again provide a list of *objects* that describe the
    pipeline components starting with a pipeline configuration object, as highlighted
    in the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们再次提供一份描述管道组件的*对象*列表，从管道配置对象开始，如以下代码所示：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we provide the definition for other pipeline objects, including the data
    nodes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提供其他管道对象的定义，包括数据节点：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this case, we are using the `#{VARIABLE_NAMES}` to declare a set of variables
    to make the pipeline definition more reusable. Once the data nodes are configured,
    we also have to define a set of actions that will trigger SNS alerts based on
    the pipeline''s success or failure. Here is a snippet of the same:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用 `#{VARIABLE_NAMES}` 来声明一组变量，使管道定义更加可重用。一旦数据节点配置完成，我们还需要定义一组操作，这些操作将在管道成功或失败时触发
    SNS 通知。以下是相关的代码片段：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With the objects defined, the second section requires the `parameters` to be
    set up, where each of the variables declared in the objects section are detailed
    and defined:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好对象后，第二部分需要设置`parameters`，在这里详细定义了在对象部分声明的每个变量：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With this in mind, let us first look at uploading this definition to AWS Data
    Pipeline using the web console:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点，接下来我们首先来看看如何通过 AWS 数据管道的 Web 控制台上传这个定义：
- en: Log in to the AWS Data Pipeline console by navigating to this URL: [https://console.aws.amazon.com/datapipeline/home?region=us-east-1](https://console.aws.amazon.com/datapipeline/home?region=us-east-1).
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录 AWS 数据管道控制台，访问此网址：[https://console.aws.amazon.com/datapipeline/home?region=us-east-1](https://console.aws.amazon.com/datapipeline/home?region=us-east-1)。
- en: We have deployed all of our pipelines so far in the US East (N. Virginia) region
    itself. You can opt to change the region, as per your requirements.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将所有管道部署在美国东部（北弗吉尼亚）区域。你可以根据需求选择更改区域。
- en: 'Once done, select the Create Pipeline option to get started. In the Create
    Pipeline page, fill in a suitable Name and Description for the new pipeline:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，选择“创建管道”选项以开始。在“创建管道”页面中，为新管道填写合适的名称和描述：
- en: '![](img/85c9a9cb-c6cc-4882-bc75-c59cfb461042.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85c9a9cb-c6cc-4882-bc75-c59cfb461042.png)'
- en: Next, select the Import a definition option and click on the Load local file as
    shown. Copy and upload the JSON file definition here.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择“导入定义”选项，并点击“加载本地文件”如所示。将 JSON 文件定义复制并上传到此处。
- en: 'With the file uploaded, fill out the Parameters section as explained here:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传文件后，填写“参数”部分，如此处所述：
- en: '**S3 bucket path to data pipeline logs**: Browse and provide the bucket path
    for storing the pipeline''s logs.'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S3 存储桶路径到数据管道日志**：浏览并提供用于存储管道日志的存储桶路径。'
- en: '**Source file path**: Browse and select a file that you wish to backup from
    an Amazon S3 bucket.'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源文件路径**：浏览并选择一个你希望从 Amazon S3 存储桶备份的文件。'
- en: '**Destination (backup) file path**: Browse and select an Amazon S3 bucket path
    where you store the backed up file. You can optionally provide a backup folder
    name as well. Each file backed up to this location will follow a standard naming
    convention: `YYYY-MM-dd-HH-mm-ss.bak`.'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标（备份）文件路径**：浏览并选择一个存储备份文件的 Amazon S3 存储桶路径。你也可以选择提供一个备份文件夹名称。每个备份到此位置的文件将遵循标准命名规则：`YYYY-MM-dd-HH-mm-ss.bak`。'
- en: '**SNS Topic ARN**: Provide a valid SNS Topic ARN here. This ARN will be used
    to notify the user whether the pipeline''s execution was a success or a failure.'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SNS 主题 ARN**：在此提供一个有效的 SNS 主题 ARN。此 ARN 将用于通知用户管道执行的成功或失败。'
- en: '**EC2 instance type**: You can optionally provide a different EC2 instance
    type as a resource here. By default, it will take the t1.micro instance type.'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EC2 实例类型**：你可以选择提供一个不同的 EC2 实例类型作为资源。默认情况下，它将使用 t1.micro 实例类型。'
- en: '**EC2 instance termination:** Once again, you can provide a different instance
    termination value here. By default, it is set to 20 minutes. The termination time
    should be changed based on the approximate time taken to back up a file. The larger
    the file, the more time required to copy it and vice versa.'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EC2 实例终止**：你可以再次提供不同的实例终止值。默认情况下，设置为 20 分钟。终止时间应根据备份文件所需的时间来调整。文件越大，复制所需的时间就越长，反之亦然。'
- en: 'Once the parameter fields are populated, select the Edit in Architect option
    to view the overall components of the pipeline definition. You should see the
    following depiction:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦参数字段被填充，选择“在 Architect 中编辑”选项查看管道定义的整体组件。你应该能看到如下图示：
- en: '![](img/a55e9e11-4fca-48b0-b2cd-4b412bb2de0b.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a55e9e11-4fca-48b0-b2cd-4b412bb2de0b.png)'
- en: Click on Save to validate the pipeline for any errors. Once done, select Activate to
    start the pipeline's execution process.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击保存以验证管道是否有错误。完成后，选择激活以启动管道的执行过程。
- en: The pipeline takes a few minutes to transition from the WAITING_FOR_RUNNER state
    to the FINISHED state. Once done, check for the backed up file in your destination
    S3 folder.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道需要几分钟时间才能从 WAITING_FOR_RUNNER 状态过渡到 FINISHED 状态。完成后，检查目标 S3 文件夹中的备份文件。
- en: 'You can further tweak this particular pipeline definition to include entire
    S3 folder paths rather than just an individual file as performed now. Additionally,
    you can also change the start of the pipeline''s execution by changing the `scheduleType` from
    `ONDEMAND` to `Schedule,` as depicted in the following code snippet:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以进一步调整这个特定的管道定义，以包括整个 S3 文件夹路径，而不仅仅是当前执行的单个文件。此外，你还可以通过将 `scheduleType` 从
    `ONDEMAND` 改为 `Schedule` 来改变管道执行的启动方式，如下面的代码片段所示：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The following snippet will execute the pipeline every hour starting from March
    1, 2018 at 00:00:00 until April 1, 2018 00:00:00.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将在 2018 年 3 月 1 日 00:00:00 开始，每小时执行一次管道，直到 2018 年 4 月 1 日 00:00:00。
- en: To know more on how you can use the `Schedule` object, visit [https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多有关如何使用 `Schedule` 对象的信息，请访问 [https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html)。
- en: 'Now that the pipeline is up and running using the console, let us also have
    a look at a few simple AWS CLI commands using which you can achieve the same results:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，管道已经通过控制台启动并运行，我们还可以看看一些简单的 AWS CLI 命令，通过这些命令，你可以实现相同的结果：
- en: 'To start with, create a blank pipeline using the following command:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令创建一个空的管道：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `<UNIQUE_TOKEN>` can be any string of characters and is used to ensure idempotency
    during repeated calls to the `create-pipeline` command.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`<UNIQUE_TOKEN>` 可以是任何字符字符串，用于确保在多次调用 `create-pipeline` 命令时的幂等性。'
- en: 'Once the pipeline is created, you will be presented with the pipeline''s ID,
    as depicted in the following screenshot. Make a note of this ID as it will be
    required in the next steps:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建管道后，你将看到管道的 ID，如下图所示。请记下这个 ID，因为接下来的步骤中会用到它：
- en: '![](img/1df71e67-b390-4ef5-bebb-e5111b4de6c6.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1df71e67-b390-4ef5-bebb-e5111b4de6c6.png)'
- en: 'Next, we need to create three separate JSON files with the following content
    in them:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建三个单独的 JSON 文件，文件内容如下：
- en: '`pipeline.json`: Copy and paste only the object definitions in this file.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline.json`：仅复制并粘贴此文件中的对象定义。'
- en: '`parameters.json`: Copy and paste the parameter definitions here.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parameters.json`：在此处复制并粘贴参数定义。'
- en: '`values.json`: Create a new file that contains the values for the parameters
    ,as shown in the following code snippet. Remember to substitute the values in
    `<>` with those of your own:'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`values.json`：创建一个新文件，其中包含参数的值，如下代码片段所示。记得将 `<>` 中的值替换为你自己的值：'
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once done, save all three files and type in the following command to attach
    the pipeline definition to the newly created pipeline:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，保存所有三个文件，并输入以下命令将管道定义附加到新创建的管道上：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is a screenshot of the command''s output for your reference:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该命令输出的截图，供你参考：
- en: '![](img/aa4b8433-202c-4685-b60c-4849910f2c9d.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa4b8433-202c-4685-b60c-4849910f2c9d.png)'
- en: 'With the pipeline definition uploaded, the final step left is to activate the
    pipeline using the following command:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once the pipeline is activated, you can view its status and last runtimes,
    using the following command:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once the pipeline''s execution completes, you can deactivate and delete the
    pipeline using the following set of commands:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is a screenshot of the command''s output for your reference:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b933193-a3ec-4217-8eb6-055fbd02d9e7.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: With this, we come towards the end of yet another interesting chapter, but before
    we wind things up, here is a quick look at some important next steps that you
    should try out on your own.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Planning your next steps
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have covered quite a lot in this chapter, there is still a lot to
    be covered with Data Pipeline. One of the fastest and easiest ways to get started
    with Data Pipeline is by using one of the ready made pipeline definition templates.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'As of date, Data Pipeline provides the following list of ready-to-use templates,
    using which you can get started with your own pipeline in a matter of minutes:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62db5de5-3e07-4e1b-9f54-8e42ca233832.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: You can additionally use these definitions as templates for further customizing
    and enhancing your own as well. Simply create a pipeline using one of the previously
    depicted templates, however do not activate them. Edit the pipeline in the architect
    mode and simple export the pipeline definition locally. Once the template's pipeline
    definition is saved locally, you can make further changes and enhancements to
    it or simply reuse components within it to make your own pipeline, as well. The
    possibilities are endless!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Another cool feature provided by pipelines is the use of *spot instances* as
    task nodes. By default, pipelines provide only on-demand instances as resources
    for your task nodes. You can optionally switch to spot instances by simply selecting
    the Task instance Bid Price option from the Resources pane of your pipeline. Provide
    a suitable amount in the adjoining field (between 0 and 20.00) and there you have
    it! The next time the pipeline activates and a task is run, it will be performed
    based on the availability of spot instances.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, that brings us to the end of yet another amazing chapter. Let's quickly
    summarize the things we have learnt so far!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: First up, we started with a brief understanding of AWS Data Pipeline along with
    its concepts and terminologies. We later also learnt a bit about pipeline definitions
    and how easy it is to compose and use them. We even built our very first simple
    Hello World pipeline using a pipeline definition, followed by a series of examples
    that you can tweak and use, according to your own use cases. Towards the end,
    we also explored a few simple AWS CLI commands required to work with pipelines
    and topped it all off with a handy guide to some next steps as well.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we will be learning and exploring AWS's versatile
    and powerful IoT services, so stick around!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
