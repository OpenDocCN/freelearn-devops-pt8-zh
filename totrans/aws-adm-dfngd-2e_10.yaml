- en: Orchestrating Data using AWS Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored the AWS analytics suite of services by
    deep diving into Amazon EMR and Amazon Redshift services.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be continuing the trend and learning about an extremely
    versatile and powerful data orchestration and transformation service called AWS
    Data Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a quick look at the various topics that we will be covering in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing AWS Data Pipeline along with a quick look at some of its concepts
    and terminologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Data Pipeline using a simple Hello World example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the Data Pipeline definition file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing scripts and commands on remote EC2 instances using a data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing up data from one S3 bucket to another using a simple, parameterized
    data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building pipelines using the AWS CLI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So without any further ado, let's get started right away!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing AWS Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS Data Pipeline is an extremely versatile web service that allows you to
    move data back and forth between various AWS services, as well as on-premise data
    sources. The service is designed specifically to provide you with an in-built
    fault tolerance and highly available platform, using which you can define and
    build your very own custom data migration workflows. AWS Data Pipeline also provides
    add-on features such as scheduling, dependency tracking, and error handling, so
    that you do not have to waste extra time and effort in writing them on your own.
    This easy-to-use and flexible service, accompanied by its low operating costs,
    make the AWS Data Pipeline service ideal for use cases such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Migrating data on a periodic basis from an Amazon EMR cluster over to Amazon
    Redshift for data warehousing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incrementally loading data from files stored in Amazon S3 directly into an Amazon
    RDS database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copying data from an Amazon MySQL database into an Amazon Redshift cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing up data from an Amazon DynamoDB table to Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing up files stored in an Amazon S3 bucket on a periodic basis, and much
    more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will be understanding and learning a bit more about AWS
    Data Pipeline by first getting to know some of its internal components, concepts
    and terminologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core foundation of AWS Data Pipeline is, as the name suggests, a pipeline.
    You can create pipelines to schedule and run your data migration or transformation
    tasks. Each pipeline relies on a pipeline definition that essentially contains
    the business logic required to drive the data migration activities. We will be
    learning more about the data pipeline definition in the upcoming sections. For
    now, let''s dive a bit into a few essential pipeline concepts and components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline components**: A single pipeline can comprise of multiple sections,
    each having its own specific place in the overall functioning of the pipeline.
    For example, a pipeline can contain sections for specifying the input data source
    from where the data has to be collected, the activity that needs to be performed
    on this data along with a few necessary conditions, the time at which the activity
    has to be triggered, and so on. Each of these sections, individually, are called
    the pipeline''s components and are used together to build a pipeline definition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task runners**: Task runners are special applications or agents that carry
    out the task assigned in a pipeline. The task runners poll AWS Data Pipeline for
    any active tasks available. If found, the task is assigned to a task runner and
    executed. Once the execution completes, the task runner will report the status
    (either success or failure) back to AWS Data Pipeline. By default, AWS provides
    a default task runner for resources that are launched and managed by AWS Data
    Pipeline. You can also install the task runner on instances or on-premise servers
    that you manage:![](img/d75df4f2-8dfb-4f52-9416-473092ea08f6.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data nodes**: Data nodes are used to define the location and the type of
    input as well as output data for the pipeline. As of now, the following data nodes
    are provided by AWS Data Pipeline:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`S3DataNode`: Used to define an Amazon S3 location as an input or output for
    storing data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SqlDataNode`: Defines a SQL table or a database query for use in the pipeline'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RedshiftDataNode`: Used to define an Amazon Redshift table as input or output
    for the pipeline'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DynamoDBDataNode`: Used to specify a DynamoDB table as input or output for
    the pipeline'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activities**: With the data''s location and type selected using the data
    nodes, the next component left to define is the type of activity to be performed
    on that data. AWS Data Pipeline provides the following set of pre-packaged activities
    that you can use and extend as per your requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CopyActivity`: Used to copy data from one data node to another'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ShellCommandActivity`: Used to run a shell command as an activity'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SqlActivity`: Executes a SQL query on a data node such as `SqlDataNode` or
    `RedhsiftDataNode`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RedshiftCopyActivity`: A specific activity that leverages the `COPY` command
    to copy data between Redshift tables'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EmrActivity`: Used to run an EMR cluster'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PigActivity`: Used to run a custom Pig script on an EMR cluster'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HiveActivity`: Runs a Hive query on an EMR cluster'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HiveCopyActivity`: Used to run a Hive `COPY` query for copying the data from
    the EMR cluster to an Amazon S3 bucket or an Amazon DynamoDB table'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resources**: With the data nodes and activities selected, the next step in
    configuring a pipeline is selecting the right resource for executing the activity.
    AWS Data Pipeline supports two types of resources:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ec2Resource`: An EC2 instance is leveraged to execute the activity selected
    in the pipeline. This resource type is common for activities, such as `CopyActivity`,
    `ShellCommandActivity`, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EmrCluster`: An Amazon EMR cluster is used to execute the activity selected
    in the pipeline. This resource is best suited for activities such as `EmrActivity`,
    `PigActivity`, `HiveActivity`, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: Actions are certain steps that a pipeline takes whenever a *success*,
    *failure* or *late activity* event occurs. You can use actions as a way to monitor
    and notify the execution status of your pipeline; for example, send an SNS notification
    in case a `CopyActivity` fails, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these concepts and terms done and dusted, let's move on to some hands-on
    action where we will be creating our very first simple and minimalistic pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with AWS Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating your own pipeline is a fairly simple process, once you get to know
    the intricacies of working with the pipeline dashboard. In this section, we will
    be exploring the AWS Data Pipeline dashboard, its various functions, and editor
    to create a simple Hello World example pipeline. To start off, here are a few
    necessary prerequisite steps that you need to complete first, starting with a
    simple Amazon S3 bucket for storing all our data pipeline logs.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Data Pipeline is only available in the EU (Ireland), Asia Pacific (Sydney),
    Asia Pacific (Tokyo), US East (N. Virginia), and the US West (Oregon) regions.
    For the purpose of the scenarios in this chapter, we will be using the US East
    (N. Virginia) region only.
  prefs: []
  type: TYPE_NORMAL
- en: From the AWS Management Console, launch the Amazon S3 console by either filtering
    the service name from the Filter option or navigating to this URL: [https://s3.console.aws.amazon.com/s3/home?region=us-east-1](https://s3.console.aws.amazon.com/s3/home?region=us-east-1).
  prefs: []
  type: TYPE_NORMAL
- en: Next, select the Create bucket option and provide a suitable value in the Bucket
    name field. Leave the rest of the fields to their default values and select Create to
    complete the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the log bucket created, the next prerequisite step involves the creation
    of a couple of IAM Roles that are required by AWS Data Pipeline for accessing
    resources, as well as what particular action it can perform over them. Since we
    are going to use the AWS Data Pipeline console for our first pipeline build, Data
    Pipeline provides two default IAM Roles that you can leverage out of the box:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataPipelineDefaultRole`: An IAM Role that grants AWS Data Pipeline access
    to all your AWS resources, including EC2, IAM, Redshift, S3, SNS, SQS and EMR.
    You can customize it to restrict the AWS services that Data Pipeline can access.
    Here is a snippet of the policy that is created:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`DataPipelineDefaultResourceRole`: This Role allows applications, scripts,
    or code executed on the Data Pipeline resources'' (EC2/EMR instances) access to
    your AWS resources:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With the prerequisites out of the way, let''s now move on to creating our very
    first pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: From the AWS Management Console, filter out Data Pipeline using the Filter option
    or alternatively, selecting this URL provided here [https://console.aws.amazon.com/datapipeline/home?region=us-east-1](https://console.aws.amazon.com/datapipeline/home?region=us-east-1).
    Select the Get started now option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will bring up the Create Pipeline wizard as displayed. Start by providing
    a suitable name for the pipeline using the Name field followed by an optional
    Description.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, select the Build using Architect option from the Source field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AWS Data Pipeline provides different ways for creating pipelines. You can leverage
    either one of the several pre-built templates using the Build using a template option,
    or opt for a more customized approach by selecting the Import a definition option,
    where you can create and upload your own data pipeline definitions. Finally, you
    can use the data pipeline architect mode to drag-drop and customize your pipeline
    using a simple intuitive dashboard, which is what we are going to do in this use
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3c7a76f-81bd-4ead-9a72-b8d59c77cfc5.png)'
  prefs: []
  type: TYPE_IMG
- en: Moving on, you can also schedule the run of your pipeline by selecting the correct
    option, provided under the Schedule section. For now, select the On pipeline activation option,
    as we want our pipeline to start its execution only when it is first activated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, browse and select the correct *S3 bucket* for logging the data pipelines'
    logs using the S3 location for logs option. This should be the same bucket that
    was created during the prerequisite section of this scenario.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, you can also provide your custom IAM Roles for Data Pipeline by
    selecting the Custom option provided under the Security/Access section. In this
    case, we have gone ahead and selected the Default IAM Roles themselves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all the required fields are populated, select the Edit in Architect option
    to continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this step completed, you should see the *architect* view of your current
    pipeline as depicted. By default, you will only have a single box called Configuration displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the Configuration box to view the various configuration options required
    by your pipeline to run. This information should be visible on the right-hand
    side navigation pane under the Others section, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ccd00977-34f2-4d55-8629-601c4a4faebe.png)'
  prefs: []
  type: TYPE_IMG
- en: You can use this Configuration to edit your pipeline's Resource Role, Pipeline
    Log Uri, Schedule Type, and many other such settings as well.
  prefs: []
  type: TYPE_NORMAL
- en: To add Resources and Activities to your pipeline, select the Add drop-down list
    as shown. Here, select `ShellCommandActivity` to get started. We will use this
    activity to echo a simple Hello World message for starters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the `ShellCommandActivity` option is selected, you should be able to see
    its corresponding configuration items in the adjoining navigation pane under the
    Activities tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type in a suitable Name for your activity. Next, from the Type section, select
    the Add an optional field drop-down list and select the Command option as shown.
    In the new Command field, type `echo "This is just a Hello World message!"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the activity in place, the final step left is to provide and associate
    a resource to the pipeline. The resource will execute the `ShellCommandActivity` on
    either an EC2 instance or an EMR instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create and associate a resource, from the Activities section, select the
    Add an optional field option once again and from the drop-down list, select the
    Runs On option. Using the Runs On option, you can create and select Resources
    for executing the task for your pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the Create new: Resource option to get started. This will create a new
    resource named `DefaultResource1,` as depicted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/64117e3f-7ddc-421c-a197-4cfe985937e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Select the newly created resource or alternatively, select the Resources option
    from the navigation pane to view and add resource specific configurations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fill in the following information as depicted in the previous screenshot in
    the Resources section of your pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Name: Provide a suitable name for your new resource.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Type: Select the Ec2Resource option from the drop-down list.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Role/Resource Role: You can choose to provide different IAM Roles, however
    I have opted to go for the default pipeline roles itself.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instance Type: Type in `t1.micro` in the adjoining field. If you do not provide
    or select the instance type field, the resource will launch a **m1.medium** instance
    by default.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Terminate After: Select the appropriate time after which the instance should
    be terminated. In this case, I have selected to terminate after `10` minutes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a screenshot of what the final pipeline would look like once the Resources section
    is filled out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61a83fb0-3221-45c2-95c6-ae3fe57d605d.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the pipeline is ready, click on Save to save the changes made. Selecting
    the Save option automatically compiles your pipeline and checks for any errors
    as well. If any errors are found, they will be displayed in the Errors/Warnings section.
    If no errors are reported, click on Activate to finally activate your pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pipeline takes a few minutes to transition from WAITING_FOR_RUNNER state
    to a FINISHED state. This process involves first spinning up the EC2 instance
    or resource, which we defined in the pipeline. Once the resource is up and running,
    Data Pipeline will automatically install the *task runner* on this particular
    resource, as Data Pipeline itself manages it. With the task runner installed,
    it starts polling the data pipeline for pending activities and executes them.
  prefs: []
  type: TYPE_NORMAL
- en: Once the pipeline's status turns to FINISHED, expand the pipeline's component
    name and select the Attempts tab, as shown. If not specified, Data Pipeline will
    try and execute your pipeline for a default three attempts before it finally stops
    the execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each attempt, you can view the corresponding Activity Logs, Stdout as well
    as the Stderr messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc482e00-5aad-413c-83fb-61841a2ed776.png)'
  prefs: []
  type: TYPE_IMG
- en: Select the Stdout option to view your Hello World message! Et voila! Your first
    pipeline is up and running!
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to try out a few other options for your pipeline by simply selecting
    the pipeline name and click on the Edit Pipeline option. You can also export your
    pipeline's definition by selecting the pipeline name and from the Actions tab,
    opting for the Export option.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline definitions are a far better and easier way of creating pipelines if
    you are a fan of working with JSON and CLI interfaces. They offer better flexibility
    and usability as compared to the standard pipeline dashboard which can take time
    to get used to for beginners. With this in mind, in the next section we will be
    exploring a few basics on how you can get started by creating your very own pipeline
    definition file.
  prefs: []
  type: TYPE_NORMAL
- en: Working with data pipeline definition Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AWS Data Pipeline console provides us with three different options to get
    started with creating a new pipeline. You could use the architect mode, which
    is exactly what we ended up working with in the earlier section, or alternatively,
    use any one of the pre-defined templates as a boilerplate and build your pipeline
    fro them. Last but not the least, the console also provides you with an ability
    to upload your very own pipeline definition file, which is basically a collection
    of various pipeline objects and conditions written in a JSON format. In this section,
    we will be learning how to write our very own pipeline definitions and later,
    use the same for building a custom pipeline as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, you will need two components to build up a pipeline definition file:
    objects and fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Objects**: An object is an individual component required to build a pipeline.
    These can be data nodes, conditions, activities, resources, schedules, and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fields**: Each object is described by one or more fields. The fields are
    made up of key-value pairs that are enclosed in double quotes and separated by
    a colon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a skeleton structure of a pipeline definition file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a look at the pipeline definition file obtained by exporting the Hello
    World pipeline example that we performed a while back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can find the complete copy of code at [https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each object generally contains an `id`, `name`, and `type` fields that are
    used to describe it and its functionality. For example, the `Resource` object
    in the Hello World scenario contains the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can also find the same fields in both the `ShellCommandActivity,` as well
    as the default configurations objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pipeline object can refer to other objects within the same pipeline using
    the `"ref" : "ID_of_referred_resource"` field. Here is an example of the `ShellCommandActivity` referencing
    to the EC2 resource, using the resource ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can additionally create custom or user-defined fields and refer them to
    other pipeline components, using the same syntax as described in the previous
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can find the detailed references for data nodes, resources, activities,
    and other objects at [https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-pipeline-objects.html).
  prefs: []
  type: TYPE_NORMAL
- en: Last but not the least; you can also leverage a parameterized template to customize
    the pipeline definition. Using this method, you can basically have one common
    pipeline definition and pass different values to it at the time of pipeline creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To parametrize a pipeline definition you need to specify a variable using the
    following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With the variable created, you can define its value in a separate `parameters` object
    which can be stored in the same pipeline definition file, or in a separate JSON
    file altogether as well. Consider the following example where we pass the same
    Hello World message in the `ShellCommandActivity` however, this time using a variable
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the variable is defined, we pass its corresponding values and expression
    in a separate `parameters` object, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the variable `myVariable` is a simple string type and we have
    also provided it with a default value, in case a value is not provided to this
    variable at the time of the pipeline's creation.
  prefs: []
  type: TYPE_NORMAL
- en: To know more about how to leverage and use variable and parameters in your pipeline
    definitions, visit [https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-custom-templates.html).
  prefs: []
  type: TYPE_NORMAL
- en: With this, we come towards the end of this section. In the next section, we
    will look at how you can leverage the AWS Data Pipeline to execute scripts and
    commands on remote EC2 instances using a parameterized pipeline definition.
  prefs: []
  type: TYPE_NORMAL
- en: Executing remote commands using AWS Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the best parts of working with Data Pipeline is that versatility of tasks
    that you can achieve by just using this one tool. In this section, we will be
    looking at a relatively simple pipeline definition using which you can execute
    remote scripts and commands on EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'How does this setup work? Well, to start with, we will be requiring one S3
    bucket (can be present in any AWS region) to be created that will store and act
    as a repository for all our shell scripts. Once the bucket is created, simply
    create and upload the following shell script to the bucket. Note however that
    in this case, the shell script is named `simplescript.sh` and the same name is
    used in the following pipeline definition, as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The script is pretty self-explanatory. It will print out a series of messages
    based on the EC2 instance it is launched from. You can substitute this script
    with any other shell script that can either be used to take backups of particular
    files, or archive existing files into a `tar.gz` and push it over to an awaiting
    S3 bucket for archiving, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the script file uploaded to the correct S3 bucket, the final step is to
    copy and paste the following pipeline definition in a file and upload it to Data
    Pipeline for execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Remember to swap out the values for `<DATAPIPELINE_LOG_BUCKET>` and `<S3_BUCKET_SCRIPT_LOCATION>` with
    their corresponding actual values, and to save the file with a JSON extension.
  prefs: []
  type: TYPE_NORMAL
- en: This particular pipeline definition relies on the `ShellCommandActivity` to
    first install the AWS CLI on the remote EC2 instance and then execute the shell
    script by copying it locally from the S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'To upload the pipeline definition, use the AWS Data Pipeline console to create
    a new pipeline. In the Create Pipeline wizard, provide a suitable Name and Description for
    the new pipeline. Once done, select the Import a definition option from the Source field,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d70473cc-a32e-44fe-80cb-00934f1f21b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the script loads, you should see the custom AWS CLI command in the Parameters section.
    With the pipeline definition successfully loaded, you can now choose to run the
    pipeline, either on a schedule or on activation. In my case, I have select to
    run the pipeline on activation itself, as this is for demo purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the *logging* is enabled for the new pipeline and the correct S3
    bucket for storing the pipeline's logs is mentioned. With all necessary fields
    filled, click on Activate to start up the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the pipeline will transition from WAITING_FOR_RUNNER state to the
    FINISHED state. This usually takes a good minute or two to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Data Pipeline console, expand on the existing pipeline and select
    the Attempts tab as shown in the following screenshot. Here, click on Stdout to
    view the output of the script''s execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9197ccab-74f1-480a-b8ea-1fc1352168be.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the output is viewed, you can optionally select the pipeline and click
    on Mark Finished option, as well. This will stop the pipeline from undertaking
    any further attempts on executions.
  prefs: []
  type: TYPE_NORMAL
- en: Simple, isn't it! You can use a similar method and approach to back up your
    files and execute some commands over managed instances. In the next section, we
    will be looking at one last pipeline definition example as well, that essentially
    helps us take periodic backups of content stored in one Amazon S3 bucket to another
    using both the Data Pipeline console, as well as the AWS CLI!
  prefs: []
  type: TYPE_NORMAL
- en: Backing up data using AWS Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most widely used use cases for AWS Data Pipeline is its ability to
    synchronize and schedule backup jobs. You can use Data Pipeline to take backups
    of data stored within EC2 instances, EBS volumes, databases and even S3 buckets.
    In this section, we will walk through a simple, parameterized pipeline definition
    using which you can effectively schedule and perform backups of files stored within
    an Amazon S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'First up, let''s have a look at the pipeline definition file itself:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete copy of code at [https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2).
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we once again provide a list of *objects* that describe the
    pipeline components starting with a pipeline configuration object, as highlighted
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we provide the definition for other pipeline objects, including the data
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we are using the `#{VARIABLE_NAMES}` to declare a set of variables
    to make the pipeline definition more reusable. Once the data nodes are configured,
    we also have to define a set of actions that will trigger SNS alerts based on
    the pipeline''s success or failure. Here is a snippet of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With the objects defined, the second section requires the `parameters` to be
    set up, where each of the variables declared in the objects section are detailed
    and defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With this in mind, let us first look at uploading this definition to AWS Data
    Pipeline using the web console:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the AWS Data Pipeline console by navigating to this URL: [https://console.aws.amazon.com/datapipeline/home?region=us-east-1](https://console.aws.amazon.com/datapipeline/home?region=us-east-1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have deployed all of our pipelines so far in the US East (N. Virginia) region
    itself. You can opt to change the region, as per your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once done, select the Create Pipeline option to get started. In the Create
    Pipeline page, fill in a suitable Name and Description for the new pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/85c9a9cb-c6cc-4882-bc75-c59cfb461042.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, select the Import a definition option and click on the Load local file as
    shown. Copy and upload the JSON file definition here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the file uploaded, fill out the Parameters section as explained here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S3 bucket path to data pipeline logs**: Browse and provide the bucket path
    for storing the pipeline''s logs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source file path**: Browse and select a file that you wish to backup from
    an Amazon S3 bucket.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Destination (backup) file path**: Browse and select an Amazon S3 bucket path
    where you store the backed up file. You can optionally provide a backup folder
    name as well. Each file backed up to this location will follow a standard naming
    convention: `YYYY-MM-dd-HH-mm-ss.bak`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SNS Topic ARN**: Provide a valid SNS Topic ARN here. This ARN will be used
    to notify the user whether the pipeline''s execution was a success or a failure.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EC2 instance type**: You can optionally provide a different EC2 instance
    type as a resource here. By default, it will take the t1.micro instance type.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EC2 instance termination:** Once again, you can provide a different instance
    termination value here. By default, it is set to 20 minutes. The termination time
    should be changed based on the approximate time taken to back up a file. The larger
    the file, the more time required to copy it and vice versa.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the parameter fields are populated, select the Edit in Architect option
    to view the overall components of the pipeline definition. You should see the
    following depiction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a55e9e11-4fca-48b0-b2cd-4b412bb2de0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Save to validate the pipeline for any errors. Once done, select Activate to
    start the pipeline's execution process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pipeline takes a few minutes to transition from the WAITING_FOR_RUNNER state
    to the FINISHED state. Once done, check for the backed up file in your destination
    S3 folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can further tweak this particular pipeline definition to include entire
    S3 folder paths rather than just an individual file as performed now. Additionally,
    you can also change the start of the pipeline''s execution by changing the `scheduleType` from
    `ONDEMAND` to `Schedule,` as depicted in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The following snippet will execute the pipeline every hour starting from March
    1, 2018 at 00:00:00 until April 1, 2018 00:00:00.
  prefs: []
  type: TYPE_NORMAL
- en: To know more on how you can use the `Schedule` object, visit [https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-schedule.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the pipeline is up and running using the console, let us also have
    a look at a few simple AWS CLI commands using which you can achieve the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, create a blank pipeline using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `<UNIQUE_TOKEN>` can be any string of characters and is used to ensure idempotency
    during repeated calls to the `create-pipeline` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the pipeline is created, you will be presented with the pipeline''s ID,
    as depicted in the following screenshot. Make a note of this ID as it will be
    required in the next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1df71e67-b390-4ef5-bebb-e5111b4de6c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we need to create three separate JSON files with the following content
    in them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pipeline.json`: Copy and paste only the object definitions in this file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parameters.json`: Copy and paste the parameter definitions here.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values.json`: Create a new file that contains the values for the parameters
    ,as shown in the following code snippet. Remember to substitute the values in
    `<>` with those of your own:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once done, save all three files and type in the following command to attach
    the pipeline definition to the newly created pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a screenshot of the command''s output for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa4b8433-202c-4685-b60c-4849910f2c9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the pipeline definition uploaded, the final step left is to activate the
    pipeline using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the pipeline is activated, you can view its status and last runtimes,
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the pipeline''s execution completes, you can deactivate and delete the
    pipeline using the following set of commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a screenshot of the command''s output for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b933193-a3ec-4217-8eb6-055fbd02d9e7.png)'
  prefs: []
  type: TYPE_IMG
- en: With this, we come towards the end of yet another interesting chapter, but before
    we wind things up, here is a quick look at some important next steps that you
    should try out on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Planning your next steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have covered quite a lot in this chapter, there is still a lot to
    be covered with Data Pipeline. One of the fastest and easiest ways to get started
    with Data Pipeline is by using one of the ready made pipeline definition templates.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of date, Data Pipeline provides the following list of ready-to-use templates,
    using which you can get started with your own pipeline in a matter of minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62db5de5-3e07-4e1b-9f54-8e42ca233832.png)'
  prefs: []
  type: TYPE_IMG
- en: You can additionally use these definitions as templates for further customizing
    and enhancing your own as well. Simply create a pipeline using one of the previously
    depicted templates, however do not activate them. Edit the pipeline in the architect
    mode and simple export the pipeline definition locally. Once the template's pipeline
    definition is saved locally, you can make further changes and enhancements to
    it or simply reuse components within it to make your own pipeline, as well. The
    possibilities are endless!
  prefs: []
  type: TYPE_NORMAL
- en: Another cool feature provided by pipelines is the use of *spot instances* as
    task nodes. By default, pipelines provide only on-demand instances as resources
    for your task nodes. You can optionally switch to spot instances by simply selecting
    the Task instance Bid Price option from the Resources pane of your pipeline. Provide
    a suitable amount in the adjoining field (between 0 and 20.00) and there you have
    it! The next time the pipeline activates and a task is run, it will be performed
    based on the availability of spot instances.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, that brings us to the end of yet another amazing chapter. Let's quickly
    summarize the things we have learnt so far!
  prefs: []
  type: TYPE_NORMAL
- en: First up, we started with a brief understanding of AWS Data Pipeline along with
    its concepts and terminologies. We later also learnt a bit about pipeline definitions
    and how easy it is to compose and use them. We even built our very first simple
    Hello World pipeline using a pipeline definition, followed by a series of examples
    that you can tweak and use, according to your own use cases. Towards the end,
    we also explored a few simple AWS CLI commands required to work with pipelines
    and topped it all off with a handy guide to some next steps as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we will be learning and exploring AWS's versatile
    and powerful IoT services, so stick around!
  prefs: []
  type: TYPE_NORMAL
