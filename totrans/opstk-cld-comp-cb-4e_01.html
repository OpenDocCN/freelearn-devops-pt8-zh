<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;1.&#xA0;Installing OpenStack with Ansible"><div class="book" id="DB7S2-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01" class="calibre1"/>Chapter 1. Installing OpenStack with Ansible</h1></div></div></div><p class="calibre10">In this chapter, we will cover the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Introduction – the OpenStack architecture</li><li class="listitem">Host network configuration</li><li class="listitem">Root SSH keys configuration</li><li class="listitem">Installing Ansible, playbooks, and dependencies</li><li class="listitem">Configuring the installation</li><li class="listitem">Running the OpenStack-Ansible playbooks</li><li class="listitem">Troubleshooting the installation</li><li class="listitem">Manually testing the installation</li><li class="listitem">Modifying the OpenStack configuration</li><li class="listitem">Virtual lab - vagrant up!</li></ul></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Installing OpenStack with Ansible">
<div class="book" title="Introduction – the OpenStack architecture"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch01lvl1sec11" class="calibre1"/>Introduction – the OpenStack architecture</h1></div></div></div><p class="calibre10">OpenStack is a suite of projects that combine into a software-defined environment to be consumed <a id="id0" class="calibre1"/>using cloud friendly tools and techniques. The popular open source software allows users to easily consume compute, network, and storage resources that have been traditionally controlled by disparate methods and tools by various teams in IT departments, big and small. While consistency of APIs can be achieved between versions of OpenStack, an administrator is free to choose which features of OpenStack to install, and as such there is no single method or architecture to install the software. This flexibility can lead to confusion when choosing how to deploy OpenStack. That said, it is universally agreed that the services that the end users interact with—the OpenStack services, supporting software (such as the databases), and APIs—must be highly available.</p><p class="calibre10">A very popular method for installing OpenStack is the OpenStack-Ansible project (<a class="calibre1" href="https://github.com/openstack/openstack-ansible">https://github.com/openstack/openstack-ansible</a>). This method of installation allows <a id="id1" class="calibre1"/>an administrator to define highly available controllers together with arrays of compute and storage, and through the use of Ansible, deploy OpenStack in a very consistent way with a small amount of dependencies. Ansible is a tool that allows for system configuration and management that operates <a id="id2" class="calibre1"/>over standard SSH connections. Ansible itself has very few dependencies, and as it uses SSH to communicate, most Linux distributions and networks are well-catered for when it comes to using this tool. It is also very popular with many system administrators around the globe, so installing OpenStack on top of what they already know lowers the barrier to entry for setting up a cloud environment for their enterprise users.</p><p class="calibre10">OpenStack can be architected in any number of ways; OpenStack-Ansible doesn't address the architecture problem directly: users are free to define any number of controller services (such as Horizon, Neutron Server, Nova Server, and MySQL). Through experience at Rackspace and feedback from users, a popular architecture is defined, which is shown here:</p><div class="mediaobject"><img src="../images/00002.jpeg" alt="Introduction – the OpenStack architecture" class="calibre16"/><div class="caption"><p class="calibre21">Figure 1: Recommended OpenStack architecture used in this book</p></div></div><p class="calibre17"> </p><p class="calibre10">As shown <a id="id3" class="calibre1"/>in the preceding diagram (<span class="strong"><em class="calibre18">Figure 1</em></span>), there are a few concepts to first understand. These are described as follows.</p></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Installing OpenStack with Ansible">
<div class="book" title="Introduction – the OpenStack architecture">
<div class="book" title="Controllers"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec10" class="calibre1"/>Controllers</h2></div></div></div><p class="calibre10">The <span class="strong"><em class="calibre18">controllers</em></span> (also <a id="id4" class="calibre1"/>referred to as <span class="strong"><em class="calibre18">infrastructure or infra nodes</em></span>) run <a id="id5" class="calibre1"/>the heart of the OpenStack services and are the only servers exposed (via load balanced pools) to your end users. The <span class="strong"><em class="calibre18">controllers</em></span> run the API services, such as Nova API, Keystone API, and Neutron API, as well as the core supporting <a id="id6" class="calibre1"/>services such as <span class="strong"><strong class="calibre2">MariaDB</strong></span> for the database required to run OpenStack, and RabbitMQ for messaging. It is this reason why, in a production setting, these servers are set up as highly available as required. This means that these are deployed as clusters <a id="id7" class="calibre1"/>behind (highly available) load balancers, starting with a minimum of 3 in the cluster. Using odd numbers starting from 3 allows <a id="id8" class="calibre1"/>clusters to lose a single server without and affecting service and still remain with quorum (minimum numbers of votes needed). This means that when the unhealthy server comes back online, the data can be replicated from the remaining 2 servers (which are, between them, consistent), thus ensuring data consistency. </p><p class="calibre10">Networking is recommended to be highly resilient, so ensure that Linux has been configured to bond or aggregate the network interfaces so that in the event of a faulty switch port, or broken cable, your services remain available. An example networking configuration for Ubuntu can be found in <span class="strong"><em class="calibre18">Appendix A</em></span>.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Installing OpenStack with Ansible">
<div class="book" title="Introduction – the OpenStack architecture">
<div class="book" title="Computes"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec11" class="calibre1"/>Computes</h2></div></div></div><p class="calibre10">These are <a id="id9" class="calibre1"/>the servers that run the hypervisor or container service that OpenStack schedules workloads to when a user requests a Nova resource (such as a virtual machine). These are not too dissimilar to hosts running a hypervisor, <a id="id10" class="calibre1"/>such as ESXi or Hyper-V, and OpenStack Compute servers can be configured in a very similar way, optionally using shared storage. However, most installations of OpenStack forgo the need for the use of shared storage in the architecture. This small detail of not using shared storage, which implies the virtual machines run from the hard disks of the compute host itself, can have a large impact on the users of your OpenStack environment when it comes to discussing the resiliency of the applications in that environment. An environment set up like this pushes most of the responsibility for application uptime to developers, which gives the greatest flexibility of a long-term cloud strategy. When an application relies on the underlying infrastructure to be 100% available, the gravity imposed by the infrastructure ties applications to specific data center technology to keep it running. However, OpenStack can be configured to introduce shared storage such as Ceph (<a class="calibre1" href="http://ceph.com/">http://ceph.com/</a>) to allow for operational features such as live-migration (the ability to move running instances from one hypervisor to another with no downtime), allowing enterprise users move their applications to a cloud environment in a very safe way. These concepts will be discussed in more detail in later chapters on compute and storage. As such, the reference architecture for a compute node is to expect virtual machines to run locally on the hard drives in the server itself.</p><p class="calibre10">With regard to networking, like the <span class="strong"><em class="calibre18">controllers</em></span>, the network must also be configured to be highly available. A compute node that has no network available might be very secure, but it would be equally useless to a cloud environment! Configure bonded interfaces in the same <a id="id11" class="calibre1"/>way as the controllers. Further information <a id="id12" class="calibre1"/>for configuring bonded interfaces under Ubuntu can be found in <span class="strong"><em class="calibre18">Appendix A</em></span>.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Installing OpenStack with Ansible">
<div class="book" title="Introduction – the OpenStack architecture">
<div class="book" title="Storage"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch01lvl2sec12" class="calibre1"/>Storage</h2></div></div></div><p class="calibre10">Storage in <a id="id13" class="calibre1"/>OpenStack refers to block storage and object storage. Block storage (providing LUNs or <span class="strong"><em class="calibre18">hard drives</em></span> to virtual machines) is provided <a id="id14" class="calibre1"/>by the Cinder service, while object storage (API driven object or blobs of storage) is provided by Swift or Ceph. Swift and Ceph manage each individual drive in a server designated as an object storage node, very much like a RAID card manages individual drives in a typical server. Each drive is an independent entity that Swift or Ceph uses to write data to. For example, if a storage node has 24 x 2.5in SAS disks in, Swift or Ceph will be configured to write to any one of those 24 disks. Cinder, however, can use a multitude of backends to store data. For example, Cinder can be configured to communicate with third-party vendors such as NetApp or Solidfire arrays, or it can be configured to talk to Sheepdog or Ceph, as well as the simplest of services such as LVM. In fact, OpenStack can be configured in such a way that Cinder uses multiple backends so that a user is able to choose the storage applicable to the service they require. This gives great flexibility to both end users and operators as it means workloads can be targeted at specific backends suitable for that workload or storage requirement.</p><p class="calibre10">This book briefly covers Ceph as the backend storage engine for Cinder. Ceph is a very popular, highly available open source storage service. Ceph has its own disk requirements to give the best performance. Each of the Ceph storage nodes in the preceding diagram are referred to as <span class="strong"><strong class="calibre2">Ceph OSDs</strong></span> (<span class="strong"><strong class="calibre2">Ceph Object Storage Daemons</strong></span>). We recommend starting with 5 of these nodes, although this is not a hard and fast rule. Performance tuning of Ceph is beyond the scope of this book, but at a minimum, we would highly recommend having SSDs for Ceph journaling and either SSD or SAS drives for the OSDs (the physical storage units).</p><p class="calibre10">The differences between a Swift node and a Ceph node in this architecture are very minimal. Both require an interface (bonded for resilience) for replication of data in the storage cluster, as well as an interface (bonded for resilience) used for data reads and writes from the client or service consuming the storage.</p><p class="calibre10">The primary difference is the recommendation to use SSDs (or NVMe) as the journaling disks.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Installing OpenStack with Ansible">
<div class="book" title="Introduction – the OpenStack architecture">
<div class="book" title="Load balancing"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch01lvl2sec13" class="calibre1"/>Load balancing</h2></div></div></div><p class="calibre10">The end <a id="id15" class="calibre1"/>users of the OpenStack environment <a id="id16" class="calibre1"/>expect services to be highly available, and OpenStack provides REST API services to all of its features. This makes the REST API services very suitable for placing behind a load balancer. In most deployments, load balancers would usually be highly available hardware appliances such as F5. For the purpose of this book, we will <a id="id17" class="calibre1"/>be using HAProxy. The premise behind <a id="id18" class="calibre1"/>this is the same though—to ensure that the services are available so your end users can continue working in the event of a failed <span class="strong"><em class="calibre18">controller</em></span> node.</p></div></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Installing OpenStack with Ansible">
<div class="book" title="Introduction – the OpenStack architecture">
<div class="book" title="OpenStack-Ansible installation requirements"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_6"><a id="ch01lvl2sec14" class="calibre1"/>OpenStack-Ansible installation requirements</h2></div></div></div><p class="calibre10">Operating <a id="id19" class="calibre1"/>installing System: Ubuntu 16.04 x86_64</p><div class="book" title="Minimal data center deployment requirements"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch01lvl3sec01" class="calibre1"/>Minimal data center deployment requirements</h3></div></div></div><p class="calibre10">For a <a id="id20" class="calibre1"/>physical installation, the following will be needed:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Controller servers (also known as infrastructure nodes)<div class="book"><ul class="itemizedlist1"><li class="listitem">At least 64 GB RAM</li><li class="listitem">At least 300 GB disk (RAID)</li><li class="listitem">4 Network Interface Cards (for creating two sets of bonded interfaces; one would be used for infrastructure and all API communication, including client, and the other would be dedicated to OpenStack networking: Neutron)</li><li class="listitem">Shared storage, or object storage service, to provide backend storage for the base OS images used</li></ul></div></li><li class="listitem">Compute servers<div class="book"><ul class="itemizedlist1"><li class="listitem">At least 64 GB RAM</li><li class="listitem">At least 600 GB disk (RAID)</li><li class="listitem">4 Network Interface Cards (for creating two sets of bonded interfaces, used in the same way as the controller servers)</li></ul></div></li><li class="listitem">Optional (if using Ceph for Cinder) 5 Ceph Servers (Ceph OSD nodes)<div class="book"><ul class="itemizedlist1"><li class="listitem">At least 64 GB RAM</li><li class="listitem">2 x SSD (RAID1) 400 GB for journaling</li><li class="listitem">8 x SAS or SSD 300 GB (No RAID) for OSD (size up requirements and adjust accordingly) </li><li class="listitem">4 Network Interface Cards (for creating two sets of bonded interfaces; one for replication and the other for data transfer in and out of Ceph)</li></ul></div></li><li class="listitem">Optional (if using Swift) 5 Swift Servers<div class="book"><ul class="itemizedlist1"><li class="listitem">At least 64 GB RAM</li><li class="listitem">8 x SAS 300 GB (No RAID) (size up requirements and adjust accordingly)</li><li class="listitem">4 Network Interface Cards (for creating two sets of bonded interfaces; one for replication and the other for data transfer in and out of Swift)</li></ul></div></li><li class="listitem">Load <a id="id21" class="calibre1"/>balancers<div class="book"><ul class="itemizedlist1"><li class="listitem">2 physical load balancers configured as a pair</li><li class="listitem">Or 2 servers running HAProxy with a Keepalived VIP to provide as the API endpoint IP address:<div class="book"><ul class="itemizedlist2"><li class="listitem">At least 16 GB RAM</li><li class="listitem">HAProxy + Keepalived</li><li class="listitem">2 Network Interface Cards (bonded)</li></ul></div></li></ul></div></li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip02" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: Setting up a physical home lab? Ensure you have a managed switch so that interfaces can have VLANs tagged.</p></div></div></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Host network configuration"><div class="book" id="E9OE2-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec12" class="calibre1"/>Host network configuration</h1></div></div></div><p class="calibre10">Installation <a id="id22" class="calibre1"/>of OpenStack using an orchestration and configuration tool such as Ansible performs a lot of tasks that would otherwise have to be undertaken manually. However, we can only use an orchestration tool if the servers we are deploying to are configured in a consistent way and described to Ansible.</p><p class="calibre10">The following section will describe a typical server setup that uses two sets of active/passive bonded interfaces for use by OpenStack. Ensure that these are cabled appropriately.</p><p class="calibre10">We assume that the following physical network cards are installed in each of the servers; adjust them to suit your environment:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">p2p1</code> and <code class="email">p2p2</code></li><li class="listitem"><code class="email">p4p1</code> and <code class="email">p4p2</code></li></ul></div><p class="calibre10">We assume that the <span class="strong"><em class="calibre18">host</em></span> network is currently using <code class="email">p2p1</code>. The <span class="strong"><em class="calibre18">host</em></span> network is the basic network that each of the servers currently resides on, and it allows you to access each one over SSH. It is assumed that this network also has a default gateway configured, and allows internet access. There should be no other networks required at this point as the servers are currently unconfigured and are not running OpenStack services.</p><p class="calibre10">At the end of this section, we will have created the following bonded interfaces:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">bond0</code>: This consists of the physical interfaces <code class="email">p2p1</code> and <code class="email">p4p1</code>. The <code class="email">bond0</code> interface will be used for host, OpenStack management, and storage traffic.</li><li class="listitem"><code class="email">bond1</code>: This consists of the physical interfaces <code class="email">p2p2</code> and <code class="email">p4p2</code>. The <code class="email">bond1</code> interface will be used for Neutron networking within OpenStack.</li></ul></div><p class="calibre10">We will <a id="id23" class="calibre1"/>have created the following VLAN tagged interfaces:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">bond0.236</code>: This will be used for the <span class="strong"><em class="calibre18">container network</em></span></li><li class="listitem"><code class="email">bond0.244</code>: This will be used for the <span class="strong"><em class="calibre18">storage network</em></span></li><li class="listitem"><code class="email">bond1.240</code>: This will be used for the <span class="strong"><em class="calibre18">VXLAN tunnel network</em></span></li></ul></div><p class="calibre10">And the following bridges:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">br-mgmt</code>: This will use the <code class="email">bond0.236</code> VLAN interface, and will be configured with an IP address from the <code class="email">172.29.236.0/24</code> range.</li><li class="listitem"><code class="email">br-storage</code>: This will use the <code class="email">bond0.244</code> VLAN interface, and will be configured with an IP address from the <code class="email">172.29.244.0/24</code> range.</li><li class="listitem"><code class="email">br-vxlan</code>: This will use the <code class="email">bond1.240 VLAN</code> interface, and will be configured with an IP address from the <code class="email">172.29.240.0/24</code> range.</li><li class="listitem"><code class="email">br-vlan</code>: This will use the untagged <code class="email">bond1</code> interface, and will not have an IP address configured.</li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip03" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: Ensure that your subnets are large enough to support your current requirements as well as future growth!</p></div><p class="calibre10">The following diagram shows the networks, interfaces, and bridges set up before we begin our installation of OpenStack:</p><div class="mediaobject"><img src="../images/00003.jpeg" alt="Host network configuration" class="calibre16"/></div><p class="calibre17"> </p></div>

<div class="book" title="Host network configuration">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec15" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">We <a id="id24" class="calibre1"/>assume that each server has Ubuntu 16.04 installed.</p><p class="calibre10">Log in, as root, onto each server that will have OpenStack installed. </p></div></div>

<div class="book" title="Host network configuration">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec16" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre10">Configuration of the host's networking, on a Ubuntu system, is performed by editing the <code class="email">/etc/network/interfaces</code> file.</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First of all, ensure that we have the right network packages installed on each server. As we are using VLANs and Bridges, the following packages must be installed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">        apt update</strong></span>
<span class="strong"><strong class="calibre2">        apt install vlan bridge-utils</strong></span>
</pre></div></li><li class="listitem" value="2">Now edit the <code class="email">/etc/network/interfaces</code> file on the first server using your preferred editor:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">        vi /etc/network/interfaces</strong></span>
</pre></div></li><li class="listitem" value="3">We will first configure the bonded interfaces. The first part of the file will describe this. Edit this file so that it looks like the following to begin with:<div class="informalexample"><pre class="programlisting">    # p2p1 + p4p1 = bond0 (used for host, container and storage)
    auto p2p1
    iface p2p1 inet manual
      bond-master bond0
      bond-primary p2p1
    auto p4p1
    iface p4p1 inet manual
      bond-master bond0
    # p2p2 + p4p2 = bond1 (used for Neutron and Storage Replication)
    auto p2p2
    iface p2p2 inet manual
      bond-master bond1
      bond-primary p2p2
    auto p4p2
    iface p4p2 inet manual
      bond-master bond1</pre></div></li><li class="listitem" value="4">Now we will <a id="id25" class="calibre1"/>configure the VLAN interfaces that are tagged against these bonds. Continue editing the file to add in the following tagged interfaces. Note that we are not assigning IP addresses to the OpenStack bonds just yet:<div class="informalexample"><pre class="programlisting">    # We're using bond0 on a native VLAN for the 'host' network.
    # This bonded interface is likely to replace the address you
    # are currently using to connect to this host.
    auto bond0
    iface bond0 inet static
      address 192.168.100.11
      netmask 255.255.255.0
      gateway 192.168.100.1
      dns-nameserver 192.168.100.1 # Update to suit/ensure you can resolve DNS
    auto bond0.236  # Container VLAN
    iface bond0.236  inet manual
    auto bond1.240  # VXLAN Tunnel VLAN
    iface bond1.240  inet manual
    auto bond0.244  # Storage (Instance to Storage) VLAN
    iface bond0.244  inet manual</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip04" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: Use appropriate VLANs as required in your own environment. The VLAN tags used here are for reference only.</p><p class="calibre10">Ensure that the correct VLAN tag is configured against the correct bonded interface. <code class="email">bond0</code> is for host-type traffic, <code class="email">bond1</code> is predominantly for Neutron-based traffic, except for storage nodes, where it is then used for storage replication.</p></div></li><li class="listitem" value="5">We will now <a id="id26" class="calibre1"/>create the bridges, and place IP addresses on here as necessary (note that <code class="email">br-vlan</code> does not have an IP address assigned). Continue editing the same file and add in the following lines:<div class="informalexample"><pre class="programlisting"># Container bridge (br-mgmt)
auto br-mgmt
iface br-mgmt inet static
  address 172.29.236.11
  netmask 255.255.255.0
  bridge_ports bond0.236        
  bridge_stp off
# Neutron's VXLAN bridge (br-vxlan)
auto br-vxlan
iface br-vxlan inet static
  address 172.29.240.11
  netmask 255.255.255.0
  bridge_ports bond1.240       
  bridge_stp off
# Neutron's VLAN bridge (br-vlan)
auto br-vlan
iface br-vlan inet manual
  bridge_ports bond1
  bridge_stp off
# Storage Bridge (br-storage)
auto br-storage
iface br-storage inet static
  address 172.29.244.11
  netmask 255.255.255.0
  bridge_ports bond0.244
  bridge_stp off</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip05" class="calibre1"/>Tip</h3><p class="calibre10">These bridge names are referenced in the OpenStack-Ansible configuration file, so ensure you name them correctly.</p><p class="calibre10">Be careful in ensuring that the correct bridge is assigned to the correct bonded interface.</p></div></li><li class="listitem" value="6">Save and exit the file, then issue the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">restart networking</strong></span>
</pre></div></li><li class="listitem" value="7">As we are configuring our OpenStack environment to be as highly available as possible, it is suggested that you also reboot your server at this point to ensure the basic server, with redundant networking in place, comes back up as expected:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">reboot</strong></span>
</pre></div></li><li class="listitem" value="8">Now <a id="id27" class="calibre1"/>repeat this for each server on your network.</li><li class="listitem" value="9">Once all the servers are done, ensure that your servers can communicate with each other over these newly created interfaces and subnets. A test like the following might be convenient:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">apt install fping</strong></span>
<span class="strong"><strong class="calibre2">fping -a -g 172.29.236.0/24</strong></span>
<span class="strong"><strong class="calibre2">fping -a -g 172.29.240.0/24</strong></span>
<span class="strong"><strong class="calibre2">fping -a -g 172.29.244.0/24</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip06" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: We also recommend that you perform a network cable unplugging exercise to ensure that the failover from one active interface to another is working as expected.</p></div></div></div>

<div class="book" title="Host network configuration">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec17" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">We have configured the physical networking of our hosts to ensure a good known state and configuration for running OpenStack. Each of the interfaces configured here is specific to OpenStack—either directly managed by OpenStack (for example, <code class="email">br-vlan</code>) or used for inter-service communication (for example, <code class="email">br-mgmt</code>). In the former case, OpenStack utilizes the <code class="email">br-vlan</code> bridge and configures tagged interfaces on <code class="email">bond1</code> directly.</p><p class="calibre10">Note that the convention used here, of VLAN tag ID using a portion of the subnet, is only to highlight a separation of VLANs to specific subnets (for example, <code class="email">bond0.236</code> is used by the <code class="email">172.29.236.0/24</code> subnet). This VLAN tag ID is arbitrary, but must be set up in accordance with your specific networking requirements.</p><p class="calibre10">Finally, we performed a fairly rudimentary test of the network. This gives you the confidence that the network configuration that will be used throughout the life of your OpenStack cloud is fit for purpose and gives assurances in the event of a failure of a cable or network card.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Root SSH keys configuration" id="F8901-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec13" class="calibre1"/>Root SSH keys configuration</h1></div></div></div><p class="calibre10">Ansible <a id="id28" class="calibre1"/>is designed to help system administrators drive greater efficiency in the datacenter by being able to configure and operate many servers using orchestration playbooks. In order for Ansible to be able to fulfill its duties, it needs an SSH connection on the Linux systems it is managing. Furthermore, in order to have a greater degree of freedom and flexibility, a hands-off approach using SSH public private key pairs is required.</p><p class="calibre10">As the installation of OpenStack is expected to run as root, this stage expects the deployment host's root public key to be propagated across all servers.</p></div>

<div class="book" title="Root SSH keys configuration" id="F8901-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec18" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">Ensure that you are <code class="email">root</code> on the deployment host. In most cases, this is the first infrastructure <span class="strong"><em class="calibre18">controller</em></span> node that we have named for the purposes of this book to be called <code class="email">infra01</code>. We will be assuming that all Ansible commands will be run from this host, and that it expects to be able to connect to the rest of the servers on this network over the host network via SSH.</p></div></div>

<div class="book" title="Root SSH keys configuration" id="F8901-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec19" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre10">In order to allow a hands-free, orchestrated OpenStack-Ansible deployment, follow these steps to create and propagate root SSH public key of <code class="email">infra01</code> across all servers required of the installation:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">As root, execute the following command to create an SSH key pair:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ssh-keygen -t rsa -f ~/.ssh/id_rsa -N ""</strong></span>
</pre></div><p class="calibre22">The output should look similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">Generating public/private rsa key pair.</strong></span>
<span class="strong"><strong class="calibre2">Your identification has been saved in /root/.ssh/id_rsa.</strong></span>
<span class="strong"><strong class="calibre2">Your public key has been saved in /root/.ssh/id_rsa.pub.</strong></span>
<span class="strong"><strong class="calibre2">The key fingerprint is:</strong></span>
<span class="strong"><strong class="calibre2">SHA256:q0mdqJI3TTFaiLrMaPABBboTsyr3pRnCaylLU5WEDCw root@infra01</strong></span>
<span class="strong"><strong class="calibre2">The key's randomart image is:</strong></span>
<span class="strong"><strong class="calibre2">+---[RSA 2048]----+</strong></span>
<span class="strong"><strong class="calibre2">|ooo ..           |</strong></span>
<span class="strong"><strong class="calibre2">|E..o. .          |</strong></span>
<span class="strong"><strong class="calibre2">|=. . +           |</strong></span>
<span class="strong"><strong class="calibre2">|.=. o +          |</strong></span>
<span class="strong"><strong class="calibre2">|+o . o oS        |</strong></span>
<span class="strong"><strong class="calibre2">|+oo . .o       |</strong></span>
<span class="strong"><strong class="calibre2">|B=++.o+ +        |</strong></span>
<span class="strong"><strong class="calibre2">|*=B+oB.o         |</strong></span>
<span class="strong"><strong class="calibre2">|o+.o=.o          |</strong></span>
<span class="strong"><strong class="calibre2">+----[SHA256]-----+</strong></span>
</pre></div></li><li class="listitem" value="2">This has created two files in <code class="email">/root/.ssh</code>, called <code class="email">id_rsa</code> and <code class="email">id_rsa.pub</code>. The file, <code class="email">id_rsa</code> is the private key, and must not be copied across the network. It is not required to be anywhere other than on this server. The file, <code class="email">id_rsa.pub</code>, is the public key and can be shared to other servers on the network. If you have other nodes (for example, named infra02), use the following to copy this key to that node in your environment:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ssh-copy-id root@infra02</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip07" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: Ensure that you can resolve <code class="email">infra02</code> and the other servers, else amend the preceding command to use its host IP address instead.</p></div></li><li class="listitem" value="3">Now <a id="id29" class="calibre1"/>repeat step 2 for all servers on your network.</li><li class="listitem" value="4">Important: finally, ensure that you execute the following command to be able to SSH to itself:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ssh-copy-id root@infra01</strong></span>
</pre></div></li><li class="listitem" value="5">Test that you can <code class="email">ssh</code>, as the root user, from <code class="email">infra01</code> to other servers on your network. You should be presented with a Terminal ready to accept commands if successful, without being prompted for a passphrase. Consult <code class="email">/var/log/auth.log</code> on the remote server if this behavior is incorrect.</li></ol><div class="calibre20"/></div></div></div>

<div class="book" title="Root SSH keys configuration" id="F8901-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec20" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">We first generated a key pair file for use by SSH. The <code class="email">-t</code> option specified the <code class="email">rsa</code> type encryption, <code class="email">-f</code> specified the output of the private key, where the public portion will get .<code class="email">pub</code> appended to its name, and <code class="email">-N ""</code> specified that no passphrase is to be used on this key. Consult your own security standards if the presented options differ from your company's requirements.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Installing Ansible, playbooks, and dependencies" id="G6PI1-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec14" class="calibre1"/>Installing Ansible, playbooks, and dependencies</h1></div></div></div><p class="calibre10">In order <a id="id30" class="calibre1"/>for us to successfully install OpenStack using Ansible, we <a id="id31" class="calibre1"/>need to ensure that Ansible and any expected dependencies<a id="id32" class="calibre1"/> are installed on the deployment host. The OpenStack-Ansible project provides a handy script to do this for us, which is part of the version of OpenStack-Ansible we will be deploying.</p></div>

<div class="book" title="Installing Ansible, playbooks, and dependencies" id="G6PI1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec21" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">Ensure that you are <code class="email">root</code> on the deployment host. In most cases, this is the first infrastructure controller node, <code class="email">infra01</code>.</p><p class="calibre10">At this point, we will be checking out the version of OpenStack-Ansible from GitHub.</p></div></div>

<div class="book" title="Installing Ansible, playbooks, and dependencies" id="G6PI1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec22" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre10">To set up Ansible and its dependencies, follow these steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">We first need to use <code class="email">git</code> to check out the OpenStack-Ansible code from GitHub, so ensure that the following packages are installed (among other needed dependencies):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">apt update</strong></span>
<span class="strong"><strong class="calibre2">apt install git python-dev bridge-tools lsof lvm2 tcpdump build-   essential ntp ntpdate python-dev libyaml-dev libpython2.7-dev libffi-dev libssl-dev python-crypto python-yaml</strong></span>
</pre></div></li><li class="listitem" value="2">We then need to grab the OpenStack-Ansible code from GitHub. At the time of writing, the Pike release branch (16.X) is described as follows, but the steps remain the same for the foreseeable future. It is recommended that you use the latest stable tag by visiting <a class="calibre1" href="https://github.com/openstack/openstack-ansible/tags">https://github.com/openstack/openstack-ansible/tags</a>. Here we're using the latest 16 (Pike) tag denoted by <code class="email">16.0.5</code>:<div class="note" title="Note"><h3 class="title2"><a id="tip08" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: To use a branch of the Queens release, use the following: <code class="email">-b 17.0.0</code>. When the Rocky release is available, use <code class="email">-b 18.0.0</code>.</p></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">git clone -b 16.0.5 https://github.com/openstack/openstack-ansible.git /opt/openstack-ansible</strong></span>
</pre></div></li><li class="listitem" value="3">Ansible and the needed dependencies to successfully install OpenStack can be found in the <code class="email">/opt/openstack-ansible/scripts</code> directory. Issue the following command to bootstrap the environment:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible</strong></span>
<span class="strong"><strong class="calibre2">scripts/boot</strong></span>
<span class="strong"><strong class="calibre2">strap-ansible.sh</strong></span>
</pre></div></li></ol><div class="calibre20"/></div></div></div>

<div class="book" title="Installing Ansible, playbooks, and dependencies" id="G6PI1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec23" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">The <a id="id33" class="calibre1"/>OpenStack-Ansible project provides a handy script to ensure that <a id="id34" class="calibre1"/>Ansible and the right dependencies are installed on the deployment host. This script (<code class="email">bootstrap-ansible.sh</code>) lives in the <code class="email">scripts/</code> directory of the checked <a id="id35" class="calibre1"/>out OpenStack-Ansible code, so at this stage we need to grab the version we want to deploy using Git. Once we have the code, we can execute the script and wait for it to complete.</p></div></div>

<div class="book" title="Installing Ansible, playbooks, and dependencies" id="G6PI1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="There's more…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch01lvl2sec24" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre10">Visit <a class="calibre1" href="https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest">https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest</a> for more information.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Configuring the installation"><div class="book" id="H5A42-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec15" class="calibre1"/>Configuring the installation</h1></div></div></div><p class="calibre10">OpenStack-Ansible is a set of official Ansible playbooks and roles that lay down OpenStack <a id="id36" class="calibre1"/>with minimal prerequisites. Like any orchestration tool, most effort is done up front with configuration, followed by a hands-free experience when the playbooks are running. The result is a tried and tested OpenStack installation suitable for any size environment, from testing to production environments.</p><p class="calibre10">When we use OpenStack-Ansible, we are basically downloading the playbooks from GitHub onto a nominated <span class="strong"><em class="calibre18">deployment server</em></span>. A deployment server is the host that has access to all the machines in the environment via SSH (and for convenience, and for the most seamless experience without hiccups, via keys). This deployment server can be one of the machines you've nominated as a part of your OpenStack environment as Ansible isn't anything that takes up any ongoing resources once run.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip09" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: Remember to back up the relevant configuration directories related to OpenStack-Ansible before you rekick an install of Ubuntu on this server!</p></div></div>

<div class="book" title="Configuring the installation">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec25" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">Ensure that you are <code class="email">root</code> on the <span class="strong"><em class="calibre18">deployment host</em></span>. In most cases, this is the first infrastructure controller node, <code class="email">infra01</code>.</p></div></div>

<div class="book" title="Configuring the installation">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec26" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre10">Let's assume that you're using the first infrastructure node, <code class="email">infra01</code>, as the deployment server.</p><p class="calibre10">If you have not followed the preceding <span class="strong"><em class="calibre18">Installing Ansible, playbooks, and dependencies</em></span> recipe review, then as <code class="email">root</code>, carry out the following if necessary:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">git clone -b 16.05 https://github.com/openstack/openstack-ansible.git /opt/openstack-ansible</strong></span>
</pre></div><p class="calibre10">This downloads the OpenStack-Ansible playbooks to the <code class="email">/opt/openstack-ansible</code> directory.</p><p class="calibre10">To configure our OpenStack deployment, carry out the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">We first copy the <code class="email">etc/openstack_deploy</code> folder out of the downloaded repository to <code class="email">/etc</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible</strong></span>
<span class="strong"><strong class="calibre2">cp -R /etc/openstack_deploy /etc</strong></span>
</pre></div></li><li class="listitem" value="2">We now have to tell Ansible which servers will do which OpenStack function, by editing the <code class="email">/etc/openstack_deploy/openstack_user_config.yml</code> file, as shown here:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cp /etc/openstack_deploy/openstack_user_variables.yml.example /etc/openstack_deploy_openstack_user_variables.yml</strong></span>
<span class="strong"><strong class="calibre2">vi /etc/openstack_deploy/openstack_user_variables.yml</strong></span>
</pre></div></li><li class="listitem" value="3">The <a id="id37" class="calibre1"/>first section, <code class="email">cidr_networks</code>, describes the subnets used by OpenStack in this installation. Here we describe the <span class="strong"><em class="calibre18">container</em></span> network (each of the OpenStack services are run inside a container, and this has its own network so that each service can communicate with each other). We describe the tunnel network (when a user creates a tenant network in this installation of OpenStack, this will create a segregated VXLAN network over this physical network). Finally, we describe the storage network subnet. Edit this file so that it looks like the following:<div class="informalexample"><pre class="programlisting">  cidr_networks:
  container: 172.29.236.0/24
  tunnel: 172.29.240.0/24
  storage: 172.29.244.0/24</pre></div></li><li class="listitem" value="4">Continue editing the file to include any IP addresses that are already used by existing physical hosts in the environment where OpenStack will be deployed (and ensuring that you've included any reserved IP addresses for physical growth too). Include the addresses we have already configured leading up to this section. Single IP addresses or ranges (start and end placed either side of a ',') can be placed here. Edit this section to look like the following, adjust as per your environment and any reserved IPs:<div class="informalexample"><pre class="programlisting">used_ips:
  - "172.29.236.20"
  - "172.29.240.20"
  - "172.29.244.20"
  - "172.29.236.101,172.29.236.117"
  - "172.29.240.101,172.29.240.117"
  - "172.29.244.101,172.29.244.117"
  - "172.29.248.101,172.29.248.117"</pre></div></li><li class="listitem" value="5">The <code class="email">global_overrides</code> section describes the bridges and other specific details of the interfaces used environment—particularly pertaining to how the <a id="id38" class="calibre1"/>container network attaches to the physical network interfaces. For the example architecture used in this book, the following output can be used. In most cases, the content in this section doesn't need to be edited apart from the load balancer information at the start, so edit to suit:<div class="informalexample"><pre class="programlisting">global_overrides:
  internal_lb_vip_address: 172.29.236.117
  external_lb_vip_address: 192.168.100.117
  lb_name: haproxy
  tunnel_bridge: "br-vxlan"
  management_bridge: "br-mgmt"
  provider_networks:
    - network:
        group_binds:
          - all_containers
          - hosts
        type: "raw"
        container_bridge: "br-mgmt"
        container_interface: "eth1"
        container_type: "veth"
        ip_from_q: "management"
        is_container_address: true
        is_ssh_address: true
    - network:
        group_binds:
          - neutron_linuxbridge_agent
        container_bridge: "br-vxlan"
        container_type: "veth"
        container_interface: "eth10"
        ip_from_q: "tunnel"
        type: "vxlan"
        range: "1:1000"
        net_name: "vxlan"
    - network:
        group_binds:
          - neutron_linuxbridge_agent
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth11"
        type: "vlan"
        range: "1:1"
        net_name: "vlan"</pre></div></li><li class="listitem" value="6">The <a id="id39" class="calibre1"/>remaining section of this file describes which server each service runs from. Most of the sections repeat, differing only in the name of the service. This is fine as the intention here is to tell OpenStack-Ansible which server (we give it a name so that Ansible can refer to it by name, and reference the IP associated with it) runs the Nova API, RabbitMQ, or the Glance service, for example. As these particular example services run on our controller nodes, and in a production setting there are at least three controllers, you can quickly see why this information repeats. Other sections refer specifically to other services, such as OpenStack compute. For brevity, a couple of sections are shown here, but continue editing the file to match your networking:<div class="informalexample"><pre class="programlisting"># Shared infrastructure parts
shared-infra_hosts:
  controller-01:
    ip: 172.29.236.110
  controller-02:
    ip: 172.29.236.111
  controller-03:
    ip: 172.29.236.112
# Compute Hosts
compute_hosts:
  compute-01:
    ip: 172.29.236.113
  compute-02:
    ip: 172.29.236.114</pre></div></li><li class="listitem" value="7">Save and exit the file. We will now need to generate some random passphrases for the various services that run in OpenStack. In OpenStack, each service—such as Nova, Glance, and Neutron (which are described through the book)—themselves have to authenticate with Keystone, and be authorized to act as a service. To do so, their own user accounts need to have passphrases generated. Carry out the following command to generate the required passphrases, which would be later used when the OpenStack playbooks are executed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/scripts</strong></span>
<span class="strong"><strong class="calibre2">python pw-token-gen.py --file /etc/openstack_deploy/user_secrets.yml</strong></span>
</pre></div></li><li class="listitem" value="8">Finally, there is another file that allows you to fine-tune the parameters of the OpenStack services, such as which backing store Glance (the OpenStack Image service) will be using, as well as configure proxy services ahead of the installation. This file is called <code class="email">/etc/openstack_deploy/user_variables.yml</code>. Let's view and edit this file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /etc/openstack_deploy</strong></span>
<span class="strong"><strong class="calibre2">vi user_variables.yml</strong></span>
</pre></div></li><li class="listitem" value="9">In a typical, highly available deployment—one in which we have three controller nodes—we need to configure Glance to use a shared storage service <a id="id40" class="calibre1"/>so that each of the three controllers have the same view of a filesystem, and therefore the images used to spin up instances. A number of shared storage backend systems that Glance can use range from NFS to Swift. We can even allow a private cloud environment to connect out over a public network and connect to a public service like Rackspace Cloud Files. If you have Swift available, add the following lines to <code class="email">user_variables.yml</code> to configure Glance to use Swift:<div class="informalexample"><pre class="programlisting">glance_swift_store_auth_version: 3
glance_default_store: swift
glance_swift_store_auth_address: http://172.29.236.117:5000/v3
glance_swift_store_container: glance_images
glance_swift_store_endpoint_type: internalURL
glance_swift_store_key: '{{ glance_service_password }}'
glance_swift_store_region: RegionOne
glance_swift_store_user: 'service:glance'</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip10" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: Latest versions of OpenStack-Ansible are smart enough to discover if Swift is being used and will update their configuration accordingly.</p></div></li><li class="listitem" value="10">View the other commented out details in the file to see if they need editing to suit your environment, then save and exit. You are now ready to start the installation of OpenStack!</li></ol><div class="calibre20"/></div></div></div>

<div class="book" title="Configuring the installation">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec27" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">Ansible is a very popular server configuration tool that is well-suited to the task of installing OpenStack. Ansible takes a set of configuration files that <span class="strong"><em class="calibre18">Playbooks</em></span> (a defined set of steps that get executed on the servers) use to control how they executed. For OpenStack-Ansible, configuration is split into two areas: describing the physical environment and describing how OpenStack is configured.</p><p class="calibre10">The first configuration file, <code class="email">/etc/openstack_deploy/openstack_user_config.yml</code>, describes the physical environment. Each section is described here:</p><div class="informalexample"><pre class="programlisting">cidr_networks:
  container: 172.29.236.0/24
  tunnel: 172.29.240.0/24
  storage: 172.29.244.0/24</pre></div><p class="calibre10">This section <a id="id41" class="calibre1"/>describes the networks required for an installation based on OpenStack-Ansible. Look at the following diagram to see the different networks and subnets.</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Container</strong></span>: Each <a id="id42" class="calibre1"/>container that gets deployed gets an IP address from this subnet. The load balancer also takes an IP address from this range.</li><li class="listitem"><span class="strong"><strong class="calibre2">Tunnel</strong></span>: This <a id="id43" class="calibre1"/>is the subnet that forms the VXLAN tunnel mesh. Each container and compute host that participates in the VXLAN tunnel gets an IP from this range (the VXLAN tunnel is used when an operator creates a Neutron subnet that specifies the <code class="email">vxlan</code> type, which creates a virtual network over this underlying subnet). Refer to <a class="calibre1" title="Chapter 4. Neutron – OpenStack Networking" href="part0048_split_000.html#1DOR01-189e69df43a248268db97cde1b1a8e47">Chapter 4</a>, <span class="strong"><em class="calibre18">Neutron – OpenStack Networking</em></span> for more details on OpenStack networking.</li><li class="listitem"><span class="strong"><strong class="calibre2">Storage</strong></span>: This is <a id="id44" class="calibre1"/>the subnet that was used when a client instance spun up in OpenStack request Cinder block storage:<div class="informalexample"><pre class="programlisting">used_ips:
  - "172.29.236.20"
  - "172.29.240.20"
  - "172.29.244.20"
  - "172.29.236.101,172.29.236.117"
  - "172.29.240.101,172.29.240.117"
  - "172.29.244.101,172.29.244.117"
  - "172.29.248.101,172.29.248.117"</pre></div></li></ul></div><p class="calibre10">The <code class="email">used_ips:</code> section refers to IP addresses that are already in use on that subnet, or reserved for use by static devices. Such devices are load balancers or other hosts that are part of the subnets that OpenStack-Ansible would otherwise have randomly allocated to containers:</p><div class="informalexample"><pre class="programlisting">global_overrides:
  internal_lb_vip_address: 172.29.236.117
  external_lb_vip_address: 192.168.1.117
  tunnel_bridge: "br-vxlan"
  management_bridge: "br-mgmt"
  storage_bridge: "br-storage"</pre></div><p class="calibre10">The <code class="email">global_overrides:</code> section describes the details around how the containers and bridged networking are set up. OpenStack-Ansible's default documentation expects Linux Bridge to be used; however, Open vSwitch can also be used. Refer to <a class="calibre1" title="Chapter 4. Neutron – OpenStack Networking" href="part0048_split_000.html#1DOR01-189e69df43a248268db97cde1b1a8e47">Chapter 4</a>, <span class="strong"><em class="calibre18">Neutron – OpenStack Networking</em></span>, for more details.</p><p class="calibre10">The <code class="email">internal_lb_vip_address:</code> and <code class="email">external_lb_vip_address:</code> sections refer to the <span class="strong"><em class="calibre18">private</em></span> and <span class="strong"><em class="calibre18">public</em></span> sides of a typical load balancer. The private (<code class="email">internal_lb_vip_address</code>) is used by the services within OpenStack (for example, Nova calls communicating with the Neutron API would use <code class="email">internal_lb_vip_address</code>, whereas a user communicating <a id="id45" class="calibre1"/>with the OpenStack environment once it has been installed would use <code class="email">external_lb_vip_address</code>). See the following diagram:</p><div class="mediaobject"><img src="../images/00004.jpeg" alt="How it works…" class="calibre16"/></div><p class="calibre17"> </p><p class="calibre10">A number of load balance pools will be created for a given <span class="strong"><strong class="calibre2">Virtual IP</strong></span> (<span class="strong"><strong class="calibre2">VIP</strong></span>) address, describing the IP addresses and ports associated with a particular service, and for each pool—one will be created on the public/external network (in the example, a VIP address of <code class="email">192.168.100.117</code> has been created for this purpose), and another VIP for use internally by OpenStack (in the preceding example, the VIP address <code class="email">172.29.236.117</code> has been created for this purpose).</p><p class="calibre10">The <code class="email">tunnel_bridge:</code> section is the name given to the bridge that is used for attaching the physical interface that participates in the VXLAN tunnel network.</p><p class="calibre10">The <code class="email">management_bridge:</code> section is the name given to the bridge that is used for all of the OpenStack services that get installed on the container network shown in the diagram.</p><p class="calibre10">The <code class="email">storage_bridge:</code> section is the name given to the bridge that is used for traffic associated with attaching storage to instances or where Swift proxied traffic would flow.</p><p class="calibre10">Each of <a id="id46" class="calibre1"/>the preceding bridges must match the names you have configured in the <code class="email">/etc/network/interfaces</code> file on each of your servers.</p><p class="calibre10">The next section, <code class="email">provider_networks</code>, remains relatively static and untouched as it describes the relationship between container networking and the physical environment. Do not adjust this section.</p><p class="calibre10">Following the <code class="email">provider_networks</code> section are the sections describing which server or group of servers run a particular service. Each block has the following syntax:</p><div class="informalexample"><pre class="programlisting">service_name:
  ansible_inventory_name_for_server:
    IP_ADDRESS
  ansible_inventory_name_for_server:
    IP_ADDRESS</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip11" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: Ensure the correct and consistent spelling of each server name (<code class="email">ansible_inventory_name_for_server</code>) to ensure correct execution of your Ansible playbooks.</p></div><p class="calibre10">A number of sections and their use are listed here:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">shared-infra_hosts</code>: This supports the shared infrastructure software, which is MariaDB/Galera and RabbitMQ</li><li class="listitem"><code class="email">repo-infra_hosts</code>: This is the specific repository containers version of OpenStack-Ansible requested</li><li class="listitem"><code class="email">haproxy_hosts</code>: When using HAProxy for load balancing, this tells the playbooks where to install and configure this service</li><li class="listitem"><code class="email">os-infra_hosts</code>: These include OpenStack API services such as Nova API and Glance API</li><li class="listitem"><code class="email">log_hosts</code>: This is where the rsyslog server runs from</li><li class="listitem"><code class="email">identity_hosts</code>: These are the servers that run the Keystone (OpenStack Identity) Service</li><li class="listitem"><code class="email">storage-infra_hosts</code>: These are the servers that run the Cinder API service</li><li class="listitem"><code class="email">storage_hosts</code>: This is the section that describes Cinder LVM nodes</li><li class="listitem"><code class="email">swift-proxy_hosts</code>: These are the hosts that would house the Swift Proxy service</li><li class="listitem"><code class="email">swift_hosts</code>: These are the Swift storage nodes</li><li class="listitem"><code class="email">compute_hosts</code>: This is the list of servers that make up your hypervisors</li><li class="listitem"><code class="email">image_hosts</code>: These are the servers that run the Glance (OpenStack Image) Service</li><li class="listitem"><code class="email">orchestration_hosts</code>: These are the servers that run the Heat API (OpenStack Orchestration) services</li><li class="listitem"><code class="email">dashboard_hosts</code>: These are the servers <a id="id47" class="calibre1"/>that run the Horizon (OpenStack Dashboard) service</li><li class="listitem"><code class="email">network_hosts</code>: These are the servers that run the Neutron (OpenStack Networking) agents and services</li><li class="listitem"><code class="email">metering-infra_hosts</code>: These are the servers that run the Ceilometer (OpenStack Telemetry) service</li><li class="listitem"><code class="email">metering-alarm_hosts</code>: These are the servers that run the Ceilometer (OpenStack Telemetry) service associated with alarms</li><li class="listitem"><code class="email">metrics_hosts</code>: The servers that run the Gnocchi component of Ceilometer</li><li class="listitem"><code class="email">metering-compute_hosts</code>: When using Ceilometer, these are the list of compute hosts that need the metering agent installed</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Running the OpenStack-Ansible playbooks" id="I3QM1-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec16" class="calibre1"/>Running the OpenStack-Ansible playbooks</h1></div></div></div><p class="calibre10">To install <a id="id48" class="calibre1"/>OpenStack, we simply run the relevant playbooks. There are three main playbooks in total that we will be using:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">setup-hosts.yml</code></li><li class="listitem"><code class="email">setup-infrastructure.yml</code></li><li class="listitem"><code class="email">setup-openstack.yml</code></li></ul></div></div>

<div class="book" title="Running the OpenStack-Ansible playbooks" id="I3QM1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec28" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">Ensure that you are the <code class="email">root</code> user on the deployment host. In most cases, this is the first infrastructure controller node, <code class="email">infra01</code>.</p></div></div>

<div class="book" title="Running the OpenStack-Ansible playbooks" id="I3QM1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec29" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre10">To install <a id="id49" class="calibre1"/>OpenStack using the OpenStack-Ansible playbooks, you navigate to the <code class="email">playbooks</code> directory of the checked out Git repository, then execute each playbook in turn:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First change to the <code class="email">playbooks</code> directory by executing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/playbooks</strong></span>
</pre></div></li><li class="listitem" value="2">The first step is to run a syntax check on your scripts and configuration. As we will be executing three playbooks, we will execute the following against each:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">openstack-ansible setup-hosts.yml --syntax-check</strong></span>
<span class="strong"><strong class="calibre2">openstack-ansible setup-infrastructure.yml --syntax-check</strong></span>
<span class="strong"><strong class="calibre2">openstack-ansible setup-openstack.yml --syntax-check</strong></span>
</pre></div></li><li class="listitem" value="3">Now we will run the first playbook using a special OpenStack-Ansible wrapper script to Ansible that configures each host that we described in the <code class="email">/etc/openstack_deploy/openstack_user_config.yml</code> file:<div class="informalexample"><pre class="programlisting">openstack-ansible setup-hosts.yml</pre></div></li><li class="listitem" value="4">After a short while, you should be greeted with a PLAY RECAP output that is all green (with yellow/blue lines indicating where any changes were made), with the output showing all changes were OK. If there are issues, review the output by scrolling back through the output and watch out for any output that was printed out in red. Refer to the <span class="strong"><em class="calibre18">Troubleshooting the installation</em></span> recipe further on in this chapter. If all is OK, we can proceed to run the next playbook for setting up load balancing. At this stage, it is important that the load balancer gets configured. OpenStack-Ansible installs the OpenStack services in LXC containers on each server, and so far we have not explicitly stated which IP address on the container network will have that particular service installed. This is because we let Ansible manage this for us. So while it might seem counter-intuitive to set up load balancing at this stage before we know where each service will be installed—Ansible has already generated a dynamic inventory ahead of any future work, so Ansible already knows how many containers are involved and knows which container will have that service installed. If you are using an F5 LTM, Brocade, or similar enterprise load balancing kit, it is recommended that you use HAProxy temporarily and view the generated configuration to be manually transferred to a physical setup. To temporarily set up HAProxy to allow an installation of OpenStack to continue, modify your <code class="email">openstack_user_config.yml</code> file to include a HAProxy host, then execute the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">openstack-ansible install-haproxy.yml</strong></span>
</pre></div></li><li class="listitem" value="5">If all is OK, we can proceed to run the next Playbook that sets up the shared infrastructure services as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">openstack-ansible setup-infrastructure.yml</strong></span>
</pre></div></li><li class="listitem" value="6">This step takes a little longer than the first Playbook. As before, inspect the output for any failures. At this stage, we should have a number of containers running on each Infrastructure Node (also known and referred to as Controller Nodes). On some of these containers, such as the ones labelled Galera or RabbitMQ, we should see services running correctly on here, waiting for OpenStack to be configured against them. We can now continue the installation by running the largest of the playbooks—the installation of OpenStack itself. To do this, execute the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">openstack-ansible setup-openstack.yml</strong></span>
</pre></div></li><li class="listitem" value="7">This may take a while to run—running to hours—so be prepared for this duration by ensuring your SSH session to the deployment host will not be interrupted after a long time, and safeguard any disconnects by running the Playbook in something like <code class="email">tmux</code> or <code class="email">screen</code>. At the end of the Playbook run, if all is OK, congratulations, you have OpenStack installed!</li></ol><div class="calibre20"/></div></div></div>

<div class="book" title="Running the OpenStack-Ansible playbooks" id="I3QM1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec30" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">Installation <a id="id50" class="calibre1"/>of OpenStack using OpenStack-Ansible is conducted using a number of playbooks. The first playbook, <code class="email">setup-hosts.yml</code>, sets up the hosts by laying down the container configurations. At this stage, Ansible knows where it will be placing all future services associated with OpenStack, so we use the dynamic inventory information to perform an installation of HAProxy and configure it for all the services used by OpenStack (that are yet to be installed). The next playbook, <code class="email">setup-infrastructure.yml</code>, configures and installs the base Infrastructure services containers that OpenStack expects to be present, such as Galera. The final playbook is the main event—the playbook that installs all the required OpenStack services we specified in the configuration. This runs for quite a while—but at the end of the run, you are left with an installation of OpenStack.</p><p class="calibre10">The OpenStack-Ansible project provides a wrapper script to the <code class="email">ansible</code> command that would ordinarily run to execute Playbooks. This is called <code class="email">openstack-ansible</code>. In essence, this ensures that the correct inventory and configuration information is passed to the <code class="email">ansible</code> command to ensure correct running of the OpenStack-Ansible playbooks.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Troubleshooting the installation" id="J2B81-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec17" class="calibre1"/>Troubleshooting the installation</h1></div></div></div><p class="calibre10">Ansible is <a id="id51" class="calibre1"/>a tool, written by people, that runs playbooks, written by people, to configure systems that would ordinarily be manually performed by people, and as such, errors can occur. The end result is only as good as the input.</p><p class="calibre10">Typical failures either occur quickly, such as connection problems, and will be relatively self-evident, or after long running jobs that may be as a result of load or network timeouts. In any case, the OpenStack-Ansible playbooks provide an efficient mechanism to rerun playbooks without having to repeat the tasks it has already completed.</p><p class="calibre10">On failure, Ansible produces a file in <code class="email">/root</code> (as we're running these playbooks as <code class="email">root</code>) called the <span class="strong"><em class="calibre18">playbook</em></span> name, with the file extension of <code class="email">.retry</code>. This file simply lists the hosts that had failed so this can be referenced when running the playbook again. This targets the single or small group of hosts, which is far more efficient than a large cluster of machines that successfully completed.</p></div>

<div class="book" title="Troubleshooting the installation" id="J2B81-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec31" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre10">We will step through a problem that caused one of the playbooks to fail.</p><p class="calibre10">Note the <a id="id52" class="calibre1"/>failed playbook and then invoke it again with the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Ensure that you're in the <code class="email">playbooks</code> directory as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/playbooks</strong></span>
</pre></div></li><li class="listitem" value="2">Now rerun that Playbook, but specify the <code class="email">retry</code> file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ansible-openstack setup-openstack.yml --retry /root/setup-openstack.retry</strong></span>
</pre></div></li><li class="listitem" value="3">In most situations, this will be enough to rectify the situation, however, OpenStack-Ansible has been written to be idempotent—meaning that the whole playbook can be run again, only modifying what it needs to. Therefore, you can run the Playbook again without specifying the <code class="email">retry</code> file.</li></ol><div class="calibre20"/></div><p class="calibre10">Should there be a failure at this first stage, execute the following:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First remove the generated <code class="email">inventory</code> files:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">rm -f /etc/openstack_deploy/openstack_inventory.json</strong></span>
<span class="strong"><strong class="calibre2">rm -f /etc/openstack_deploy/openstack_hostnames_ips.yml</strong></span>
</pre></div></li><li class="listitem" value="2">Now rerun the <code class="email">setup-hosts.yml</code> playbook:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/playbooks</strong></span>
<span class="strong"><strong class="calibre2">openstack-ansible setup-hosts.yml</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><p class="calibre10">In some situations, it might be applicable to destroy the installation and begin again. As each service gets installed in LXC containers, it is very easy to wipe an installation and start from the beginning. To do so, carry out the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">We first destroy all of the containers in the environment:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/playbooks</strong></span>
<span class="strong"><strong class="calibre2">openstack-ansible lxc-containers-destroy.yml</strong></span>
</pre></div><p class="calibre22">You will be asked to confirm this action. Follow the ons-screen prompts.</p></li><li class="listitem" value="2">We recommend you to uninstall the following package to avoid any conflicts with the future running of the playbooks, and also clear out any remnants of containers on each host:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ansible hosts -m shell -a "pip uninstall -y appdirs"</strong></span>
</pre></div></li><li class="listitem" value="3">Finally, remove the inventory information:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">rm -f /etc/openstack_deploy/openstack_inventory.json </strong></span>
<span class="strong"><strong class="calibre2">/etc/openstack_deploy/openstack_hostnames_ips.yml</strong></span>
</pre></div></li></ol><div class="calibre20"/></div></div></div>

<div class="book" title="Troubleshooting the installation" id="J2B81-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec32" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">Ansible is not perfect and so are computers. Sometimes failures occur in the environment <a id="id53" class="calibre1"/>due to SSH timeouts, or some other transient failure. Also, despite Ansible trying its best to retry the execution of a playbook, the result might be a failure. Failure in Ansible is quite obvious—it is usually predicated by outputs of red text on the screen. In most cases, rerunning the offending playbook may get over some transient problems. Each playbook runs a specific task, and Ansible will state which task has failed. Troubleshooting why that particular task had failed will eventually lead to a good outcome. Worst case, you can reset your installation from the beginning.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Manually testing the installation" id="K0RQ1-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec18" class="calibre1"/>Manually testing the installation</h1></div></div></div><p class="calibre10">Once the <a id="id54" class="calibre1"/>installation has completed successfully, the first step is to test the install. Testing OpenStack involves both automated and manual checks.</p><p class="calibre10">Manual tests verify user-journeys that may not normally be picked up through automated testing, such as ensuring horizon is displayed properly.</p><p class="calibre10">Automated tests can be invoked using a testing framework such as tempest or the OpenStack benchmarking tool—rally.</p></div>

<div class="book" title="Manually testing the installation" id="K0RQ1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec33" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">Ensure that you are <code class="email">root</code> on the first infrastructure controller node, <code class="email">infra01</code>.</p></div></div>

<div class="book" title="Manually testing the installation" id="K0RQ1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec34" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre10">The installation of OpenStack-Ansible creates several <code class="email">utility</code> containers on each of the infra nodes. These utility hosts provide all the command-line tools needed to try out OpenStack, using the command line of course. Carry out the following steps to get access to a utility host and run various commands in order to verify an installation of OpenStack manually:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, view the running containers by issuing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">lxc-ls -f</strong></span>
</pre></div></li><li class="listitem" value="2">As you can see, this lists a number of containers because the OpenStack-Ansible installation uses isolated Linux containers for running each service. By the side of each one its IP address and running state will be listed. You can see here that the container network of <code class="email">172.29.236.0/24</code> was used in this chapter and why this was named this way. One of the containers on here is the utility container, named with the following format: <code class="email">nodename_utility_container_randomuuid</code>. To access this container, you can SSH to it, or you can issue the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">lxc-attach -n infra01_utility_container_34u477d</strong></span>
</pre></div></li><li class="listitem" value="3">You will <a id="id55" class="calibre1"/>now be running a Terminal inside this container, with access only to the tools and services belonging to that containers. In this case, we have access to the required OpenStack clients. The first thing you need to do is source in your OpenStack credentials. The OpenStack-Ansible project writes out a generated bash environment file with an <code class="email">admin</code> user and project that was set up during the installation. Load this into your bash environment with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">source openrc</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip12" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: you can also use the following syntax in Bash: . <code class="email">openrc</code>
</p></div></li><li class="listitem" value="4">Now you can use the OpenStack CLI to view the services and status of the environment, as well as create networks, and launch instances. A few handy commands are listed here:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">openstack server list</strong></span>
<span class="strong"><strong class="calibre2">openstack network list</strong></span>
<span class="strong"><strong class="calibre2">openstack endpoint list</strong></span>
<span class="strong"><strong class="calibre2">openstack network agent list</strong></span>
</pre></div></li></ol><div class="calibre20"/></div></div></div>

<div class="book" title="Manually testing the installation" id="K0RQ1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec35" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">The OpenStack-Ansible method of installing OpenStack installs OpenStack services into isolated containers on our Linux servers. On each of the controller (or infra) nodes are about 12 containers, each running a single service such as nova-api or RabbitMQ. You can view the running containers by logging into any of the servers as root and issuing a <code class="email">lxc-ls -f</code> command. The <code class="email">-f</code> parameter gives you a full listing showing the status of the instance such as whether it is running or stopped.</p><p class="calibre10">One of the containers on the infra nodes has <code class="email">utility</code> in its name, and this is known as a <span class="strong"><em class="calibre18">utility container</em></span> in OpenStack-Ansible terminology. This container has OpenStack client tools installed, which makes it a great place to start manually testing an installation of OpenStack. Each container has at least an IP address on the container network—in the example <a id="id56" class="calibre1"/>used in this chapter this is the <code class="email">172.29.236.0/24</code> subnet. You can SSH to the IP address of this container, or use another <code class="email">lxc</code> command to attach to it: <code class="email">lxc-attach -n</code> <code class="email">&lt;name_of_container&gt;</code>. Once you have a session inside the container, you can use it like any other system, provided those tools are available to the restricted four-walls of the container. To use OpenStack commands, however, you first need to source the resource environment file which is named <code class="email">openrc</code>. This is a normal bash environment file that has been prepopulated during the installation and provides all the required credentials needed to use OpenStack straight away.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Modifying the OpenStack configuration" id="KVCC1-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec19" class="calibre1"/>Modifying the OpenStack configuration</h1></div></div></div><p class="calibre10">It would <a id="id57" class="calibre1"/>be ludicrous to think that all of the playbooks would be needed to run again for a small change such as changing the CPU contention ratio from 4:1 to 8:1. So instead, the playbooks have been developed and tagged so that specific playbooks can be run associated with that particular project that would reconfigure and restart the associated services to pick up the changes. </p></div>

<div class="book" title="Modifying the OpenStack configuration" id="KVCC1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec36" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">Ensure that you are <code class="email">root</code> on the <span class="strong"><em class="calibre18">deployment host</em></span>. In most cases, this is the first infrastructure controller node, <code class="email">infra01</code>.</p></div></div>

<div class="book" title="Modifying the OpenStack configuration" id="KVCC1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec37" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre10">The following are the common changes and how they can be changed using Ansible. As we'll adjust the configuration, all of these commands are executed from the same host you used to perform the installation.</p><p class="calibre10">To adjust the CPU overcommit/allocation ratio, carry out the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Edit the <code class="email">/etc/openstack_deploy/user_variables.yml</code> file and modify (or add) the following line (adjust the figure to suit):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">nova_cpu_allocation_ratio: 8.0</strong></span>
</pre></div></li><li class="listitem" value="2">Now execute the following commands to make changes in the environment:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/playbooks</strong></span>
<span class="strong"><strong class="calibre2">openstack-ansible os-nova-install.yml --tags 'nova-config'</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><p class="calibre10">For more complex changes, for example, to add configuration that isn't a simple one-line change in a template, we can use an alternative in the form of overrides. To make changes to the default Nova Quotas, carry out the following as an example:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Edit the <code class="email">/etc/openstack_deploy/user_variables.yml</code> file and modify (or add) the following line (adjust the figure to suit):<div class="informalexample"><pre class="programlisting">nova_nova_conf_overrides:  
  DEFAULT:
   quota_fixed_ips = -1
   quota_floating_ips = 20
   quota_instances = 20</pre></div></li><li class="listitem" value="2">Now execute the following commands to make changes in the environment:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/playbooks</strong></span>
<span class="strong"><strong class="calibre2">openstack-ansible os-nova-install.yml --tag 'nova-config'</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><p class="calibre10">Changes <a id="id58" class="calibre1"/>for Neutron, Glance, Cinder, and all other services are modified in a similar way. Adjust the name of the service in the syntax used. For example, to change a configuration item in the <code class="email">neutron.conf</code> file, you would use the following syntax:</p><div class="informalexample"><pre class="programlisting">neutron_neutron_conf_overrides:  
  DEFAULT:
   dhcp_lease_duration = -1</pre></div><p class="calibre10">Then execute the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd /opt/openstack-ansible/playbooks</strong></span>
<span class="strong"><strong class="calibre2">openstack-ansible os-neutron-install.yml --tag 'neutron-config'</strong></span>
</pre></div></div></div>

<div class="book" title="Modifying the OpenStack configuration" id="KVCC1-189e69df43a248268db97cde1b1a8e47">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec38" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">We modified the same OpenStack-Ansible configuration files as in the <span class="strong"><em class="calibre18">Configuring the installation</em></span> recipe and executed the <code class="email">openstack-ansible playbook</code> command, specifying the playbook that corresponded to the service we wanted to change. As we were making configuration changes, we notified Ansible of this through the <code class="email">--tag</code> parameter.</p><p class="calibre10">Refer to <a class="calibre1" href="https://docs.openstack.org/">https://docs.openstack.org/</a> for all configuration options for each service.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Virtual lab - vagrant up!"><div class="book" id="LTSU2-189e69df43a248268db97cde1b1a8e47"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec20" class="calibre1"/>Virtual lab - vagrant up!</h1></div></div></div><p class="calibre10">In an <a id="id59" class="calibre1"/>ideal world, each of us would have access to physical servers and the network kit in order to learn, test, and experiment with OpenStack. However, most of the time this isn't the case. By using an orchestrated virtual lab, using Vagrant and VirtualBox, allows you to experience this chapter on OpenStack-Ansible using your laptop.</p><p class="calibre10">The following Vagrant lab can be found at <a class="calibre1" href="http://openstackbook.online/">http://openstackbook.online/</a>.</p><p class="calibre10">This is <a id="id60" class="calibre1"/>the architecture of the Vagrant-based OpenStack environment:</p><div class="mediaobject"><img src="../images/00005.jpeg" alt="Virtual lab - vagrant up!" class="calibre16"/></div><p class="calibre17"> </p><p class="calibre10">Essentially there are three virtual machines that are created (a controller node, a compute node and a client machine), and each host has four network cards (plus an internal bridged interface used by VirtualBox itself). The four network cards represent the networks described in this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Eth1</strong></span>: This is included in the <code class="email">br-mgmt</code> bridge, and used by the container network</li><li class="listitem"><span class="strong"><strong class="calibre2">Eth2</strong></span>: This is included in the <code class="email">br-vlan</code> bridge, and used when a VLAN-based Neutron network is created once OpenStack is up and running</li><li class="listitem"><span class="strong"><strong class="calibre2">Eth3</strong></span>: This is the client or host network—the network we would be using to interact with OpenStack services (for example, the public/external side of the load balancer)</li><li class="listitem"><span class="strong"><strong class="calibre2">Eth4</strong></span>: This is included in the <code class="email">br-vxlan</code> bridge, and used when a VXLAN-based Neutron overlay network is created once OpenStack is up and running</li></ul></div><p class="calibre10">Note that the virtual machine called <code class="email">openstack-client</code>, which gets created in this lab, provides you with all the command-line tools to conveniently get you started with working with OpenStack.</p></div>

<div class="book" title="Virtual lab - vagrant up!">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec39" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre10">In order <a id="id61" class="calibre1"/>to run a multi-node OpenStack environment, running as a virtual environment on your laptop or designated host, the following set of requirements are needed:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A Linux, Mac, or Windows desktop, laptop or server. The authors of this book use macOS and Linux, with Windows as the host desktop being the least tested configuration.</li><li class="listitem">At least 16GB RAM. 24GB is recommended.</li><li class="listitem">About 50 GB of disk space. The virtual machines that provide the infra and compute nodes in this virtual environment are thin provisioned, so this requirement is just a guide depending on your use.</li><li class="listitem">An internet connection. The faster the better, as the installation relies on downloading files and packages directly from the internet.</li></ul></div></div></div>

<div class="book" title="Virtual lab - vagrant up!">
<div class="book" title="How to do it…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec40" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre10">To run the OpenStack environment within the virtual environment, we need a few programs installed, all of which are free to download and use: VirtualBox, Vagrant, and Git. VirtualBox provides the virtual servers representing the servers in a normal OpenStack installation; Vagrant describes the installation in a fully orchestrated way; Git allows us to check out all of the scripts that we provide as part of the book to easily test a virtual OpenStack installation. The following instructions describe an installation of these tools on Ubuntu Linux.</p><p class="calibre10">We first need to install VirtualBox if it is not already installed. We recommend downloading the latest available releases of the software. To do so on Ubuntu Linux as <code class="email">root</code>, follow these steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">We first add the <code class="email">virtualbox.org</code> repository key with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add –</strong></span>
</pre></div></li><li class="listitem" value="2">Next we add the repository file to our <code class="email">apt</code> configuration, by creating a file called <code class="email">/etc/apt/sources.list.d/virtualbox.conf</code> with the following contents:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">deb http://download.virtualbox.org/virtualbox/debian xenial contrib</strong></span>
</pre></div></li><li class="listitem" value="3">We now run an <code class="email">apt update</code> to refresh and update the <code class="email">apt</code> cache with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">apt update</strong></span>
</pre></div></li><li class="listitem" value="4">Now install VirtualBox with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">apt install virtualbox-5.1</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><p class="calibre10">Once VirtualBox is installed, we can install Vagrant. Follow these steps to install Vagrant:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Vagrant is downloaded from <a class="calibre1" href="https://www.vagrantup.com/downloads.html">https://www.vagrantup.com/downloads.html</a>. The version we want is Debian 64-Bit. At the time of writing, this is version 2.0.1. To download it on our desktop issue the following command: <div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.deb</strong></span>
</pre></div></li><li class="listitem" value="2">We can now install the file with the following command<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">dpkg -i ./vagrant_2.0.1_x86_64.deb</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><p class="calibre10">The lab <a id="id62" class="calibre1"/>utilizes two vagrant plugins: <code class="email">vagrant-hostmanager</code> and <code class="email">vagrant-triggers</code>. To install these, carry out the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Install <code class="email">vagrant-hostmanager</code> using the <code class="email">vagrant</code> tool:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant plugin install vagrant-hostmanager</strong></span>
</pre></div></li><li class="listitem" value="2">Install <code class="email">vagrant-triggers</code> using the <code class="email">vagrant</code> tool:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant plugin install vagrant-triggers</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><p class="calibre10">If Git is not currently installed, issue the following command to install <code class="email">git</code> on a Ubuntu machine:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">apt update</strong></span>
<span class="strong"><strong class="calibre2">apt install git</strong></span>
</pre></div><p class="calibre10">Now that we have the required tools, we can use the <code class="email">OpenStackCookbook</code> Vagrant lab environment to perform a fully orchestrated installation of OpenStack in a VirtualBox environment:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">We will first checkout the lab environment scripts and supporting files with <code class="email">git</code> by issuing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">git clone https://github.com/OpenStackCookbook/vagrant-openstack</strong></span>
</pre></div></li><li class="listitem" value="2">We will change into the <code class="email">vagrant-openstack</code> directory that was just created:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">cd vagrant-openstack</strong></span>
</pre></div></li><li class="listitem" value="3">We can now orchestrate the creation of the virtual machines and installation of OpenStack using one simple command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant up</strong></span>
</pre></div></li></ol><div class="calibre20"/></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip13" class="calibre1"/>Tip</h3><p class="calibre10">
<span class="strong"><strong class="calibre2">Tip</strong></span>: This will take quite a while as it creates the virtual machines and runs through all the same playbook steps described in this chapter.</p></div></div></div>

<div class="book" title="Virtual lab - vagrant up!">
<div class="book" title="How it works…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec41" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre10">Vagrant <a id="id63" class="calibre1"/>is an awesome tool for orchestrating many different virtual and cloud environments. It allows us to describe what virtual servers need to be created, and using Vagrant's provisioner allows us to run scripts once a virtual <a id="id64" class="calibre1"/>machine has been created.</p><p class="calibre10">Vagrant's environment file is called <span class="strong"><strong class="calibre2">Vagrantfile</strong></span>. You can edit this file to adjust the settings of the virtual machine, for example, to increase the RAM or number of available CPUs.</p><p class="calibre10">This allows us to describe a complete OpenStack environment using one command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant up</strong></span>
</pre></div><p class="calibre10">The environment consists of the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A controller node, <code class="email">infra-01</code></li><li class="listitem">A compute node, <code class="email">compute-01</code></li><li class="listitem">A client virtual machine, <code class="email">openstack-client</code></li></ul></div><p class="calibre10">Once the environment has finished installing, you can use the environment by navigating to <code class="email">http://192.168.100.10/</code> in your web browser. To retrieve the admin password, follow the steps given here and view the file named <code class="email">openrc</code>.</p><p class="calibre10">There is a single controller node that has a <code class="email">utility</code> container configured for use in this environment. Attach to this with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant ssh controller-01</strong></span>
<span class="strong"><strong class="calibre2">sudo -i</strong></span>
<span class="strong"><strong class="calibre2">lxc-attach -n (lxc-ls | grep utility)</strong></span>
<span class="strong"><strong class="calibre2">openrc</strong></span>
</pre></div><p class="calibre10">Once you have retrieved the <code class="email">openrc</code> details, copy these to your <code class="email">openstack-client</code> virtual machine. From here you can operate OpenStack, mimicking a desktop machine accessing an installation of OpenStack utilizing the command line.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant ssh openstack-client</strong></span>
<span class="strong"><strong class="calibre2">openrc</strong></span>
</pre></div><p class="calibre10">You should now be able to use OpenStack CLI tools to operate the environment.</p></div></div></body></html>