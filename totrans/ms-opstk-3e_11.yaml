- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: A Hybrid Cloud Hyperscale Use Case – Scaling a Kubernetes Workload
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合云超大规模用例——扩展Kubernetes工作负载
- en: “There are not more than five cardinal tastes, yet combinations of them yield
    more flavors than can ever be tasted.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “虽然味觉的基本味道不超过五种，但它们的组合产生的风味远超过能尝试的种类。”
- en: – Sun Tzu, The Art of War
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 孙子，《孙子兵法》
- en: 'In the previous chapter, we briefly explored hybrid cloud design patterns and
    how hybrid deployment has become a cloud trend for several organizations. The
    hybrid cloud model reflects a complex mix of services that work in tandem to satisfy
    different workload requirements. When you join an OpenStack private environment,
    a hybrid model can be more challenging to set up due to the complexity of the
    dynamic mix of resources spread across private and public environments. Building
    hybrid cloud networks is the first challenge that an organization would face,
    which requires stable, dedicated, and consistent connectivity between both public
    and private clouds. Moreover, applications that can run on bare metal, virtual
    machines, or containers require a way to ensure configuration consistency between
    different cloud environments. Several implementation challenges covered in the
    previous chapter still present a blocker for many organizations from enjoying
    the benefits of hybrid clouds. The good news is that with the evolution of **application
    architecture patterns** , **containerization** technology has shifted how and
    where applications can be deployed. If we are looking for application migration,
    cloud bursting, or running for a backup and disaster recovery strategy, adopting
    containerization technology is the way to go. In this chapter, we will put into
    practice what was covered in the previous chapter and uncover the power of containerization
    in a hybrid cloud setup by covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们简要探讨了混合云设计模式，以及混合部署如何成为许多组织的云趋势。混合云模型反映了服务的复杂混合，这些服务协同工作以满足不同的工作负载需求。当你加入OpenStack私有环境时，由于私有云和公有云中分布的资源的动态混合，混合模型的搭建可能更具挑战性。构建混合云网络是组织面临的第一个挑战，这需要公有云和私有云之间稳定、专用和一致的连接。此外，能够在裸金属、虚拟机或容器上运行的应用程序需要一种方法来确保不同云环境之间的配置一致性。在上一章中提到的若干实施挑战仍然是许多组织享受混合云优势的障碍。好消息是，随着**应用架构模式**的演进，**容器化**技术已经改变了应用程序的部署方式和位置。如果我们寻求应用迁移、云爆发或备份与灾难恢复策略，采用容器化技术是最佳选择。在本章中，我们将实践上一章所涉及的内容，并通过以下主题揭示容器化在混合云设置中的强大力量：
- en: Designing hybrid cloud networking between OpenStack private cloud and a public
    cloud environment such as AWS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计OpenStack私有云与公有云环境（如AWS）之间的混合云网络
- en: Revisiting briefly the main concepts of Kubernetes (also referred to as K8s)
    to prepare for a cloud-agnostic workload model across OpenStack and AWS clouds
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要回顾Kubernetes（也称为K8s）的主要概念，为OpenStack和AWS云之间的云无关工作负载模型做准备
- en: Exploring different approaches to implementing a hybrid cloud setup
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索实现混合云设置的不同方法
- en: Deploying Kubernetes across an OpenStack private cloud and AWS public cloud
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在OpenStack私有云和AWS公有云之间部署Kubernetes
- en: Learning orchestration tooling for hybrid cloud deployment using Juju
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习使用Juju进行混合云部署的编排工具
- en: Connecting Kubernetes clusters across private and public clouds by federating
    its control plane and running a hybrid deployment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过联邦控制平面连接私有和公有云中的Kubernetes集群，并运行混合部署
- en: Qualifying a hybrid architecture
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资格验证混合架构
- en: In this section, we will explore the main motivation for adopting hybrid cloud
    implementation. We will briefly illustrate how containerization helps achieve
    most of the hybrid model benefits and enables companies to move between cloud
    environments most efficiently. The networking and security of connected cloud
    environments is another important pillar that must be looked at carefully when
    adopting the hybrid approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨采用混合云实施的主要动机。我们将简要说明容器化如何帮助实现混合模型的大部分优势，并使企业能够在云环境之间高效地迁移。连接的云环境的网络和安全性是采用混合方法时必须仔细考虑的另一个重要支柱。
- en: Adopting a cloud-agnostic stack
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用云无关堆栈
- en: 'Being cloud-agnostic is a key motivator for several organizations that seek
    the best cloud experience when being onboarded into a cloud journey. Thanks to
    the **pay-as-you-go** model, organizations can use cloud resources and pay only
    for what they use. However, some business workloads might require changes, and
    switching or expanding to another cloud provider might become necessary. Enjoying
    free-pass movement between cloud providers might not sound straightforward at
    first. This is due to the technology that a cloud provider offers. A workload
    naturally becomes locked into more dependencies that run on a specific cloud provider.
    Moving or expanding to another cloud provider can be an expensive, complex, and
    error-prone process. For that reason, cloud agnosticism should be addressed in
    advance, starting with the workload architecture itself. The best example of this
    is the emergent **microservices architecture** . One of the main benefits of opting
    for a microservice design is that it breaks an application into smaller and independent
    modules, making it easy to test and deploy them separately without the need to
    rely on a specific technology stack. Another benefit of a microservice design
    is the infrastructure that will run it. That is where containerization comes into
    the picture. Containers come with much more efficient compute utilization and
    time optimization compared to virtual machines. That makes the microservice and
    containerization concepts an ideal pair to adopt cloud agnosticism. Hence, the
    hybrid cloud model is much easier to implement, with some additions. As illustrated
    in the following diagram, those additions include a container orchestration layer
    and automated deployment, making the cloud-agnostic model almost complete:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '云中立性是许多组织在进入云计算旅程时，寻求最佳云体验的关键动因。得益于**按需付费**模式，组织可以使用云资源，并仅为实际使用的部分付费。然而，一些业务负载可能需要进行调整，切换或扩展到另一个云服务提供商可能变得必要。云服务提供商之间的自由迁移一开始可能并不简单。这是由于每个云服务提供商所提供的技术，业务负载自然会被绑定到某个特定云服务提供商的更多依赖上。迁移或扩展到另一个云服务提供商可能是一个昂贵、复杂且容易出错的过程。因此，云中立性应提前考虑，从负载架构本身开始着手。一个很好的例子就是新兴的**微服务架构**。选择微服务设计的主要好处之一是它将应用程序拆分为更小且独立的模块，使得它们可以单独进行测试和部署，而无需依赖特定的技术栈。微服务设计的另一个好处是其运行的基础设施。这就是容器化的作用所在。与虚拟机相比，容器具有更高效的计算资源利用和时间优化。这使得微服务与容器化概念成为实现云中立性的理想组合。因此，混合云模型在加入一些附加功能后，更容易实现。如以下图所示，这些附加功能包括容器编排层和自动化部署，使得云中立模型几乎完整：  '
- en: '![Figure 11.1 – The cloud-agnostic architecture of a hybrid model](img/B21716_11_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 混合模型的云中立架构](img/B21716_11_01.jpg)'
- en: Figure 11.1 – The cloud-agnostic architecture of a hybrid model
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 混合模型的云中立架构
- en: Container orchestration engines manage the life cycle of the workloads with
    almost no administration need. Most of the famous container orchestration engines
    such as Docker and Kubernetes are considered part of the cloud-agnostic container
    orchestration platform family. While container orchestration brings more container
    management automation on its own, it would not be nearly as valuable without the
    **Infrastructure as Code** ( **IaC** ) concept. Each cloud provider offers its
    own infrastructure templating mechanism ( **CloudFormation** for AWS, **Cloud
    Deployment Manager** for Google, **Deployment Manager** for Azure, and **Heat**
    for OpenStack). The vendor-specific syntax of each service presents a challenge
    as switching between cloud deployment environments becomes more complex. The solution
    is obviously to take a cloud-agnostic approach by employing third-party tools
    that allow us to deploy the same workload infrastructure in more than one cloud
    environment, using the same code to abstract the infrastructure layer. These tools
    became available only after the IaC concept had emerged as a DevOps principle.
    **Terraform** from Hashicorp has become the most commonly used cloud-agnostic
    tool for infrastructure management. Today, more open source projects have been
    inspired by Terraform, and recently, the **Pulumi** tool has appeared on the DevOps
    horizon. In addition to managing infrastructure using code, Pulumi allows the
    description of the infrastructure in any o the most popular programming languages
    such as Python, Java, Go, .NET, and Typescript.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排引擎几乎不需要管理即可管理工作负载的生命周期。大多数著名的容器编排引擎，如Docker和Kubernetes，都被视为云无关的容器编排平台的一部分。虽然容器编排本身带来了更多的容器管理自动化，但没有**基础设施即代码**（**IaC**）概念，它的价值将大大降低。每个云服务商都提供自己的基础设施模板机制（AWS的**CloudFormation**，Google的**Cloud
    Deployment Manager**，Azure的**Deployment Manager**，以及OpenStack的**Heat**）。每个服务的厂商特定语法构成了一个挑战，因为在不同云部署环境之间切换变得更加复杂。显然，解决方案是采用云无关的方法，通过使用第三方工具来实现，这些工具允许我们在多个云环境中部署相同的工作负载基础设施，并使用相同的代码来抽象基础设施层。这些工具只有在IaC概念作为DevOps原则出现后才得以实现。Hashicorp的**Terraform**已成为最常用的云无关工具，用于基础设施管理。今天，更多的开源项目受到了Terraform的启发，最近，**Pulumi**工具已经出现在DevOps领域。除了使用代码管理基础设施外，Pulumi还允许使用Python、Java、Go、.NET和Typescript等最流行的编程语言来描述基础设施。
- en: Important note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: On August 10, 2023, Hashicorp announced that Terraform is no longer under the
    Mozilla Public License v2.0. Pulumi has emerged as a successor of Terraform in
    the open source world and has gained massive popularity in the last year. To read
    more about Pulumi, check the official website at [https://www.pulumi.com/docs/get-started/](https://www.pulumi.com/docs/get-started/)
    .
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年8月10日，Hashicorp宣布Terraform不再受Mozilla公共许可证v2.0的约束。Pulumi已成为Terraform在开源世界中的继任者，并在过去一年里获得了极大的关注和流行。欲了解更多关于Pulumi的信息，请访问其官方网站
    [https://www.pulumi.com/docs/get-started/](https://www.pulumi.com/docs/get-started/)。
- en: To orchestrate the whole infrastructure management process, CI/CD tools define
    the engines to automate everything across different cloud environments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了协调整个基础设施管理过程，CI/CD工具定义了在不同云环境中自动化一切的引擎。
- en: Now that we have covered the different pieces of the hybrid cloud puzzle, we
    will focus on how to connect the dots between two cloud environments – in our
    case, OpenStack and AWS.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了混合云拼图的不同部分，我们将重点讨论如何连接两个云环境——在我们的案例中是OpenStack和AWS之间的连接。
- en: Preparing for a hybrid model
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备混合模型
- en: 'Establishing a hybrid cloud architecture first requires an adequate network
    connection between the private and public clouds. Such a network connection is
    critical to maintaining business continuity. For that reason, continuous evaluation
    of the connection that looks at latency, uptime, and bandwidth is a must. Security
    is the other essential aspect of the hybrid connection. Depending on the hardware
    and network service used for such a connection, operators should ensure encryption
    in transit is enabled and redundancy is considered. Implementing a cloud hybrid
    connection can be greatly simplified when using cloud network-managed services.
    As shown in the next simplified network diagram, an OpenStack private cloud endpoint
    is connected to an AWS **Virtual Private Cloud** ( **VPC** ) through two different
    connections:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 建立混合云架构首先需要私有云和公有云之间的适当网络连接。这种网络连接对于保持业务连续性至关重要。因此，必须对连接进行持续评估，评估内容包括延迟、正常运行时间和带宽。安全性是混合连接的另一个关键方面。根据用于此类连接的硬件和网络服务，操作员应确保启用传输加密并考虑冗余。当使用云网络托管服务时，实现云混合连接可以大大简化。如下一张简化的网络图所示，OpenStack
    私有云终端通过两种不同的连接与 AWS **虚拟私有云**（**VPC**）相连：
- en: '![Figure 11.2 – A hybrid connection between the OpenStack and AWS clouds](img/B21716_11_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – OpenStack 与 AWS 云之间的混合连接](img/B21716_11_02.jpg)'
- en: Figure 11.2 – A hybrid connection between the OpenStack and AWS clouds
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – OpenStack 与 AWS 云之间的混合连接
- en: The type of connection in the upper part of the figure illustrates a site-to-site
    VPN connection over the internet using an IPSec tunnel. Without exposing any resources
    in both cloud environments (the OpenStack private network range and AWS VPC) to
    the public world, communication occurs in an encrypted tunnel traveling over the
    internet. The implementation of a VPN connection between an OpenStack tenant environment
    and AWS VPC is much more automated than in a classic data center. As we covered
    in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking – Connectivity
    and Managed Service Options* , Neutron offers a managed network service, VPNaaS,
    that can be deployed as a site-to-site VPN connection in minutes. AWS offers a
    similar managed VPN feature as part of its VPC service that can establish a VPN
    connection by simply creating a VPN gateway and configuring a customer gateway
    (an OpenStack endpoint).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图上方的连接类型展示了通过互联网使用 IPSec 隧道的站点到站点 VPN 连接。在两个云环境（OpenStack 私有网络范围和 AWS VPC）中没有暴露任何资源的情况下，通信发生在通过互联网传输的加密隧道中。OpenStack
    租户环境和 AWS VPC 之间的 VPN 连接实现比经典数据中心更加自动化。正如我们在 [*第 6 章*](B21716_06.xhtml#_idTextAnchor159)
    中提到的，*OpenStack 网络 – 连接性和托管服务选项*，Neutron 提供了一种托管网络服务 VPNaaS，可以在几分钟内部署为站点到站点 VPN
    连接。AWS 提供类似的托管 VPN 功能，作为其 VPC 服务的一部分，只需创建 VPN 网关并配置客户网关（即 OpenStack 终端）即可建立 VPN
    连接。
- en: Important note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When using a site-to-site VPN connection in AWS, two tunnels will be created
    for redundancy. Each tunnel terminates in a different Availability Zone. If one
    tunnel is down, traffic is automatically shifted to the second active tunnel.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS 中的站点到站点 VPN 连接时，将创建两个隧道以实现冗余。每个隧道终止于不同的可用区。如果一个隧道出现故障，流量会自动切换到第二个活动隧道。
- en: VPN can be the quickest way to establish such a hybrid cloud connection. A major
    pitfall of a site-to-site VPN connection linking two different locations across
    different regions is limited performance. In AWS’ case, each VPN tunnel can support
    a bandwidth of up to 1.25 Gbps. Other public cloud providers might offer different
    options. For example, Azure allows cloud users to specify the type of VPN gateway
    they require, and depending on its size, the bandwidth can vary between 100 Mbps
    and 10 Gbps.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: VPN 可以是建立这种混合云连接的最快方式。连接两个不同地区的不同位置的站点到站点 VPN 连接的一个主要陷阱是性能有限。在 AWS 的情况下，每个 VPN
    隧道最多可以支持 1.25 Gbps 的带宽。其他公共云提供商可能提供不同的选项。例如，Azure 允许云用户指定所需的 VPN 网关类型，根据其大小，带宽可以在
    100 Mbps 到 10 Gbps 之间变化。
- en: On the OpenStack side, Neutron will provision a virtual appliance with a certain
    bandwidth limit. Neutron offers a **Quality of Service** ( **QoS** ) feature that
    can be used to set network traffic policies for better network bandwidth control.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 端，Neutron 会配置一个具有特定带宽限制的虚拟设备。Neutron 提供了 **服务质量**（**QoS**）功能，可用于设置网络流量策略，以更好地控制网络带宽。
- en: When dealing with complex and large-scale workloads that need to exist and move
    between different cloud environments, a limited bandwidth capacity can present
    a bottleneck. Moreover, a consistent connection between both cloud environments
    using a simple VPN connection cannot always be guaranteed. This is due to the
    long path taken across shared internet devices. For that reason, public cloud
    providers offer a second category network connection to establish a connection
    to a private environment. In the AWS world, **DirectConnect** is an ideal option
    when a stable, dedicated, isolated, and higher-performance connection is required.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理需要在不同云环境之间存在并移动的复杂和大规模工作负载时，有限的带宽容量可能会成为瓶颈。此外，使用简单的 VPN 连接来保持两个云环境之间的一致连接并不总是能够得到保障。这是因为数据需要经过共享的互联网设备，路径较长。因此，公共云服务提供商提供了第二种类型的网络连接，以便与私有环境建立连接。在
    AWS 世界中，当需要稳定、专用、隔离且高性能的连接时，**DirectConnect** 是一个理想的选择。
- en: 'Implementing a DirectConnect connection requires more steps in the deployment
    process, unlike a VPN connection setup. Cloud operators can order a DirectConnect
    connection via the AWS console or CLI and decide which type of connection they
    will put in place. DirectConnect offers two kinds of connections:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实施 DirectConnect 连接需要比 VPN 连接设置更多的部署步骤。云操作员可以通过 AWS 控制台或 CLI 订购 DirectConnect
    连接，并决定要使用哪种类型的连接。DirectConnect 提供两种类型的连接：
- en: '**A hosted connection** : An AWS partner will handle the physical connection
    and link it to the device sitting at the edge of the OpenStack private cloud environment.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托管连接**：AWS 合作伙伴将处理物理连接，并将其链接到位于 OpenStack 私有云环境边缘的设备。'
- en: '**A dedicated connection** : This establishes a physical connection from the
    OpenStack endpoint to an AWS DirectConnect location. This type of connection comes
    with multiple bandwidth capacity options.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专用连接**：这建立了从 OpenStack 端点到 AWS DirectConnect 位置的物理连接。此类型的连接提供多个带宽容量选项。'
- en: 'Which of these connectivity options you choose depends on many factors, such
    as cost, geographic location, and bandwidth throughput. To learn more about AWS
    DirectConnect technical details, check out the following AWS URL: [https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html](https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这些连接选项中的哪一种取决于许多因素，如成本、地理位置和带宽吞吐量。要了解有关 AWS DirectConnect 技术细节的更多信息，请查看以下
    AWS URL：[https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html](https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html)
- en: Hybrid connectivity requires a certain level of redundancy. If one link fails,
    you still have a second to carry the traffic, even with lower bandwidth throughput.
    The previous architecture diagram exposed a classic network design that leverages
    two connections, established using DirectConnect and VPN. Operators can configure
    their devices at the OpenStack private cloud end to primarily receive traffic
    on the DirectConnect interface. That will terminate both connections into two
    different devices in the same location endpoint. If the DirectConnect connection
    fails for some reason, traffic will be switched to flow across the VPN connection
    as a backup. This design pattern is considered a good compromise that grants high
    availability at a fair price. If a hybrid connection requires the same consistent
    bandwidth with exact configurations in both links, two DirectConnect links will
    be put in place. That will maximize resiliency for critical production workloads.
    However, that comes of course with a higher bill. AWS recommends using dynamic
    routing to allow connections to fail over automatically and leverage the best
    available routes. This routing capability requires that the OpenStack hardware
    endpoint supports dynamic routing and auto-failover features.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 混合连接需要一定程度的冗余。如果一条链路失败，你仍然可以使用第二条链路来传输流量，即使带宽吞吐量较低。之前的架构图展示了经典的网络设计，利用 DirectConnect
    和 VPN 建立了两条连接。操作员可以在 OpenStack 私有云端配置设备，主要通过 DirectConnect 接口接收流量。这将把两条连接终止到同一位置端点的两个不同设备上。如果由于某种原因，DirectConnect
    连接失败，流量将切换到通过 VPN 连接传输，作为备份。这种设计模式被认为是一种良好的折衷方案，能够在合理的价格下提供高可用性。如果混合连接需要相同的一致带宽，并且两个链路的配置完全一致，那么将会配置两条
    DirectConnect 链路。这样可以最大化关键生产工作负载的弹性。然而，当然这也会带来更高的费用。AWS 建议使用动态路由，以便连接能够自动故障转移，并利用最佳可用路径。此路由功能要求
    OpenStack 硬件端点支持动态路由和自动故障转移功能。
- en: Now that we have covered one of the most critical pieces of hybrid cloud design,
    networking, we will move on to the next part to cover different ways to design
    a containerized hybrid cloud setup.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了混合云设计中最关键的部分之一——网络，我们将继续讨论设计容器化混合云设置的不同方法。
- en: Designing a containerized hybrid cloud
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计容器化混合云
- en: 'Container technology is the way to go to develop modern applications with all
    the benefits of portability, speed of deployment, and less operational overhead.
    The evolution of containerization has allowed organizations to run and scale their
    workloads in an almost zero-touch, reliable, and rapid way. With the rise of several
    forms of container engines such as Docker and LXD, a few container orchestration
    systems such as Kubernetes, Mesos, and Docker Swarm have come to light and empowered
    the way of developing applications. Today, we can find organizations deploying
    production workloads with more confidence than ever before. Container orchestration
    systems have shifted the gears on how applications are developed with the evolution
    of microservices architecture. Public cloud providers seized the opportunity to
    provide a platform to run a fully containerized environment. AWS, for example,
    offers (at the time of writing) three main container-related services, which are
    summarized as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 容器技术是开发现代应用程序的最佳选择，具备可移植性、快速部署和更少操作开销等优势。容器化的发展使得组织能够以几乎零接触、可靠和快速的方式运行和扩展其工作负载。随着
    Docker 和 LXD 等多种容器引擎的兴起，Kubernetes、Mesos 和 Docker Swarm 等容器编排系统也应运而生，推动了应用开发的方式。如今，组织部署生产工作负载的信心比以往任何时候都要强。容器编排系统通过微服务架构的演变，改变了应用程序开发的方式。公共云服务商抓住了这个机会，提供了运行完全容器化环境的平台。例如，AWS（截至本文写作时）提供了三大主要的与容器相关的服务，概述如下：
- en: '**Elastic Container Service** ( **ECS** ): Deploys and manages containerized
    workloads through a fully managed container orchestration service'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性容器服务** (**ECS**)：通过完全托管的容器编排服务，部署和管理容器化工作负载。'
- en: '**Elastic Kubernetes Service** ( **EKS** ): Runs and manages a Kubernetes control
    plane that is responsible for container deployment, scheduling, and availability'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性 Kubernetes 服务** (**EKS**)：运行和管理 Kubernetes 控制平面，负责容器部署、调度和可用性。'
- en: '**Fargate** : Acts as a serverless service that deploys and manages applications
    without the need to manage the underlying infrastructure'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fargate**：充当一种无服务器服务，部署和管理应用程序，而无需管理底层基础设施。'
- en: Azure has similar managed container services such as **Azure Container Instances**
    ( **ACI** ), **Azure Container Apps** ( **ACA** ), and **Azure Kubernetes Service**
    ( **AKS** ). Kubernetes has been a great deal for GCP, which has developed **Google
    Kubernetes Engine** ( **GKE** ) to reliably and efficiently deploy and scale containerized
    workloads on Kubernetes. The history of Kubernetes adoption could fill an entire
    book.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 也提供类似的托管容器服务，例如 **Azure 容器实例** (**ACI**)、**Azure 容器应用** (**ACA**) 和 **Azure
    Kubernetes 服务** (**AKS**)。Kubernetes 对 GCP 来说至关重要，GCP 开发了 **Google Kubernetes
    Engine** (**GKE**)，用于可靠且高效地部署和扩展容器化工作负载。Kubernetes 的采用历史足以写成一本书。
- en: Deploying applications on containers in a hybrid cloud architecture requires
    a good understanding of a few design patterns. Organizations should seek the right
    tools that fit into their strategic initiative to take advantage of the hybrid
    model. In the following sections, we will explore a few implemented design patterns
    to deploy a hybrid containerized architecture using Kubernetes. Before tackling
    the different design models, we will pick up Kubernetes as a containerization
    platform. One of many reasons to choose Kubernetes is that it is widely used,
    with plenty of features and tools available in its ecosystem. Another great benefit
    of adopting Kubernetes is its flexibility to design clusters and run containers,
    surrounded by several stable open source tools for container management across
    platforms, as well as its support federation feature that fits into a hybrid cloud
    architecture. Before delving into the previously mentioned design benefits of
    building a hybrid model, let’s briefly look at Kubernetes jargon.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合云架构中部署容器化应用程序需要对一些设计模式有较好的理解。组织应该选择适合其战略目标的工具，以利用混合模型。在接下来的章节中，我们将探讨几种实现的设计模式，用于使用
    Kubernetes 部署混合容器化架构。在处理不同的设计模型之前，我们将选择 Kubernetes 作为容器化平台。选择 Kubernetes 的众多原因之一是它被广泛使用，且其生态系统中有许多功能和工具。采用
    Kubernetes 的另一个重要好处是它在设计集群和运行容器方面的灵活性，周围有多个稳定的开源容器管理工具，跨平台使用，并且支持适应混合云架构的联合功能。在深入探讨前面提到的构建混合模型的设计优势之前，让我们简要了解
    Kubernetes 的术语。
- en: Kubernetes in a nutshell
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 简介
- en: Today, Kubernetes is everywhere. Eight years ago, companies were hesitant to
    start a production journey running Kubernetes, as it was new to the market since
    Google had recently made it publicly available. The concept of a container orchestration
    engine was not new at the time, since Docker Swarm and Mesos were quite evolved
    and widely used. Kubernetes, as expected, came with a different engine concept.
    Understanding the basic concepts of how it works is an essential step to confidently
    run it in production environments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，Kubernetes 无处不在。八年前，企业对于使用 Kubernetes 开始生产旅程犹豫不决，因为它刚刚由 Google 公布在市场上，还是个新生事物。容器编排引擎的概念在当时并不新鲜，因为
    Docker Swarm 和 Mesos 已经相当成熟并被广泛使用。正如预期的那样，Kubernetes 提供了一种不同的引擎概念。理解其基本概念如何运作是自信地在生产环境中运行
    Kubernetes 的重要一步。
- en: For container enthusiasts, using cloud services will facilitate the adoption
    of a containerization model based on Kubernetes. As mentioned in the previous
    section, cloud hyperscalers offer a managed service that supports Kubernetes.
    The Kubernetes PaaS model simply removes any overhead to manage and install the
    containerization orchestration engine. This also includes additional networking,
    storage, and availability aspects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于容器爱好者来说，使用云服务将有助于采用基于 Kubernetes 的容器化模型。如前一节所述，云超大规模厂商提供支持 Kubernetes 的托管服务。Kubernetes
    PaaS 模型简单地消除了管理和安装容器编排引擎的开销。这还包括额外的网络、存储和可用性方面。
- en: 'As shown in the following figure, Kubernetes is designed to run different types
    of nodes within a cluster:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，Kubernetes 旨在在集群内运行不同类型的节点：
- en: '![Figure 11.3 – The Kubernetes engine architecture at a high level](img/B21716_11_03.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – Kubernetes 引擎架构（高层次）](img/B21716_11_03.jpg)'
- en: Figure 11.3 – The Kubernetes engine architecture at a high level
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – Kubernetes 引擎架构（高层次）
- en: 'The node categories can be summarized as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 节点类别可以总结如下：
- en: '**Master node** : This is responsible for running most of the essential processes
    that manage a Kubernetes cluster, including the following:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主节点** ：它负责运行管理 Kubernetes 集群的大多数关键进程，包括以下内容：'
- en: '**Scheduler** : This runs the **kube-scheduler** process that manages the placement
    of containers on proper hosts based on resource availability and load.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器** ：它运行 **kube-scheduler** 进程，管理容器根据资源可用性和负载在适当的主机上部署。'
- en: '**Controller manager** : This runs the **kube-controller-manager** process
    that keeps an eye on the cluster activities.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器管理器** ：它运行 **kube-controller-manager** 进程，负责监控集群活动。'
- en: '**API server** : This runs the **kube-apiserver** process that allows communication
    between a Kubernetes client and the cluster. The master node can be reached via
    the API server.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 服务器** ：它运行 **kube-apiserver** 进程，允许 Kubernetes 客户端与集群之间进行通信。可以通过 API
    服务器访问主节点。'
- en: '**etcd** : This is a key-value storage that handles the state of the cluster.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd** ：这是一个键值存储系统，用于处理集群的状态。'
- en: '**Worker node** : This runs the workloads in containers and is composed of
    two components:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：这是运行容器中的工作负载的节点，包含两个组件：'
- en: '**Kubelet service** : This runs the K **ubelet** process that allows cluster
    inter-communication. A Kubelet process also listens from the API server to manage
    containers in the worker node.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubelet服务**：这是运行**Kubelet**进程的服务，允许集群内部的通信。Kubelet进程还会从API服务器监听，管理工作节点中的容器。'
- en: '**Kube-proxy service** : This runs the **kube-proxy** service that handles
    networking between services in a Kubernetes cluster.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kube-proxy服务**：这是运行**kube-proxy**服务的组件，负责Kubernetes集群中服务之间的网络通信。'
- en: 'Kubernetes jargon introduces a few concepts as well as those related to its
    cluster-based architecture description, as illustrated in the following diagram:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes术语引入了几个概念，以及与其基于集群架构描述相关的内容，如下图所示：
- en: '![Figure 11.4 – The Kubernetes engine architecture at a low level](img/B21716_11_04.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – Kubernetes引擎的低级架构](img/B21716_11_04.jpg)'
- en: Figure 11.4 – The Kubernetes engine architecture at a low level
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – Kubernetes引擎的低级架构
- en: 'Different cluster components can be described as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的集群组件可以描述如下：
- en: '**Pod** : This is defined as the smallest and most basic component in a Kubernetes
    environment. A pod combines and runs containers, attaches storage as required,
    and assigns a unique IP address for networking.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod**：这是定义为Kubernetes环境中最小且最基本的组件。Pod将容器结合在一起并运行，按需附加存储，并为网络通信分配唯一的IP地址。'
- en: '**Controller** : This presents the main piece of the container orchestration
    engine. It manages all different aspects of a pod such as creation, deletion,
    replication, and rollouts. A Kubernetes controller keeps track of workload placements
    and allocated resources. There are different kinds of controllers, including **ReplicaSet**
    , **StatefulSet** , **DaemonSet** , **Deployment** , **Job** , and **CronJob**
    . Each controller type has a different use for specific pod management purposes.
    For example, the **ReplicaSet** controller creates a set of pods that all run
    the same application. **CronJob** is more dedicated to creating scheduled jobs.
    To read more about the Kubernetes controller types, refer to the official Kubernetes
    page at [https://kubernetes.io/docs/concepts/workloads/controllers/](https://kubernetes.io/docs/concepts/workloads/controllers/)
    .'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器**：这是容器编排引擎的主要部分。它管理Pod的各个方面，如创建、删除、复制和发布。Kubernetes控制器跟踪工作负载的位置和分配的资源。控制器有不同的类型，包括**ReplicaSet**、**StatefulSet**、**DaemonSet**、**Deployment**、**Job**和**CronJob**。每种控制器类型有不同的用途，针对特定的Pod管理需求。例如，**ReplicaSet**控制器创建一组运行相同应用程序的Pod。**CronJob**更专注于创建定时任务。要了解更多关于Kubernetes控制器类型的信息，请参考Kubernetes官方网站的页面：[https://kubernetes.io/docs/concepts/workloads/controllers/](https://kubernetes.io/docs/concepts/workloads/controllers/)。'
- en: '**Service** : This exposes an application that runs in one or multiple Pods
    for incoming requests. A Kubernetes service routes each request to the respective
    pod(s) and provides additional capabilities, such as DNS and load balancing to
    access a pod.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Service**：这是暴露在一个或多个Pod中运行的应用程序，以便接收传入请求。Kubernetes服务将每个请求路由到相应的Pod，并提供额外的功能，如DNS和负载均衡来访问Pod。'
- en: '**Namespace** : This is a scoped logical layer to isolate Kubernetes resource
    names, thus avoiding name collisions when many users launch several Kubernetes
    clusters in the *same* physical cluster.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Namespace**：这是一个作用域逻辑层，用于隔离Kubernetes资源名称，从而避免在多个用户在*同一*物理集群中启动多个Kubernetes集群时发生名称冲突。'
- en: '**Volume** : This provides persistent storage when required by a workload running
    in Pods. The latest version of Kubernetes supports more storage and volume features,
    as well as an exhaustive list of volume types. A full list of supported types
    of volumes can be found at [https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)
    .'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Volume**：当Pod中的工作负载需要时，提供持久存储。Kubernetes的最新版本支持更多存储和卷功能，以及一个详尽的卷类型列表。可以在[https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)找到受支持的卷类型的完整列表。'
- en: Now that we have covered most of the important aspects of the Kubernetes world,
    we can verify how it fits into a hybrid cloud setup by exploring two of the most
    widely adopted deployment models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了Kubernetes世界中的大部分重要方面，我们可以通过探索两种最广泛采用的部署模型，验证它如何适应混合云架构。
- en: Designing a decentralized models
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计去中心化模型
- en: 'The decentralized Kubernetes model, which is adapted for cloud hybrid architecture,
    is referred to as a *bursting* model. Organizations adopt such a design to overcome
    the over-provisioning of resources in their private clouds by leveraging public
    cloud resources. In such a layout, each cloud environment has its own Kubernetes
    infrastructure deployed and managed separately. In a second iteration, operators
    need to manage network connectivity between clusters that reside in both cloud
    environments. The next figure illustrates public cloud bursting in a hybrid setup
    where a Kubernetes cluster that runs in a private cloud extends its resources
    by reaching public resources (additional worker nodes):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化的 Kubernetes 模型，适用于云混合架构，称为*爆发*模型。组织采用这种设计，通过利用公共云资源来克服其私有云中资源的过度配置。在这种布局中，每个云环境都有自己的
    Kubernetes 基础设施，分别进行部署和管理。在第二次迭代中，操作员需要管理位于两个云环境中的集群之间的网络连接。下图展示了在混合设置中，私有云中运行的
    Kubernetes 集群通过接入公共资源（额外的工作节点）来扩展其资源的公共云爆发。
- en: '![Figure 11.5 – A hybrid bursting model for OpenStack and a public cloud provider](img/B21716_11_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – OpenStack 和公共云提供商的混合爆发模型](img/B21716_11_05.jpg)'
- en: Figure 11.5 – A hybrid bursting model for OpenStack and a public cloud provider
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – OpenStack 和公共云提供商的混合爆发模型
- en: Depending on each public Kubernetes managed service, operators will need to
    define extra parameters, such as DNS, cluster API endpoint, and namespace configurations,
    so that public resources can be used within the same workload or application context.
    The implementation of such a model can be far from complex and requires additional
    tweaks to properly synchronize the Kubernetes cluster resources, such as services
    and deployments. Operators must ensure that both clusters are consistent when
    it comes to configuration, identity management, and workload deployments.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每个公共 Kubernetes 托管服务，操作员需要定义额外的参数，例如 DNS、集群 API 端点和命名空间配置，以便在同一工作负载或应用上下文中使用公共资源。实现这种模型可能并不复杂，但需要额外的调整来正确同步
    Kubernetes 集群资源，如服务和部署。操作员必须确保两个集群在配置、身份管理和工作负载部署方面的一致性。
- en: 'As highlighted at the beginning of this section, AWS offers two managed services
    for workloads based on Kubernetes: EKS and Fargate. Before drafting a bursting
    Kubernetes layout joining the OpenStack and AWS environments, it is essential
    to recognize the use cases for each AWS EKS and AWS Fargate service, as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节开头所强调的，AWS 提供了两种托管服务，供基于 Kubernetes 的工作负载使用：EKS 和 Fargate。在设计一个结合 OpenStack
    和 AWS 环境的爆发式 Kubernetes 布局之前，必须认识到每个 AWS EKS 和 AWS Fargate 服务的使用场景，如下所示：
- en: 'Use EKS if you need to do the following:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要执行以下操作，请使用 EKS：
- en: Provision a managed Kubernetes cluster by AWS without the need to manage its
    control plane and operate its underlying infrastructure and scaling and security
    configurations
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 AWS 提供托管的 Kubernetes 集群，无需管理其控制平面，并且可以操作其底层基础设施及扩展和安全配置。
- en: Have more flexibility in the networking, storage, and scaling options
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络、存储和扩展选项上有更多的灵活性。
- en: Deploy complex and large workloads
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署复杂和大型工作负载。
- en: Have more control of its deployment of cross-regional, on-premises, and private
    and public clouds
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好地控制跨区域、本地部署以及私有云和公共云的部署。
- en: Have a wider integration with AWS native services for advanced configuration,
    such as Elastic Load Balancing and AWS App Mesh
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 AWS 原生服务有更广泛的集成，支持高级配置，如弹性负载均衡和 AWS App Mesh。
- en: 'Use Fargate if you *don’t* need to do the following:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你*不*需要执行以下操作，请使用 Fargate：
- en: Provision a Kubernetes infrastructure and its associated resources
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供 Kubernetes 基础设施及其相关资源
- en: Manage workloads by provisioning resources (EC2 instances) but via task definitions
    instead
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过任务定义来管理工作负载并提供资源（EC2 实例）。
- en: Run containers based on cluster and instance sizes but on the CPU and RAM resources
    you need instead
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于集群和实例大小运行容器，但使用你所需的 CPU 和 RAM 资源。
- en: Not focus on managing the cluster nodes but the workloads instead
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不专注于管理集群节点，而是专注于管理工作负载。
- en: Include additional features that come with the Kubernetes engine
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括 Kubernetes 引擎附带的额外功能。
- en: If you require advanced configurations that support a cloud-agnostic setup and
    more networking customization, then EKS would be a suitable choice for a hybrid
    layout.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要支持云无关设置和更多网络定制的高级配置，那么 EKS 将是适合混合布局的选择。
- en: Another key consideration is that when employing storage in a Kubernetes hybrid
    decentralized model, both storage (volume) endpoints should have the same data.
    There are several ways to ensure data synchronization across both storage endpoints
    by leveraging some public cloud-managed storage services. For example, a volume
    attached to Pods running in a Kubernetes cluster in OpenStack can be copied over
    a private connection to AWS using **DataSync** ( [https://aws.amazon.com/datasync/](https://aws.amazon.com/datasync/)
    ).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键考虑因素是，在采用 Kubernetes 混合分布式模型时，两个存储（卷）端点应该具有相同的数据。有几种方法可以通过利用一些公共云托管存储服务来确保两个存储端点之间的数据同步。例如，附加到
    OpenStack 中 Kubernetes 集群上运行的 Pods 的卷，可以通过私有连接使用 **DataSync**（[https://aws.amazon.com/datasync/](https://aws.amazon.com/datasync/)）复制到
    AWS。
- en: As shown in the following architecture diagram, DataSync requires an endpoint
    for the inbound data transfer from a private cloud storage endpoint. You can create
    an **Elastic File System** ( **EFS** ) as a destination location in your AWS VPC.
    The EFS location can be configured in the **DataSync** task so that any data being
    transferred from a volume located in the private cloud is synced in an EFS share.
    It is important to note that it is possible to mount an EFS filesystem on Fargate,
    but it is only limited to volume static provisioning, whereas, with EKS nodes,
    it is possible to use dynamic persistent volume provisioning.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，DataSync 需要一个端点用于从私有云存储端点传入数据。你可以在 AWS VPC 中创建一个 **弹性文件系统**（**EFS**）作为目标位置。EFS
    位置可以在 **DataSync** 任务中配置，以便从私有云中位于卷上的任何数据都能同步到 EFS 共享。需要注意的是，虽然可以在 Fargate 上挂载
    EFS 文件系统，但仅限于卷静态配置，而在 EKS 节点上，则可以使用动态持久卷配置。
- en: '![Figure 11.6 – A hybrid Kubernetes bursting model with volume synchronization](img/B21716_11_06.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 具有卷同步的混合 Kubernetes 溢出模型](img/B21716_11_06.jpg)'
- en: Figure 11.6 – A hybrid Kubernetes bursting model with volume synchronization
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 具有卷同步的混合 Kubernetes 溢出模型
- en: This hybrid model is suitable for workloads that do not require low latency
    with immediate scaling resources dynamically during peak demand. In such a model,
    cloud operators should expect more networking management, as well as ensuring
    data consistency across hybrid cloud environments. Managing multiple Kubernetes
    clusters across different cloud environments can be overwhelming, even when you
    employ public Kubernetes-managed services such as EKS and Fargate. The good news
    is that a few open source tools and wrappers based on the Kubernetes ecosystem
    have been developed to empower more extended designs. **Kubernetes Cloud Instance
    Provider** ( **KIP** ) enables you to manage Pods in cloud instances from one
    single and simplified interface. The next section will explore an abstracted layout
    that measures a decentralized hybrid Kubernetes layout between AWS and OpenStack
    using KIP.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混合模型适用于不要求低延迟和在高峰需求期间动态立即扩展资源的工作负载。在这种模型中，云运营商应该预期需要更多的网络管理，并确保混合云环境中的数据一致性。即使你使用像
    EKS 和 Fargate 这样的公共 Kubernetes 托管服务，管理跨不同云环境的多个 Kubernetes 集群也可能会感到压力很大。好消息是，一些基于
    Kubernetes 生态系统的开源工具和封装器已经被开发出来，以支持更广泛的设计。**Kubernetes 云实例提供商**（**KIP**）使你能够通过一个简化的接口管理云实例中的
    Pods。下一节将探索一个抽象的布局，展示使用 KIP 在 AWS 和 OpenStack 之间实现去中心化混合 Kubernetes 布局。
- en: Hybrid cloud bursting
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合云溢出
- en: Before tackling the idea behind KIP in this section, it is essential to cover
    briefly the term *Kubelet* as another key concept of the Kubernetes framework.
    The Kubelet is simply an agent that runs Kubernetes at the node level. It provides
    all kinds of features that deal with Pods, such as deployment, management, and
    communication between nodes and Pods. A Kubelet can be considered a coordinator
    between the Kubernetes control plane and the containers running on nodes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中探讨 KIP 背后的思想之前，有必要简要介绍 *Kubelet* 这一 Kubernetes 框架中的另一个关键概念。Kubelet 只是一个在节点级别运行
    Kubernetes 的代理。它提供了处理 Pods 的各种功能，如部署、管理以及节点与 Pods 之间的通信。Kubelet 可以被看作是 Kubernetes
    控制平面与运行在节点上的容器之间的协调者。
- en: Important note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To read more about the Kubelet, refer to the official Kubernetes page found
    at [https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Kubelet 的内容，请参阅官方 Kubernetes 页面：[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)
- en: Based on the same Kubelet concept, the **Cloud Native Computing Foundation**
    ( **CNCF** ) has initiated an open source project that focuses on a **Virtual
    Kubelet** . The official Virtual Kubelet project web page can be found at [https://virtual-kubelet.io/](https://virtual-kubelet.io/)
    . It defines it as *“an open source Kubernetes kubelet implementation that masquerades
    as a kubelet.”* The term *masquerades* is quite metaphoric, but one way to understand
    its concept is to consider the difference between a Kubelet and a Virtual Kubelet.
    The Virtual Kubelet provides more capabilities than the standard Kubelet by scheduling
    the provisioning of containers in other cloud solutions, but not on the nodes
    themselves. Under the hood, the Virtual Kubelet is highly customizable, allowing
    users to use other cloud environments and deploy Pods with their own APIs. The
    other piece of the Virtual Kubelet is the **Kubelet providers** . A Virtual Kubelet
    provider is a pluggable interface that provides all the necessary operational
    features and management of Pods and containers. A few of the providers commonly
    used to back Kubernetes nodes on cloud container platforms are AWS Fargate, Azure
    Container Instances, and OpenStack’s Zun. An up-to-date list of supported providers
    can be found at [https://virtual-kubelet.io/docs/providers/#current-providers](https://virtual-kubelet.io/docs/providers/#current-providers)
    .
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基于相同的Kubelet概念，**云原生计算基金会**（**CNCF**）发起了一个专注于**虚拟Kubelet**的开源项目。该项目的官方网站可以在[https://virtual-kubelet.io/](https://virtual-kubelet.io/)找到。它将其定义为*“一种开源的Kubernetes
    Kubelet实现，伪装成一个Kubelet。”* 术语*伪装*具有相当的隐喻性，但理解其概念的一种方式是考虑Kubelet和虚拟Kubelet之间的区别。虚拟Kubelet通过调度在其他云解决方案中提供容器的资源，但不在节点上执行，从而提供比标准Kubelet更多的功能。在幕后，虚拟Kubelet是高度可定制的，允许用户使用其他云环境并通过自己的API部署Pod。虚拟Kubelet的另一个部分是**Kubelet提供者**。虚拟Kubelet提供者是一个可插拔接口，提供所有必要的操作特性和Pod及容器的管理。用于支持云容器平台上Kubernetes节点的一些常见提供者包括AWS
    Fargate、Azure容器实例和OpenStack的Zun。一个最新的支持提供者列表可以在[https://virtual-kubelet.io/docs/providers/#current-providers](https://virtual-kubelet.io/docs/providers/#current-providers)找到。
- en: The following diagram illustrates a simple cloud bursting example, from a private
    OpenStack cloud environment to the AWS public cloud. In an OpenStack tenant environment,
    a Kubernetes cluster runs alongside a Virtual Kubelet pod.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个简单的云爆发示例，从私有的OpenStack云环境到AWS公共云。在OpenStack租户环境中，一个Kubernetes集群与一个虚拟Kubelet
    pod一起运行。
- en: '![Figure 11.7 – Hybrid Kubernetes bursting between OpenStack and AWS via KIP](img/B21716_11_07.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7 – 通过KIP在OpenStack和AWS之间的混合Kubernetes云爆发](img/B21716_11_07.jpg)'
- en: Figure 11.7 – Hybrid Kubernetes bursting between OpenStack and AWS via KIP
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 通过KIP在OpenStack和AWS之间的混合Kubernetes云爆发
- en: The other usage of a hybrid decentralized model is to deploy different Kubernetes
    clusters in more than one cloud environment (private and public) from a single
    deployer. This setup would require additional networking by connecting different
    clusters at the network layer. However, controlling and operating the clusters
    is performed individually. To centralize the management of workloads running across
    multiple cloud environments, we would need to provide a common control plane layer.
    This design brings the bursting model to the next level by adding a centralized
    control aspect to the hybrid model, which will be explored in more detail in the
    next subsection.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 混合去中心化模型的另一种用途是通过一个单一部署者在多个云环境（私有和公共）中部署不同的Kubernetes集群。这种设置需要额外的网络连接来在网络层连接不同的集群。然而，集群的控制和操作是独立执行的。为了集中管理在多个云环境中运行的工作负载，我们需要提供一个公共的控制平面层。这种设计通过在混合模型中加入集中的控制层次，提升了云爆发模型的层次，下一小节将详细探讨这一点。
- en: Designing a centralized model
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计一个集中式模型
- en: The second hybrid model consolidates different Kubernetes clusters that run
    in both the private and public clouds. This type of architecture is referred to
    as **Kubernetes Federation** , which is primarily based on multi-cluster deployments
    with additional and common control planes. Under the Kubernetes Federation setup,
    one single source of a workload deployment is applied to a **host cluster** as
    a central deployment location that reaches different clusters across different
    connected cloud endpoints. The deployment of the application is then propagated
    to all environments with a view of a single target cluster. That removes any need
    to configure or maintain the state of an application in each cluster separately.
    The **Federation model** is a desirable hybrid option when you need to manage
    several clusters through a single API interface. An application is packaged and
    deployed from a single source. It spreads out its deployments equally across all
    visible clusters. From a host cluster (master configuration host) perspective,
    all connected clusters under its view are considered member clusters, so each
    deployed application will have its replicas throughout all worker nodes at scale.
    Kubernetes Federation can also be customized during the application deployment.
    Some member clusters are part of different environments and might require specific
    local variables, such as network policies. Such member cluster-specific configurations
    can be supplemented by the host cluster holding the Federation configuration.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种混合模型整合了在私有云和公有云中运行的不同 Kubernetes 集群。这种架构被称为**Kubernetes 联邦**，它主要基于多集群部署，并具有额外的共用控制平面。在
    Kubernetes 联邦设置下，单一的工作负载部署源会应用到**主集群**，作为一个集中部署位置，能够覆盖跨越不同云端点的多个集群。应用的部署随后会传播到所有环境，且视图呈现为单一目标集群。这消除了在每个集群中分别配置或维护应用状态的需求。**联邦模型**是一个理想的混合选项，特别是在你需要通过单一
    API 接口管理多个集群时。应用从单一源打包并部署，且将其部署均匀分布在所有可见集群中。从主集群（主配置主机）的角度来看，所有其视野下的连接集群都被视为成员集群，因此每个已部署的应用将在所有工作节点上按比例拥有副本。Kubernetes
    联邦也可以在应用部署过程中进行自定义。一些成员集群属于不同环境，可能需要特定的本地变量，比如网络策略。这类特定于成员集群的配置可以由持有联邦配置的主集群进行补充。
- en: There are a variety of tools to deploy a hybrid Kubernetes centralized model.
    In the next section, we will focus on deploying a production-grade Kubernetes
    cluster across the OpenStack and AWS clouds using some trending open source solutions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种工具可以部署混合 Kubernetes 集中模型。在接下来的部分，我们将专注于使用一些流行的开源解决方案，在 OpenStack 和 AWS 云上部署生产级
    Kubernetes 集群。
- en: Deploying Kubernetes everywhere
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 到处部署 Kubernetes
- en: There are several ways to orchestrate the deployment of Kubernetes clusters.
    Using **PaaS** and **SaaS** can be the fastest way that uses the least operational
    overhead. The open source world has also contributed to facilitating the deployment
    across clouds, but that involves preparation and understanding how each solution
    works under the hood. In the following section, we will focus on an open source
    solution named **Juju** from Canonical.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以编排 Kubernetes 集群的部署。使用**PaaS**和**SaaS**可以是最快且操作开销最小的方式。开源界也为跨云部署提供了便利，但这涉及到准备工作和理解每种解决方案的底层实现。在接下来的部分，我们将重点介绍
    Canonical 提供的一个名为**Juju**的开源解决方案。
- en: Juju integrates very well with a hybrid cloud deployment model and, more specifically,
    Kubernetes workloads across different cloud environments. Under the hood, a **Juju
    controller** host should exist per environment, whereas a **Juju client** performs
    deployment actions and other functions. As shown in the following diagram, a Juju
    cloud controller runs in each cloud environment and supports multiple cloud environments,
    including AWS and OpenStack.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Juju 与混合云部署模型非常契合，尤其是在跨不同云环境的 Kubernetes 工作负载方面。在底层，每个环境应该有一个**Juju 控制器**主机，而**Juju
    客户端**则执行部署操作和其他功能。如下面的图所示，Juju 云控制器在每个云环境中运行，并支持包括 AWS 和 OpenStack 在内的多个云环境。
- en: '![Figure 11.8 – A Juju workflow in a hybrid deployment model](img/B21716_11_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – 混合部署模型中的 Juju 工作流](img/B21716_11_08.jpg)'
- en: Figure 11.8 – A Juju workflow in a hybrid deployment model
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 混合部署模型中的 Juju 工作流
- en: The latest list of Juju’s supported clouds can be found at [https://juju.is/docs/juju/juju-supported-clouds](https://juju.is/docs/juju/juju-supported-clouds)
    . This architecture can be compared to the **Puppet** or **Chef** system management
    tools. Puppet uses master-slave architecture, in which the slave component can
    be compared to a Juju client and the master component is akin to a Juju cloud
    controller.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 Juju 支持的云列表可以在[https://juju.is/docs/juju/juju-supported-clouds](https://juju.is/docs/juju/juju-supported-clouds)找到。这个架构可以与**Puppet**或**Chef**系统管理工具进行比较。Puppet
    使用主从架构，其中从组件可以与 Juju 客户端进行比较，主组件则类似于 Juju 云控制器。
- en: Important note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Several projects and open-source solutions exist that support hybrid and multi-cloud
    architecture using containers. Use cases of hybrid deployments using containers
    can be found at [https://openinfra.dev/hybrid-cloud/](https://openinfra.dev/hybrid-cloud/)
    .
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多个支持使用容器的混合云和多云架构的项目和开源解决方案。使用容器的混合部署用例可以在[https://openinfra.dev/hybrid-cloud/](https://openinfra.dev/hybrid-cloud/)找到。
- en: Juju, similar to Puppet (manifests) or Chef (cookbooks), uses its own script
    operators, which are named **charms** . Charms deploy applications across different
    infrastructure resources including containers, virtual machines, and even bare
    metal machines. Juju charms have been designed to be flexible for deployment and
    can be written in several programming languages.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Juju 类似于 Puppet（清单）或 Chef（食谱），使用其自己的脚本操作符，这些操作符被称为**charms**。Charms 在不同的基础设施资源上部署应用程序，包括容器、虚拟机，甚至裸金属机器。Juju
    charms 设计得非常灵活，支持多种编程语言进行编写。
- en: 'The following steps describe how a hybrid Kubernetes cluster can be deployed
    across public and private clouds:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了如何在公有云和私有云之间部署混合 Kubernetes 集群：
- en: Install the Juju client in a management/deployer host.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在管理/部署主机上安装 Juju 客户端。
- en: Configure the Juju client in the management host to bootstrap the AWS and OpenStack
    clouds.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在管理主机中配置 Juju 客户端，以引导 AWS 和 OpenStack 云。
- en: Deploy the Juju cloud controller in the AWS public cloud.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 公有云中部署 Juju 云控制器。
- en: Deploy the Juju cloud controller in the OpenStack private cloud.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 OpenStack 私有云中部署 Juju 云控制器。
- en: Deploy Kubernetes clusters across both private and public clouds using Juju.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Juju 在私有云和公有云之间部署 Kubernetes 集群。
- en: Connect both Kubernetes clusters.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接两个 Kubernetes 集群。
- en: Configure both Kubernetes clusters for federation.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置两个 Kubernetes 集群进行联合。
- en: 'In *step 1* , we will need to pick up a dedicated host for Juju cloud management
    and install the Juju client. This can be any machine that has connectivity to
    both the OpenStack and AWS environments. The Juju client supports several operating
    systems, a list of which can be found at [https://juju.is/docs/juju/install-juju](https://juju.is/docs/juju/install-juju)
    . The following step-by-step walkthrough installs the Juju client in a machine
    running on macOS:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 1 步*中，我们需要选择一台专门用于 Juju 云管理的主机，并安装 Juju 客户端。这可以是任何一台能够连接 OpenStack 和 AWS
    环境的机器。Juju 客户端支持多个操作系统，可以在[https://juju.is/docs/juju/install-juju](https://juju.is/docs/juju/install-juju)找到支持的操作系统列表。以下逐步指南展示了如何在运行
    macOS 的机器上安装 Juju 客户端：
- en: '[PRE0]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To install the Juju client on a Linux machine that runs the Ubuntu operating
    system, use the following command lines:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 Ubuntu 操作系统的 Linux 主机上安装 Juju 客户端时，使用以下命令行：
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Juju client installation can be checked by running its interactive command
    line interface, as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过运行交互式命令行界面来检查 Juju 客户端的安装情况，具体如下：
- en: '[PRE2]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![Figure 11.9 – Verifying the Juju installation](img/B21716_11_09.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – 验证 Juju 安装](img/B21716_11_09.jpg)'
- en: Figure 11.9 – Verifying the Juju installation
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – 验证 Juju 安装
- en: Important note
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The Juju command line does not return any Juju cloud controller, as we have
    fresh-installed, and so far, no cloud controller in any cloud has been configured.
    Juju automatically registers each newly configured cloud controller, as we will
    see later.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Juju 命令行不会返回任何 Juju 云控制器，因为我们是全新安装的，到目前为止，还没有在任何云中配置云控制器。Juju 会自动注册每个新配置的云控制器，稍后我们将看到。
- en: 'You can also list the currently supported list of cloud environments by running
    the following command line:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过运行以下命令行列出当前支持的云环境列表：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We get the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 11.10 – The Juju supported cloud providers list](img/B21716_11_10.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – Juju 支持的云服务提供商列表](img/B21716_11_10.jpg)'
- en: Figure 11.10 – The Juju supported cloud providers list
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – Juju 支持的云服务提供商列表
- en: The next step is to bootstrap our cloud environments using Juju. To interact
    with AWS resources and bootstrap the first Juju cloud controller, we will need
    to generate a key pair of EC2 instances in AWS, constructed by access and secret
    keys.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用Juju引导我们的云环境。为了与AWS资源进行交互并引导第一个Juju云控制器，我们需要生成AWS中EC2实例的密钥对，密钥对由访问密钥和密钥ID构成。
- en: Important note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: OpenStack is not listed here, as Juju lists only public cloud providers. Private
    clouds, including OpenStack, would require adding it via the Juju command-line
    interface, which will be covered in the following sections.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack没有列出在此处，因为Juju仅列出公共云提供商。包括OpenStack在内的私有云将需要通过Juju命令行界面添加，这将在接下来的章节中讲解。
- en: 'The following part of the walkthrough requires you to have an AWS account and
    an **Identity Access Management** ( **IAM** ) user with programmatic access. Access
    your AWS account and make sure that you have sufficient IAM permissions to create
    an IAM user with programmatic access. From the main console services tab, click
    **IAM** and select **Users** from the **Access management** drop-down option list
    in the bottom-left corner of the console:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分的操作步骤需要你拥有一个AWS账户，并且需要一个具有编程访问权限的**身份与访问管理**（**IAM**）用户。访问你的AWS账户，确保你拥有足够的IAM权限来创建一个具有编程访问权限的IAM用户。从主控制台的服务标签中，点击**IAM**，然后在控制台左下角的**访问管理**下拉菜单中选择**用户**：
- en: '![Figure 11.11 – The AWS IAM console](img/B21716_11_11.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – AWS IAM控制台](img/B21716_11_11.jpg)'
- en: Figure 11.11 – The AWS IAM console
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – AWS IAM控制台
- en: 'Next, click on the **Create** **user** button:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击**创建** **用户**按钮：
- en: '![Figure 11.12 – Creating an IAM user in the AWS console](img/B21716_11_12.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图11.12 – 在AWS控制台中创建IAM用户](img/B21716_11_12.jpg)'
- en: Figure 11.12 – Creating an IAM user in the AWS console
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12 – 在AWS控制台中创建IAM用户
- en: 'Provide an IAM user in the **User name** field and click on **Next** . Leave
    the **Provide user access to the AWS Management Console** checkbox unchecked,
    as we just need a user with programmatic access:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在**用户名**字段中提供一个IAM用户名，然后点击**下一步**。保持**提供用户访问AWS管理控制台**复选框不选中，因为我们只需要一个具有编程访问权限的用户：
- en: '![Figure 11.13 – Specifying user details in the AWS console](img/B21716_11_13.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13 – 在AWS控制台中指定用户详细信息](img/B21716_11_13.jpg)'
- en: Figure 11.13 – Specifying user details in the AWS console
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 – 在AWS控制台中指定用户详细信息
- en: 'The next step involves allowing our Juju user to perform on AWS resources.
    The Juju client will bootstrap a cloud controller in an EC2 instance. For the
    sake of simplicity and based on the least privilege principle, we will assign
    an AWS-managed policy with full permissions on EC2 resources in AWS by attaching
    an **AmazonEC2FullAccess** policy to the created user, as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是允许我们的Juju用户在AWS资源上执行操作。Juju客户端将在EC2实例中引导一个云控制器。为了简单起见，并且基于最小权限原则，我们将通过附加**AmazonEC2FullAccess**策略来为创建的用户分配对AWS中EC2资源的完全权限，如下所示：
- en: '![Figure 11.14 – Assigning an IAM policy to the Juju IAM user in the AWS console](img/B21716_11_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14 – 在AWS控制台中为Juju IAM用户分配IAM策略](img/B21716_11_14.jpg)'
- en: Figure 11.14 – Assigning an IAM policy to the Juju IAM user in the AWS console
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – 在AWS控制台中为Juju IAM用户分配IAM策略
- en: Important note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: From a best security practice perspective in AWS, it is not recommended to attach
    a full array of permissions to an IAM user or role. Custom policies can restrict
    the allowed actions on services and resources. As a rule of thumb, always use
    the least privilege principle when creating policies and assigning permissions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从AWS最佳安全实践的角度来看，不建议向IAM用户或角色附加一整套权限。自定义策略可以限制对服务和资源的允许操作。作为经验法则，在创建策略和分配权限时，始终遵循最小权限原则。
- en: 'Click on **Next** and on the **Review and create** page, click on **Create
    user** :'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**下一步**，然后在**审查并创建**页面上，点击**创建用户**：
- en: '![Figure 11.15 – Reviewing and creating the Juju IAM user in the AWS console](img/B21716_11_15.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15 – 在AWS控制台中审查并创建Juju IAM用户](img/B21716_11_15.jpg)'
- en: Figure 11.15 – Reviewing and creating the Juju IAM user in the AWS console
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – 在AWS控制台中审查并创建Juju IAM用户
- en: 'On the **IAM** | **Users** dashboard, select the newly created Juju IAM user.
    In the **Summary** tab, click on the **Create access** **key** link:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在**IAM** | **用户**仪表板中，选择新创建的Juju IAM用户。在**摘要**标签下，点击**创建访问** **密钥**链接：
- en: '![Figure 11.16 – Creating an access key for the Juju IAM user in the AWS console](img/B21716_11_16.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图11.16 – 在AWS控制台中为Juju IAM用户创建访问密钥](img/B21716_11_16.jpg)'
- en: Figure 11.16 – Creating an access key for the Juju IAM user in the AWS console
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16 – 在 AWS 控制台中为 Juju IAM 用户创建访问密钥
- en: 'The next page in the IAM user wizard will require you to select the credentials
    use case. Select the **Application running outside** **AWS** option:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: IAM 用户向导的下一页将要求你选择凭证的使用案例。请选择 **在 AWS 外部运行的应用程序** 选项：
- en: '![Figure 11.17 – Selecting an access key use case for the Juju IAM user in
    the AWS console](img/B21716_11_17.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.17 – 在 AWS 控制台中为 Juju IAM 用户选择访问密钥用例](img/B21716_11_17.jpg)'
- en: Figure 11.17 – Selecting an access key use case for the Juju IAM user in the
    AWS console
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17 – 在 AWS 控制台中为 Juju IAM 用户选择访问密钥用例
- en: 'The last step of the creation of the IAM user credentials is optional: providing
    a tag value for the AWS credentials resources. Add a tag value and click on **Create**
    **access key** :'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 IAM 用户凭证的最后一步是可选的：为 AWS 凭证资源提供标签值。添加标签值并点击**创建** **访问密钥**：
- en: '![Figure 11.18 – Adding an AWS tag resource for the Juju IAM user in the AWS
    console](img/B21716_11_18.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.18 – 在 AWS 控制台中为 Juju IAM 用户添加 AWS 标签资源](img/B21716_11_18.jpg)'
- en: Figure 11.18 – Adding an AWS tag resource for the Juju IAM user in the AWS console
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 – 在 AWS 控制台中为 Juju IAM 用户添加 AWS 标签资源
- en: 'This will generate a pair of access and secret keys. You can copy and save
    them or download a CSV file containing the credentials in a secure location locally:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一对访问密钥和秘密密钥。你可以将它们复制并保存在安全的位置，或者下载包含凭证的 CSV 文件到本地：
- en: '![Figure 11.19 – Retrieving the generated keys for the Juju IAM user in the
    AWS console](img/B21716_11_19.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.19 – 在 AWS 控制台中检索为 Juju IAM 用户生成的密钥](img/B21716_11_19.jpg)'
- en: Figure 11.19 – Retrieving the generated keys for the Juju IAM user in the AWS
    console
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 – 在 AWS 控制台中检索为 Juju IAM 用户生成的密钥
- en: 'Switch back to the Juju command-line interface in the management host and run
    the following command line to provide a key pair, including the access and secret
    keys created in the previous step:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 切换回管理主机中的 Juju 命令行界面，并运行以下命令行，提供一对密钥，包括在前一步中创建的访问密钥和秘密密钥：
- en: '[PRE4]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 11.20 – Adding AWS IAM user cloud credentials using the Juju client](img/B21716_11_20.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.20 – 使用 Juju 客户端添加 AWS IAM 用户云凭证](img/B21716_11_20.jpg)'
- en: Figure 11.20 – Adding AWS IAM user cloud credentials using the Juju client
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.20 – 使用 Juju 客户端添加 AWS IAM 用户云凭证
- en: The previous command line will optionally request which region Juju will be
    operating within by providing the generated access and secrets previously. Juju
    will create a local AWS profile named **packetpub-kube** .
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令行将可选地请求提供 Juju 将要操作的区域，通过提供先前生成的访问密钥和秘密密钥。Juju 将创建一个名为 **packetpub-kube**
    的本地 AWS 配置文件。
- en: 'The next step is to create our first Juju cloud controller in AWS by running
    the following command line:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是通过运行以下命令行，在 AWS 中创建我们的第一个 Juju 云控制器：
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We get the following output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 11.21 – Creating a Juju cloud controller in AWS](img/B21716_11_21.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.21 – 在 AWS 中创建 Juju 云控制器](img/B21716_11_21.jpg)'
- en: Figure 11.21 – Creating a Juju cloud controller in AWS
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.21 – 在 AWS 中创建 Juju 云控制器
- en: 'Under the hood, Juju performs an API call to the AWS environment and spawns
    an EC2 instance. The cloud controller instance can be checked in the AWS console
    in an initializing status from the EC2 dashboard:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，Juju 执行 API 调用至 AWS 环境，并启动一个 EC2 实例。可以通过 AWS 控制台中的 EC2 仪表盘查看云控制器实例的初始化状态：
- en: '![Figure 11.22 – A Juju cloud controller EC2 instance created in the AWS console](img/B21716_11_22.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.22 – 在 AWS 控制台中创建的 Juju 云控制器 EC2 实例](img/B21716_11_22.jpg)'
- en: Figure 11.22 – A Juju cloud controller EC2 instance created in the AWS console
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.22 – 在 AWS 控制台中创建的 Juju 云控制器 EC2 实例
- en: 'Note that Juju bootstraps the Juju controller with default configurations,
    such as the size of the instance ( **m7g.medium** ). Make sure, in the Juju command
    line output, that the bootstrapping process completes, as shown in the following
    figure:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Juju 使用默认配置引导 Juju 控制器，例如实例的大小（**m7g.medium**）。确保在 Juju 命令行输出中，启动过程完成，如下图所示：
- en: '![Figure 11.23 – The completion of Juju cloud controller bootstrapping in AWS](img/B21716_11_23.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.23 – Juju 云控制器在 AWS 中引导完成](img/B21716_11_23.jpg)'
- en: Figure 11.23 – The completion of Juju cloud controller bootstrapping in AWS
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.23 – Juju 云控制器在 AWS 中引导完成
- en: Our created controller host is assigned a public IP ( **34.233.124.117** ) that
    the Juju client communicates with.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的控制器主机被分配了一个公共 IP 地址（**34.233.124.117**），Juju 客户端与其通信。
- en: Important note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Juju bootstrap command can be customized further by specifying the size
    of the instance and the region where the cloud controller will be deployed. That
    can be achieved by using the **constraint** option in the Juju command-line interface,
    which uses a key-value format. A full list of Juju constraints can be found at
    [https://juju.is/docs/juju/constraint](https://juju.is/docs/juju/constraint) .
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Juju引导命令可以通过指定实例大小和云控制器部署的区域来进一步定制。可以通过在Juju命令行界面中使用**constraint**选项来实现，选项采用键值格式。有关Juju约束的完整列表，请访问[https://juju.is/docs/juju/constraint](https://juju.is/docs/juju/constraint)。
- en: 'The Juju cloud controller can be listed by running the following command line:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过运行以下命令行列出Juju云控制器：
- en: '[PRE6]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![Figure 11.24 – Listing the deployed Juju controller using the Juju CLI](img/B21716_11_24.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.24 – 使用Juju CLI列出已部署的Juju控制器](img/B21716_11_24.jpg)'
- en: Figure 11.24 – Listing the deployed Juju controller using the Juju CLI
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.24 – 使用Juju CLI列出已部署的Juju控制器
- en: Another amazing benefit of Juju deployment is the support of a user graphical
    interface that comes with several features, such as the interactive creation of
    controller systems and the status of deployments. The user interface link can
    be accessed by default via the assigned public IP of the EC2 instance, which is
    generated by running the Juju dashboard command line. If you are running Juju
    version 3.0 or later, you will need to add the **juju-dashboard** charm to the
    deployed controller.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Juju部署的另一个惊人优势是支持用户图形界面，提供了多个功能，例如交互式创建控制器系统和部署状态。用户界面链接默认可以通过EC2实例分配的公共IP访问，该IP通过运行Juju仪表板命令行生成。如果你运行的是Juju版本3.0或更高版本，你需要将**juju-dashboard**魅力添加到已部署的控制器。
- en: Important note
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Prior to version 3.0.0, the Juju dashboard charm was deployed by default when
    installing the Juju controller, and access details were generated by running the
    **$ juju guis** command line. The previous command line was dropped from Juju
    3.0.0 and later versions, and was replaced by the **$ juju dashboard** command-line
    option.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在版本3.0.0之前，Juju控制器安装时默认会部署Juju仪表板魅力，并通过运行**$ juju guis**命令行生成访问详情。此命令行已从Juju
    3.0.0及更高版本中删除，取而代之的是**$ juju dashboard**命令行选项。
- en: 'As we aim to expose Kubernetes resources, we will add the Kubernetes dashboard
    charm. Switch to the created controller by running the following command line:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们计划公开Kubernetes资源，我们将添加Kubernetes仪表板魅力。通过运行以下命令行切换到已创建的控制器：
- en: '[PRE7]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 11.25 – Switching Juju controllers using the Juju CLI](img/B21716_11_25.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.25 – 使用Juju CLI切换Juju控制器](img/B21716_11_25.jpg)'
- en: Figure 11.25 – Switching Juju controllers using the Juju CLI
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.25 – 使用Juju CLI切换Juju控制器
- en: 'Deploy the **juju-dashboard** charm for Kubernetes on the selected controller:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在选定的控制器上部署**juju-dashboard**魅力以支持Kubernetes：
- en: '[PRE8]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, link the dashboard to the selected controller as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过以下方式将仪表板链接到选定的控制器：
- en: '[PRE9]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, make the dashboard accessible by running the following command line:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过运行以下命令行使仪表板可访问：
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The Juju dashboard for Kubernetes should be deployed on the controller instance
    and accessed by pointing to the generated URL and login credentials provided in
    the previous output.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的Juju仪表板应部署在控制器实例上，并通过指向前述输出中生成的URL及登录凭证进行访问。
- en: Important note
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The output of the dashboard link and associated login credentials are no longer
    valid. Make sure to use your own generated link and credentials to access the
    deployed dashboard.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板链接的输出和相关的登录凭证不再有效。请确保使用自己生成的链接和凭证来访问已部署的仪表板。
- en: 'Once the Juju client and cloud controller are configured, we can start by deploying
    the first Kubernetes cluster in AWS. Juju provides different alternatives to deploy
    a Kubernetes cluster in AWS by simply deploying the respective charm. That can
    be a cluster deployed via a managed Kubernetes service, such as EKS, or a custom
    Kubernetes cluster running on an EC2 instance. Any of those aforementioned alternatives
    can be deployed simply via the Juju charms. In the following steps, we will deploy
    a self-managed Kubernetes cluster in AWS. Juju charms come with a production-grade
    Kubernetes cluster that can be deployed by simply running the following command
    line in the deployer host:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Juju客户端和云控制器配置完成，我们就可以开始在AWS中部署第一个Kubernetes集群。Juju提供了不同的选项，通过简单地部署相应的charm来在AWS中部署Kubernetes集群。可以通过托管的Kubernetes服务（例如EKS）部署集群，或者在EC2实例上运行自定义的Kubernetes集群。上述任何一种选项都可以通过Juju
    charms轻松部署。在接下来的步骤中，我们将部署一个自管理的Kubernetes集群在AWS中。Juju charms提供了一个生产级的Kubernetes集群，只需在部署主机上运行以下命令行即可部署：
- en: '[PRE11]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The **charmed-kubernetes** Juju charm deploys several components in AWS, including
    the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**charmed-kubernetes** Juju charm会在AWS中部署多个组件，包括以下内容：'
- en: Kubernetes master node
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes主节点
- en: Kubernetes slave node
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes从节点
- en: Certificate authority
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 证书颁发机构
- en: etcd
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd
- en: Kubernetes API load balancer
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes API负载均衡器
- en: Containerd engine
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Containerd引擎
- en: Virtual networking service running Calico
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Calico的虚拟网络服务
- en: 'A full list with respective links for each component can be found at the Juju
    Charmhub at [https://charmhub.io/charmed-kubernetes](https://charmhub.io/charmed-kubernetes)
    . The previous command line will deploy the Kubernetes cluster on the created
    cloud controller. We can check each component’s status and details by running
    the following command line in the Juju deployer host:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组件的完整列表及相应链接可以在Juju Charmhub上找到，地址是[https://charmhub.io/charmed-kubernetes](https://charmhub.io/charmed-kubernetes)。之前的命令行将会在创建的云控制器上部署Kubernetes集群。我们可以通过在Juju部署主机上运行以下命令来检查每个组件的状态和详细信息：
- en: '[PRE12]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '![Figure 11.26 – Listing the K8s cluster status in AWS using the Juju CLI](img/B21716_11_26.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图11.26 – 使用Juju CLI列出AWS中的K8s集群状态](img/B21716_11_26.jpg)'
- en: Figure 11.26 – Listing the K8s cluster status in AWS using the Juju CLI
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.26 – 使用Juju CLI列出AWS中的K8s集群状态
- en: 'Alternatively, a new Kubernetes cluster can be checked through the installed
    dashboard. First, make sure that you have **kubectl** installed locally:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以通过已安装的仪表板检查新的Kubernetes集群。首先，确保在本地安装了**kubectl**：
- en: '[PRE13]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once installed, run the **kubectl** proxy command line locally on **127.0.0.1**
    and port **8001** :'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，在**127.0.0.1**和端口**8001**上本地运行**kubectl**代理命令：
- en: '[PRE14]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In a browser, enter [http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/](http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/)
    . This will show you a graphical user interface showing different deployed K8s
    cluster components as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中输入[http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/](http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/)。这将显示一个图形用户界面，展示不同的K8s集群组件，如下所示：
- en: '![Figure 11.27 – The Juju dashboard for the K8s cluster deployed in AWS](img/B21716_11_27.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图11.27 – AWS中部署的K8s集群的Juju仪表板](img/B21716_11_27.jpg)'
- en: Figure 11.27 – The Juju dashboard for the K8s cluster deployed in AWS
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.27 – AWS中部署的K8s集群的Juju仪表板
- en: Once we have achieved a fully deployed and running Kubernetes cluster in AWS,
    we can move to do the same steps in the OpenStack private cloud.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在AWS中实现了完全部署并正在运行的Kubernetes集群，就可以开始在OpenStack私有云中执行相同的步骤。
- en: 'The **juju list-clouds** command line, as mentioned earlier, does not return
    OpenStack as a supported cloud by default. As a first step, add the OpenStack
    private cloud to Juju by running the following command line:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，**juju list-clouds**命令默认不会返回OpenStack作为支持的云。作为第一步，通过运行以下命令将OpenStack私有云添加到Juju：
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output we get is this:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出是：
- en: '![Figure 11.28 – Adding the OpenStack cloud using the Juju client](img/B21716_11_28.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图11.28 – 使用Juju客户端添加OpenStack云](img/B21716_11_28.jpg)'
- en: Figure 11.28 – Adding the OpenStack cloud using the Juju client
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.28 – 使用Juju客户端添加OpenStack云
- en: 'Once the OpenStack cloud is added, use the same **juju add-credentials** command
    line to authenticate against the OpenStack API:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦添加了OpenStack云，使用相同的**juju add-credentials**命令行对OpenStack API进行身份验证：
- en: '[PRE16]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run the following command line to bootstrap the OpenStack cloud in Juju:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令行以引导Juju中的OpenStack云：
- en: '[PRE17]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As with the bootstrapping process in the AWS world, Juju will create a new cloud
    controller instance in OpenStack named **juju-controller2** .
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 与AWS中的引导过程类似，Juju将在OpenStack中创建一个名为**juju-controller2**的新云控制器实例。
- en: 'Once created, we can deploy a Kubernetes cluster using the same Juju charm
    deployed in AWS. Use the **juju deploy** command line targeting the OpenStack
    Juju model as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建完成，我们可以使用在AWS中部署的相同Juju魅力部署Kubernetes集群。使用以下**juju deploy**命令行，目标是OpenStack
    Juju模型：
- en: '[PRE18]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The deployment process can take several minutes, and it can be checked by running
    the Juju status command line with the controller name as a flag, as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 部署过程可能需要几分钟，可以通过运行带有控制器名称标志的Juju状态命令行来检查，如下所示：
- en: '[PRE19]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will be the output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是输出：
- en: '![Figure 11.29 – Checking the K8s cluster status in OpenStack using the Juju
    client](img/B21716_11_29.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图11.29 – 使用Juju客户端检查OpenStack中的K8s集群状态](img/B21716_11_29.jpg)'
- en: Figure 11.29 – Checking the K8s cluster status in OpenStack using the Juju client
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.29 – 使用Juju客户端检查OpenStack中的K8s集群状态
- en: As we deployed the same Kubernetes Juju charm in OpenStack as AWS, we should
    expect to see the same components listed in AWS up and running after the process
    has finished successfully. The previous command line provides the current state
    of each unit of the Kubernetes cluster. The cluster is fully deployed when all
    agents in each unit have the **idle** value.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在OpenStack中部署的Kubernetes Juju魅力与AWS中相同，因此在过程成功完成后，应该看到与AWS中相同的组件正在运行。之前的命令行提供了Kubernetes集群中每个单元的当前状态。当每个单元中的所有代理都显示**idle**值时，集群即完全部署。
- en: Important note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The deployment process can take a while, and a real-time view of the deployment
    can be performed by running a **watch -c juju status** command line.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 部署过程可能需要一些时间，并且可以通过运行**watch -c juju status**命令行来实时查看部署状态。
- en: 'Once the cluster is deployed, it can be accessed from either the Kubernetes
    master or worker node. To take control of the cluster, we will need to set up
    the client and credentials access. Create a dedicated config directory in the
    deployer workstation:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群部署完成，可以从Kubernetes主节点或工作节点访问它。要控制集群，我们需要设置客户端和凭证访问权限。在部署工作站中创建一个专用的配置目录：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The next step aims to centralize the management of both clusters created in
    AWS and OpenStack through the same configuration location. To do this, we will
    copy the configuration files of each Kubernetes master deployed in both the AWS
    and OpenStack environments over an **scp** command line. The following command
    lines will switch the Juju controller to the respective Kubernetes environment
    and copy the configuration file to the **~/.kube** directory. Let’s start with
    the AWS Juju controller:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步的目标是通过相同的配置位置，集中管理在AWS和OpenStack中创建的两个集群。为此，我们将通过**scp**命令行复制每个在AWS和OpenStack环境中部署的Kubernetes主节点的配置文件。以下命令行将切换Juju控制器到相应的Kubernetes环境，并将配置文件复制到**~/.kube**目录。我们从AWS的Juju控制器开始：
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Run the previous command but switch to the OpenStack Juju controller:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 运行之前的命令，但切换到OpenStack Juju控制器：
- en: '[PRE22]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To query the Kubernetes resources for both cloud environments, we can run the
    **kubectl** native tool that is, by default, installed in each master and worker
    node. In the management host, make sure that **kubectl** is installed (if this
    was not done previously) by running the following command line on a macOS machine:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询两个云环境中的Kubernetes资源，我们可以运行默认安装在每个主节点和工作节点中的**kubectl**本地工具。在管理主机中，确保**kubectl**已安装（如果之前未安装），可以在macOS机器上运行以下命令行：
- en: '[PRE23]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tip
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The Kubectl setup guide for other platforms is detailed on the official Kubernetes
    website page at [https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)
    .
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其他平台的Kubectl设置指南详见Kubernetes官网页面：[https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)。
- en: 'We can then query each Kubernetes cluster from the Juju client machine by specifying
    the respective configuration path of each cloud environment. The following command
    line displays the status of the deployed Kubernetes cluster in AWS:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过指定每个云环境的相应配置路径，从Juju客户端机器查询每个Kubernetes集群。以下命令行显示了在AWS中部署的Kubernetes集群的状态：
- en: '[PRE24]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here is the output:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 11.30 – Listing the deployed K8s cluster components in AWS using kubectl](img/B21716_11_30.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图11.30 – 使用kubectl列出在AWS中部署的K8s集群组件](img/B21716_11_30.jpg)'
- en: Figure 11.30 – Listing the deployed K8s cluster components in AWS using kubectl
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can access the Kubernetes dashboard on the Juju client machine
    by running **$ kubectl** **proxy 127.0.0.1:8001** .
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have two fully deployed Kubernetes clusters in AWS and OpenStack
    clouds, we will focus on connecting them. To use hybrid Kubernetes jargon, we
    will rely on the Federation feature.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Bringing clusters together
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Kubernetes Federation, a functionality that is known as **KubeFed** , enables
    workloads to run over multiple clusters, spread across multiple data centers and
    geographical locations. KubeFed leverages the control of different clusters from
    one central host that propagates any configuration changes to all or a selected
    member of the clusters. From an architecture perspective, the federated control
    plane leverages a set of standard APIs to control and manage cluster-wide operations.
    Since isolated clusters can be associated with DNS entries, KubeFed takes care
    of managing them by automatically configuring DNS for all discovered nodes. As
    shown in the next figure (in our case, with the hybrid model), KubeFed enables
    multiple clusters to be managed through a single control plane:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.31 – A Kubernetes federation layout across OpenStack and AWS clouds](img/B21716_11_31.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Figure 11.31 – A Kubernetes federation layout across OpenStack and AWS clouds
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: KubeFed can coordinate the state and configuration of many clusters running
    the standard Kubernetes API. In our use case, and for the AWS public cloud, any
    Kubernetes cluster deployed in EC2, EKS, or Fargate can be federated and linked
    to the cluster deployed in OpenStack’s private cloud.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at the federation implementation, let’s take a quick detour to
    look at some technical concepts of the KubeFed. The Kubernetes federation piece
    runs a regular Pod into a Kubernetes cluster on its own. **Federation Pods** are
    purely used for federation purposes and live within a created federated service.
    Functions such as service deployments, health monitoring, and DNS management are
    handled by the federated Pods.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The following steps will demonstrate how to use KubeFed to bring the AWS and
    OpenStack worlds together.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'To start working with Federation, we will need to deploy a dedicated Kubernetes
    cluster on the host cluster. The simplest and quickest way to do so is by setting
    a Federation controller using Juju. Kubernetes’ Federation cluster can be deployed
    in either OpenStack or AWS. You can also choose to deploy it in a different management
    environment and interact with it through a local client that can be installed
    later. For the sake of simplicity, we will use the OpenStack environment to run
    the Federation cluster. Using Juju, we can switch to the OpenStack private cloud
    environment and deploy a new federation cluster, using the same Kubernetes charm
    as earlier, as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Make sure to follow the status of the cluster deployment using the **$juju
    status** command line. Once the nine-component clusters are deployed, we can proceed
    to set up the federation cluster locally. With the same command lines we used
    for the OpenStack and AWS Kubernetes clusters, use **scp** to copy the cluster
    config file from any node under the ~/. kube/ directory:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: We copied the config file by renaming it, as we did with the AWS and OpenStack
    Kube files, to **fed-config** .
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'A good practice when dealing with multiple Kubernetes clusters is to centralize
    the management of all of them within a single command line. That will remove extra
    overhead when switching between clusters every time we need to operate a specific
    cluster. One way to do this is simply by merging all Kubernetes config files into
    one file. The latest releases of **kubectl** support the management of multiple
    cluster views. Start by creating an empty config file if it does not exist yet:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, set the **KUBECONFIG** environment variable with a list of paths to all
    configuration files, as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once exported, run the following command line to merge all the **kubeconfig**
    files into one:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Using the **kubectl get-contexts** command line, we can display available contexts
    from the merged configuration step:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here is the output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.32 – A listing of the K8s contexts of the Juju controllers](img/B21716_11_32.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: Figure 11.32 – A listing of the K8s contexts of the Juju controllers
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: We can display all clusters defined in the **kubeconfig** file by running the
    **kubectl config get-clusters** command line.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving to construct the federated control plane, we will need to expose
    the federated service controller via DNS names. In our case, we have deployed
    the federation cluster in the OpenStack environment. Hence, a DNS provider configuration
    should be available to the federated service controller. When deploying the federated
    service controller in other cloud providers, DNS provider configuration is generated
    automatically if the deployed host cluster is the same as the DNS provider. For
    example, if the federated service controller is running in AWS and pre-configurated
    with a Route 53 hosted zone, the DNS provider will be automatically generated
    and pass DNS names to the federated service controller. In the following setup,
    we will configure **CoreDNS** as a federation DNS provider in the OpenStack federation
    cluster. To enable CoreDNS on **juju-controller-fed** , run the following command
    lines:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, create a **coredns-provider.conf** file that contains the IP address
    of the etcd service, obtained from the cluster status of the **juju-controller-fed**
    cluster and the DNS zone. The content of the **CoreDNS** file is configured as
    follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Important note
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The zone configured in this example is based on a local DNS configuration in
    the OpenStack environment. Feel free to use a custom DNS provider or a local one
    of your choice to create and manage zones for private cloud resources. Public
    cloud providers also offer a managed DNS service to create zones and DNS records,
    such as Route53 for AWS and Azure DNS for the Azure cloud.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is time to deploy the federation control plane on **juju-controller-fed**
    . We briefly discussed Kubefed, the tool for Kubernetes federation, but we have
    not installed it yet. To do so, run the following command lines on macOS to install
    Kubefed:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, extract binaries to one of the directories and set the necessary executable
    permissions for the Kubefed binary:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Run the **kubefed init** command line to deploy and initialize the required
    service in the federation cluster:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here is the output:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.33 – Running the Kubefed Federation control plane](img/B21716_11_33.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: Figure 11.33 – Running the Kubefed Federation control plane
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'The **kubefed init** Federation command line will install a new API server
    by exposing the Federation service, as shown previously. It also takes care of
    installing the controller manager for Federation in addition to a dedicated namespace,
    referred to as **federation-system** in the host cluster. Now that we have Federation
    ready for usage, we can start controlling the Kubernetes clusters for Federation.
    Proceed by switching to the Federation context:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, add Kubernetes clusters running in OpenStack and AWS clouds to the joined
    Federation context. Let’s start by adding AWS:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.34 – Joining an AWS K8s cluster to the federation](img/B21716_11_34.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: Figure 11.34 – Joining an AWS K8s cluster to the federation
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'The next command line will join the Kubernetes cluster running on OpenStack:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This is the output:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.35 – Joining an OpenStack K8s cluster to the federation](img/B21716_11_35.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: Figure 11.35 – Joining an OpenStack K8s cluster to the federation
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Both previous command lines will add the necessary configuration in the Federation
    control plane so we can interact with the new construct through the Federating
    API as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.36 – Listing federated K8s clusters running in OpenStack and AWS](img/B21716_11_36.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: Figure 11.36 – Listing federated K8s clusters running in OpenStack and AWS
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'At this level, we have constructed a few federated Kubernetes clusters running
    in public AWS and private OpenStack clouds. Further operations on both clusters
    can be performed using the standard Kubernetes APIs. This federation enables the
    deployment of all the Kubernetes primitives such as namespaces, services, deployments,
    ConfigMaps, and so on. For example, through the same federating endpoint, we can
    deploy a Kubernetes namespace that will be spread across the hybrid cloud environment.
    Let’s create a new YAML namespace called **fed-ns.yaml** with the following content:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, run the following command line to create the namespace in the federating
    context as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here is the output:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.37 – Creating a federated K8s namespace](img/B21716_11_37.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: Figure 11.37 – Creating a federated K8s namespace
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the creation of the namespace in both the AWS and OpenStack worlds.
    Starting with AWS, simply switch the context by getting the list of deployed namespaces:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following is the output:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.38 – Listing the namespace for the K8s context running in AWS](img/B21716_11_38.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: Figure 11.38 – Listing the namespace for the K8s context running in AWS
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, run the same command line but switch to the OpenStack context:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here is the output:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.39 – Listing the namespace for the K8s context running in OpenStack](img/B21716_11_39.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: Figure 11.39 – Listing the namespace for the K8s context running in OpenStack
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! Our federation is ready to handle workloads in a hybrid setup
    across the OpenStack and AWS clouds. Deploying services and Pods will only require
    creating them via the federating context, and the federation controller will take
    care of sharing the services on both the private and public clouds.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored more cloud opportunities with OpenStack. Leveraging container
    technology to deploy workloads across multiple cloud environments has been adopted
    by several organizations that seek the best usage of the *cloud* paradigm. This
    chapter highlighted some of the adopted approaches to deploying a hybrid cloud
    based on containers. The decentralized model can be considered a simple version
    of the centralized one where cloud operators can deploy workloads in multiple
    cloud environments. With the centralized model, firing one deployment from one
    place propagates a workload across multiple environments. The chapter demonstrated
    a use case of a hybrid model between AWS and OpenStack. Both worlds can meet,
    and having the capability to move between both cloud platforms is a great *cloudy
    power* . Containerization allows companies to deploy, manage, and scale their
    workloads in more than a cloud environment, as we learned in this chapter. Federation
    mode has proven to be a rapid, reliable, and zero-touch way to operate workloads
    managed by Kubernetes, which we also learned about in this chapter. We further
    learned that OpenStack did not miss the chance again, and that with the rise of
    hybrid cloud adoption over the last few years, OpenStack has come to light. Due
    to the maturity of its ecosystem and rich set of APIs, OpenStack has become in
    demand to manage private cloud infrastructure. We learned that the marriage of
    both private and public clouds has, so far, been a successful experience. Let’s
    keep it *cloudy* , where OpenStack keeps *shining* .
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
