- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Hybrid Cloud Hyperscale Use Case – Scaling a Kubernetes Workload
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “There are not more than five cardinal tastes, yet combinations of them yield
    more flavors than can ever be tasted.”
  prefs: []
  type: TYPE_NORMAL
- en: – Sun Tzu, The Art of War
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we briefly explored hybrid cloud design patterns and
    how hybrid deployment has become a cloud trend for several organizations. The
    hybrid cloud model reflects a complex mix of services that work in tandem to satisfy
    different workload requirements. When you join an OpenStack private environment,
    a hybrid model can be more challenging to set up due to the complexity of the
    dynamic mix of resources spread across private and public environments. Building
    hybrid cloud networks is the first challenge that an organization would face,
    which requires stable, dedicated, and consistent connectivity between both public
    and private clouds. Moreover, applications that can run on bare metal, virtual
    machines, or containers require a way to ensure configuration consistency between
    different cloud environments. Several implementation challenges covered in the
    previous chapter still present a blocker for many organizations from enjoying
    the benefits of hybrid clouds. The good news is that with the evolution of **application
    architecture patterns** , **containerization** technology has shifted how and
    where applications can be deployed. If we are looking for application migration,
    cloud bursting, or running for a backup and disaster recovery strategy, adopting
    containerization technology is the way to go. In this chapter, we will put into
    practice what was covered in the previous chapter and uncover the power of containerization
    in a hybrid cloud setup by covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing hybrid cloud networking between OpenStack private cloud and a public
    cloud environment such as AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting briefly the main concepts of Kubernetes (also referred to as K8s)
    to prepare for a cloud-agnostic workload model across OpenStack and AWS clouds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring different approaches to implementing a hybrid cloud setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Kubernetes across an OpenStack private cloud and AWS public cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning orchestration tooling for hybrid cloud deployment using Juju
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting Kubernetes clusters across private and public clouds by federating
    its control plane and running a hybrid deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qualifying a hybrid architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the main motivation for adopting hybrid cloud
    implementation. We will briefly illustrate how containerization helps achieve
    most of the hybrid model benefits and enables companies to move between cloud
    environments most efficiently. The networking and security of connected cloud
    environments is another important pillar that must be looked at carefully when
    adopting the hybrid approach.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting a cloud-agnostic stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Being cloud-agnostic is a key motivator for several organizations that seek
    the best cloud experience when being onboarded into a cloud journey. Thanks to
    the **pay-as-you-go** model, organizations can use cloud resources and pay only
    for what they use. However, some business workloads might require changes, and
    switching or expanding to another cloud provider might become necessary. Enjoying
    free-pass movement between cloud providers might not sound straightforward at
    first. This is due to the technology that a cloud provider offers. A workload
    naturally becomes locked into more dependencies that run on a specific cloud provider.
    Moving or expanding to another cloud provider can be an expensive, complex, and
    error-prone process. For that reason, cloud agnosticism should be addressed in
    advance, starting with the workload architecture itself. The best example of this
    is the emergent **microservices architecture** . One of the main benefits of opting
    for a microservice design is that it breaks an application into smaller and independent
    modules, making it easy to test and deploy them separately without the need to
    rely on a specific technology stack. Another benefit of a microservice design
    is the infrastructure that will run it. That is where containerization comes into
    the picture. Containers come with much more efficient compute utilization and
    time optimization compared to virtual machines. That makes the microservice and
    containerization concepts an ideal pair to adopt cloud agnosticism. Hence, the
    hybrid cloud model is much easier to implement, with some additions. As illustrated
    in the following diagram, those additions include a container orchestration layer
    and automated deployment, making the cloud-agnostic model almost complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – The cloud-agnostic architecture of a hybrid model](img/B21716_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – The cloud-agnostic architecture of a hybrid model
  prefs: []
  type: TYPE_NORMAL
- en: Container orchestration engines manage the life cycle of the workloads with
    almost no administration need. Most of the famous container orchestration engines
    such as Docker and Kubernetes are considered part of the cloud-agnostic container
    orchestration platform family. While container orchestration brings more container
    management automation on its own, it would not be nearly as valuable without the
    **Infrastructure as Code** ( **IaC** ) concept. Each cloud provider offers its
    own infrastructure templating mechanism ( **CloudFormation** for AWS, **Cloud
    Deployment Manager** for Google, **Deployment Manager** for Azure, and **Heat**
    for OpenStack). The vendor-specific syntax of each service presents a challenge
    as switching between cloud deployment environments becomes more complex. The solution
    is obviously to take a cloud-agnostic approach by employing third-party tools
    that allow us to deploy the same workload infrastructure in more than one cloud
    environment, using the same code to abstract the infrastructure layer. These tools
    became available only after the IaC concept had emerged as a DevOps principle.
    **Terraform** from Hashicorp has become the most commonly used cloud-agnostic
    tool for infrastructure management. Today, more open source projects have been
    inspired by Terraform, and recently, the **Pulumi** tool has appeared on the DevOps
    horizon. In addition to managing infrastructure using code, Pulumi allows the
    description of the infrastructure in any o the most popular programming languages
    such as Python, Java, Go, .NET, and Typescript.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: On August 10, 2023, Hashicorp announced that Terraform is no longer under the
    Mozilla Public License v2.0. Pulumi has emerged as a successor of Terraform in
    the open source world and has gained massive popularity in the last year. To read
    more about Pulumi, check the official website at [https://www.pulumi.com/docs/get-started/](https://www.pulumi.com/docs/get-started/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: To orchestrate the whole infrastructure management process, CI/CD tools define
    the engines to automate everything across different cloud environments.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the different pieces of the hybrid cloud puzzle, we
    will focus on how to connect the dots between two cloud environments – in our
    case, OpenStack and AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for a hybrid model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Establishing a hybrid cloud architecture first requires an adequate network
    connection between the private and public clouds. Such a network connection is
    critical to maintaining business continuity. For that reason, continuous evaluation
    of the connection that looks at latency, uptime, and bandwidth is a must. Security
    is the other essential aspect of the hybrid connection. Depending on the hardware
    and network service used for such a connection, operators should ensure encryption
    in transit is enabled and redundancy is considered. Implementing a cloud hybrid
    connection can be greatly simplified when using cloud network-managed services.
    As shown in the next simplified network diagram, an OpenStack private cloud endpoint
    is connected to an AWS **Virtual Private Cloud** ( **VPC** ) through two different
    connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – A hybrid connection between the OpenStack and AWS clouds](img/B21716_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – A hybrid connection between the OpenStack and AWS clouds
  prefs: []
  type: TYPE_NORMAL
- en: The type of connection in the upper part of the figure illustrates a site-to-site
    VPN connection over the internet using an IPSec tunnel. Without exposing any resources
    in both cloud environments (the OpenStack private network range and AWS VPC) to
    the public world, communication occurs in an encrypted tunnel traveling over the
    internet. The implementation of a VPN connection between an OpenStack tenant environment
    and AWS VPC is much more automated than in a classic data center. As we covered
    in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking – Connectivity
    and Managed Service Options* , Neutron offers a managed network service, VPNaaS,
    that can be deployed as a site-to-site VPN connection in minutes. AWS offers a
    similar managed VPN feature as part of its VPC service that can establish a VPN
    connection by simply creating a VPN gateway and configuring a customer gateway
    (an OpenStack endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When using a site-to-site VPN connection in AWS, two tunnels will be created
    for redundancy. Each tunnel terminates in a different Availability Zone. If one
    tunnel is down, traffic is automatically shifted to the second active tunnel.
  prefs: []
  type: TYPE_NORMAL
- en: VPN can be the quickest way to establish such a hybrid cloud connection. A major
    pitfall of a site-to-site VPN connection linking two different locations across
    different regions is limited performance. In AWS’ case, each VPN tunnel can support
    a bandwidth of up to 1.25 Gbps. Other public cloud providers might offer different
    options. For example, Azure allows cloud users to specify the type of VPN gateway
    they require, and depending on its size, the bandwidth can vary between 100 Mbps
    and 10 Gbps.
  prefs: []
  type: TYPE_NORMAL
- en: On the OpenStack side, Neutron will provision a virtual appliance with a certain
    bandwidth limit. Neutron offers a **Quality of Service** ( **QoS** ) feature that
    can be used to set network traffic policies for better network bandwidth control.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with complex and large-scale workloads that need to exist and move
    between different cloud environments, a limited bandwidth capacity can present
    a bottleneck. Moreover, a consistent connection between both cloud environments
    using a simple VPN connection cannot always be guaranteed. This is due to the
    long path taken across shared internet devices. For that reason, public cloud
    providers offer a second category network connection to establish a connection
    to a private environment. In the AWS world, **DirectConnect** is an ideal option
    when a stable, dedicated, isolated, and higher-performance connection is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing a DirectConnect connection requires more steps in the deployment
    process, unlike a VPN connection setup. Cloud operators can order a DirectConnect
    connection via the AWS console or CLI and decide which type of connection they
    will put in place. DirectConnect offers two kinds of connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A hosted connection** : An AWS partner will handle the physical connection
    and link it to the device sitting at the edge of the OpenStack private cloud environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A dedicated connection** : This establishes a physical connection from the
    OpenStack endpoint to an AWS DirectConnect location. This type of connection comes
    with multiple bandwidth capacity options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Which of these connectivity options you choose depends on many factors, such
    as cost, geographic location, and bandwidth throughput. To learn more about AWS
    DirectConnect technical details, check out the following AWS URL: [https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html](https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid connectivity requires a certain level of redundancy. If one link fails,
    you still have a second to carry the traffic, even with lower bandwidth throughput.
    The previous architecture diagram exposed a classic network design that leverages
    two connections, established using DirectConnect and VPN. Operators can configure
    their devices at the OpenStack private cloud end to primarily receive traffic
    on the DirectConnect interface. That will terminate both connections into two
    different devices in the same location endpoint. If the DirectConnect connection
    fails for some reason, traffic will be switched to flow across the VPN connection
    as a backup. This design pattern is considered a good compromise that grants high
    availability at a fair price. If a hybrid connection requires the same consistent
    bandwidth with exact configurations in both links, two DirectConnect links will
    be put in place. That will maximize resiliency for critical production workloads.
    However, that comes of course with a higher bill. AWS recommends using dynamic
    routing to allow connections to fail over automatically and leverage the best
    available routes. This routing capability requires that the OpenStack hardware
    endpoint supports dynamic routing and auto-failover features.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered one of the most critical pieces of hybrid cloud design,
    networking, we will move on to the next part to cover different ways to design
    a containerized hybrid cloud setup.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a containerized hybrid cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Container technology is the way to go to develop modern applications with all
    the benefits of portability, speed of deployment, and less operational overhead.
    The evolution of containerization has allowed organizations to run and scale their
    workloads in an almost zero-touch, reliable, and rapid way. With the rise of several
    forms of container engines such as Docker and LXD, a few container orchestration
    systems such as Kubernetes, Mesos, and Docker Swarm have come to light and empowered
    the way of developing applications. Today, we can find organizations deploying
    production workloads with more confidence than ever before. Container orchestration
    systems have shifted the gears on how applications are developed with the evolution
    of microservices architecture. Public cloud providers seized the opportunity to
    provide a platform to run a fully containerized environment. AWS, for example,
    offers (at the time of writing) three main container-related services, which are
    summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic Container Service** ( **ECS** ): Deploys and manages containerized
    workloads through a fully managed container orchestration service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic Kubernetes Service** ( **EKS** ): Runs and manages a Kubernetes control
    plane that is responsible for container deployment, scheduling, and availability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fargate** : Acts as a serverless service that deploys and manages applications
    without the need to manage the underlying infrastructure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure has similar managed container services such as **Azure Container Instances**
    ( **ACI** ), **Azure Container Apps** ( **ACA** ), and **Azure Kubernetes Service**
    ( **AKS** ). Kubernetes has been a great deal for GCP, which has developed **Google
    Kubernetes Engine** ( **GKE** ) to reliably and efficiently deploy and scale containerized
    workloads on Kubernetes. The history of Kubernetes adoption could fill an entire
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying applications on containers in a hybrid cloud architecture requires
    a good understanding of a few design patterns. Organizations should seek the right
    tools that fit into their strategic initiative to take advantage of the hybrid
    model. In the following sections, we will explore a few implemented design patterns
    to deploy a hybrid containerized architecture using Kubernetes. Before tackling
    the different design models, we will pick up Kubernetes as a containerization
    platform. One of many reasons to choose Kubernetes is that it is widely used,
    with plenty of features and tools available in its ecosystem. Another great benefit
    of adopting Kubernetes is its flexibility to design clusters and run containers,
    surrounded by several stable open source tools for container management across
    platforms, as well as its support federation feature that fits into a hybrid cloud
    architecture. Before delving into the previously mentioned design benefits of
    building a hybrid model, let’s briefly look at Kubernetes jargon.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes in a nutshell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, Kubernetes is everywhere. Eight years ago, companies were hesitant to
    start a production journey running Kubernetes, as it was new to the market since
    Google had recently made it publicly available. The concept of a container orchestration
    engine was not new at the time, since Docker Swarm and Mesos were quite evolved
    and widely used. Kubernetes, as expected, came with a different engine concept.
    Understanding the basic concepts of how it works is an essential step to confidently
    run it in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: For container enthusiasts, using cloud services will facilitate the adoption
    of a containerization model based on Kubernetes. As mentioned in the previous
    section, cloud hyperscalers offer a managed service that supports Kubernetes.
    The Kubernetes PaaS model simply removes any overhead to manage and install the
    containerization orchestration engine. This also includes additional networking,
    storage, and availability aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following figure, Kubernetes is designed to run different types
    of nodes within a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – The Kubernetes engine architecture at a high level](img/B21716_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – The Kubernetes engine architecture at a high level
  prefs: []
  type: TYPE_NORMAL
- en: 'The node categories can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Master node** : This is responsible for running most of the essential processes
    that manage a Kubernetes cluster, including the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduler** : This runs the **kube-scheduler** process that manages the placement
    of containers on proper hosts based on resource availability and load.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller manager** : This runs the **kube-controller-manager** process
    that keeps an eye on the cluster activities.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API server** : This runs the **kube-apiserver** process that allows communication
    between a Kubernetes client and the cluster. The master node can be reached via
    the API server.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd** : This is a key-value storage that handles the state of the cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Worker node** : This runs the workloads in containers and is composed of
    two components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubelet service** : This runs the K **ubelet** process that allows cluster
    inter-communication. A Kubelet process also listens from the API server to manage
    containers in the worker node.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kube-proxy service** : This runs the **kube-proxy** service that handles
    networking between services in a Kubernetes cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes jargon introduces a few concepts as well as those related to its
    cluster-based architecture description, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The Kubernetes engine architecture at a low level](img/B21716_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – The Kubernetes engine architecture at a low level
  prefs: []
  type: TYPE_NORMAL
- en: 'Different cluster components can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pod** : This is defined as the smallest and most basic component in a Kubernetes
    environment. A pod combines and runs containers, attaches storage as required,
    and assigns a unique IP address for networking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller** : This presents the main piece of the container orchestration
    engine. It manages all different aspects of a pod such as creation, deletion,
    replication, and rollouts. A Kubernetes controller keeps track of workload placements
    and allocated resources. There are different kinds of controllers, including **ReplicaSet**
    , **StatefulSet** , **DaemonSet** , **Deployment** , **Job** , and **CronJob**
    . Each controller type has a different use for specific pod management purposes.
    For example, the **ReplicaSet** controller creates a set of pods that all run
    the same application. **CronJob** is more dedicated to creating scheduled jobs.
    To read more about the Kubernetes controller types, refer to the official Kubernetes
    page at [https://kubernetes.io/docs/concepts/workloads/controllers/](https://kubernetes.io/docs/concepts/workloads/controllers/)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service** : This exposes an application that runs in one or multiple Pods
    for incoming requests. A Kubernetes service routes each request to the respective
    pod(s) and provides additional capabilities, such as DNS and load balancing to
    access a pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Namespace** : This is a scoped logical layer to isolate Kubernetes resource
    names, thus avoiding name collisions when many users launch several Kubernetes
    clusters in the *same* physical cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume** : This provides persistent storage when required by a workload running
    in Pods. The latest version of Kubernetes supports more storage and volume features,
    as well as an exhaustive list of volume types. A full list of supported types
    of volumes can be found at [https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have covered most of the important aspects of the Kubernetes world,
    we can verify how it fits into a hybrid cloud setup by exploring two of the most
    widely adopted deployment models.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a decentralized models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The decentralized Kubernetes model, which is adapted for cloud hybrid architecture,
    is referred to as a *bursting* model. Organizations adopt such a design to overcome
    the over-provisioning of resources in their private clouds by leveraging public
    cloud resources. In such a layout, each cloud environment has its own Kubernetes
    infrastructure deployed and managed separately. In a second iteration, operators
    need to manage network connectivity between clusters that reside in both cloud
    environments. The next figure illustrates public cloud bursting in a hybrid setup
    where a Kubernetes cluster that runs in a private cloud extends its resources
    by reaching public resources (additional worker nodes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – A hybrid bursting model for OpenStack and a public cloud provider](img/B21716_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – A hybrid bursting model for OpenStack and a public cloud provider
  prefs: []
  type: TYPE_NORMAL
- en: Depending on each public Kubernetes managed service, operators will need to
    define extra parameters, such as DNS, cluster API endpoint, and namespace configurations,
    so that public resources can be used within the same workload or application context.
    The implementation of such a model can be far from complex and requires additional
    tweaks to properly synchronize the Kubernetes cluster resources, such as services
    and deployments. Operators must ensure that both clusters are consistent when
    it comes to configuration, identity management, and workload deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'As highlighted at the beginning of this section, AWS offers two managed services
    for workloads based on Kubernetes: EKS and Fargate. Before drafting a bursting
    Kubernetes layout joining the OpenStack and AWS environments, it is essential
    to recognize the use cases for each AWS EKS and AWS Fargate service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use EKS if you need to do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provision a managed Kubernetes cluster by AWS without the need to manage its
    control plane and operate its underlying infrastructure and scaling and security
    configurations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Have more flexibility in the networking, storage, and scaling options
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy complex and large workloads
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Have more control of its deployment of cross-regional, on-premises, and private
    and public clouds
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a wider integration with AWS native services for advanced configuration,
    such as Elastic Load Balancing and AWS App Mesh
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use Fargate if you *don’t* need to do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provision a Kubernetes infrastructure and its associated resources
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage workloads by provisioning resources (EC2 instances) but via task definitions
    instead
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Run containers based on cluster and instance sizes but on the CPU and RAM resources
    you need instead
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Not focus on managing the cluster nodes but the workloads instead
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include additional features that come with the Kubernetes engine
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you require advanced configurations that support a cloud-agnostic setup and
    more networking customization, then EKS would be a suitable choice for a hybrid
    layout.
  prefs: []
  type: TYPE_NORMAL
- en: Another key consideration is that when employing storage in a Kubernetes hybrid
    decentralized model, both storage (volume) endpoints should have the same data.
    There are several ways to ensure data synchronization across both storage endpoints
    by leveraging some public cloud-managed storage services. For example, a volume
    attached to Pods running in a Kubernetes cluster in OpenStack can be copied over
    a private connection to AWS using **DataSync** ( [https://aws.amazon.com/datasync/](https://aws.amazon.com/datasync/)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following architecture diagram, DataSync requires an endpoint
    for the inbound data transfer from a private cloud storage endpoint. You can create
    an **Elastic File System** ( **EFS** ) as a destination location in your AWS VPC.
    The EFS location can be configured in the **DataSync** task so that any data being
    transferred from a volume located in the private cloud is synced in an EFS share.
    It is important to note that it is possible to mount an EFS filesystem on Fargate,
    but it is only limited to volume static provisioning, whereas, with EKS nodes,
    it is possible to use dynamic persistent volume provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – A hybrid Kubernetes bursting model with volume synchronization](img/B21716_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – A hybrid Kubernetes bursting model with volume synchronization
  prefs: []
  type: TYPE_NORMAL
- en: This hybrid model is suitable for workloads that do not require low latency
    with immediate scaling resources dynamically during peak demand. In such a model,
    cloud operators should expect more networking management, as well as ensuring
    data consistency across hybrid cloud environments. Managing multiple Kubernetes
    clusters across different cloud environments can be overwhelming, even when you
    employ public Kubernetes-managed services such as EKS and Fargate. The good news
    is that a few open source tools and wrappers based on the Kubernetes ecosystem
    have been developed to empower more extended designs. **Kubernetes Cloud Instance
    Provider** ( **KIP** ) enables you to manage Pods in cloud instances from one
    single and simplified interface. The next section will explore an abstracted layout
    that measures a decentralized hybrid Kubernetes layout between AWS and OpenStack
    using KIP.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid cloud bursting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before tackling the idea behind KIP in this section, it is essential to cover
    briefly the term *Kubelet* as another key concept of the Kubernetes framework.
    The Kubelet is simply an agent that runs Kubernetes at the node level. It provides
    all kinds of features that deal with Pods, such as deployment, management, and
    communication between nodes and Pods. A Kubelet can be considered a coordinator
    between the Kubernetes control plane and the containers running on nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To read more about the Kubelet, refer to the official Kubernetes page found
    at [https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)
  prefs: []
  type: TYPE_NORMAL
- en: Based on the same Kubelet concept, the **Cloud Native Computing Foundation**
    ( **CNCF** ) has initiated an open source project that focuses on a **Virtual
    Kubelet** . The official Virtual Kubelet project web page can be found at [https://virtual-kubelet.io/](https://virtual-kubelet.io/)
    . It defines it as *“an open source Kubernetes kubelet implementation that masquerades
    as a kubelet.”* The term *masquerades* is quite metaphoric, but one way to understand
    its concept is to consider the difference between a Kubelet and a Virtual Kubelet.
    The Virtual Kubelet provides more capabilities than the standard Kubelet by scheduling
    the provisioning of containers in other cloud solutions, but not on the nodes
    themselves. Under the hood, the Virtual Kubelet is highly customizable, allowing
    users to use other cloud environments and deploy Pods with their own APIs. The
    other piece of the Virtual Kubelet is the **Kubelet providers** . A Virtual Kubelet
    provider is a pluggable interface that provides all the necessary operational
    features and management of Pods and containers. A few of the providers commonly
    used to back Kubernetes nodes on cloud container platforms are AWS Fargate, Azure
    Container Instances, and OpenStack’s Zun. An up-to-date list of supported providers
    can be found at [https://virtual-kubelet.io/docs/providers/#current-providers](https://virtual-kubelet.io/docs/providers/#current-providers)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram illustrates a simple cloud bursting example, from a private
    OpenStack cloud environment to the AWS public cloud. In an OpenStack tenant environment,
    a Kubernetes cluster runs alongside a Virtual Kubelet pod.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Hybrid Kubernetes bursting between OpenStack and AWS via KIP](img/B21716_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Hybrid Kubernetes bursting between OpenStack and AWS via KIP
  prefs: []
  type: TYPE_NORMAL
- en: The other usage of a hybrid decentralized model is to deploy different Kubernetes
    clusters in more than one cloud environment (private and public) from a single
    deployer. This setup would require additional networking by connecting different
    clusters at the network layer. However, controlling and operating the clusters
    is performed individually. To centralize the management of workloads running across
    multiple cloud environments, we would need to provide a common control plane layer.
    This design brings the bursting model to the next level by adding a centralized
    control aspect to the hybrid model, which will be explored in more detail in the
    next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a centralized model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second hybrid model consolidates different Kubernetes clusters that run
    in both the private and public clouds. This type of architecture is referred to
    as **Kubernetes Federation** , which is primarily based on multi-cluster deployments
    with additional and common control planes. Under the Kubernetes Federation setup,
    one single source of a workload deployment is applied to a **host cluster** as
    a central deployment location that reaches different clusters across different
    connected cloud endpoints. The deployment of the application is then propagated
    to all environments with a view of a single target cluster. That removes any need
    to configure or maintain the state of an application in each cluster separately.
    The **Federation model** is a desirable hybrid option when you need to manage
    several clusters through a single API interface. An application is packaged and
    deployed from a single source. It spreads out its deployments equally across all
    visible clusters. From a host cluster (master configuration host) perspective,
    all connected clusters under its view are considered member clusters, so each
    deployed application will have its replicas throughout all worker nodes at scale.
    Kubernetes Federation can also be customized during the application deployment.
    Some member clusters are part of different environments and might require specific
    local variables, such as network policies. Such member cluster-specific configurations
    can be supplemented by the host cluster holding the Federation configuration.
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of tools to deploy a hybrid Kubernetes centralized model.
    In the next section, we will focus on deploying a production-grade Kubernetes
    cluster across the OpenStack and AWS clouds using some trending open source solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Kubernetes everywhere
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several ways to orchestrate the deployment of Kubernetes clusters.
    Using **PaaS** and **SaaS** can be the fastest way that uses the least operational
    overhead. The open source world has also contributed to facilitating the deployment
    across clouds, but that involves preparation and understanding how each solution
    works under the hood. In the following section, we will focus on an open source
    solution named **Juju** from Canonical.
  prefs: []
  type: TYPE_NORMAL
- en: Juju integrates very well with a hybrid cloud deployment model and, more specifically,
    Kubernetes workloads across different cloud environments. Under the hood, a **Juju
    controller** host should exist per environment, whereas a **Juju client** performs
    deployment actions and other functions. As shown in the following diagram, a Juju
    cloud controller runs in each cloud environment and supports multiple cloud environments,
    including AWS and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – A Juju workflow in a hybrid deployment model](img/B21716_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – A Juju workflow in a hybrid deployment model
  prefs: []
  type: TYPE_NORMAL
- en: The latest list of Juju’s supported clouds can be found at [https://juju.is/docs/juju/juju-supported-clouds](https://juju.is/docs/juju/juju-supported-clouds)
    . This architecture can be compared to the **Puppet** or **Chef** system management
    tools. Puppet uses master-slave architecture, in which the slave component can
    be compared to a Juju client and the master component is akin to a Juju cloud
    controller.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Several projects and open-source solutions exist that support hybrid and multi-cloud
    architecture using containers. Use cases of hybrid deployments using containers
    can be found at [https://openinfra.dev/hybrid-cloud/](https://openinfra.dev/hybrid-cloud/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Juju, similar to Puppet (manifests) or Chef (cookbooks), uses its own script
    operators, which are named **charms** . Charms deploy applications across different
    infrastructure resources including containers, virtual machines, and even bare
    metal machines. Juju charms have been designed to be flexible for deployment and
    can be written in several programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe how a hybrid Kubernetes cluster can be deployed
    across public and private clouds:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the Juju client in a management/deployer host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the Juju client in the management host to bootstrap the AWS and OpenStack
    clouds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the Juju cloud controller in the AWS public cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the Juju cloud controller in the OpenStack private cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy Kubernetes clusters across both private and public clouds using Juju.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect both Kubernetes clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure both Kubernetes clusters for federation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In *step 1* , we will need to pick up a dedicated host for Juju cloud management
    and install the Juju client. This can be any machine that has connectivity to
    both the OpenStack and AWS environments. The Juju client supports several operating
    systems, a list of which can be found at [https://juju.is/docs/juju/install-juju](https://juju.is/docs/juju/install-juju)
    . The following step-by-step walkthrough installs the Juju client in a machine
    running on macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the Juju client on a Linux machine that runs the Ubuntu operating
    system, use the following command lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Juju client installation can be checked by running its interactive command
    line interface, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Verifying the Juju installation](img/B21716_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Verifying the Juju installation
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Juju command line does not return any Juju cloud controller, as we have
    fresh-installed, and so far, no cloud controller in any cloud has been configured.
    Juju automatically registers each newly configured cloud controller, as we will
    see later.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also list the currently supported list of cloud environments by running
    the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – The Juju supported cloud providers list](img/B21716_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – The Juju supported cloud providers list
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to bootstrap our cloud environments using Juju. To interact
    with AWS resources and bootstrap the first Juju cloud controller, we will need
    to generate a key pair of EC2 instances in AWS, constructed by access and secret
    keys.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack is not listed here, as Juju lists only public cloud providers. Private
    clouds, including OpenStack, would require adding it via the Juju command-line
    interface, which will be covered in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following part of the walkthrough requires you to have an AWS account and
    an **Identity Access Management** ( **IAM** ) user with programmatic access. Access
    your AWS account and make sure that you have sufficient IAM permissions to create
    an IAM user with programmatic access. From the main console services tab, click
    **IAM** and select **Users** from the **Access management** drop-down option list
    in the bottom-left corner of the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – The AWS IAM console](img/B21716_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – The AWS IAM console
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, click on the **Create** **user** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Creating an IAM user in the AWS console](img/B21716_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Creating an IAM user in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: 'Provide an IAM user in the **User name** field and click on **Next** . Leave
    the **Provide user access to the AWS Management Console** checkbox unchecked,
    as we just need a user with programmatic access:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Specifying user details in the AWS console](img/B21716_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Specifying user details in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step involves allowing our Juju user to perform on AWS resources.
    The Juju client will bootstrap a cloud controller in an EC2 instance. For the
    sake of simplicity and based on the least privilege principle, we will assign
    an AWS-managed policy with full permissions on EC2 resources in AWS by attaching
    an **AmazonEC2FullAccess** policy to the created user, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Assigning an IAM policy to the Juju IAM user in the AWS console](img/B21716_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Assigning an IAM policy to the Juju IAM user in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: From a best security practice perspective in AWS, it is not recommended to attach
    a full array of permissions to an IAM user or role. Custom policies can restrict
    the allowed actions on services and resources. As a rule of thumb, always use
    the least privilege principle when creating policies and assigning permissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Next** and on the **Review and create** page, click on **Create
    user** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Reviewing and creating the Juju IAM user in the AWS console](img/B21716_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Reviewing and creating the Juju IAM user in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **IAM** | **Users** dashboard, select the newly created Juju IAM user.
    In the **Summary** tab, click on the **Create access** **key** link:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Creating an access key for the Juju IAM user in the AWS console](img/B21716_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Creating an access key for the Juju IAM user in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: 'The next page in the IAM user wizard will require you to select the credentials
    use case. Select the **Application running outside** **AWS** option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Selecting an access key use case for the Juju IAM user in
    the AWS console](img/B21716_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Selecting an access key use case for the Juju IAM user in the
    AWS console
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step of the creation of the IAM user credentials is optional: providing
    a tag value for the AWS credentials resources. Add a tag value and click on **Create**
    **access key** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Adding an AWS tag resource for the Juju IAM user in the AWS
    console](img/B21716_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Adding an AWS tag resource for the Juju IAM user in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: 'This will generate a pair of access and secret keys. You can copy and save
    them or download a CSV file containing the credentials in a secure location locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Retrieving the generated keys for the Juju IAM user in the
    AWS console](img/B21716_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Retrieving the generated keys for the Juju IAM user in the AWS
    console
  prefs: []
  type: TYPE_NORMAL
- en: 'Switch back to the Juju command-line interface in the management host and run
    the following command line to provide a key pair, including the access and secret
    keys created in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – Adding AWS IAM user cloud credentials using the Juju client](img/B21716_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – Adding AWS IAM user cloud credentials using the Juju client
  prefs: []
  type: TYPE_NORMAL
- en: The previous command line will optionally request which region Juju will be
    operating within by providing the generated access and secrets previously. Juju
    will create a local AWS profile named **packetpub-kube** .
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create our first Juju cloud controller in AWS by running
    the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – Creating a Juju cloud controller in AWS](img/B21716_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – Creating a Juju cloud controller in AWS
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, Juju performs an API call to the AWS environment and spawns
    an EC2 instance. The cloud controller instance can be checked in the AWS console
    in an initializing status from the EC2 dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.22 – A Juju cloud controller EC2 instance created in the AWS console](img/B21716_11_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.22 – A Juju cloud controller EC2 instance created in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that Juju bootstraps the Juju controller with default configurations,
    such as the size of the instance ( **m7g.medium** ). Make sure, in the Juju command
    line output, that the bootstrapping process completes, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.23 – The completion of Juju cloud controller bootstrapping in AWS](img/B21716_11_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.23 – The completion of Juju cloud controller bootstrapping in AWS
  prefs: []
  type: TYPE_NORMAL
- en: Our created controller host is assigned a public IP ( **34.233.124.117** ) that
    the Juju client communicates with.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Juju bootstrap command can be customized further by specifying the size
    of the instance and the region where the cloud controller will be deployed. That
    can be achieved by using the **constraint** option in the Juju command-line interface,
    which uses a key-value format. A full list of Juju constraints can be found at
    [https://juju.is/docs/juju/constraint](https://juju.is/docs/juju/constraint) .
  prefs: []
  type: TYPE_NORMAL
- en: 'The Juju cloud controller can be listed by running the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.24 – Listing the deployed Juju controller using the Juju CLI](img/B21716_11_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.24 – Listing the deployed Juju controller using the Juju CLI
  prefs: []
  type: TYPE_NORMAL
- en: Another amazing benefit of Juju deployment is the support of a user graphical
    interface that comes with several features, such as the interactive creation of
    controller systems and the status of deployments. The user interface link can
    be accessed by default via the assigned public IP of the EC2 instance, which is
    generated by running the Juju dashboard command line. If you are running Juju
    version 3.0 or later, you will need to add the **juju-dashboard** charm to the
    deployed controller.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Prior to version 3.0.0, the Juju dashboard charm was deployed by default when
    installing the Juju controller, and access details were generated by running the
    **$ juju guis** command line. The previous command line was dropped from Juju
    3.0.0 and later versions, and was replaced by the **$ juju dashboard** command-line
    option.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we aim to expose Kubernetes resources, we will add the Kubernetes dashboard
    charm. Switch to the created controller by running the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.25 – Switching Juju controllers using the Juju CLI](img/B21716_11_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.25 – Switching Juju controllers using the Juju CLI
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy the **juju-dashboard** charm for Kubernetes on the selected controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, link the dashboard to the selected controller as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, make the dashboard accessible by running the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The Juju dashboard for Kubernetes should be deployed on the controller instance
    and accessed by pointing to the generated URL and login credentials provided in
    the previous output.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The output of the dashboard link and associated login credentials are no longer
    valid. Make sure to use your own generated link and credentials to access the
    deployed dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the Juju client and cloud controller are configured, we can start by deploying
    the first Kubernetes cluster in AWS. Juju provides different alternatives to deploy
    a Kubernetes cluster in AWS by simply deploying the respective charm. That can
    be a cluster deployed via a managed Kubernetes service, such as EKS, or a custom
    Kubernetes cluster running on an EC2 instance. Any of those aforementioned alternatives
    can be deployed simply via the Juju charms. In the following steps, we will deploy
    a self-managed Kubernetes cluster in AWS. Juju charms come with a production-grade
    Kubernetes cluster that can be deployed by simply running the following command
    line in the deployer host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The **charmed-kubernetes** Juju charm deploys several components in AWS, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes master node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes slave node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certificate authority
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etcd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes API load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerd engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual networking service running Calico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A full list with respective links for each component can be found at the Juju
    Charmhub at [https://charmhub.io/charmed-kubernetes](https://charmhub.io/charmed-kubernetes)
    . The previous command line will deploy the Kubernetes cluster on the created
    cloud controller. We can check each component’s status and details by running
    the following command line in the Juju deployer host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.26 – Listing the K8s cluster status in AWS using the Juju CLI](img/B21716_11_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.26 – Listing the K8s cluster status in AWS using the Juju CLI
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, a new Kubernetes cluster can be checked through the installed
    dashboard. First, make sure that you have **kubectl** installed locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, run the **kubectl** proxy command line locally on **127.0.0.1**
    and port **8001** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In a browser, enter [http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/](http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/)
    . This will show you a graphical user interface showing different deployed K8s
    cluster components as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.27 – The Juju dashboard for the K8s cluster deployed in AWS](img/B21716_11_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.27 – The Juju dashboard for the K8s cluster deployed in AWS
  prefs: []
  type: TYPE_NORMAL
- en: Once we have achieved a fully deployed and running Kubernetes cluster in AWS,
    we can move to do the same steps in the OpenStack private cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **juju list-clouds** command line, as mentioned earlier, does not return
    OpenStack as a supported cloud by default. As a first step, add the OpenStack
    private cloud to Juju by running the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.28 – Adding the OpenStack cloud using the Juju client](img/B21716_11_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.28 – Adding the OpenStack cloud using the Juju client
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the OpenStack cloud is added, use the same **juju add-credentials** command
    line to authenticate against the OpenStack API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command line to bootstrap the OpenStack cloud in Juju:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As with the bootstrapping process in the AWS world, Juju will create a new cloud
    controller instance in OpenStack named **juju-controller2** .
  prefs: []
  type: TYPE_NORMAL
- en: 'Once created, we can deploy a Kubernetes cluster using the same Juju charm
    deployed in AWS. Use the **juju deploy** command line targeting the OpenStack
    Juju model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The deployment process can take several minutes, and it can be checked by running
    the Juju status command line with the controller name as a flag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.29 – Checking the K8s cluster status in OpenStack using the Juju
    client](img/B21716_11_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.29 – Checking the K8s cluster status in OpenStack using the Juju client
  prefs: []
  type: TYPE_NORMAL
- en: As we deployed the same Kubernetes Juju charm in OpenStack as AWS, we should
    expect to see the same components listed in AWS up and running after the process
    has finished successfully. The previous command line provides the current state
    of each unit of the Kubernetes cluster. The cluster is fully deployed when all
    agents in each unit have the **idle** value.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The deployment process can take a while, and a real-time view of the deployment
    can be performed by running a **watch -c juju status** command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the cluster is deployed, it can be accessed from either the Kubernetes
    master or worker node. To take control of the cluster, we will need to set up
    the client and credentials access. Create a dedicated config directory in the
    deployer workstation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step aims to centralize the management of both clusters created in
    AWS and OpenStack through the same configuration location. To do this, we will
    copy the configuration files of each Kubernetes master deployed in both the AWS
    and OpenStack environments over an **scp** command line. The following command
    lines will switch the Juju controller to the respective Kubernetes environment
    and copy the configuration file to the **~/.kube** directory. Let’s start with
    the AWS Juju controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the previous command but switch to the OpenStack Juju controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To query the Kubernetes resources for both cloud environments, we can run the
    **kubectl** native tool that is, by default, installed in each master and worker
    node. In the management host, make sure that **kubectl** is installed (if this
    was not done previously) by running the following command line on a macOS machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The Kubectl setup guide for other platforms is detailed on the official Kubernetes
    website page at [https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then query each Kubernetes cluster from the Juju client machine by specifying
    the respective configuration path of each cloud environment. The following command
    line displays the status of the deployed Kubernetes cluster in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.30 – Listing the deployed K8s cluster components in AWS using kubectl](img/B21716_11_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.30 – Listing the deployed K8s cluster components in AWS using kubectl
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can access the Kubernetes dashboard on the Juju client machine
    by running **$ kubectl** **proxy 127.0.0.1:8001** .
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have two fully deployed Kubernetes clusters in AWS and OpenStack
    clouds, we will focus on connecting them. To use hybrid Kubernetes jargon, we
    will rely on the Federation feature.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing clusters together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Kubernetes Federation, a functionality that is known as **KubeFed** , enables
    workloads to run over multiple clusters, spread across multiple data centers and
    geographical locations. KubeFed leverages the control of different clusters from
    one central host that propagates any configuration changes to all or a selected
    member of the clusters. From an architecture perspective, the federated control
    plane leverages a set of standard APIs to control and manage cluster-wide operations.
    Since isolated clusters can be associated with DNS entries, KubeFed takes care
    of managing them by automatically configuring DNS for all discovered nodes. As
    shown in the next figure (in our case, with the hybrid model), KubeFed enables
    multiple clusters to be managed through a single control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.31 – A Kubernetes federation layout across OpenStack and AWS clouds](img/B21716_11_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.31 – A Kubernetes federation layout across OpenStack and AWS clouds
  prefs: []
  type: TYPE_NORMAL
- en: KubeFed can coordinate the state and configuration of many clusters running
    the standard Kubernetes API. In our use case, and for the AWS public cloud, any
    Kubernetes cluster deployed in EC2, EKS, or Fargate can be federated and linked
    to the cluster deployed in OpenStack’s private cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at the federation implementation, let’s take a quick detour to
    look at some technical concepts of the KubeFed. The Kubernetes federation piece
    runs a regular Pod into a Kubernetes cluster on its own. **Federation Pods** are
    purely used for federation purposes and live within a created federated service.
    Functions such as service deployments, health monitoring, and DNS management are
    handled by the federated Pods.
  prefs: []
  type: TYPE_NORMAL
- en: The following steps will demonstrate how to use KubeFed to bring the AWS and
    OpenStack worlds together.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start working with Federation, we will need to deploy a dedicated Kubernetes
    cluster on the host cluster. The simplest and quickest way to do so is by setting
    a Federation controller using Juju. Kubernetes’ Federation cluster can be deployed
    in either OpenStack or AWS. You can also choose to deploy it in a different management
    environment and interact with it through a local client that can be installed
    later. For the sake of simplicity, we will use the OpenStack environment to run
    the Federation cluster. Using Juju, we can switch to the OpenStack private cloud
    environment and deploy a new federation cluster, using the same Kubernetes charm
    as earlier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure to follow the status of the cluster deployment using the **$juju
    status** command line. Once the nine-component clusters are deployed, we can proceed
    to set up the federation cluster locally. With the same command lines we used
    for the OpenStack and AWS Kubernetes clusters, use **scp** to copy the cluster
    config file from any node under the ~/. kube/ directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We copied the config file by renaming it, as we did with the AWS and OpenStack
    Kube files, to **fed-config** .
  prefs: []
  type: TYPE_NORMAL
- en: 'A good practice when dealing with multiple Kubernetes clusters is to centralize
    the management of all of them within a single command line. That will remove extra
    overhead when switching between clusters every time we need to operate a specific
    cluster. One way to do this is simply by merging all Kubernetes config files into
    one file. The latest releases of **kubectl** support the management of multiple
    cluster views. Start by creating an empty config file if it does not exist yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, set the **KUBECONFIG** environment variable with a list of paths to all
    configuration files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once exported, run the following command line to merge all the **kubeconfig**
    files into one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the **kubectl get-contexts** command line, we can display available contexts
    from the merged configuration step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.32 – A listing of the K8s contexts of the Juju controllers](img/B21716_11_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.32 – A listing of the K8s contexts of the Juju controllers
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We can display all clusters defined in the **kubeconfig** file by running the
    **kubectl config get-clusters** command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving to construct the federated control plane, we will need to expose
    the federated service controller via DNS names. In our case, we have deployed
    the federation cluster in the OpenStack environment. Hence, a DNS provider configuration
    should be available to the federated service controller. When deploying the federated
    service controller in other cloud providers, DNS provider configuration is generated
    automatically if the deployed host cluster is the same as the DNS provider. For
    example, if the federated service controller is running in AWS and pre-configurated
    with a Route 53 hosted zone, the DNS provider will be automatically generated
    and pass DNS names to the federated service controller. In the following setup,
    we will configure **CoreDNS** as a federation DNS provider in the OpenStack federation
    cluster. To enable CoreDNS on **juju-controller-fed** , run the following command
    lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a **coredns-provider.conf** file that contains the IP address
    of the etcd service, obtained from the cluster status of the **juju-controller-fed**
    cluster and the DNS zone. The content of the **CoreDNS** file is configured as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The zone configured in this example is based on a local DNS configuration in
    the OpenStack environment. Feel free to use a custom DNS provider or a local one
    of your choice to create and manage zones for private cloud resources. Public
    cloud providers also offer a managed DNS service to create zones and DNS records,
    such as Route53 for AWS and Azure DNS for the Azure cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is time to deploy the federation control plane on **juju-controller-fed**
    . We briefly discussed Kubefed, the tool for Kubernetes federation, but we have
    not installed it yet. To do so, run the following command lines on macOS to install
    Kubefed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, extract binaries to one of the directories and set the necessary executable
    permissions for the Kubefed binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the **kubefed init** command line to deploy and initialize the required
    service in the federation cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.33 – Running the Kubefed Federation control plane](img/B21716_11_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.33 – Running the Kubefed Federation control plane
  prefs: []
  type: TYPE_NORMAL
- en: 'The **kubefed init** Federation command line will install a new API server
    by exposing the Federation service, as shown previously. It also takes care of
    installing the controller manager for Federation in addition to a dedicated namespace,
    referred to as **federation-system** in the host cluster. Now that we have Federation
    ready for usage, we can start controlling the Kubernetes clusters for Federation.
    Proceed by switching to the Federation context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add Kubernetes clusters running in OpenStack and AWS clouds to the joined
    Federation context. Let’s start by adding AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.34 – Joining an AWS K8s cluster to the federation](img/B21716_11_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.34 – Joining an AWS K8s cluster to the federation
  prefs: []
  type: TYPE_NORMAL
- en: 'The next command line will join the Kubernetes cluster running on OpenStack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.35 – Joining an OpenStack K8s cluster to the federation](img/B21716_11_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.35 – Joining an OpenStack K8s cluster to the federation
  prefs: []
  type: TYPE_NORMAL
- en: 'Both previous command lines will add the necessary configuration in the Federation
    control plane so we can interact with the new construct through the Federating
    API as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.36 – Listing federated K8s clusters running in OpenStack and AWS](img/B21716_11_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.36 – Listing federated K8s clusters running in OpenStack and AWS
  prefs: []
  type: TYPE_NORMAL
- en: 'At this level, we have constructed a few federated Kubernetes clusters running
    in public AWS and private OpenStack clouds. Further operations on both clusters
    can be performed using the standard Kubernetes APIs. This federation enables the
    deployment of all the Kubernetes primitives such as namespaces, services, deployments,
    ConfigMaps, and so on. For example, through the same federating endpoint, we can
    deploy a Kubernetes namespace that will be spread across the hybrid cloud environment.
    Let’s create a new YAML namespace called **fed-ns.yaml** with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following command line to create the namespace in the federating
    context as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.37 – Creating a federated K8s namespace](img/B21716_11_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.37 – Creating a federated K8s namespace
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the creation of the namespace in both the AWS and OpenStack worlds.
    Starting with AWS, simply switch the context by getting the list of deployed namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.38 – Listing the namespace for the K8s context running in AWS](img/B21716_11_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.38 – Listing the namespace for the K8s context running in AWS
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, run the same command line but switch to the OpenStack context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.39 – Listing the namespace for the K8s context running in OpenStack](img/B21716_11_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.39 – Listing the namespace for the K8s context running in OpenStack
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! Our federation is ready to handle workloads in a hybrid setup
    across the OpenStack and AWS clouds. Deploying services and Pods will only require
    creating them via the federating context, and the federation controller will take
    care of sharing the services on both the private and public clouds.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored more cloud opportunities with OpenStack. Leveraging container
    technology to deploy workloads across multiple cloud environments has been adopted
    by several organizations that seek the best usage of the *cloud* paradigm. This
    chapter highlighted some of the adopted approaches to deploying a hybrid cloud
    based on containers. The decentralized model can be considered a simple version
    of the centralized one where cloud operators can deploy workloads in multiple
    cloud environments. With the centralized model, firing one deployment from one
    place propagates a workload across multiple environments. The chapter demonstrated
    a use case of a hybrid model between AWS and OpenStack. Both worlds can meet,
    and having the capability to move between both cloud platforms is a great *cloudy
    power* . Containerization allows companies to deploy, manage, and scale their
    workloads in more than a cloud environment, as we learned in this chapter. Federation
    mode has proven to be a rapid, reliable, and zero-touch way to operate workloads
    managed by Kubernetes, which we also learned about in this chapter. We further
    learned that OpenStack did not miss the chance again, and that with the rise of
    hybrid cloud adoption over the last few years, OpenStack has come to light. Due
    to the maturity of its ecosystem and rich set of APIs, OpenStack has become in
    demand to manage private cloud infrastructure. We learned that the marriage of
    both private and public clouds has, so far, been a successful experience. Let’s
    keep it *cloudy* , where OpenStack keeps *shining* .
  prefs: []
  type: TYPE_NORMAL
