<html><head></head><body>
<div id="_idContainer213">
<h1 class="chapter-number" id="_idParaDest-296"><a id="_idTextAnchor700"/><span class="koboSpan" id="kobo.1.1">17</span></h1>
<h1 id="_idParaDest-297"><a id="_idTextAnchor701"/><span class="koboSpan" id="kobo.2.1">Managing Production Environments with Terraform</span></h1>
<p><span class="koboSpan" id="kobo.3.1">It is fitting that the capstone of this book is managing your environments with Terraform as that is probably the most important operational aspect of our solutions’ infrastructure: managing it. </span><span class="koboSpan" id="kobo.3.2">All too often, infrastructure as code is used as an expedient way to turn meters and blast solutions into the cloud without a thought given to what would happen the next day and every other day </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">going forward.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">This </span><em class="italic"><span class="koboSpan" id="kobo.6.1">day 1 ops</span></em><span class="koboSpan" id="kobo.7.1"> mindset is rampant, and while understandable from a psychological standpoint, the people working with infrastructure as code are inherently builders. </span><span class="koboSpan" id="kobo.7.2">We love building new things and are constantly looking for ways to improve how we do so. </span><span class="koboSpan" id="kobo.7.3">But I would argue that one of the most important (and often neglected) design considerations for infrastructure-as-code solutions is not scalability, performance, security, or even high </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">availability—it’s operability.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">Can we effectively operate our environments without outages and delays that can impact the health of our environments and ultimately the commitments we make to our customers? </span><span class="koboSpan" id="kobo.9.2">If the answer is no, then we have failed as infrastructure-as-code developers, cloud engineers, and </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">cloud architects.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">In this chapter, we will look at how we can infuse infrastructure as code with processes and techniques empowered by Terraform to achieve </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">these goals.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">The chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">following topics:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.15.1">Operations models</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.16.1">Applying changes</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.17.1">Breakfixing</span></span></li>
</ul>
<h1 id="_idParaDest-298"><a id="_idTextAnchor702"/><span class="koboSpan" id="kobo.18.1">Operating models</span></h1>
<p><span class="koboSpan" id="kobo.19.1">In this section, we’ll delve </span><a id="_idIndexMarker1196"/><span class="koboSpan" id="kobo.20.1">into the different operating models that fit common usage patterns for teams and organizations employing Terraform to provision and manage their infrastructure. </span><span class="koboSpan" id="kobo.20.2">Let’s start with the basics of Terraform operations: </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">state management</span></strong><span class="koboSpan" id="kobo.22.1">. </span><span class="koboSpan" id="kobo.22.2">We’ll then explore how teams can incorporate Terraform into their operating models. </span><span class="koboSpan" id="kobo.22.3">Depending on a team’s role within an organization and the cloud infrastructure they are managing, the team’s dynamics may vary. </span><span class="koboSpan" id="kobo.22.4">This can also affect how they collaborate with other parts of the organization that may or may not use Terraform in </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">their workflow.</span></span><a id="_idTextAnchor703"/></p>
<h2 id="_idParaDest-299"><a id="_idTextAnchor704"/><span class="koboSpan" id="kobo.24.1">State management</span></h2>
<p><span class="koboSpan" id="kobo.25.1">When</span><a id="_idIndexMarker1197"/><span class="koboSpan" id="kobo.26.1"> starting to manage long-lived environments </span><a id="_idIndexMarker1198"/><span class="koboSpan" id="kobo.27.1">using Terraform, whether they are just for development, testing, or actual production workloads, the foundational change to your operating model is the </span><a id="_idIndexMarker1199"/><span class="koboSpan" id="kobo.28.1">introduction of </span><strong class="bold"><span class="koboSpan" id="kobo.29.1">Terraform state</span></strong><span class="koboSpan" id="kobo.30.1">. </span><span class="koboSpan" id="kobo.30.2">We discussed Terraform state in </span><a href="B21183_01.xhtml#_idTextAnchor039"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.31.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.32.1"> of this book when we delved into Terraform’s architecture, so we already know the value it brings as the arbiter of what the environment should look like, but creating state and managing it is part of the day-to-day operations of managing environments </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">with Terrafo</span><a id="_idTextAnchor705"/><span class="koboSpan" id="kobo.34.1">rm.</span></span></p>
<h3><span class="koboSpan" id="kobo.35.1">Just say no to manual state manipulation</span></h3>
<p><span class="koboSpan" id="kobo.36.1">As we</span><a id="_idIndexMarker1200"/><span class="koboSpan" id="kobo.37.1"> have established, Terraform state files are essentially just JSON text files that contain an inventory of the resources as they were provisioned during the last Terraform Apply. </span><span class="koboSpan" id="kobo.37.2">It might be tempting to manually manipulate the state file just by opening it up in a text editor and changing things around. </span><span class="koboSpan" id="kobo.37.3">However, this is ill advised. </span><span class="koboSpan" id="kobo.37.4">The Terraform CLI has many commands that provide a safe way to perform state manipulation operations, and HashiCorp is even starting to aggressively roll out HashiCorp Configuration Language features to enable state manipulation through the code base itself rather than bespoke administrator tinkering through the CLI. </span><span class="koboSpan" id="kobo.37.5">Besides transitioning from an imperative approach to a declarative one, it also has the added benefit for module authors to make version upgrade paths more seamless by building a safe way to update without costly blue-green deployments or implementing short-term </span><em class="italic"><span class="koboSpan" id="kobo.38.1">fixes</span></em><span class="koboSpan" id="kobo.39.1"> that become </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">long-term probl</span><a id="_idTextAnchor706"/><span class="koboSpan" id="kobo.41.1">ems.</span></span></p>
<h3><span class="koboSpan" id="kobo.42.1">Access control</span></h3>
<p><span class="koboSpan" id="kobo.43.1">Due to Terraform’s </span><a id="_idIndexMarker1201"/><span class="koboSpan" id="kobo.44.1">nature of being this extensible chameleon of an infrastructure-as-code tool that adapts to each target platform through its provider plugins, it also adapts to the target platforms by way of the backend provider that is used. </span><span class="koboSpan" id="kobo.44.2">By default, Terraform uses the local filesystem for the state, but this, of course, is not used when managing long-lived environments. </span><span class="koboSpan" id="kobo.44.3">As we discussed, when we implemented the solution across each of the three cloud platforms covered in this book, we used a different backend provider to store the state on the </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">corresponding cloud.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">In AWS, we used</span><a id="_idIndexMarker1202"/><span class="koboSpan" id="kobo.47.1"> the </span><strong class="source-inline"><span class="koboSpan" id="kobo.48.1">s3</span></strong><span class="koboSpan" id="kobo.49.1"> backend, which stored our state on AWS’s </span><strong class="bold"><span class="koboSpan" id="kobo.50.1">Simple Storage Service</span></strong><span class="koboSpan" id="kobo.51.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.52.1">S3</span></strong><span class="koboSpan" id="kobo.53.1">). </span><span class="koboSpan" id="kobo.53.2">By default, only users with sufficient </span><a id="_idIndexMarker1203"/><span class="koboSpan" id="kobo.54.1">identity and access management permissions are able to access the data. </span><span class="koboSpan" id="kobo.54.2">This allows us to have fine-grained control over the users (and the machines) who can access the state files. </span><span class="koboSpan" id="kobo.54.3">Likewise, on Azure, when we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.55.1">azurerm</span></strong><span class="koboSpan" id="kobo.56.1"> backend, and on Google Cloud Platform, when we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.57.1">gcs</span></strong><span class="koboSpan" id="kobo.58.1"> backend, Terraform stored the state files on the corresponding storage service for each of these cloud platforms. </span><span class="koboSpan" id="kobo.58.2">Like AWS, the other cloud platforms implement similar access controls to prevent unprivileged access. </span><span class="koboSpan" id="kobo.58.3">On Azure, this takes </span><a id="_idIndexMarker1204"/><span class="koboSpan" id="kobo.59.1">the form of Azure </span><strong class="bold"><span class="koboSpan" id="kobo.60.1">Role-Based Access Controls</span></strong><span class="koboSpan" id="kobo.61.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.62.1">RBACs</span></strong><span class="koboSpan" id="kobo.63.1">) specified at either the resource group or the subscription level. </span><span class="koboSpan" id="kobo.63.2">On Google Cloud, this takes the form of access control lists that are driven at the </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">projec</span><a id="_idTextAnchor707"/><span class="koboSpan" id="kobo.65.1">t level.</span></span></p>
<h3><span class="koboSpan" id="kobo.66.1">Encryption</span></h3>
<p><span class="koboSpan" id="kobo.67.1">In </span><a id="_idIndexMarker1205"/><span class="koboSpan" id="kobo.68.1">addition to identity-based access controls that we can apply on the cloud services that are hosting our Terraform state files, we can also employ built-in capabilities of these services to leverage various levels of encryption. </span><span class="koboSpan" id="kobo.68.2">The simplest level is the built-in transparent data encryption that protects us if the cloud provider has a physical data breach. </span><span class="koboSpan" id="kobo.68.3">This is a nice insurance policy but it’s one of the more unlikely </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">attack vectors.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">The more likely way our Terraform state files will become vulnerable is if we have leaky identity and access management controls. </span><span class="koboSpan" id="kobo.70.2">One method for adding an additional layer of security is by leveraging encryption of the data within the storage service itself. </span><span class="koboSpan" id="kobo.70.3">When you do this, access to the files is not enough; you need access to the encryption </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">keys themselves.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">On AWS, this is done </span><a id="_idIndexMarker1206"/><span class="koboSpan" id="kobo.73.1">using AWS </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">Key Management Service</span></strong><span class="koboSpan" id="kobo.75.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.76.1">KMS</span></strong><span class="koboSpan" id="kobo.77.1">), which allows you to create and manage your own keys that can be used to encrypt your Terraform state files. </span><span class="koboSpan" id="kobo.77.2">Similar capabilities exist on both Azure and Google Cloud. </span><span class="koboSpan" id="kobo.77.3">On Azure, you would employ customer-managed keys created in Azure Key Vault, and on Google, you would employ the same approach but, of course, use the equivalent Google Cloud service called Google Cloud KMS. </span><span class="koboSpan" id="kobo.77.4">If you want a cloud-agnostic approach, you could leverage a multi-cloud key management solution such as </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">HashiC</span><a id="_idTextAnchor708"/><span class="koboSpan" id="kobo.79.1">orp Vault.</span></span></p>
<h3><span class="koboSpan" id="kobo.80.1">Backup</span></h3>
<p><span class="koboSpan" id="kobo.81.1">In the previous </span><a id="_idIndexMarker1207"/><span class="koboSpan" id="kobo.82.1">chapter, we looked at how to import an existing environment into Terraform and saw that even while there are built-in tools to do this, it can be tedious and error-prone. </span><span class="koboSpan" id="kobo.82.2">Unfortunately, the only thing keeping your environment in the classification of environments </span><em class="italic"><span class="koboSpan" id="kobo.83.1">managed by Terraform</span></em><span class="koboSpan" id="kobo.84.1"> is the state file. </span><span class="koboSpan" id="kobo.84.2">If you lose your state file or if it becomes corrupted or out of sync beyond all recollection, your clean environment that was provisioned by Terraform could very easily become an orphaned environment, no longer managed by Terraform and requiring you to consider your options when it comes to importing </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">or re-provisioning.</span></span></p>
<p><span class="koboSpan" id="kobo.86.1">Don’t let that happen! </span><span class="koboSpan" id="kobo.86.2">You should keep backups of your state files. </span><span class="koboSpan" id="kobo.86.3">Most of the Terraform backends that we looked at support this out of the box in several different ways. </span><span class="koboSpan" id="kobo.86.4">First, it does this by enabling version tracking so you actually have a versioned history of the state file within the storage service itself. </span><span class="koboSpan" id="kobo.86.5">This is a very convenient and cost-effective way to help you overcome small issues such as human error or transient </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">deployment failures.</span></span></p>
<p><span class="koboSpan" id="kobo.88.1">However, you should also consider more advanced cross-region replication features of the cloud storage service hosting your Terraform state backend to help you in case of a broader outage. </span><span class="koboSpan" id="kobo.88.2">Terraform state going temporarily offline or unavailable doesn’t impact your solution’s availability, but it does impact your ability to exert control in the environment in the case of an outage. </span><span class="koboSpan" id="kobo.88.3">So, it’s important to think about implementing cross-region replication and a backup strategy to ensure all scenarios </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">a</span><a id="_idTextAnchor709"/><span class="koboSpan" id="kobo.90.1">re covered.</span></span></p>
<h3><span class="koboSpan" id="kobo.91.1">Organization</span></h3>
<p><span class="koboSpan" id="kobo.92.1">One of the</span><a id="_idIndexMarker1208"/><span class="koboSpan" id="kobo.93.1"> easiest things you can control is where your Terraform workspaces are stored. </span><span class="koboSpan" id="kobo.93.2">It doesn’t take a whole bunch of bells and whistles to protect your state files if you properly segment your Terraform workspaces and work within the security boundaries that your cloud has </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">to offer.</span></span></p>
<p><span class="koboSpan" id="kobo.95.1">On AWS, you may want to create more S3 buckets and place those buckets in different AWS accounts to ensure there isn’t secret leakage due to overly benevolent </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">IAM policies.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">Likewise, on Azure, more storage accounts can be provisioned and placed in Azure subscriptions to isolate them more effectively against overly generous </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">subscription-level permissions.</span></span></p>
<p><span class="koboSpan" id="kobo.99.1">On </span><a id="_idIndexMarker1209"/><span class="koboSpan" id="kobo.100.1">Google Cloud, consider carefully what project the Google Cloud Storage service should be provisioned within and opt for an isolated project for Terraform state. </span><span class="koboSpan" id="kobo.100.2">This will ensure that the application and its administrators don’t necessarily have access to the secrets that may be in the Terrafor</span><a id="_idTextAnchor710"/><span class="koboSpan" id="kobo.101.1">m </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">state file.</span></span></p>
<h2 id="_idParaDest-300"><a id="_idTextAnchor711"/><span class="koboSpan" id="kobo.103.1">Standalone application</span></h2>
<p><span class="koboSpan" id="kobo.104.1">For</span><a id="_idIndexMarker1210"/><span class="koboSpan" id="kobo.105.1"> most of this book, we have been operating as a small team at the elusive billionaire magnate Keyser Söze’s firm building a next-generation fleet operations platform. </span><span class="koboSpan" id="kobo.105.2">In these scenarios, we worked across multiple clouds and implemented our solution using three different cloud computing paradigms along </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">the way:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<span class="koboSpan" id="kobo.107.1"><img alt="Figure 17.1 – Small team operating model for a small team deploying a standalone application" src="image/B21183_17_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.108.1">Figure 17.1 – Small team operating model for a small team deploying a standalone application</span></p>
<p><span class="koboSpan" id="kobo.109.1">In this scenario, we saw an application development team that was working on a typical N-tier architecture application. </span><span class="koboSpan" id="kobo.109.2">The team consisted of probably 6-8 people who were software developers or software testers. </span><span class="koboSpan" id="kobo.109.3">The ultimate goal was not to provision infrastructure but to facilitate a release process for the application software the team </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">is developing.</span></span></p>
<p><span class="koboSpan" id="kobo.111.1">In these types of teams, it’s not uncommon for both the application code and the infrastructure as code to be maintained within the same source code repository. </span><span class="koboSpan" id="kobo.111.2">This simple approach recognizes the natural dependencies between the infrastructure and the application as a result of the deployment process. </span><span class="koboSpan" id="kobo.111.3">The presence of well-known secrets is provisioned by the infrastructure as code but referenced by the application during its initialization. </span><span class="koboSpan" id="kobo.111.4">Keeping it all in our source code repository allows us to minimize the mechanics of making changes that cascade across the infrastructure and application code base in a single feature branch, pull request, and ultimately merge </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">into </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">main</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">We </span><a id="_idIndexMarker1211"/><span class="koboSpan" id="kobo.116.1">have a single Terraform root module that we use to deploy our environments, and we alter the input variables to configure it appropriately for different instances of the environment: </span><strong class="source-inline"><span class="koboSpan" id="kobo.117.1">DEV</span></strong><span class="koboSpan" id="kobo.118.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.119.1">PROD</span></strong><span class="koboSpan" id="kobo.120.1">. </span><span class="koboSpan" id="kobo.120.2">This allows us to manage a multitude of environments simply by changing which workspace we point at either using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">terraform workspace</span></strong><span class="koboSpan" id="kobo.122.1"> command or by changing the backend key that we use to partition the workspace within </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">the backend.</span></span></p>
<p><span class="koboSpan" id="kobo.124.1">The solution that we built and deployed was an end-to-end solution with multiple architectural components that made up the entire application—in this case, an application with a web frontend and a REST API as its backend, which is not an uncommon scenario. </span><span class="koboSpan" id="kobo.124.2">Because our solution was so simple, we were able to operate in a completely self-contained manner. </span><span class="koboSpan" id="kobo.124.3">This isn’t always the case, as we’ll see later, in larger teams and larger environments—particularly in </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">the enterprise.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">During the solution development that we did in </span><em class="italic"><span class="koboSpan" id="kobo.127.1">Chapters 7</span></em><span class="koboSpan" id="kobo.128.1"> through </span><em class="italic"><span class="koboSpan" id="kobo.129.1">15</span></em><span class="koboSpan" id="kobo.130.1">, we didn’t really address how those environments would be managed in production. </span><span class="koboSpan" id="kobo.130.2">In a normal product development process, we would need to provision multiple environments for various purposes and manage our release life cycle across these environments until we finally ship the product by deploying it </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">into production.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">As we saw along this journey, in addition to the subtle and sometimes not-so-subtle differences between cloud platforms, depending on the cloud computing paradigm, we would use different mediums for packaging our application deployment, which sometimes allowed us to integrate the deployment artifact into our Terraform configuration by referencing the virtual machine image or the container image; but sometimes, as with serverless, we had to implement an additional standalone deployment procedure that would execute after Terraform provisioned </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">our</span><a id="_idTextAnchor712"/><span class="koboSpan" id="kobo.134.1"> environments.</span></span></p>
<h2 id="_idParaDest-301"><a id="_idTextAnchor713"/><span class="koboSpan" id="kobo.135.1">Shared infrastructure</span></h2>
<p><span class="koboSpan" id="kobo.136.1">Unlike </span><a id="_idIndexMarker1212"/><span class="koboSpan" id="kobo.137.1">the application development team that we followed along on their journey through the multiverse of cloud platforms and cloud computing paradigms, there is an entire engineering group that isn’t building application code, but they are still heavy users of Terraform. </span><span class="koboSpan" id="kobo.137.2">These teams manage an organization’s shared infrastructure. </span><span class="koboSpan" id="kobo.137.3">These teams might be made up of traditional infrastructure engineers who may have managed the on-premises virtualization environment, network security, or other similar realms within IT infrastructure. </span><span class="koboSpan" id="kobo.137.4">Depending on the size of the organization, this can be a big job, spanning many teams and organizations, each with its own scope and realm of responsibilities, or it could be a </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">single team:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<span class="koboSpan" id="kobo.139.1"><img alt="Figure 17.2 – Shared infrastructure team operating model for a shared infrastructure team deploying infrastructure that supports one or more application teams within an organization" src="image/B21183_17_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.140.1">Figure 17.2 – Shared infrastructure team operating model for a shared infrastructure team deploying infrastructure that supports one or more application teams within an organization</span></p>
<p><span class="koboSpan" id="kobo.141.1">This operating model differs from a simple project with stand-alone application development in that there are some inherent dependencies between what this team and other teams in the organization provision. </span><span class="koboSpan" id="kobo.141.2">Those external teams draw their dependencies without committing to any type of operating model that conforms to the shared infrastructure team’s. </span><span class="koboSpan" id="kobo.141.3">Therefore, these teams might not be using Terraform or any automation for </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">that matter.</span></span></p>
<p><span class="koboSpan" id="kobo.143.1">The environments that they are managing could be a shared network, centralized monitoring and logging, databases, data lakes, data warehouses, or even pools of shared compute, such as Kubernetes clusters. </span><span class="koboSpan" id="kobo.143.2">In most scenarios, they won’t have their own application code, but they will often have their own deployment packages—whether these are virtual machine or container images of their own creation or third-party </span><strong class="bold"><span class="koboSpan" id="kobo.144.1">Commercial Off-the-Shelf</span></strong><span class="koboSpan" id="kobo.145.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.146.1">COTS</span></strong><span class="koboSpan" id="kobo.147.1">) software </span><a id="_idIndexMarker1213"/><span class="koboSpan" id="kobo.148.1">packages provided to them by software vendors through either a commercial or open </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">source relationship.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">In large organizations, virtual machine and container image repositories themselves are usually built and managed as shared infrastructure that is built and maintained by a platform team to be reused across </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">the organization.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">These workloads will likely also have multiple environments but may not have as many as an application development team and may opt to delineate environments simply by a non-production/production dimension. </span><span class="koboSpan" id="kobo.152.2">This approach enables maximum reuse for non-production workloads and reduces the overhead of further fragmenting the shared infrastructure for every use case that dependent teams </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">might have.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">The</span><a id="_idIndexMarker1214"/><span class="koboSpan" id="kobo.155.1"> deployment process is simplified due to the absence of application code, but shared infrastructure teams should carefully consider how to organize their Terraform workspaces to minimize friction between the external teams, which draw dependencies on them. </span><span class="koboSpan" id="kobo.155.2">This is where blast radius plays an important role in the design and segmentation of shared infrastructure workloads into discrete and manageable </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">Terra</span><a id="_idTextAnchor714"/><span class="koboSpan" id="kobo.157.1">form workspaces.</span></span></p>
<h2 id="_idParaDest-302"><a id="_idTextAnchor715"/><span class="koboSpan" id="kobo.158.1">Shared services</span></h2>
<p><span class="koboSpan" id="kobo.159.1">Finally, the </span><a id="_idIndexMarker1215"/><span class="koboSpan" id="kobo.160.1">most complex operating model is that of a </span><strong class="bold"><span class="koboSpan" id="kobo.161.1">shared service</span></strong><span class="koboSpan" id="kobo.162.1">. </span><span class="koboSpan" id="kobo.162.2">In</span><a id="_idIndexMarker1216"/><span class="koboSpan" id="kobo.163.1"> this scenario, we are combining aspects of both our standalone application and our shared infrastructure. </span><span class="koboSpan" id="kobo.163.2">Shared services not only have the application code base that they need to build and deploy but also have other teams within the organization that draw dependencies on them. </span><span class="koboSpan" id="kobo.163.3">However, unlike the shared infrastructure team, where those dependencies might be at the network or configuration layer, shared services often have dependencies at the application interface layer, embedded within the message-based protocols that the two systems use for interoperability. </span><span class="koboSpan" id="kobo.163.4">The shared services team is likely made up of developers and testers responsible for maintaining one (or more) services within a portfolio </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">of microservices:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<span class="koboSpan" id="kobo.165.1"><img alt="Figure 17.3 – Shared services team" src="image/B21183_17_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.166.1">Figure 17.3 – Shared services team</span></p>
<p><span class="koboSpan" id="kobo.167.1">Shared services teams </span><a id="_idIndexMarker1217"/><span class="koboSpan" id="kobo.168.1">are commonplace at large organizations and, as a result, often operate in an environment where they may draw their own dependencies on both other shared services</span><a id="_idIndexMarker1218"/><span class="koboSpan" id="kobo.169.1"> and shared infrastructure teams within their organization. </span><span class="koboSpan" id="kobo.169.2">This helps reduce the scope of responsibility of the shared services team as they can shed responsibilities that are picked up by shared infrastructure teams operating lower-level infrastructure, such as the wide area network, security, and logging and monitoring, as well as higher-level infrastructure such as Kubernetes or even shared Kafka or </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">Cassandra clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">While the distribution of this responsibility helps focus a shared services team’s energy on the development and maintenance of their service, it also creates additional coordination effort to synchronize changes and release processes as well as versioning compatibility between both downstream and</span><a id="_idTextAnchor716"/> <span class="No-Break"><span class="koboSpan" id="kobo.172.1">upstream services.</span></span></p>
<p><span class="koboSpan" id="kobo.173.1">Now that we’ve looked at several different operating models for using Terraform to manage existing environments, we’ll take a deeper look at some of the common operations that you’ll need to perform as you manage your environments. </span><span class="koboSpan" id="kobo.173.2">No matter what your team looks like and what type of workload you are managing with Terraform, these scenarios are bound to </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">come up!</span></span></p>
<h1 id="_idParaDest-303"><a id="_idTextAnchor717"/><span class="koboSpan" id="kobo.175.1">Applying changes</span></h1>
<p><span class="koboSpan" id="kobo.176.1">When we </span><a id="_idIndexMarker1219"/><span class="koboSpan" id="kobo.177.1">manage a long-lived environment, it’s inevitable that we will have to eventually make changes to that environment—whether they are large or small. </span><span class="koboSpan" id="kobo.177.2">Change happens. </span><span class="koboSpan" id="kobo.177.3">It can be a change related to our solution itself, or it can be a change needed due to upgrades to our tools and the underlying platform itself. </span><span class="koboSpan" id="kobo.177.4">It can be expected—such as planned releases—or unexpected—such as zone or regional outages. </span><span class="koboSpan" id="kobo.177.5">In </span><a id="_idIndexMarker1220"/><span class="koboSpan" id="kobo.178.1">this section, we’ll look at all of the different types of changes that often happen to our environments and how we should best handle them while managing our environm</span><a id="_idTextAnchor718"/><span class="koboSpan" id="kobo.179.1">ents </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">using Terraform.</span></span></p>
<h2 id="_idParaDest-304"><a id="_idTextAnchor719"/><span class="koboSpan" id="kobo.181.1">Patching</span></h2>
<p><span class="koboSpan" id="kobo.182.1">When </span><a id="_idIndexMarker1221"/><span class="koboSpan" id="kobo.183.1">using Terraform, there are several places within our code that we will need to make conscious decisions about what versions of what components we want to use. </span><span class="koboSpan" id="kobo.183.2">These places include the version of Terraform’s executable and the providers and modules that you use with</span><a id="_idTextAnchor720"/><span class="koboSpan" id="kobo.184.1">in </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">your configuration.</span></span></p>
<h3><span class="koboSpan" id="kobo.186.1">Upgrading the Terraform executable</span></h3>
<p><span class="koboSpan" id="kobo.187.1">The </span><a id="_idIndexMarker1222"/><span class="koboSpan" id="kobo.188.1">first thing we need to consider is what version of Terraform we want to use. </span><span class="koboSpan" id="kobo.188.2">This may seem surprising—I mean, why wouldn’t you always want to use the latest and greatest version of Terraform? </span><span class="koboSpan" id="kobo.188.3">However, there are some pretty important reasons why upgrading the version of Terraform you are using is something you should be careful about when managing </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">existing systems.</span></span></p>
<p><span class="koboSpan" id="kobo.190.1">The version of Terraform you are using could impact the versions of the providers you are using that are supported, which could result in cascading upgrade requirements that may require you to take on more change in your code base than you were </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">originally planning.</span></span></p>
<p><span class="koboSpan" id="kobo.192.1">While new versions of Terraform often bring exciting new features, capabilities, and bug fixes, they can also bring deprecations and backward incompatibilities. </span><span class="koboSpan" id="kobo.192.2">This is something that HashiCorp has historically done an excellent job of managing, minimizing the impact of the change, but it is nonetheless something to keep an eye on as it does occasionally happen. </span><span class="koboSpan" id="kobo.192.3">The most recent example of where the version of Terraform had major implications was with version </span><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">0.12</span></strong><span class="koboSpan" id="kobo.194.1"> of Terraform. </span><span class="koboSpan" id="kobo.194.2">In this situation, if you were using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">aws</span></strong><span class="koboSpan" id="kobo.196.1"> provider, if you upgraded to version </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">0.12</span></strong><span class="koboSpan" id="kobo.198.1"> of Terraform, you would need to upgrade to version </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">2.20.0</span></strong><span class="koboSpan" id="kobo.200.1"> of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">aws</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.202.1"> provider.</span></span></p>
<p><span class="koboSpan" id="kobo.203.1">The version of Terraform is often referenced in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">required_versions</span></strong><span class="koboSpan" id="kobo.205.1"> block of both root modules and reusable modules alike. </span><span class="koboSpan" id="kobo.205.2">Therefore, you should also evaluate the upgrade’s implications on your Terraform-managed environments and any modules th</span><a id="_idTextAnchor721"/><span class="koboSpan" id="kobo.206.1">at you </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">are referencing.</span></span></p>
<h2 id="_idParaDest-305"><a id="_idTextAnchor722"/><span class="koboSpan" id="kobo.208.1">Upgrading providers</span></h2>
<p><span class="koboSpan" id="kobo.209.1">Like </span><a id="_idIndexMarker1223"/><span class="koboSpan" id="kobo.210.1">Terraform itself, each of the providers we use to provision resources to various clouds and other platforms has its own version. </span><span class="koboSpan" id="kobo.210.2">This compounds the issues we experienced when upgrading Terraform itself across every provider we use within our Terraform solutions. </span><span class="koboSpan" id="kobo.210.3">However, most Terraform deployments use the provider for one cloud platform but also might include other providers for different control planes that the </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">solution targets.</span></span></p>
<p><span class="koboSpan" id="kobo.212.1">The cloud platforms, in particular, are problematic just because they move so fast and are so far-reaching. </span><span class="koboSpan" id="kobo.212.2">For example, the AWS, Azure, and Google Cloud resource providers have over 700, 600, and 400 different resource types, respectively! </span><span class="koboSpan" id="kobo.212.3">Now, you probably won’t be using all of those resource types in one of your Terraform solutions, but with so many different resource types offered by a provider, there is an opportunity for change anytime one of those services adds a new feature. </span><span class="koboSpan" id="kobo.212.4">Hence, they change frequently, with new versions of the provider released weekly and sometimes </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">even faster!</span></span></p>
<p><span class="koboSpan" id="kobo.214.1">It’s a good idea to be purposeful about upgrading the versions of your providers. </span><span class="koboSpan" id="kobo.214.2">While you shouldn’t necessarily follow the provider’s weekly release cadence, it’s best not to let the version of your provider stagnate, as this just builds up technical debt until it becomes an emergency. </span><span class="koboSpan" id="kobo.214.3">Emergencies can arise in one of two ways. </span><span class="koboSpan" id="kobo.214.4">First, you could be leveraging deprecated resources, blocks, or attributes within your configuration that will eventually have their support removed. </span><span class="koboSpan" id="kobo.214.5">Second, you might want to take advantage of a new feature or capability of one of the resources you are using, but it’s unsupported </span><a id="_idTextAnchor723"/><span class="koboSpan" id="kobo.215.1">in your </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">current version.</span></span></p>
<h2 id="_idParaDest-306"><a id="_idTextAnchor724"/><span class="koboSpan" id="kobo.217.1">Upgrading modules</span></h2>
<p><span class="koboSpan" id="kobo.218.1">Modules</span><a id="_idIndexMarker1224"/><span class="koboSpan" id="kobo.219.1"> are another place where you need to think about versions. </span><span class="koboSpan" id="kobo.219.2">When you reference a module from the Terraform Registry, you explicitly set the version you want to use. </span><span class="koboSpan" id="kobo.219.3">If you are using modules stored in other, less structured locations, such as Git repositories, you should be careful to reference them using a </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">specific tag.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">The impact of upgrading a module version, like each of the resource types within a provider, depends on the breaking changes—or lack thereof—within the module’s new version. </span><span class="koboSpan" id="kobo.221.2">Sometimes, modules can differ radically between versions, and this can result in a significant negative impact on consumers of these modules who naively upgrade, assuming everything will work </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">out okay.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">For modules, Terraform</span><a id="_idIndexMarker1225"/><span class="koboSpan" id="kobo.224.1"> Plan is usually sufficient to detect whether there is a major change being introduced, but when provider and module version changes overlap, it is often a good idea to perform test deployments in order to verify upgrades. </span><span class="koboSpan" id="kobo.224.2">This can be done for any type of change you are trying to intro</span><a id="_idTextAnchor725"/><span class="koboSpan" id="kobo.225.1">duce into </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">the environment.</span></span></p>
<h2 id="_idParaDest-307"><a id="_idTextAnchor726"/><span class="koboSpan" id="kobo.227.1">Refactoring</span></h2>
<p><span class="koboSpan" id="kobo.228.1">As we develop </span><a id="_idIndexMarker1226"/><span class="koboSpan" id="kobo.229.1">more advanced configurations, we can often find ourselves in a situation where there are components of our module—whether it’s a root module or a reusable module—that can ideally be extracted into their own module because they implement a repeatable pattern that could be reusable in a more granular context in other modules and </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">other deployments.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">It is in these situations that we will likely need to move resources from one module to another. </span><span class="koboSpan" id="kobo.231.2">If we do this within our code, any new environments that we provision immediately can reap the benefits, but our existing environments will suffer because they will detect the change. </span><span class="koboSpan" id="kobo.231.3">The resource that we moved from one module to another will now have an entirely new path when Terraform does its plan. </span><span class="koboSpan" id="kobo.231.4">From Terraform’s perspective, the resource at the old location was deleted, and a new resource needs to be created at the new location. </span><span class="koboSpan" id="kobo.231.5">This drop-create motion creates a tremendous amount of disruption when managing </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">existing environments.</span></span></p>
<p><span class="koboSpan" id="kobo.233.1">Like with the importing of resources, we have two methods for moving resources. </span><span class="koboSpan" id="kobo.233.2">There is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">terraform state mv</span></strong><span class="koboSpan" id="kobo.235.1"> command-line operation and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.236.1">moved</span></strong><span class="koboSpan" id="kobo.237.1"> block, the latter of which we can define in our </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">HCL configuration:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.239.1">
moved {
  from = module.foo.azurerm_network_security_rule.nsg_443
  to   = module.bar.azurerm_network_security_rule.main[0]
}</span></pre> <p><span class="koboSpan" id="kobo.240.1">The command-line operation is quite simple and is structured how you </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">would expect:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.242.1">
terraform state mv SOURCE DESTINATION</span></pre> <p><span class="koboSpan" id="kobo.243.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">SOURCE</span></strong><span class="koboSpan" id="kobo.245.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">DESTINATION</span></strong><span class="koboSpan" id="kobo.247.1"> command-line parameters correspond to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">moved</span></strong><span class="koboSpan" id="kobo.249.1"> block’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">from</span></strong><span class="koboSpan" id="kobo.251.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">to</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.253.1">attributes, respectively.</span></span></p>
<p><span class="koboSpan" id="kobo.254.1">Let’s look </span><a id="_idIndexMarker1227"/><span class="koboSpan" id="kobo.255.1">at a specific example. </span><span class="koboSpan" id="kobo.255.2">In the chapters where we built solutions using Kubernetes, we saw several resources get repeated with nearly identical configurations for both the frontend and backend components of our application architecture. </span><span class="koboSpan" id="kobo.255.3">These resources were </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">kubernetes_deployment</span></strong><span class="koboSpan" id="kobo.257.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">kubernetes_service</span></strong><span class="koboSpan" id="kobo.259.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">kubernetes_config_map</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<span class="koboSpan" id="kobo.263.1"><img alt="Figure 17.4 – Visible repeating pattern of resources" src="image/B21183_17_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.264.1">Figure 17.4 – Visible repeating pattern of resources</span></p>
<p><span class="koboSpan" id="kobo.265.1">Before we can refactor this solution, we need to create a module that will replace the three </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">repeating resources:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<span class="koboSpan" id="kobo.267.1"><img alt="Figure 17.5 – Refactor step 2 – construct a reusable module that can be configured to replace each of the instances of the repeating pattern" src="image/B21183_17_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.268.1">Figure 17.5 – Refactor step 2 – construct a reusable module that can be configured to replace each of the instances of the repeating pattern</span></p>
<p><span class="koboSpan" id="kobo.269.1">Now that the </span><a id="_idIndexMarker1228"/><span class="koboSpan" id="kobo.270.1">module has been created, we need to create an instance of the module in the root module and delete the previous resources within the </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">repeating pattern:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer203">
<span class="koboSpan" id="kobo.272.1"><img alt="Figure 17.6 – Refactor step 3 – replace the loose resources with module references and moved blocks" src="image/B21183_17_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.273.1">Figure 17.6 – Refactor step 3 – replace the loose resources with module references and moved blocks</span></p>
<p><span class="koboSpan" id="kobo.274.1">Finally, we</span><a id="_idIndexMarker1229"/><span class="koboSpan" id="kobo.275.1"> create </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">moved</span></strong><span class="koboSpan" id="kobo.277.1"> blocks that will facilitate Terraform recognizing that the resources don’t need to be destroyed and recreated because they have already been provisione</span><a id="_idTextAnchor727"/><span class="koboSpan" id="kobo.278.1">d but the path </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">has changed.</span></span></p>
<h2 id="_idParaDest-308"><a id="_idTextAnchor728"/><span class="koboSpan" id="kobo.280.1">Planning for failure</span></h2>
<p><span class="koboSpan" id="kobo.281.1">Sometimes, the</span><a id="_idIndexMarker1230"/><span class="koboSpan" id="kobo.282.1"> unexpected happens and part of our infrastructure is impacted due to an outage of some kind on the target cloud platform. </span><span class="koboSpan" id="kobo.282.2">In these situations, we need to be able to react and bring change to our existing environments in order to minimize the damage</span><a id="_idTextAnchor729"/><span class="koboSpan" id="kobo.283.1"> or recover from </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">the outage.</span></span></p>
<h3><span class="koboSpan" id="kobo.285.1">Active-passive</span></h3>
<p><span class="koboSpan" id="kobo.286.1">First, let us look </span><a id="_idIndexMarker1231"/><span class="koboSpan" id="kobo.287.1">at the active-passive workload deployed within a single </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">Terraform workspace:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer204">
<span class="koboSpan" id="kobo.289.1"><img alt="Figure 17.7 – Active-passive workload deployed within a single Terraform workspace" src="image/B21183_17_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.290.1">Figure 17.7 – Active-passive workload deployed within a single Terraform workspace</span></p>
<p><span class="koboSpan" id="kobo.291.1">Here is what it looks like during </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">an outage:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<span class="koboSpan" id="kobo.293.1"><img alt="Figure 17.8 – Active-passive workload when disaster strikes!" src="image/B21183_17_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.294.1">Figure 17.8 – Active-passive workload when disaster strikes!</span></p>
<p><span class="koboSpan" id="kobo.295.1">The application and </span><a id="_idIndexMarker1232"/><span class="koboSpan" id="kobo.296.1">database in US West are unavailable. </span><span class="koboSpan" id="kobo.296.2">Luckily, we have the database in US East that we were replicating to. </span><span class="koboSpan" id="kobo.296.3">However, we need to create an online environment to start serving our customers using </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">this database:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<span class="koboSpan" id="kobo.298.1"><img alt="Figure 17.9 – Recovery step 1: Provision a new environment in a different region" src="image/B21183_17_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.299.1">Figure 17.9 – Recovery step 1: Provision a new environment in a different region</span></p>
<p><span class="koboSpan" id="kobo.300.1">We use </span><a id="_idIndexMarker1233"/><span class="koboSpan" id="kobo.301.1">Terraform to provision a new environment in a new Terraform workspace. </span><span class="koboSpan" id="kobo.301.2">We configure the new root module to use the US East as the primary region and the secondary region as another healthy region nearby, in this case, US Central. </span><span class="koboSpan" id="kobo.301.3">This environment is healthy, but it’s missing </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">our data.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<span class="koboSpan" id="kobo.303.1"><img alt="Figure 17.10 – Recovery step 2: Replicate data from the old environment to the new environment" src="image/B21183_17_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.304.1">Figure 17.10 – Recovery step 2: Replicate data from the old environment to the new environment</span></p>
<p><span class="koboSpan" id="kobo.305.1">We reconfigure our new workspace to reference the </span><em class="italic"><span class="koboSpan" id="kobo.306.1">old</span></em><span class="koboSpan" id="kobo.307.1"> database by importing it into the state, essentially replacing the new empty database with the old database. </span><span class="koboSpan" id="kobo.307.2">This will also likely cause a replacement of the replication configuration to start replication from the old database to</span><a id="_idIndexMarker1234"/><span class="koboSpan" id="kobo.308.1"> the new disaster recovery database in the US </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">Central region:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer208">
<span class="koboSpan" id="kobo.310.1"><img alt="Figure 17.11 – Recovery step 3: Cut over to new environment and decommission the old environment entirely" src="image/B21183_17_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.311.1">Figure 17.11 – Recovery step 3: Cut over to new environment and decommission the old environment entirely</span></p>
<p><span class="koboSpan" id="kobo.312.1">Now, the old disaster recovery database in US East is our main production database, and we have a new disaster recovery site in US Central in case we need to perform this same operation again. </span><span class="koboSpan" id="kobo.312.2">At this point, we are ready to resume service with our customers by allowing traffic back to our application. </span><span class="koboSpan" id="kobo.312.3">The database will be up to date because of the previous replication that was in place from US West to US East. </span><span class="koboSpan" id="kobo.312.4">There might be minor data loss for some customers during the small window when the requests were recorded in US West but may not have made it over to US Ea</span><a id="_idTextAnchor730"/><span class="koboSpan" id="kobo.313.1">st through </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">the replication.</span></span></p>
<h3><span class="koboSpan" id="kobo.315.1">Active-active</span></h3>
<p><span class="koboSpan" id="kobo.316.1">Now, here’s </span><a id="_idIndexMarker1235"/><span class="koboSpan" id="kobo.317.1">an active-active deployment in a single Terraform workspace without using </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">any modules:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer209">
<span class="koboSpan" id="kobo.319.1"><img alt="Figure 17.12 – Active-active deployment in a single Terraform workspace without using any modules" src="image/B21183_17_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.320.1">Figure 17.12 – Active-active deployment in a single Terraform workspace without using any modules</span></p>
<p><span class="koboSpan" id="kobo.321.1">To achieve</span><a id="_idIndexMarker1236"/><span class="koboSpan" id="kobo.322.1"> higher levels of system availability, we can opt for an active-active cross-region deployment. </span><span class="koboSpan" id="kobo.322.2">In this situation, we will have two instances of our application deployed across two regions and replication between the databases. </span><span class="koboSpan" id="kobo.322.3">This will ensure that in case of an outage in one region, our customers will continue to be served by routing traffic to the </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">healthy region.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">In the preceding approach, we created our multi-region deployment in a single Terraform workspace, which means both regions will be updated on a single Terraform application. </span><span class="koboSpan" id="kobo.324.2">This can be problematic because if one region is down, then half of our deployment will potentially be unresponsive, thus impacting our ability to enact change across the entire environment. </span><span class="koboSpan" id="kobo.324.3">This could impact our ability to failover, increase capacity, or adjust auto-scale settings in the </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">unaffected region.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">In order to start moving away from deploying all of our regions into a single Terraform workspace, it is a good idea to encapsulate an entire regional deployment into a single reusable module. </span><span class="koboSpan" id="kobo.326.2">In doing so, we make it much easier to segment our Terraform workspaces across regions and easily add additional regions as we </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">scale out:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer210">
<span class="koboSpan" id="kobo.328.1"><img alt="Figure 17.13 – Module design to encapsulate our application deployment within a single region" src="image/B21183_17_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.329.1">Figure 17.13 – Module design to encapsulate our application deployment within a single region</span></p>
<p><span class="koboSpan" id="kobo.330.1">The module </span><a id="_idIndexMarker1237"/><span class="koboSpan" id="kobo.331.1">will have everything that needs to be deployed into a single region. </span><span class="koboSpan" id="kobo.331.2">In addition, there may be optional components, such as the database replication configuration, which may not need to be enabled depending on whether this region is the primary or one of the secondary endpoints. </span><span class="koboSpan" id="kobo.331.3">Therefore, our module needs to take two input variables. </span><span class="koboSpan" id="kobo.331.4">First, there is the region that this instance of our application will be deployed into. </span><span class="koboSpan" id="kobo.331.5">Then, there is a feature flag to enable or disable database replication. </span><span class="koboSpan" id="kobo.331.6">This will be enabled when the region is our primary, but it will be disabled when it is set up as </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">a secondary.</span></span></p>
<p><span class="koboSpan" id="kobo.333.1">This is an example; your mileage may vary depending on the database or technologies that you are using, but it’s important to recognize that it is a common scenario in such modules to leverage feature flags to allow the customization of each instance of the module to fulfill its </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">specific role:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer211">
<span class="koboSpan" id="kobo.335.1"><img alt="Figure 17.14 – Active-active deployment in a single Terraform workspace using modules to provision each region" src="image/B21183_17_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.336.1">Figure 17.14 – Active-active deployment in a single Terraform workspace using modules to provision each region</span></p>
<p><span class="koboSpan" id="kobo.337.1">Now that </span><a id="_idIndexMarker1238"/><span class="koboSpan" id="kobo.338.1">we have our module, we can use it within our single Terraform workspace to provision both regions. </span><span class="koboSpan" id="kobo.338.2">This approach allows for additional regions to be provisioned quite easily within a single Terraform Apply, but it is susceptible to operational impact when an outage occurs. </span><span class="koboSpan" id="kobo.338.3">If you have designed your failover mechanism and secondary regions to be self-sufficient, then this approach may not be unreasonable, but just remember that you may lose the ability to perform Terraform Apply operations during </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">the outage.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">Even when performing a targeted apply, it will execute a plan across the entire workspace. </span><span class="koboSpan" id="kobo.340.2">So, even though, in theory, a targeted </span><strong class="source-inline"><span class="koboSpan" id="kobo.341.1">terraform apply</span></strong><span class="koboSpan" id="kobo.342.1"> will only change resources that you target because it has to perform a full plan, if the control plane you are </span><a id="_idIndexMarker1239"/><span class="koboSpan" id="kobo.343.1">targeting is impacted in certain regions or zones, then you will be unable to </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">do so.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer212">
<span class="koboSpan" id="kobo.345.1"><img alt="Figure 17.15 – Active-active with separate workspaces" src="image/B21183_17_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.346.1">Figure 17.15 – Active-active with separate workspaces</span></p>
<p><span class="koboSpan" id="kobo.347.1">Transitioning to completely separate workspaces for each region can help you maintain control over your environments through Terraform because you will be performing a </span><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">terraform apply</span></strong><span class="koboSpan" id="kobo.349.1"> operation within the context of each region. </span><span class="koboSpan" id="kobo.349.2">This adds additional operational overhead during steady state as it creates additional Terraform workspaces to manage and additional mechanics when performing day-to-day maintenance of your environment, so many might still opt for a single workspace to manage </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">multi-region environments.</span></span></p>
<p><span class="koboSpan" id="kobo.351.1">As we saw</span><a id="_idIndexMarker1240"/><span class="koboSpan" id="kobo.352.1"> in this section, even when change is planned, things can be challenging. </span><span class="koboSpan" id="kobo.352.2">There are changes to the modules we build and consume in our solutions, there are changes to the design and capabilities of the cloud services we employ, which translates into changes within the individual resources we use, and finally, there are changes to the Terraform executable itself. </span><span class="koboSpan" id="kobo.352.3">And these are only changes that we plan and control! </span><span class="koboSpan" id="kobo.352.4">Additional changes can come in the form of unexpected outages within the availability zones or regions where our solutions are deployed. </span><span class="koboSpan" id="kobo.352.5">In the next section, we’ll look at how we can respond to unexpected errors and perform some </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">routine </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.354.1">breakfixing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">.</span></span></p>
<h1 id="_idParaDest-309"><a id="_idTextAnchor731"/><span class="koboSpan" id="kobo.356.1">Breakfixing</span></h1>
<p><span class="koboSpan" id="kobo.357.1">Now that </span><a id="_idIndexMarker1241"/><span class="koboSpan" id="kobo.358.1">we’ve looked at the change we know is coming, and the change that we know is coming but we can’t control when, we need to take a look through a more tactical lens to help us respond to the inevitable </span><em class="italic"><span class="koboSpan" id="kobo.359.1">little</span></em><span class="koboSpan" id="kobo.360.1"> bumps along the way of our journey of managing existing environments with Terraform. </span><span class="koboSpan" id="kobo.360.2">These are going to be smaller issues that are not massively impactful but can definitely become a burden if we are ill prepared. </span><span class="koboSpan" id="kobo.360.3">But once you get used to them—and how to respo</span><a id="_idTextAnchor732"/><span class="koboSpan" id="kobo.361.1">nd—they become easy </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">to manage!</span></span></p>
<h2 id="_idParaDest-310"><a id="_idTextAnchor733"/><span class="koboSpan" id="kobo.363.1">Apply-time failures</span></h2>
<p><span class="koboSpan" id="kobo.364.1">While </span><strong class="source-inline"><span class="koboSpan" id="kobo.365.1">terraform plan</span></strong><span class="koboSpan" id="kobo.366.1"> provides</span><a id="_idIndexMarker1242"/><span class="koboSpan" id="kobo.367.1"> us with excellent intelligence on what changes (or lack thereof) need to be made to our environments, sometimes things can go wrong in unexpected ways during the </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">terraform apply</span></strong><span class="koboSpan" id="kobo.369.1"> operation even for the most well-intentioned plan. </span><span class="koboSpan" id="kobo.369.2">There are some things that you can do to try to stay ahead of these issues and lessen the frequency of </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">encountering them.</span></span></p>
<p><span class="koboSpan" id="kobo.371.1">As we know, Terraform executes under an identity on whatever cloud platform we are targeting, and as a result, whatever permissions or privileges that identity has—so does Terraform. </span><span class="koboSpan" id="kobo.371.2">Being aware of what permissions Terraform’s identity has and what your code is doing well helps you identify whether there are gaps that will result in authorization failures, as these are often implicit and hard to detect by Terraform. </span><span class="koboSpan" id="kobo.371.3">Some cloud platforms even require you to explicitly enable entire categories of cloud services before you use them. </span><span class="koboSpan" id="kobo.371.4">As we saw in </span><em class="italic"><span class="koboSpan" id="kobo.372.1">Chapters 13</span></em><span class="koboSpan" id="kobo.373.1"> through </span><em class="italic"><span class="koboSpan" id="kobo.374.1">15</span></em><span class="koboSpan" id="kobo.375.1">, Google Cloud is notorious for this and we were required to enable each of the relevant Google Cloud APIs within our Google Cloud project before we could even attempt to provision something. </span><span class="koboSpan" id="kobo.375.2">On Azure, most common services are enabled by default, but some more obscure resource providers need to be </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">explicitly enabled.</span></span></p>
<p><span class="koboSpan" id="kobo.377.1">Beyond </span><a id="_idIndexMarker1243"/><span class="koboSpan" id="kobo.378.1">simply enabling the services you want to use, all cloud platforms implement default quotas that will restrict how much you can provision within certain contexts. </span><span class="koboSpan" id="kobo.378.2">These contexts are usually region- or SKU-based. </span><span class="koboSpan" id="kobo.378.3">They provide joint protection for the cloud platform from a capacity planning standpoint but also for us as customers to prevent us from accidentally provisioning extremely expensive resources or too many of one kind of resource. </span><span class="koboSpan" id="kobo.378.4">Quotas are not the only limits imposed by the cloud platforms, there are often resource limits set for each service within a given deployment context, such as within an AWS account, an Azure subscription, or a Google </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">Cloud project.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">In addition to quotas and service limits, often, when working with cloud services that employ private networking, you may run into issues if network settings are misconfigured, such as incorrect virtual network and subnet configurations, or security group rules that prevent resources from being created or accessed. </span><span class="koboSpan" id="kobo.380.2">Sometimes, Terraform operates on a cloud service’s data plane, which might become unavailable when you configure it with private networking. </span><span class="koboSpan" id="kobo.380.3">Ensure that Terraform has the proper private networking in place to have a line of sight for necessary </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">data planes.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">Other issues can arise with implicit resource dependencies that Terraform can’t determine through the configuration and plan. </span><span class="koboSpan" id="kobo.382.2">This can occur when a resource relies on another resource, but that relationship or dependency is not known to Terraform through direct references between the resources within the configuration. </span><span class="koboSpan" id="kobo.382.3">There could also be conflicts with existing resources, such as trying to create resources that already exist with the same name or other settings that can’t exist in more than one resource within the given scope—be it at a networking level or at the cloud platform’s control </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">plane level.</span></span></p>
<p><span class="koboSpan" id="kobo.384.1">Operations might take longer than expected, and other transient platform errors lead to timeouts. </span><span class="koboSpan" id="kobo.384.2">Timeouts can result in the resources eventually being successfully provisioned, but </span><a id="_idIndexMarker1244"/><span class="koboSpan" id="kobo.385.1">because it happened after the operation timed out, Terraform won’t know about it. </span><span class="koboSpan" id="kobo.385.2">This can happen when large resources are being provisioned </span><a id="_idTextAnchor734"/><span class="koboSpan" id="kobo.386.1">or when there are </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">network delays.</span></span></p>
<h2 id="_idParaDest-311"><a id="_idTextAnchor735"/><span class="koboSpan" id="kobo.388.1">Removing from state</span></h2>
<p><span class="koboSpan" id="kobo.389.1">In the</span><a id="_idIndexMarker1245"/><span class="koboSpan" id="kobo.390.1"> previous section, we discussed apply-time failures and their impact on Terraform infrastructure management. </span><span class="koboSpan" id="kobo.390.2">Another common situation that can arise when managing environments with Terraform is the need to remove a resource from the state. </span><span class="koboSpan" id="kobo.390.3">As we’ve discussed in previous chapters, this can be achieved both imperatively using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">terraform state rm</span></strong><span class="koboSpan" id="kobo.392.1"> command or declaratively by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.393.1">removed</span></strong><span class="koboSpan" id="kobo.394.1"> block in your HashiCorp Configuration </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">Language code.</span></span></p>
<p><span class="koboSpan" id="kobo.396.1">One such scenario is when you need to decommission resources. </span><span class="koboSpan" id="kobo.396.2">If you manually delete a resource outside of Terraform, it needs to be removed from the state file to prevent errors during the next </span><strong class="source-inline"><span class="koboSpan" id="kobo.397.1">terraform apply</span></strong><span class="koboSpan" id="kobo.398.1">. </span><span class="koboSpan" id="kobo.398.2">Similarly, if a resource is accidentally imported into the wrong location in your Terraform configuration, it can be removed before re-importing </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">it correctly.</span></span></p>
<p><span class="koboSpan" id="kobo.400.1">When working in a team, if someone else has already removed a resource but your local state file is not yet updated, resolving the discrepancy may involve removing the resource from your state file. </span><span class="koboSpan" id="kobo.400.2">Cleaning up orphaned resources is another important use case. </span><span class="koboSpan" id="kobo.400.3">If a resource becomes orphaned (no longer managed by Terraform) due to manual changes or configuration errors, it can be removed from the </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">state file.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">Another place where you may need to remove resources is during the refactoring process. </span><span class="koboSpan" id="kobo.402.2">Of course, as we’ve discussed, it’s more common to move resources in this scenario, but there are cases where removal might be necessary as well, such as splitting a large configuration into smaller modules; that is, resources might need to be removed from the state file before re-importing them into their new locations. </span><span class="koboSpan" id="kobo.402.3">Additionally, if a resource needs to be replaced with a new one (for example, due to a change in the resource’s configuration that requires recreation), the old resource might be removed from the state file before creating the new one. </span><span class="koboSpan" id="kobo.402.4">During testing or debugging, temporarily removing resources from the state file can help isolate issues or test specific scenarios. </span><span class="koboSpan" id="kobo.402.5">If you’re consolidating multiple similar resources into a single resource (e.g., merging several security groups into one), the old resources mig</span><a id="_idTextAnchor736"/><span class="koboSpan" id="kobo.403.1">ht be removed from the </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">state file.</span></span></p>
<h2 id="_idParaDest-312"><a id="_idTextAnchor737"/><span class="koboSpan" id="kobo.405.1">Importing into state</span></h2>
<p><span class="koboSpan" id="kobo.406.1">In </span><a href="B21183_16.xhtml#_idTextAnchor665"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.407.1">Chapter 16</span></em></span></a><span class="koboSpan" id="kobo.408.1">, we delved </span><a id="_idIndexMarker1246"/><span class="koboSpan" id="kobo.409.1">into the intricacies of importing existing environments that were provisioned outside of Terraform. </span><span class="koboSpan" id="kobo.409.2">As we continue our exploration of managing existing environments in this chapter, we will encounter situations where importing resources becomes essential for breakfixing within environments already under </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">Terraform’s management.</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">One common situation where importing resources becomes necessary, even in environments initially provisioned with Terraform, is when transient errors occur during the </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">terraform apply</span></strong><span class="koboSpan" id="kobo.413.1"> process. </span><span class="koboSpan" id="kobo.413.2">These errors can lead to a peculiar state where resources are provisioned but reported as unhealthy to Terraform, causing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">terraform apply</span></strong><span class="koboSpan" id="kobo.415.1"> process to fail. </span><span class="koboSpan" id="kobo.415.2">However, these resources may eventually finish provisioning or be recovered by the cloud platform later. </span><span class="koboSpan" id="kobo.415.3">In such instances, we are faced with the decision to delete these resources and rerun </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">terraform apply</span></strong><span class="koboSpan" id="kobo.417.1"> or import them into </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">the state.</span></span></p>
<p><span class="koboSpan" id="kobo.419.1">Importing resources in this context serves as a form of breakfixing, akin to patching up a leak in a well-oiled machine. </span><span class="koboSpan" id="kobo.419.2">It allows us to reconcile the discrepancies between the actual state of our cloud environment and Terraform’s understanding of it, much like how we would address apply-time failures by ensuring proper permissions, quotas, and </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">network configurations.</span></span></p>
<p><span class="koboSpan" id="kobo.421.1">Another scenario where importing might be necessary is when dealing with resources that have been manually or by an automated process created or modified outside of Terraform. </span><span class="koboSpan" id="kobo.421.2">This can lead to a drift between the Terraform state and the actual infrastructure, similar to how unexpected changes in cloud service limits or network settings can cause issues. </span><span class="koboSpan" id="kobo.421.3">This can arise from human operators working outside the bounds of our infrastructure-as-code process, or it could arise from automated systems enforcing enterprise governance standards. </span><span class="koboSpan" id="kobo.421.4">By importing the newly created or modified resources back into the Terraform state, we can realign our configuration with the current state of the infrastructure, ensuring that subsequent Ter</span><a id="_idTextAnchor738"/><span class="koboSpan" id="kobo.422.1">raform operations </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">proceed smoothly.</span></span></p>
<h1 id="_idParaDest-313"><a id="_idTextAnchor739"/><span class="koboSpan" id="kobo.424.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.425.1">In this chapter, we explored how to manage existing environments using Terraform. </span><span class="koboSpan" id="kobo.425.2">We began with a comprehensive examination of the various operating models employed by teams with differing roles and responsibilities within organizations of varying sizes. </span><span class="koboSpan" id="kobo.425.3">We investigated how these teams integrate Terraform into their day-to-day operations—from managing a simple standalone application to navigating the complexities of shared infrastructure services, such as a centralized network, and addressing the nuances of building shared services that intertwine across the enterprise. </span><span class="koboSpan" id="kobo.425.4">We discussed the challenges of interdependencies experienced with shared infrastructure, coupled with their own application development </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">release processes.</span></span></p>
<p><span class="koboSpan" id="kobo.427.1">A significant portion of this chapter was dedicated to simply applying changes to our existing environments. </span><span class="koboSpan" id="kobo.427.2">This included the seemingly mundane process of upgrading our Terraform tools—ranging from the Terraform executable itself to the Terraform providers we use and the modules we consume within our solutions. </span><span class="koboSpan" id="kobo.427.3">We also discussed the refactoring that may be necessary with our own code and addressed how to handle unplanned changes—such as when disaster strikes. </span><span class="koboSpan" id="kobo.427.4">This discussion was akin to preparing for a storm; just as one would secure their windows and doors, we explored how to use Terraform to prepare our environments for when we needed to take action during </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">an outage.</span></span></p>
<p><span class="koboSpan" id="kobo.429.1">We concluded the chapter by discussing the more common break-fixing scenarios that you will encounter in your day-to-day operations of managing existing environments </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">with Terraform.</span></span></p>
<p><span class="koboSpan" id="kobo.431.1">In the next chapter, we’ll be looking to close out the book by discussing some important things to consider as you take your next steps in </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">mastering Terraform.</span></span></p>
</div>
</body></html>