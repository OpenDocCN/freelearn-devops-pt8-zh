- en: Introducing Cloud Run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have discussed many things relating to building serverless
    technologies in the cloud. In this chapter, we'll look at the latest offering
    from Google, which provides a stateless environment for your applications. Unlike
    Cloud Functions, Cloud Run explicitly utilizes container technology to provide
    a constrained environment for HTTP endpoints. Cloud Functions, on the other hand,
    provides an opinionated view of serverless workloads, for example, runtime language
    limitations. Cloud Run removes many of those restrictions in order to meet developers
    where they are. If you follow these things carefully, you will know that containers
    and Kubernetes are both the top skills any cloud professional can have.
  prefs: []
  type: TYPE_NORMAL
- en: To commence our discussion, we will outline the Cloud Run component architecture.
    In doing so, we will discuss several topics in order to try and set the scene
    for Cloud Run. The primary objective of this chapter is to present the supporting
    technologies. You should take the time to understand the use cases and be aware
    of how Cloud Run leverages each.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the Cloud Run component architecture, we will lay some of
    the groundwork in terms of outlining some key technologies. To commence this discussion,
    we'll start with microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Cloud Run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Run versus Cloud Run for Anthos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete the exercises in this chapter, you will need a Google Cloud project
    or a Qwiklabs account.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code files for this chapter in this book's GitHub repository,
    under the `ch07` subdirectory, at [https://github.com/PacktPublishing/Hands-on-Serverless-Computing-with-Google-Cloud/tree/master/ch07](https://github.com/PacktPublishing/Hands-on-Serverless-Computing-with-Google-Cloud/tree/master/ch07).
  prefs: []
  type: TYPE_NORMAL
- en: While you are going through the code snippets in this book, you will notice
    that, in a few instances, a few lines from the code/output have been removed and
    replaced with dots (`...`). The use of ellipses is only to show relevant code/output.
    The complete code is available on GitHub at the link mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: Working with microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There has been a lot of discussion about the critical benefits of monoliths
    versus microservices. The possibility associated with creating smaller code packages
    has apparent advantages in that they are typically easier to debug, more straightforward
    to integrate, and have a consistent message interface. Those benefits, by themselves,
    would not be sufficient to warrant a wholesale migration to microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we''re contrasting a typical monolithic software
    construct to a microservice architecture. The first thing to notice is that the
    microservice architecture has a lot more component services available. A key point
    to note is the deconstruction of the single application into the delivery of services
    focus on business operation. Over the next couple of paragraphs, we will discuss
    the reasoning behind this approach and how it is beneficial (and highly relevant)
    when moving to environments such as Cloud Run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/504e87fd-c396-4877-8b59-5ed1c38e53e1.png)'
  prefs: []
  type: TYPE_IMG
- en: First and foremost, microservices should be autonomous; that is, they provide
    an isolated and independent component. Providing a standardized interface allows
    the microservice to participate in the broader ecosystem seamlessly. Ensuring
    the components achieve inter-component communication provides the basis for building
    scalable and flexible solutions.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of microservices, they typically serve multiple container
    components that have been deployed within a loosely coupled architecture. Each
    microservice represents the decomposition of an application into a series of functions.
    Consider how we focused on building lightweight tasks with a single purpose for
    Cloud Functions (reference chapters three, four and five). In this conversation,
    we elevated containers as the artifact of choice. The communication mechanism
    that's used delivers consistent communication across components in some cases,
    acting as an **Application Programming Interface** (**API**). The use of containers
    provides a layer of abstraction to ensure compatibility with any runtime language.
  prefs: []
  type: TYPE_NORMAL
- en: Contrast this to a single monolithic application in which tightly coupled constituent
    components exist. Tightly coupled indicates it would be challenging to pull the
    various modules apart or create new integrations. Due to the single application
    structure, the language runtime is typically consistent across the monolith. The
    inability to use different runtime languages can lead to issues as the best option
    for the task cannot necessarily be used. Similarly, scaling can also be an issue
    when incurring specific performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: The application architecture patterns shown in the preceding diagram are essential
    considerations since we will be using containers for Cloud Run. In doing so, we'll
    commit ourselves to continue building lightweight and loosely coupled functions.
    This will help you to solidify your understanding of building and specific design
    approaches that are taken while you work through the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, that is not to say that microservices are for every occasion. There
    are occasions where one size does not fit all. Pragmatically selecting the architecture
    and the approach lends itself to better-designed applications and increases the
    skills of the designer. Microservices are far from simple to write and do not
    give themselves to every occasion. Modeling the services, the correct scope, and
    the correct content for a microservice is a significant challenge if you wish
    to deliver the benefits we outlined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To assist with this process, it is often helpful to consider how microservice
    design patterns can cater to requirements and help you to build scalable solutions.
    As you might expect, this subject is both broad and varied and has been covered
    in many presentations and books to try and define a consensus on the base level
    of knowledge required for the subject. Since we will be predominantly dealing
    with HTTP-related communication, we will provide a quick overview of the event
    processing patterns we will need to consider. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The asynchronous event processing pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synchronous event processing pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These models are the most relevant types of communication that you will experience.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous event processing pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While not explicitly called out on the platform, in truth, you will already
    be familiar with most of the patterns. Asynchronous communication will typically
    utilize a publisher/subscriber pattern. In this pattern, listeners are activated
    through an event that they subscribe to. Messages in this partnership are suitable
    for one-to-many relationships. On Google Cloud, Cloud Pub/Sub provides this service,
    which is where a defined topic and the subscribers to this topic present information
    for each matching event on the publisher. A service, such as Cloud Pub/Sub, will
    need a model like this to provide an asynchronous communication pattern that's
    suitable for streaming information.
  prefs: []
  type: TYPE_NORMAL
- en: In a situation where a batch-oriented or one-to-one communication flow is desirable,
    a job queue pattern is more appropriate. In this (winner takes all) model, a queue
    mechanism is used to hold information while the queue consumer determines when
    the information will be retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous event processing pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to note that, with any design method, the design does not create
    a perfect situation for every situation. Generalizing code in this way may introduce
    a need to repeat content/code, and this can sometimes be unavoidable. Focusing
    on keeping microservices isolated and independent should be the primary focus
    and you need to accept that there will always be exceptional cases. In the case
    of synchronous event processing, there are two patterns that you need to be familiar
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: The request/response pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sidewinder pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For synchronous messaging, an immediate response demand by the calling service
    delivers the acknowledgment. Most commonly associated with the HTTP model, this
    pattern is the one most people are familiar with. In this request/response situation,
    the message is to be consumed as part of point-to-point communication.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative state to manage is one where synchronous communication is to
    be observed rather than consumed. This is known as the **sidewinder** pattern
    and can be useful if there are multiple endpoints ready to consume the message.
    However, only a specific endpoint address can be responsible for the generation
    of a response.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into more detail on Cloud Run, we will take a quick tour of containers
    and explain why they are an essential piece of technology. For this discussion,
    we will focus on Docker containers; however, it is good to know that other containers
    exist and offer similar benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Working with containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While applications can run anywhere, working in different environments has traditionally
    led to issues in terms of general consistency. Deploying code from one environment
    to another falls foul of a change that renders it incompatible with the underlying
    infrastructure. The industry's focus on moving away from a monolithic application
    to small, integrated components (that is, microservices) has, in general, led
    to the consideration of generating loosely coupled artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional development in an environment based on virtualized hardware provides
    a well-understood platform on which many successful deployments exist. However,
    the inefficiency of deploying microservice components has meant this approach
    has become less attractive due to the unnecessary replication of underlying resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7eb6f8c3-2294-4074-8c11-7b9953eb9a15.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, you can see that, with the virtualized hardware, each
    virtual machine invocation requires replication of resources, both for the operating
    system and libraries. While virtual machines do continue to provide advantages
    for large-scale machines, for a microservice-based architecture, a more lightweight
    approach is desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Containers provide access to the underlying hardware through a shared resource
    model. An approach such as this, which allows the host hardware to share its existing
    resources among the containers, is more desirable. Through the use of containers,
    the host can allocate its resources to the container that will be executed in
    this environment. If you are working in an environment that utilizes microservices,
    it is likely that this environment is looking to deliver the efficiencies associated
    with containers.
  prefs: []
  type: TYPE_NORMAL
- en: Consistently building these components is only half the story; how do you ensure
    the artifact remains consistent across each deployment? For this, we use containers
    to define a software package in which we can control the environment (for example,
    memory, disk, network, filesystem, and so on). The underlying cloud environment
    utilizes the existing filesystem to create a partition that enacts isolation specifically
    for your container. So, rather than installing applications directly onto a host,
    we can install the container and run this on our host. Since the application exists
    in the host, any incompatibility is likely to be platform-related. A consequence
    of this is that your application can now provide consistency between deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will go through a lightning-fast overview of Docker
    and how to use it with Google Cloud. In this discussion, we will cover the basics
    so that those of you who are unfamiliar with containers can get up to speed.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common ways to enact containers is by using the Docker container
    runtime. Docker provides all of the advantages of containers. It presents a simple
    interface that you can use to manage your application while it's running inside
    a container. For the sake of brevity, our discussion will focus on using Docker
    in a Linux environment. However, keep in mind that other options exist and can
    be just as effective.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the container has three main elements to consider. Primarily, containers
    utilize a base image that an application is run on. The base image represents
    the operating system that the application will run on, for example, Debian, Ubuntu,
    or Alpine. Also, dependency packages for the base image will need to be installed
    to ensure the environment can achieve compatibility with the application. Packages
    are typically compatible libraries that are applied to the container, such as
    SSL, cURL, and the GCloud SDK. Finally, there is command execution, which indicates
    what runs at the point of execution. The addition of an entry point defines what
    happens when the container runs.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, containers are an excellent way for us to isolate
    application functionality. However, they also offer an elegant means to define
    a signature for your application. You may be wondering what I mean by this. Imagine
    we have an application running inside a container. The image is built using a
    file that is used to define the environment that the application should run in.
    In this context, an image represents the executable package and holds the necessary
    dependencies for the application. By going through the necessary process, we have
    isolated our application requirements into a transportable environment (container
    image). Doing this is extremely powerful. But that's enough theoretical discussion—let's
    build something.
  prefs: []
  type: TYPE_NORMAL
- en: The base element we will start with is the manifest. A manifest represents the
    image specification, including the application to be created. This environment
    incorporates a base image (for example, Scratch, Alpine, Ubuntu, Debian, and so
    on) denoted by a FROM statement. Choosing a base image is a topic in itself, but
    note that the more lightweight an image is, the easier it will be to deploy the
    workload.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the base image, we will also incorporate the packages and libraries
    that are necessary for the task at hand. If you are working with a modern language,
    you will potentially have this information already as they will have been installed
    locally (thank you Node.js). If you are making an image from someone else's application,
    this is the part where relationships become frayed. In our example, we won't be
    installing any additional packages; instead, we will be using the existing capabilities
    of the base Alpine image.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in our configuration, we will set out what our image should do when
    the application container starts. The `ENTRYPOINT` command indicates invocation
    when the container starts up. The `CMD` label indicates the parameter to the entry
    point. Note that this configuration is being used to allow the image to be extended
    so that it can print other messages. It is highly recommended to read up on the
    usage of both `ENTRYPOINT` and `CMD` as they can save a significant amount of
    time when it comes to processing commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of this process, you will have a manifest file that incorporates
    each of these elements. For the sake of our example, we can create a simple Dockerfile.
    Follow these steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Dockerfile manifest file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If we were to build the preceding manifest, it would take the instructions we
    laid out and create an image based on each of the manifest lines. Consider the
    previous statement for a moment and what that means for us; we have built a host
    machine for a single application based on a file.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding manifest content can tell us a lot about the requirements
    of the application and little about the execution. From the manifest, we can reference
    the base image (that is, OS), the dependencies (that is, libraries/packages),
    and the command to be run (that is, the application to be run). The next step
    would be to run the build process to turn this manifest into an image that is
    representative of the information contained in the file.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Docker image is managed through the command line. Turning a manifest
    into something useful means we need to generate an image. The build process goes
    through each line of the manifest and adds it to the final image. As the build
    process is running, each line will create an archive layer that represents the
    command executed. Using the example manifest presented earlier, we can build an
    image for our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the Dockerfile manifest file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, we're telling Docker that we want to create an image
    by initiating the process with the verb `build`. Docker will, by default, assume
    there is a local file named `Dockerfile` present in the current directory. If
    you wish to use an alternative manifest naming convention, you can, for example,
    append `-f [FILENAME]` to the command line, in which case you would need the following
    command which is technically equivalent to *step 2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a manifest file named `myDockerfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Following the `build` parameter, we tell Docker to label the images by using
    the `-t [label]` command. In this example, we have provided both the name and
    version to be applied to the image that will be generated.
  prefs: []
  type: TYPE_NORMAL
- en: It is considered good practice to incorporate a revision of all of the images
    that are created.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we indicate where the new image will find the manifest by adding a
    period (full stop) to the end of the command, indicating that the local directory
    contains the source information. You can replace this with a more specific destination
    if you wish to.
  prefs: []
  type: TYPE_NORMAL
- en: Running the preceding command will initiate the Docker tool that will be used
    to build the local manifest. On successful completion of this process, we can
    confirm that a new image is present on our machine by asking Docker to list the
    available images.
  prefs: []
  type: TYPE_NORMAL
- en: 'List the images that are held locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can view how the output looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eefb2f4e-2f9c-4446-b662-cbbb1d8cb44a.png)'
  prefs: []
  type: TYPE_IMG
- en: From the resulting list, we will see that our image has been successfully created
    and is now available to access. In addition the latest alpine image has been downloaded
    and used as the base image for your new image. Congratulations—building an image
    from a manifest is a great thing and increases your general understanding of a
    range of subjects. For reference, containers are images that run on Linux subsystems
    and share the kernel of the host machine. In this respect, we can see that containers
    provide a lightweight mechanism for running discrete processes on a host.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to build an image, we can move on to running a container
    on the host. A point of confusion when starting with containers is switching between
    the terms image and container. For reference, an image references a non-running
    container. Once the image is running, it is a container. These terms are used
    interchangeably all of the time, but now you know the difference. Please don't
    lose any sleep over this.
  prefs: []
  type: TYPE_NORMAL
- en: To run a container on our host, we need to tell Docker which image we wish to
    initiate and state the parameters that are necessary for the application to run.
    At a minimum, we need the Docker command that will be used to launch a container
    on the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can view how the output looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c7cd7be-3693-43f7-9a97-f1956bfb2991.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding command, we're using the verb, `run`, to indicate to Docker
    that we want to initiate an image. Note that, at this point, the image's location
    (that is, local or remote) is not important. If the image is not found locally,
    a remote repository search will proceed automatically. Finally, if the image does
    not exist locally or remotely, an error will be returned.
  prefs: []
  type: TYPE_NORMAL
- en: Note the preceding application is actually quite useful. If you specify an additional
    argument, it will print that instead of the default message associated with the
    command. Try `docker run hello-docker:1.0 "I love working on Google Cloud"`
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier in this section, we built our image, meaning it should be accessible
    to Docker. This process can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78235e4b-9fc2-44cd-8033-0268c6723f23.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see a container, which has a running state
    and a container ID assigned to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have performed the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Created a manifest file (for example, a Dockerfile)
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: Built the image from the manifest
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the image to create a container
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Hopefully, all of these actions are clear to you, and you can see how straightforward
    it is to incorporate Docker within your development workflow. As the container
    is running on the host machine, it is sharing its resources with the host. To
    confirm that the Docker container has been started successfully, you will need
    to use the Docker process command to list all the running containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'List all the Docker processes available on the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `ps` command option relates to the process that's initiated by the Docker
    application. In this example, we want to see all of the active containers on the
    host. Being able to track which processes are currently running on the host is
    very important. In the preceding command, we're listing all of the processes in
    the Docker namespace. Doing this allows us to see what is active on a host and
    gives us valuable insight into the dynamic process that's occurring on the host.
    Running operations on a localhost doesn't come without a price. The machine resource
    state that holds the active containers will need to be stopped to restore the
    machine's overall resources.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the topic of microservices, in this example, I am outputting information
    to the screen. It may be more desirable to not output the status on the screen
    for a majority of situations. Unless you have a genuine requirement to output
    the status (that is, it's a frontend HTTP application), try and avoid providing
    feedback via the screen. Sending information directly to the logging infrastructure
    is a more scalable approach. In the upcoming sections, we will work on more sophisticated
    examples that require specific network ports to be exposed and information to
    be written directly to the logs. Adopting this best practice at the earlier stages
    of using containers is an excellent habit to get into, and minimizes any potential
    rework associated with removing screen content.
  prefs: []
  type: TYPE_NORMAL
- en: Before releasing the resource associated with the container, take a minute to
    observe the logs that have been generated by the running application. To do this,
    we need to use a specific command and insert the actual container ID for the active
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Show the logs associated with a specific container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Accessing containers logs in this way is a great strategy if we wish to investigate
    what is happening during the active life cycle of a container. In an instance
    where runtime information is not available as a direct output, Docker can be used
    to ascertain what is happening in the application so that any errors that occur
    can be addressed. Now that we have examined the properties of an active container,
    we should look at how to release resources.
  prefs: []
  type: TYPE_NORMAL
- en: To stop an active container, we need to use a specific command that will halt
    the active process from running. Entering the following at the command line will
    stop the active container from running on the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop the container running on the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, the container identifier is the one that was presented
    by the `ps` command that we initiated earlier. Whenever Docker requests an identifier,
    it is more than likely referring to this helpful reference title to distinguish
    it from the active component. Once the container stops, the associated resources
    for our simple container example will be released back to the host machine. Now
    that you know how to build and invoke an image, we can look at how to increase
    our productivity by introducing two developer tools: **Google Cloud Build** and
    **Container Registry**.'
  prefs: []
  type: TYPE_NORMAL
- en: Populating Container Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we provided a high-level introduction to Docker and
    containers. In this section, we will expand on this discussion by looking at Google
    developer tools and how these increase developer productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Before we continue, let's outline our assumptions for this section since there
    are going to be some dependencies. First and foremost, your environment should
    already have been set up to use the GCloud SDK and should be pointing to a valid
    project on Google Cloud. Also, the Docker application should be installed and
    capable of building images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a typical development environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a19b63a2-8111-46b7-ad2c-7992e9414afd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, rather than using a local repository, we have defined a remote
    repository based on **Google Container** **Registry** (**GCR**). The remote registry
    replaces the use of Docker Hub for Google-based projects and gives us access to
    a multi-regional repository. In this example, we will use a simple manifest to
    build a small image and populate the Google Cloud Repository. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Dockerfile manifest file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: From here, we can initiate a local build to test the manifest file using the
    default Dockerfile, which is available in the `build` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the Docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first difference in this process is populating the repository based on the
    build output. The manual path to achieve this is by tagging the image with the
    identifier for the repository endpoint. We need to apply a `tag` to indicate that
    the created artifact resides in GCR. We do this by appending the `gcr.io/[PROJECT_ID]`
    label. This will tell the GCloud SDK to use the US repository and a particular
    Google Cloud project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tag the Docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that the image has been labeled correctly, we can push the image to the
    remote repository on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Push the image to GCR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the locally held image will be pushed to GCR and hence be available
    remotely. Remote repository image access requires a `pull` command to be used
    if we wish to retrieve it on the localhost. It is essential to note that authenticated
    access uses IAM to control access (even if you make the repository public).
  prefs: []
  type: TYPE_NORMAL
- en: 'Pull the image from GCR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the preceding example, it is clear that this process is incredibly
    similar to building and using the Docker Hub repository. Images that are stored
    locally will require storage, which means where disk space is tight, remotely
    hosting your images is a worthwhile endeavor. As we learned earlier, Docker images
    are very flexible and the convenience of remote repositories provides for more
    flexible deployment strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at remote repositories and how to populate Container Registry has taught
    us that it involves some additional steps. Fortunately, Google has created a versatile
    tool named Cloud Build that helps to remove some of that effort.
  prefs: []
  type: TYPE_NORMAL
- en: Using Cloud Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enhance the build process, tools such as Cloud Build can help us to build
    more sophisticated scripts for the creation of images. Cloud Build is a developer
    tool that doesn't get much fanfare, but it is beneficial for things such as offloading
    and automating mundane tasks such as building images. In terms of image creation,
    the images that are built will reside in Google Container Registry and be maintained
    within a project-bound repository. Information that's stored in these repositories
    can be declared public or private on an individual basis, which establishes a
    simple but effective way to manage the images that are generated by the build
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud Build is incredibly straightforward to integrate into your development
    workflow. The package is described as a language-independent manifest that is
    used to script the desired automation flow. Some key features of Cloud Build to
    consider are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Native Docker support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports multiple repositories (for example, Cloud Source Repositories, Bitbucket,
    and GitHub)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom pipeline workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customized package support (for example, Docker, Maven, and Gradle)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local or cloud-based builds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Package vulnerability scanning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we are going to use some of these tools to build our images and add them
    to a remote repository hosted on Google Cloud. To start, we will update our example
    manifest once more and amend the message''s output via the command parameter.
    Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Docker manifest file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When using Cloud Build, we no longer directly call Docker from the command line.
    Instead, to build the artifact, it uses the GCloud SDK command to create an image
    on the remote repository. The default Dockerfile needs to be present locally and
    should be used as the basis for image creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initiate the build process based on the Docker manifest file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: An additional option that starts to show the value of Cloud Build is that we
    can also create a file that will be responsible for automating the build process.
    Creating a `cloudbuild.yaml` file allows the developer to specify a series of
    steps to perform as part of the build process. The arguments for this process
    include a rich set of functionality that goes beyond Docker. It is highly recommended
    to investigate this at your leisure. In the following example, we're essentially
    replicating the `docker` command to build our image and tell it to hold the output
    in the Cloud Repository. The `images` line denotes the label associated with the
    build artifact. On completion, a new version (that is, `hello-docker:1.3`) is
    created and available on Container Registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Cloud Build manifest file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To build the preceding file using Cloud Build, we need to run the following
    from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the image with Cloud Build and submit the image to Google Container Registry:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we outlined a simple way to incorporate a Docker manifest
    into Cloud Build. There are a variety of ways in which you can enhance this model
    so that you can include more sophisticated options that can be combined. For now,
    that's all we need to cover in terms of establishing a Docker workflow. Having
    enhanced our general understanding of Docker and some of the development tools
    associated with Google Cloud, we will turn our attention to Cloud Run.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Cloud Run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud Run (and Cloud Run for Anthos) is a container-based serverless technology.
    A distinct advantage here is that containerization is a widely adopted approach.
    Being able to package your application as a container and then subsequently migrate
    it to a fully managed serverless environment without any additional work is a
    desirable proposition.
  prefs: []
  type: TYPE_NORMAL
- en: When working with any technology, it is always good to have an understanding
    of the constituent parts. In this respect, Google Cloud has chosen to base its
    technology on several open source technologies that the community can contribute
    to. Underestimating the ability to move between cloud providers occurs frequently.
    When developing an application, an important consideration is how that product/service
    technology can be adapted and the support it will receive.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the fundamental proposition of running containers in the cloud, Cloud
    Run provides a fully managed, serverless execution environment. Similar to both
    App Engine and Cloud Functions, Google have predominantly done all of the heavy
    lifting in terms of infrastructure management. I say this mostly due to the inclusion
    of Cloud Run for Anthos, which requires the addition of a Kubernetes (Google Kubernetes
    Engine) cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Building full-stack serverless applications is a reality right now, and the
    tools and patterns that allow you to take advantage are within your grasp. Integrating
    with other services and platforms should not require significant code rewrites.
    Similarly, moving between different products and cloud providers should not present
    an issue when they're based on standard components and compatible architectural
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Before we continue our discussion of Cloud Run, we'll turn our attention to
    some key features that are used to enable this flexible serverless environment.
  prefs: []
  type: TYPE_NORMAL
- en: gVisor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The gVisor open source project provides a sandboxed runtime environment for
    containers. In this environment, the containers that are created are run against
    a userspace kernel in which compatibility exists through the use of the **Open
    Container Initiative** (**OCI**) runtime specification. Intercepting application
    system calls provides a layer of isolation so that interaction can occur with
    the controlled host. The central tenet of this approach is to limit the system
    call surface area to minimize the attack radius. For a container environment,
    being able to exploit kernel space provides access to the host machines. To reduce
    the possibility of that eventuality, gVisor seeks to restrict this access and
    limit untrusted userspace code. As shown in the following diagram, a sandboxing
    technique is used with gVisor to provide a virtualized environment for application
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2894b7f-ad0c-4ed3-bcff-96aea1b2dde8.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the system calls from the application get passed to gVisor, and
    it is here that it's determined whether they're permissible or not. Restricting
    the system calls via gVisor means permission is only given to verified access
    at the **Host Kernel** level. An approach such as this is described as defense
    in depth, meaning multiple layers are used to provide increased isolation from
    the host.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing an environment such as this allows us to run untrusted containers.
    In this instance, gVisor limits the possible interactions with the host kernel
    through the use of an independent operating system kernel. The beauty of OCI makes
    this type of integration possible and establishes an elegant way to interchange
    solutions such as Docker and gVisor seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Knative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin our discussion, we'll delve into what Knative provides and then follow
    this up with an overview of the components within the project. Knative delivers
    APIs for close integration on the Kubernetes platform for both developers and
    operators. I would highly encourage further reading in this area to achieve greater
    insight than what would be possible given the brief synopsis provided in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knative offers a multifaceted solution for the Kubernetes platform by providing
    a series of components. These components are responsible for many standard aspects
    of working with a platform, such as deployment, routing, and scaling. As you might
    expect with something associated with Kubernetes, the components offer compatibility
    across both frameworks and application tiers, making it relatively simple to incorporate
    into any design:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2039ff58-dfc3-4e71-a763-2ff0b3b7fe07.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see that different persona are involved in
    a Kubernetes workflow. Operators are typically responsible for infrastructure
    maintenance. Developers focus on creating application workloads that reside on
    the platform and interact with the API. It is at this level that Knative allows
    developers to deliver greater efficiency for their applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussions of Knative typically describe it as middleware since it sits between
    the Kubernetes platform and your application. Earlier in this chapter, we looked
    at microservice design patterns; Knative is essentially a fully realized expression
    of this approach for serverless workloads. In this relationship, two primary components
    are essential to the discussion, that is, Knative Serving and Knative Events:'
  prefs: []
  type: TYPE_NORMAL
- en: Serving relates to access to the **Custom Resource Definitions** (**CRDs**)
    that control workload interaction with the underlying Kubernetes cluster. The
    supporting compute resource for this service will be capable of scaling to zero.
    Note that, on Kubernetes, this relates to the resource, not the cluster. Interaction
    with the platform API provides us with an opportunity to enact more granular control.
    In this respect, being able to control elements such as service, route, configuration,
    and revision is possible using Knative serving. Each element is used to provide
    specific management of the desired state and communication via the rules put in
    place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knative Serving is capable of abstracting services such as ingress between different
    environments and cloud providers. By doing this, the interaction between application
    developers and Kubernetes becomes significantly more straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Events follow the notion of producers and consumers in which responsibility
    is required for shared activities. Enabling the late binding of generated artifacts
    allows us to incorporate a loosely coupled service that is capable of interacting
    with other services. The list of event artifacts uses an event registry, thereby
    allowing a consumer to be triggered without the need to reference other objects.
    In this respect, the event consumer must be addressable; that is, they must be
    capable of receiving and acknowledging messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While on the subject of Knative, I will briefly mention Istio, which is a service
    mesh that provides policy enforcement and traffic management, among other things.
    So, what is a service mesh? A service mesh represents a network of microservices
    typically deployed on Kubernetes. Istio provides several sophisticated features,
    including metrics that support the overall management of the mesh network. For
    serverless workloads that are deployed on Cloud Run for Anthos, Knative, together
    with Istio, provides an extension to the Kubernetes platform to enable more granular
    control of the microservice architecture being implemented.
  prefs: []
  type: TYPE_NORMAL
- en: This brief overview of the lower-level components should have provided you with
    some additional context about the underlying Cloud Run architecture. In the next
    section, we'll return to the topic of Cloud Run and Cloud Run on Anthos to perform
    a brief comparison of the products.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Run versus Cloud Run for Anthos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentally, Cloud Run is a serverless platform for stateless workloads. For
    this solution, there is no requirement for infrastructure management. Alternatively,
    you may have an existing Kubernetes cluster. In this scenario, all of your workloads
    run from this environment. Additionally, you may need features such as namespacing,
    control over pod colocation, or additional telemetry. In this case, Cloud Run
    on Anthos provides a more considered choice. In both instances, the workloads
    to be deployed remain the same, so as a developer, the associated effort does
    not increase, despite the apparent differences in terms of deployment platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what we mean in terms of Cloud Run/Cloud Run for Anthos, let''s
    start with a diagram. This will help us to observe the technology stack of each
    so that we can understand the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74ee304c-2231-4da7-a35a-d05cadddd062.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, it is clear that there is a lot of commonality between
    the two forms of Cloud Run. At the forefront of communication is a gateway that's
    used to consume HTTP traffic. Here, we can route traffic to the underlying product.
  prefs: []
  type: TYPE_NORMAL
- en: At the start of our diagram, we can discern that HTTP traffic is routed to our
    environment. Traffic to Google environments typically routes through the **Google
    Front End** (**GFE**). For Cloud Run for Anthos traffic, there is additional routing
    configuration based on a Google Cloud Load Balancer that's active at the network
    layer (and potentially an Istio gateway).
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of the container, we can see a crucial difference at this
    level. The management of the artifact has explicit dependencies, based on which
    the platform takes precedent. This is a central difference between the compute
    platform that's used to run the objects. On Kubernetes, the deployment process
    uses **Google Kubernetes Engine** (**GKE**). As we discussed earlier, the container
    artifact that's deployed uses the OCI to deliver the runtime and image specification.
    To access the broader services of Google, the Knative Serving API is used to communicate
    with Google APIs.
  prefs: []
  type: TYPE_NORMAL
- en: We already know that Knative is used to deliver both a portable and extensible
    API across different runtime environments in support of the development of serverless
    applications. Utilizing the Knative Serving API to provide portability and access
    to backend Google APIs is inherent with Cloud Run. Don't underestimate the power
    of portability, whether you are already reaping the benefits of Kubernetes or
    still undecided; having a core component to manage the transition seamlessly is
    a welcome addition. We touched on these high-level aspects of Knative earlier
    in this chapter; however, incorporating this capability makes for a great platform
    that we can use to extend applications to take advantage of orchestrated workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of the underlying architecture, we know that
    there are many moving parts that provide this serverless architecture on Google
    Cloud. In the next couple of chapters, we will turn our attention to the specifics
    of Cloud Run and Cloud Run for Anthos.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed Cloud Run at a high level and introduced the constituent
    components that make all of this a reality. Just like Google's other serverless
    products, Cloud Run scales to zero, except here, the deployment artifact is now
    a container. Utilizing a container artifact provides additional benefits as Cloud
    Run can be deployed with Kubernetes or without it. In addition, any language runtime
    can be used, making for a very flexible product.
  prefs: []
  type: TYPE_NORMAL
- en: Familiarity with container environments (for example, Docker) is a real advantage
    here, but Cloud Run removes much of the complexity of deploying code. Once the
    container has been successfully built, it can be deployed. Support for serverless
    request/response messages is inherent in Cloud Run, so there is always a simple
    and consistent method for developing components. For those of you who weren't
    previously familiar with containers, hopefully, you now know enough to be able
    to utilize them.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, we provided a common grounding for working
    with Cloud Run and containers. Whether or not you believe that containers are
    the future, they are an important topic to grasp. Now that we have gone through
    the basics of Cloud Run, we can move on to more interesting projects. In the next
    chapter, we will continue to investigate this serverless product and build some
    example projects.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Describe some differences between a monolith and a microservice application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What function does the GFE perform?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name two synchronous event processing patterns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using Docker, what is the `ENTRYPOINT` keyword used for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What Docker command is used to build an image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name the product that Google Cloud uses for image management?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What purpose does Cloud Build fulfill?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the Knative API an important component of Cloud Run?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is OCI and what is it used for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name some different operating systems that support containers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Migrating monolithic application microservices on GKE**: [https://cloud.google.com/solutions/migrating-a-monolithic-app-to-microservices-gke](https://cloud.google.com/solutions/migrating-a-monolithic-app-to-microservices-gke)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knative**: [https://cloud.google.com/knative/](https://cloud.google.com/knative/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gVisor**: [https://gvisor.dev/](https://gvisor.dev/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Istio**: [https://istio.io/docs/concepts/](https://istio.io/docs/concepts/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Infrastructure Security Design Overview**: [https://cloud.google.com/security/infrastructure/design/](https://cloud.google.com/security/infrastructure/design/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Load Balancing**: [https://cloud.google.com/load-balancing/](https://cloud.google.com/load-balancing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quickstart for Docker**: [https://cloud.google.com/cloud-build/docs/quickstart-docker](https://cloud.google.com/cloud-build/docs/quickstart-docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
