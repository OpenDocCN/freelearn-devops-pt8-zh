- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Data Engineering for Solution Architecture
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案架构的数据工程
- en: In the previous chapter, you learned about the DevOps process, which automates
    the application deployment pipeline and fosters a culture of collaboration among
    development, operations, and security teams. This chapter will introduce you to
    data engineering, including the various tools and techniques used to collect data
    from different parts of your application to gain insights that can drive your
    business.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您了解了DevOps过程，它自动化了应用程序部署流水线，并促进了开发、运维和安全团队之间的协作文化。本章将介绍数据工程，包括用于从应用程序的不同部分收集数据的各种工具和技术，从而获得可以推动您业务的见解。
- en: Data is being generated everywhere with high velocity and volume in the internet
    and digitization era. Getting insights from these enormous amounts of data at
    a fast pace is challenging. We must continuously innovate to ingest, store, and
    process this data to derive business outcomes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网和数字化时代，数据正在以极高的速度和量级产生。以快速的节奏从这些海量数据中获得洞察力是一个挑战。我们必须不断创新，摄取、存储和处理这些数据，以获得业务成果。
- en: With the convergence of cloud, mobile, and social technologies, advancements
    in many fields, such as genomics and life sciences, are growing ever-increasingly.
    Tremendous value is found in mining this data for more insight. Modern stream
    processing systems must produce continual results based on data with high input
    rates at low latency.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着云计算、移动技术和社交技术的融合，基因组学和生命科学等多个领域的进展日益增长。从中挖掘数据获得更多见解的巨大价值不断增加。现代流处理系统必须基于高速输入数据在低延迟的情况下持续产生结果。
- en: The concept of *big data* refers to more than just the collection and analysis
    of data. The actual value for organizations in their data can be used to gain
    insight and create competitive advantages. Not all big data solutions must end
    in visualization. Many solutions, such as **machine learning** (**ML**) and other
    predictive analytics, feed these answers programmatically into other software
    or applications, extracting the information and responding as designed.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据*的概念不仅仅是数据的收集和分析。组织在数据中的实际价值可以用于获得洞察力并创造竞争优势。并非所有的大数据解决方案都必须以可视化为终点。许多解决方案，如**机器学习**（**ML**）和其他预测分析，将这些答案以程序化方式输入到其他软件或应用程序中，提取信息并按设计做出响应。'
- en: As with most things, getting faster results costs more, and big data is no exception.
    Some answers might not be needed immediately, so the solution’s latency and throughput
    can be flexible enough to take hours to complete. Other responses, such as in
    predictive analytics, may be needed as soon as the data is available.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数事物一样，获取更快的结果需要更多的成本，大数据也不例外。有些答案可能不需要立即得到，因此解决方案的延迟和吞吐量可以灵活调整，可能需要几个小时才能完成。其他响应，比如预测分析中的响应，可能在数据可用时就需要立刻得到。
- en: 'In this chapter, you will learn about the following topics to handle and manage
    your big data needs:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下主题，以处理和管理您的大数据需求：
- en: What is big data architecture?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是大数据架构？
- en: Designing big data processing pipelines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计大数据处理管道
- en: Data ingestion, storage, processing, and analytics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取、存储、处理和分析
- en: Visualizing data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Designing big data architectures
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计大数据架构
- en: Big data architecture best practices
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据架构最佳实践
- en: By the end of this chapter, you will know how to design big data and analytics
    architecture. You will learn about the big data pipeline steps, including data
    ingestion, storage, processing, visualization, and architecture patterns.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将了解如何设计大数据和分析架构。您将学习大数据管道的步骤，包括数据摄取、存储、处理、可视化和架构模式。
- en: What is big data architecture?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是大数据架构？
- en: 'The sheer volume of collected data can cause problems. With the accumulation
    of more and more data, managing and moving data along with its underlying big
    data infrastructure becomes increasingly difficult. The rise of cloud providers
    has facilitated the ability to move applications to the cloud. Multiple sources
    of data result in increased volumes, velocity, and variety. The following are
    some common computer-generated data sources:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的数据量庞大可能会带来问题。随着数据的不断积累，管理和移动数据及其底层的大数据基础设施变得越来越困难。云服务提供商的崛起促进了将应用程序迁移到云端的能力。多个数据源导致数据量、速度和多样性增加。以下是一些常见的计算机生成数据来源：
- en: '**Application server logs**: Application logs and games'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用服务器日志**：应用程序日志和游戏'
- en: '**Clickstream logs**: From website clicks and browsing'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点击流日志**：来自网站点击和浏览'
- en: '**Sensor data**: Weather, water, wind energy, and smart grids'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器数据**：天气、水、风能和智能电网'
- en: '**Images and videos**: Traffic and security cameras'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像和视频**：交通和安全摄像头'
- en: Computer-generated data can vary from semi-structured logs to unstructured binaries.
    Computer-generated data sources can produce pattern matching or correlations in
    data that generate recommendations for social networking and online gaming. You
    can also use computer-generated data, such as blogs, reviews, emails, pictures,
    and brand perceptions, to track applications or service behavior.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机生成的数据可以从半结构化日志到非结构化的二进制数据。计算机生成的数据源可以在数据中产生模式匹配或相关性，生成社交网络和在线游戏的推荐。你还可以使用计算机生成的数据，如博客、评论、电子邮件、图片和品牌感知，来跟踪应用程序或服务的行为。
- en: Human-generated data includes email searches, natural language queries, sentiment
    analysis on products or companies, and product recommendations. Social graph analysis
    can produce product recommendations based on your circle of friends, jobs you
    may find interesting, or even reminders based on your circle of friends’ birthdays,
    anniversaries, and so on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工生成的数据包括电子邮件搜索、自然语言查询、产品或公司情感分析以及产品推荐。社交图谱分析可以根据你的朋友圈生成产品推荐、可能感兴趣的工作，甚至根据朋友的生日、纪念日等生成提醒。
- en: 'Typical barriers you hear from analytics teams that prevent them from delivering
    the most value to their organizations are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来自分析团队的典型障碍是阻止他们为组织提供最大价值的原因：
- en: '**Limited insight into customer experiences and operations**: To create new
    customer experiences, organizations need better visibility into their business.
    Complex and costly data collection, processing systems, and added scale costs
    require organizations to limit the types and amount of data they collect and analyze.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对客户体验和运营的洞察有限**：为了创造新的客户体验，组织需要更好地了解他们的业务。复杂且昂贵的数据收集、处理系统和增加的规模成本迫使组织限制其收集和分析的数据类型和数量。'
- en: '**Need to make quicker decisions**: This is a two-part problem:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要做出更快的决策**：这是一个两部分的问题：'
- en: Traditional data systems are overwhelmed, resulting in workloads taking a long
    time to complete.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统数据系统不堪重负，导致工作负载需要很长时间才能完成。
- en: More decisions need to be made in seconds or minutes, requiring systems to collect
    and process data in real time.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多的决策需要在秒或分钟内做出，这要求系统实时收集和处理数据。
- en: '**Enabling innovation with ML**: Organizations are adding and growing their
    data science teams to help optimize and grow their business. These users need
    more access to data with their choice of tools without the traditional red tape
    and processes that will slow them down.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过机器学习推动创新**：组织正在增加并发展他们的数据科学团队，以帮助优化和发展他们的业务。这些用户需要更多的数据访问权限和选择的工具，而不必通过传统的繁琐流程，这些流程可能会拖慢他们的工作进度。'
- en: '**Technical staff and cost to scale self-managed infrastructures**: Customers
    who manage infrastructure on-premises need help to quickly scale to meet business
    demand. Managing infrastructure, high availability, scaling, and operational monitoring
    takes time, especially at scale.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术人员和扩展自管基础设施的成本**：需要管理本地基础设施的客户，需要帮助快速扩展以满足业务需求。管理基础设施、高可用性、扩展和运营监控需要时间，特别是在大规模时。'
- en: 'In **big data architecture**, the general flow of a significant data pipeline
    starts with data and ends with insight. How you get from start to finish depends
    on a lot of factors. The following diagram illustrates a data workflow pipeline
    that will help you design your data architecture:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在**大数据架构**中，一个重要数据管道的整体流程从数据开始，到洞察结束。你从开始到结束的过程取决于很多因素。下图展示了一个数据工作流管道，帮助你设计数据架构：
- en: '![](img/B21336_12_01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21336_12_01.png)'
- en: 'Figure 12.1: Big data pipeline for data architecture design'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：数据架构设计的大数据管道
- en: 'As shown in the preceding diagram, the standard workflow of the big data pipeline
    includes the following steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，大数据管道的标准工作流程包括以下步骤：
- en: Data is collected (ingested) by an appropriate tool.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据通过适当的工具被收集（引入）。
- en: The data is stored persistently.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据被持久存储。
- en: The data is processed or analyzed. The data processing/analysis solution takes
    the data from storage, performs operations, and then stores the processed data
    again.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据被处理或分析。数据处理/分析解决方案从存储中获取数据，进行操作，然后再次存储处理后的数据。
- en: The data is then used by other processing/analysis tools or by the same tool
    again to get further answers from the data.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据随后被其他处理/分析工具或同一工具再次使用，以从数据中获取进一步的答案。
- en: To make answers useful to business users, they are visualized using a **business
    intelligence** (**BI**) tool or fed into an ML algorithm to make future predictions.
    Once the appropriate answers have been presented to the user, this gives them
    insight into the data they can use to make further business decisions.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使答案对业务用户有用，这些答案通常通过**商业智能**（**BI**）工具进行可视化，或被输入到机器学习算法中以做出未来预测。一旦适当的答案展示给用户，这将为他们提供能够进一步用于商业决策的数据洞察。
- en: The tools you deploy in your pipeline determine your *time to answer*, which
    is the latency between when your data was created and when you can get insight
    from it. The best way to architect data solutions while considering latency is
    to determine how to balance throughput with cost because a higher performance
    and reduced latency usually result in a higher price. For example, a financial
    trading platform requires real-time analytics to provide its users with immediate
    insights for quick decision making.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你在管道中部署的工具决定了你的*响应时间*，即数据创建时与你能够从中获得洞察时之间的延迟。考虑延迟时设计数据解决方案的最佳方式是确定如何平衡吞吐量和成本，因为更高的性能和更低的延迟通常会导致更高的价格。例如，金融交易平台需要实时分析，以便为用户提供即时洞察，从而做出快速决策。
- en: To achieve this, the platform might employ an expensive data processing pipeline
    that includes in-memory databases, real-time stream processing, and high-speed
    data ingestion services. This setup ensures low latency, allowing traders to respond
    to market changes instantaneously. Here, the business necessity for real-time
    analytics justifies the high costs associated with the low-latency architecture.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，平台可能会采用昂贵的数据处理管道，其中包括内存数据库、实时流处理和高速数据摄取服务。该设置确保低延迟，使交易者能够即时响应市场变化。在这里，实时分析的业务需求证明了与低延迟架构相关的高成本是合理的。
- en: Designing big data processing pipelines
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计大数据处理管道
- en: 'One of the critical mistakes many big data architectures make is handling multiple
    data pipeline stages with one tool. A fleet of servers managing the end-to-end
    data pipeline, from data storage and transformation to visualization, may be the
    most straightforward architecture, but it is also the most vulnerable to breakdowns
    in the pipeline. Such tightly coupled big data architecture typically does not
    provide the best possible balance of throughput and cost for your needs. When
    you are designing a data architecture, use FLAIR data principles as explained
    in the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 许多大数据架构犯的一个关键错误是使用一个工具处理多个数据管道阶段。从数据存储和转换到可视化，管理端到端数据管道的服务器群可能是最直接的架构，但也是最容易出现管道故障的架构。这种紧密耦合的大数据架构通常无法为你的需求提供最佳的吞吐量和成本平衡。在设计数据架构时，请使用以下解释的FLAIR数据原则：
- en: '**F – Findability**: This refers to the capability to easily locate available
    data assets and access their metadata, which includes information like ownership
    and data classification, along with other crucial attributes necessary for data
    governance and compliance.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F – 可查找性**：指的是轻松定位可用数据资产并访问其元数据的能力，包括诸如所有权、数据分类以及其他对于数据治理和合规性至关重要的属性。'
- en: '**L – Lineage**: The ability to trace the origin of data, track its movement
    and history, and understand as well as visualize how data flows from its sources
    to its points of consumption.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L – 血统**：追溯数据来源、跟踪其流动和历史的能力，并理解及可视化数据从源头到消费点的流动方式。'
- en: '**A – Accessibility**: This involves the facility to request and obtain security
    credentials that grant the right to access a specific data asset. It also implies
    the need for a networking infrastructure that supports efficient data access.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A – 可访问性**：这涉及到请求和获取安全凭证的能力，凭证授予访问特定数据资产的权限。它还意味着需要支持高效数据访问的网络基础设施。'
- en: '**I – Interoperability**: Ensuring that data is stored in formats that are
    accessible and usable by most, if not all, internal processing systems within
    the organization.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I – 互操作性**：确保数据以大多数（如果不是全部）组织内部处理系统可以访问和使用的格式存储。'
- en: '**R – Reusability**: Data should be documented with a known schema, and the
    source of the data should be clearly attributed. This aspect often includes principles
    of **master data management** (**MDM**), which focuses on the management of critical
    data from different domains to provide, with accuracy and consistency, a single
    point of reference.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R – 可重用性**：数据应使用已知的模式进行文档化，并且数据的来源应清晰注明。这个方面通常包括**主数据管理**（**MDM**）的原则，主数据管理关注于来自不同领域的关键数据的管理，以准确性和一致性为基础，提供一个单一的参考点。'
- en: Big data architects recommend decoupling the pipeline between ingestion, storage,
    processing, and getting insight. There are several advantages to decoupling storage
    and processing in multiple stages, including increased *fault tolerance*. For
    example, if something goes wrong in the second round of processing and the hardware
    dedicated to that task fails, you won’t have to start again from the beginning
    of the pipeline; your system can resume from the second storage stage. Decoupling
    your storage from various processing tiers allows you to read and write to multiple
    data stores.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据架构师建议解耦摄取、存储、处理和获取洞察之间的管道。将存储和处理解耦成多个阶段有几个优点，包括提高*容错能力*。例如，如果第二轮处理出现问题并且专门用于该任务的硬件发生故障，你就不必从管道的开始重新开始；系统可以从第二个存储阶段恢复。将存储从各个处理层解耦，允许你读写多个数据存储。
- en: 'The following diagram illustrates various tools and processes to consider when
    designing a big data architecture pipeline:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了在设计大数据架构管道时需要考虑的各种工具和过程：
- en: '![](img/B21336_12_02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21336_12_02.png)'
- en: 'Figure 12.2: Tools and processes for big data architecture design'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：大数据架构设计的工具与过程
- en: 'The things you should consider when determining the right tools for your big
    data architectures include the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定适合你大数据架构的工具时，应该考虑以下因素：
- en: The structure of your data
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据结构
- en: Maximum acceptable latency
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大可接受延迟
- en: Minimum acceptable throughput
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最低可接受吞吐量
- en: Typical access patterns of your system’s end users
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统终端用户的典型访问模式
- en: Your data structure impacts both the tools you use to process it and where you
    store it. The ordering of your data and the size of each object you’re storing
    and retrieving are also essential considerations. How your solution weighs latency/throughput
    and cost determines the time to answer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据结构不仅影响你用来处理它的工具，还影响你存储它的位置。数据的排序以及每个存储和检索的对象大小也是重要的考虑因素。你的解决方案如何权衡延迟/吞吐量与成本决定了回答问题所需的时间。
- en: User access patterns are another essential component to consider. Some jobs
    require regularly joining many related tables, and others require daily or less
    frequent data storage. Some jobs require comparing data from a wide range of data
    sources, and other jobs pull data from only one unstructured table. Knowing how
    your end users will most often use the data will help you determine the breadth
    and depth of your big data architecture. Let’s dive deep into each process and
    the tools involved in big data architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用户访问模式是另一个需要考虑的关键因素。有些任务需要定期连接多个相关表，而其他任务则需要进行日常或不太频繁的数据存储。有些任务需要比较来自广泛数据源的数据，而其他任务只从一个非结构化表中提取数据。了解终端用户最常如何使用数据将帮助你确定大数据架构的广度和深度。让我们深入探讨每个过程及其涉及的工具。
- en: Data ingestion, storage, processing, and analytics
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄取、存储、处理和分析
- en: To turn raw data into actionable intelligence that can inform decision making
    and strategic planning for businesses, data needs to be managed through several
    key stages, beginning with **data ingestion**—the collection of data from various
    sources. This can include everything from user-generated data to machine logs,
    or real-time streaming data. Once collected, the data needs to be stored in **data
    storage**, which can be done in databases, data lakes, or cloud storage solutions,
    depending on the data type and intended use.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将原始数据转化为可以为企业决策和战略规划提供信息的可操作智能，数据需要通过几个关键阶段进行管理，从**数据摄取**开始——即从各种来源收集数据。这可以包括从用户生成的数据到机器日志，或者实时流数据。一旦收集，数据需要存储在**数据存储**中，这可以通过数据库、数据湖或云存储解决方案来实现，具体取决于数据类型和预期用途。
- en: Following storage, **data processing and analytics** come into play, which involves
    sorting, aggregating, or transforming the data into a more usable form, where
    analytics can be performed on the processed data to extract meaningful insights.
    Analytics can range from simple queries and reporting to complex ML algorithms
    and predictive modeling. Let’s learn about these stages in detail.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储之后，**数据处理和分析**进入关键环节，涉及对数据进行排序、聚合或转换为更易用的形式，之后可以对处理后的数据进行分析，提取有意义的洞察。分析可以从简单的查询和报告到复杂的机器学习算法和预测建模等多种形式。让我们详细了解这些阶段。
- en: Data ingestion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data ingestion is the act of collecting data for transfer and storage. There
    are lots of places from where data can be onboarded. Predominantly, data ingestion
    falls into one of the categories of databases, streams, logs, and files. Among
    these, databases are the most popular. These typically consist of your main upstream
    transactional systems that are the primary data storage for your applications.
    They take on both relational and non-relational flavors, and several techniques
    for extracting data from them exist.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取是收集数据以进行传输和存储的行为。数据可以从很多地方进行摄取，主要分为数据库、流、日志和文件这几类。在这些类别中，数据库最为常见。这些数据库通常由应用程序的主要上游事务性系统组成，是数据的主要存储方式。它们可以是关系型数据库或非关系型数据库，并且有多种技术可以从中提取数据。
- en: 'Streams are open-ended sequences of time-series data, such as clickstream data
    from websites or **Internet of Things** (**IoT**) devices, usually published in
    an API we host. Applications, services, and operating systems generate logs. As
    shown in the following diagram, use the type of data your environment collects,
    and how it is collected, to determine what kind of ingestion solution is ideal
    for your needs:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 流是开放式的时间序列数据序列，例如来自网站的点击流数据或**物联网**（**IoT**）设备数据，通常通过我们托管的API发布。应用程序、服务和操作系统会生成日志。如下面的图所示，使用你所在环境收集的数据类型及其收集方式，来决定最适合你需求的摄取解决方案：
- en: '![](img/B21336_12_03.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21336_12_03.png)'
- en: 'Figure 12.3: Type of data ingestion'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：数据摄取类型
- en: As shown, transactional data storage must be able to store and retrieve data
    quickly. End users need quick and straightforward data access, making app and
    web servers the ideal ingestion methods. For the same reasons, NoSQL and **relational
    database management system** (**RDBMS**) databases are usually the best solutions
    for these kinds of processes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，事务性数据存储必须能够快速存储和检索数据。最终用户需要快速、简便的数据访问，这使得应用程序和Web服务器成为理想的摄取方式。出于同样的原因，NoSQL和**关系数据库管理系统**（**RDBMS**）数据库通常是此类过程的最佳解决方案。
- en: Data transmitted through individual files is typically ingested from connected
    devices. A large amount of file data does not require fast storage and retrieval
    compared to transactional data. For file data, often a transfer is one way, where
    data is produced by multiple resources and ingested into a single object or file
    storage for later use.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过单个文件传输的数据通常来自连接的设备。与事务性数据相比，大量文件数据不需要快速存储和检索。对于文件数据，通常是单向传输，其中数据由多个资源生成，并摄取到单一的对象或文件存储中以备后用。
- en: Stream data such as clickstream logs should be ingested through an appropriate
    solution such as **Apache Kafka** or **Fluentd**. Apache Kafka is a popular choice
    for this purpose, offering robust publish-subscribe capabilities that can handle
    massive amounts of data efficiently. Fluentd is another tool that can be used
    for data ingestion, particularly known for its log aggregation capabilities.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据，如点击流日志，应通过适当的解决方案摄取，例如**Apache Kafka**或**Fluentd**。Apache Kafka是一个流行的选择，提供强大的发布-订阅功能，能够高效地处理大量数据。Fluentd是另一种可用于数据摄取的工具，特别以其日志聚合能力而闻名。
- en: Initially, these logs are stored in stream storage solutions such as Kafka,
    so they’re available for real-time processing and analysis. Long-term storage
    of these logs is best in a low-cost solution such as object storage.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这些日志存储在流存储解决方案中，例如Kafka，这样它们就可以进行实时处理和分析。长期存储这些日志，最好的方式是选择低成本的存储方案，如对象存储。
- en: Streaming storage decouples your collection system (producers) from the processing
    system (consumers). It provides a persistent buffer for your incoming data. The
    data can be processed, and you can pump the data at a rate dependent on your needs.
    Let’s learn about some popular data ingestion technologies.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 流式存储将你的数据采集系统（生产者）与处理系统（消费者）解耦。它为你的来数据提供持久的缓冲区。数据可以根据你的需求以不同的速率进行处理和传输。让我们了解一些流行的数据导入技术。
- en: Technology choices for data ingestion
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据导入的技术选择
- en: 'Let’s look at some popular open source tools for data ingestion and transfer:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些流行的开源数据导入和传输工具：
- en: '**Apache DistCp**: DistCp stands for *distributed copy* and is part of the
    Hadoop ecosystem. The DistCp tool is used to copy large data within a cluster
    or between clusters. DistCp achieves efficient and fast data copying by utilizing
    MapReduce’s parallel processing distribution capability. It distributes directories
    and files into map tasks to copy file partitions from source to target. DistCp
    also does error handling, recovery, and reporting across clusters.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache DistCp**：DistCp代表*分布式复制*，是Hadoop生态系统的一部分。DistCp工具用于在集群内或集群之间复制大量数据。DistCp通过利用MapReduce的并行处理分发能力，实现了高效和快速的数据复制。它将目录和文件分发为映射任务，从源到目标复制文件分区。DistCp还在集群间进行错误处理、恢复和报告。'
- en: '**Apache Sqoop**: Sqoop is also part of the Hadoop ecosystem project and helps
    to transfer data between Hadoop and relational data stores such as RDBMS. Sqoop
    allows you to import data from a structured data store into the **Hadoop Distributed
    File System** (**HDFS**) and to export data from the HDFS into a structured data
    store. Sqoop uses plugin connectors to connect to relational databases. You can
    use the Sqoop extension API to build a new connector or use one of the included
    connectors that support data exchange between Hadoop and standard relational database
    systems.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Sqoop**：Sqoop也是Hadoop生态系统的一部分，帮助在Hadoop和关系型数据存储（如RDBMS）之间转移数据。Sqoop允许将数据从结构化数据存储导入到**Hadoop分布式文件系统**（**HDFS**），并将数据从HDFS导出到结构化数据存储。Sqoop使用插件连接器连接到关系数据库。你可以使用Sqoop扩展API构建新的连接器，或者使用其中之一支持Hadoop和标准关系数据库系统之间数据交换的连接器。'
- en: '**Apache Flume**: Flume is open source software mainly used to ingest a large
    amount of log data. Apache Flume collects and aggregates data to Hadoop reliably
    and distributes it. Flume facilitates streaming data ingestion and allows analytics.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Flume**：Flume是开源软件，主要用于导入大量日志数据。Apache Flume可靠地收集和汇总数据，并将其分发到Hadoop。Flume促进了流式数据导入，并支持分析。'
- en: More open source projects, such as Apache Storm and Apache Samza, are available
    for streaming to process unbounded data streams reliably.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 更多开源项目，如Apache Storm和Apache Samza，可用于流处理，可靠地处理无界数据流。
- en: Ingesting data to the cloud
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据导入云端
- en: 'Ingesting data into the cloud is critical to managing and leveraging big data.
    The three major cloud providers—AWS, **Google Cloud Platform** (**GCP**), and
    Azure—offer various data ingestion services. Each has unique features and capabilities
    tailored to different needs and data volumes. Let’s look at some of the unique
    features of these three cloud providers:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据导入云端对于管理和利用大数据至关重要。三大云服务提供商——AWS、**Google Cloud Platform**（**GCP**）和Azure——提供各种数据导入服务。每个服务商都有独特的功能和能力，适用于不同的需求和数据量。让我们来看看这三家云服务商的一些独特特点：
- en: 'AWS data ingestion services:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS数据导入服务：
- en: '**AWS Direct Connect**: This offers a high-speed, private network connection
    to AWS, reducing latency and increasing bandwidth. It’s ideal for transferring
    large volumes of data and provides a more consistent network speed than internet-based
    transfers.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Direct Connect**：提供一个高速、私有的网络连接到AWS，减少延迟并增加带宽。它非常适合转移大量数据，并提供比基于互联网的传输更稳定的网络速度。'
- en: '**AWS Snowball and Snowmobile**: These services provide physical devices for
    transferring vast volumes of data (in terabytes and **petabytes** (**PBs**)) to
    AWS. Snowball is suitable for hundreds of terabytes, while Snowmobile can handle
    up to 100 PBs in a single transfer, ideal for huge datasets.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Snowball和Snowmobile**：这些服务提供用于将大量数据（以TB和**PB**（**PBs**）为单位）转移到AWS的物理设备。Snowball适用于数百TB，而Snowmobile则可以处理单次转移高达100PB的数据，适合处理庞大的数据集。'
- en: '**AWS Database Migration Service (DMS)**: This facilitates the migration of
    databases to AWS. It supports both homogeneous and heterogeneous migrations and
    can handle ongoing data replication through **change data capture** (**CDC**).'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS数据库迁移服务（DMS）**：该服务帮助将数据库迁移到AWS，支持同构和异构迁移，并能通过**变更数据捕获**（**CDC**）处理持续的数据复制。'
- en: 'GCP data ingestion services:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP数据摄取服务：
- en: '**Google Cloud Storage Transfer Service**: This allows for the transfer of
    large volumes of data to Google Cloud Storage from online data sources like Amazon
    S3 and HTTP/HTTPS locations, as well as from on-premises data storage.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud存储转移服务**：该服务允许从在线数据源（如Amazon S3和HTTP/HTTPS位置）以及本地数据存储将大量数据转移到Google
    Cloud Storage。'
- en: '**Pub/Sub**: This offers real-time messaging and streaming data ingestion.
    It’s a scalable and flexible service that enables the ingestion of streaming data
    like logs and event data for real-time analytics.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pub/Sub**：该服务提供实时消息传递和流数据摄取。它是一个可扩展且灵活的服务，能够摄取如日志和事件数据等流数据，以进行实时分析。'
- en: '**Dataflow**: An integrated service for both data ingestion and processing.
    It’s handy for **extract, transform, and load** (**ETL**) tasks and real-time
    event stream processing.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据流**：一个集成服务，既支持数据摄取也支持数据处理。它非常适合**提取、转换和加载**（**ETL**）任务以及实时事件流处理。'
- en: 'Azure data ingestion services:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure数据摄取服务：
- en: '**Azure Data Factory**: This data integration service supports both on-premises
    and cloud data movements and transformations. It enables the ingestion of data
    from a variety of sources, processing it using computing services like Azure HDInsight
    and Azure Batch, and subsequently publishing the processed data to storage solutions
    such as Azure SQL Data Warehouse.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure数据工厂**：该数据集成服务支持本地和云端数据的迁移与转换。它可以从各种数据源摄取数据，利用计算服务如Azure HDInsight和Azure
    Batch处理数据，并将处理后的数据发布到如Azure SQL数据仓库等存储解决方案。'
- en: '**Azure Event Hubs**: A robust and scalable data streaming platform and event
    ingestion service, Azure Event Hubs is capable of handling millions of events
    per second. This makes it an ideal solution for real-time analytics on data originating
    from various sources like applications, websites, or IoT devices.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure事件中心**：一个强大且可扩展的数据流平台和事件摄取服务，Azure事件中心能够每秒处理数百万个事件，使其成为处理来自各种来源（如应用程序、网站或物联网设备）的实时数据分析的理想解决方案。'
- en: '**Azure Import/Export service**: This service is designed for the bulk transfer
    of large data volumes to and from Azure Blob Storage and Azure Files. It leverages
    physical disks for data transfer, making it a viable option for situations where
    transferring large amounts of data over a network might be too slow or expensive.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure导入/导出服务**：该服务旨在支持大数据量的批量传输到Azure Blob存储和Azure Files，利用物理磁盘进行数据传输，是在网络传输大数据量可能过慢或过于昂贵的情况下的理想选择。'
- en: Each cloud provider offers a unique set of tools to meet various data ingestion
    needs, from real-time streaming to large-scale data migration, ensuring flexibility
    and scalability in big data management.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 每个云服务提供商都提供一套独特的工具，以满足各种数据摄取需求，从实时流数据到大规模数据迁移，确保大数据管理中的灵活性和可扩展性。
- en: Streaming data is also becoming very important to ingest and analyze. You will
    learn more about streaming data in the *Streaming data stores* section. Let’s
    learn more about the techniques you can use to choose the right storage and the
    available storage choices.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据的摄取与分析变得越来越重要。你将在*流数据存储*部分了解更多关于流数据的内容。让我们进一步了解你可以使用的技术，帮助你选择合适的存储及其可用的选择。
- en: Storing data
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储数据
- en: One of the most common mistakes when setting up storage for a big data environment
    is using one solution, frequently an RDBMS, to handle all of your data storage
    requirements.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在为大数据环境配置存储时，最常见的错误之一是使用单一解决方案，通常是关系数据库管理系统（RDBMS）来处理所有数据存储需求。
- en: 'You will have many tools available, but they need to be optimized for the task
    they need to complete. One solution is not necessarily the best for all of your
    needs; the best solution for your environment might be a combination of storage
    solutions that carefully balance latency with cost. An ideal storage solution
    uses the right tool for the right job. The following diagram combines multiple
    factors related to your data and the storage choice associated with it:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你将有许多工具可用，但它们需要针对它们需要完成的任务进行优化。一个解决方案不一定是满足你所有需求的最佳选择；对于你的环境，最好的解决方案可能是多种存储方案的结合，这些方案能够仔细平衡延迟与成本。理想的存储解决方案使用适合特定任务的工具。下图结合了与你的数据和相关存储选择相关的多个因素：
- en: '![](img/B21336_12_04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21336_12_04.png)'
- en: 'Figure 12.4: Understanding data storage'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4：理解数据存储
- en: 'As shown in the proceeding diagram, choosing a data store depends upon the
    following factors:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，选择数据存储方案取决于以下因素：
- en: '**How structured is your data?** Does it adhere to a specific, well-formed
    schema, as with Apache weblogs (logs are generally poorly structured and unsuitable
    for relational databases), standardized data protocols, and contractual interfaces?
    Is it completely arbitrary binary data, as in images, audio, video, and PDF documents?
    Or is it semi-structured with a general structure but potentially high variability
    across the records, as in JSON or CSV?'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**你的数据结构化程度如何？** 它是否遵循特定的、格式良好的架构，例如 Apache 日志（日志通常结构不佳，不适合关系型数据库）、标准化的数据协议和合同接口？还是完全任意的二进制数据，例如图像、音频、视频和
    PDF 文档？或者它是半结构化的，具有一般的结构，但记录间可能存在较大的变化，例如 JSON 或 CSV？'
- en: '**How quickly does new data need to be available for querying?** Is it a real-time
    scenario where decisions are made as new records stream in, such as campaign managers
    adjusting based on conversion rates or a website making product recommendations
    based on user behavior similarity? Is it a daily, weekly, or monthly batch scenario,
    such as model training, financial statement preparation, or product performance
    reporting? Or is it somewhere in between, such as with user engagement emails,
    where it doesn’t require real-time action, and you can have a buffer of a few
    minutes or even a few hours between the user action and the touchpoint?'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新数据需要多快才能供查询使用？** 是实时场景，决策会随着新记录的流入而做出，例如活动经理根据转化率调整，或者网站根据用户行为相似性做出产品推荐？还是日常、每周或每月批量处理场景，例如模型训练、财务报表准备或产品性能报告？或者是介于两者之间的场景，例如用户参与邮件，它不需要实时响应，你可以在用户行为与触发点之间有几分钟甚至几小时的缓冲时间？'
- en: '**What is the size of the data ingest?** Is the data ingested recorded by the
    record as data comes in, such as with JSON payloads from REST APIs that measure
    at least a few KBs at best? Is it a large batch of records arriving simultaneously,
    such as system integrations and third-party data feeds? Or is it somewhere in
    between, such as with a few micro-batches of clickstream data aggregated together
    for more efficient processing?'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据摄取的大小是多少？** 数据是否随着记录的到来而被摄取，例如 REST API 中的 JSON 负载，最佳情况下至少为几 KB？是大量记录同时到达，例如系统集成和第三方数据源？还是介于两者之间，例如通过聚合几个微批次的点击流数据来进行更高效的处理？'
- en: '**What is the total volume of data and its growth rate?** Are you in GBs and
    TBs, or do you intend to store data in PBs or **exabytes** (**EBs**)? How much
    of this data is required for your specific analytics use cases? Do most of your
    queries only require a specific rolling window of time? Or, do you need a mechanism
    to query the entirety of your historical dataset?'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据的总量及其增长率是多少？** 你是在 GB 或 TB 范围内，还是打算存储 PB 或 **EB**（**艾克萨字节**）级别的数据？这些数据中有多少是你的特定分析用例所需的？你大多数查询是否仅需要特定的滚动时间窗口？或者，你是否需要一个机制来查询整个历史数据集？'
- en: '**What will the cost be to store and query the data in any particular location**?
    When it comes to any computing environment, we generally see a *triangle of constraints*
    between performance, resilience, and low cost. The better the performance and
    the higher the resilience you want your storage to have, the more expensive it
    will be. You can have quick queries over PBs of data but settle on querying TBs
    of data in a compressed format to meet your cost requirements.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储和查询数据在特定位置的成本是多少**？在任何计算环境中，我们通常会看到性能、韧性和低成本之间的*约束三角*。你希望存储的性能和韧性越好，它的成本就越高。你可以对PB级别的数据进行快速查询，但为了满足成本要求，你可以选择以压缩格式查询TB级别的数据。'
- en: Finally, what type of analytic queries will run against the data? Will it power
    a dashboard with a fixed set of metrics and drill down? Will it participate in
    large numerical aggregations rolled up by various business dimensions? Or will
    it be used for diagnostics, leveraging string tokenization for full-text searching
    and pattern analysis?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，针对数据将运行什么类型的分析查询？它是为具有固定指标集并支持深入分析的仪表盘提供数据吗？它会参与由不同业务维度汇总的大型数值聚合吗？还是它将用于诊断，利用字符串标记化进行全文搜索和模式分析？
- en: When you determine your data’s characteristics and understand the data structure,
    you can assess which solution you need to use for your data storage. Let’s learn
    about the various solutions for storing data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当你确定了数据的特征并了解了数据结构后，你可以评估需要使用什么解决方案来存储数据。让我们了解一下用于存储数据的各种解决方案。
- en: Technology choices for data storage
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据存储的技术选择
- en: As we discussed, a single tool can only do a few things. It would be best if
    you used the right tool for the right job, and a data lake enables you to build
    a highly configurable big data architecture to meet your specific needs. Business
    problems need to be narrower, deeper, and more complex for one tool to solve everything,
    especially big data and analytics.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，单一工具只能完成少数任务。你应该根据不同的工作选择合适的工具，数据湖让你能够构建一个高度可配置的大数据架构，以满足你的特定需求。业务问题需要更具体、更深刻、更复杂，单一工具无法解决一切，尤其是在大数据和分析方面。
- en: For example, hot data will need to be stored and processed in memory, so caches
    or in-memory databases like Redis or SAP HANA are appropriate. AWS offers the
    ElastiCache service, providing a managed Redis or memcached environment. NoSQL
    databases are ideal when facing high-velocity but small-sized records, for example,
    user-session information or IoT data. NoSQL databases are also useful for content
    management to store data catalogs. Let’s learn about the most popular and commonly
    used storage for structured data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，热数据需要在内存中存储和处理，因此缓存或内存数据库如Redis或SAP HANA是合适的选择。AWS提供了ElastiCache服务，提供托管的Redis或memcached环境。当面对高速但小型的记录时，NoSQL数据库是理想的选择，例如用户会话信息或物联网数据。NoSQL数据库也适用于内容管理，用于存储数据目录。让我们了解一下最常用的结构化数据存储。
- en: Structured data stores
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化数据存储
- en: Structured data stores have been around for decades and are the most familiar
    technology choice for storing data. Most transactional databases such as Oracle,
    MySQL, SQL Server, and PostgreSQL are row-based due to dealing with frequent data
    writes from software applications. Organizations often repurpose transactional
    databases for reporting purposes, requiring frequent data reads but much fewer
    data writes. Looking at high data-read requirements, more innovation is coming
    into querying on structured data stores, such as the columnar file format, which
    helps to enhance data-read performance for analytics requirements.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据存储已经存在了几十年，是存储数据最熟悉的技术选择。大多数事务性数据库，如Oracle、MySQL、SQL Server和PostgreSQL，都是基于行的，因为它们需要处理来自软件应用程序的频繁数据写入。组织通常会重新利用事务性数据库用于报告目的，这需要频繁的数据读取，但数据写入较少。随着对数据读取需求的增加，更多的创新正进入结构化数据存储的查询方式，例如列式文件格式，这有助于提高分析需求的数据读取性能。
- en: Row-based formats store the data in rows in a file. Row-based writing is the
    fastest way to write the data to the disk, but it is not necessarily the quickest
    read option because you need to skip over a lot of irrelevant data. Column-based
    formats store all the column values together in the file. This leads to better
    compression because the same data types are grouped. It also typically provides
    better read performance because you can skip columns that are not required.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基于行的格式将数据以行的形式存储在文件中。基于行的写入是将数据写入磁盘的最快方式，但并不一定是最快的读取选项，因为你需要跳过大量不相关的数据。基于列的格式将所有列的值一起存储在文件中，这有助于更好的压缩，因为相同的数据类型会被分组。它通常还提供更好的读取性能，因为你可以跳过不需要的列。
- en: Let’s look at common choices for the structured data store. For example, you
    need to query the total number of sales in a given month from the order table,
    which has fifty columns. The query will scan the entire table with all fifty columns
    in a row-based architecture. In columnar architecture, the query will scan the
    order sales column, thus improving data query performance. Let’s look into more
    details about relational databases, focusing on transaction data and data warehousing
    to handle data analytics needs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结构化数据存储的常见选择。例如，您需要从包含五十列的订单表中查询某个月的总销售额。使用行式架构时，查询将扫描整个表的所有五十列。而在列式架构中，查询将只扫描订单销售列，从而提高数据查询性能。让我们进一步了解关系型数据库，专注于事务数据和数据仓库，以应对数据分析的需求。
- en: Relational databases
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关系型数据库
- en: 'A RDBMS is more suitable for **online transaction processing** (**OLTP**) applications.
    Some popular relational databases are Oracle, MSSQL, MariaDB, PostgreSQL, and
    so on. Some of these traditional databases have been around for decades. Many
    applications, including e-commerce, banking, and hotel booking, are backed by
    relational databases. Relational databases are very good at handling transaction
    data where complex joint queries between tables are required. Looking at transaction
    data needs, the relational database should adhere to the **atomicity, consistency,
    isolation, and durability** (**ACID**) principles as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库管理系统（RDBMS）更适用于**在线事务处理**（**OLTP**）应用。一些流行的关系型数据库包括Oracle、MSSQL、MariaDB、PostgreSQL等。这些传统数据库中的一些已经存在了几十年。许多应用程序，包括电子商务、银行业务和酒店预订，都是由关系型数据库支撑的。关系型数据库非常擅长处理需要在表之间进行复杂连接查询的事务数据。从事务数据的需求来看，关系型数据库应遵循**原子性、一致性、隔离性和持久性**（**ACID**）原则，如下所示：
- en: '**Atomicity**: Atomicity means the transaction will be executed fully from
    end to end, and, in the case of any error, the entire transaction will roll back.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原子性**：原子性意味着事务将从头到尾完整执行，如果发生任何错误，整个事务将回滚。'
- en: '**Consistency**: Consistency means that all data should be committed to the
    database when transactions are completed.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：一致性意味着所有数据在事务完成时应该被提交到数据库。'
- en: '**Isolation**: Isolation requires multiple transactions to run concurrently
    in isolation without interfering with each other.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隔离性**：隔离性要求多个事务可以并发执行，并且互不干扰。'
- en: '**Durability**: In case of any interruption, such as a network or power failure,
    the transaction should be able to resume to the last known state.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久性**：在发生中断（如网络或电力故障）时，事务应能恢复到最后已知的状态。'
- en: Data from relational databases is often offloaded to data warehousing solutions
    for reporting and aggregation purposes. Let’s learn more about data warehousing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库中的数据通常会被卸载到数据仓库解决方案中，以进行报告和聚合处理。让我们深入了解数据仓库的更多内容。
- en: Data warehousing
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据仓库
- en: Data warehouses are central repositories that store accumulations of data from
    one or multiple sources. They store current and historical data to help create
    analytical reports for business data analytics. However, data warehouses store
    data centrally from various systems, but they cannot be treated as data lakes.
    Data warehouses handle only structured relational data, while data lakes work
    with structured and unstructured data, such as JSON logs and CSV data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库是存储来自一个或多个来源的数据积累的中央存储库。它们存储当前和历史数据，帮助创建业务数据分析的分析报告。然而，数据仓库集中存储来自各种系统的数据，但不能将其视为数据湖。数据仓库只处理结构化的关系型数据，而数据湖则可以处理结构化和非结构化数据，如JSON日志和CSV数据。
- en: Data warehouse databases are more suitable for **online analytical processing**
    (**OLAP**) applications. These databases are optimized for operations that involve
    reading large amounts of data, allowing for the aggregation and summarization
    of data to extract valuable business insights.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库数据库更适用于**在线分析处理**（**OLAP**）应用。这些数据库经过优化，能够处理涉及大量数据读取的操作，从而实现数据的聚合和汇总，提取有价值的业务洞察。
- en: Take, for example, a banking scenario where a bank maintains a data warehouse
    that stores comprehensive information about customer accounts, transactions, loan
    details, and branch information. Additionally, the bank collects and stores data
    on customer interactions, service usage, and online banking activities in a related
    system.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以银行场景为例，假设某银行维护一个数据仓库，存储关于客户账户、交易、贷款详情和分支信息的全面数据。此外，银行还收集并存储客户互动、服务使用和在线银行活动的数据，存在于相关系统中。
- en: Through OLAP, the bank can perform complex analyses of this combined data. It
    can query the data warehouse to uncover trends, such as identifying the most popular
    types of accounts or loans, analyzing transaction volumes over time, or assessing
    the usage patterns of online versus in-branch banking services. This analytical
    capability enables the bank to make informed decisions on product offerings, customer
    service improvements, and operational strategies, ultimately enhancing customer
    satisfaction and driving business growth.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 OLAP，银行可以对这些合并的数据进行复杂分析。它可以查询数据仓库，发现趋势，例如识别最受欢迎的账户或贷款类型、分析交易量随时间的变化，或评估线上与分行银行服务的使用模式。这种分析能力使得银行能够做出有关产品提供、客户服务改进和运营策略的明智决策，从而提升客户满意度并推动业务增长。
- en: Data warehouses provide fast aggregation capabilities over vast volumes of structured
    data. While these technologies, such as Amazon **Redshift**, **Netezza**, and
    **Teradata**, are designed to execute complex aggregate queries quickly, they
    must be optimized for high volumes of concurrent writes. So, data needs to be
    loaded in batches, preventing warehouses from serving real-time insights over
    hot data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库提供对大量结构化数据的快速聚合能力。虽然像 Amazon **Redshift**、**Netezza** 和 **Teradata** 这样的技术被设计用来快速执行复杂的聚合查询，但它们必须针对高并发写入进行优化。因此，数据需要分批加载，这使得数据仓库无法提供对热数据的实时洞察。
- en: Modern data warehouses use a columnar base to enhance query performance. Examples
    of this include Amazon Redshift, Snowflake, and Google BigQuery. These data warehouses
    provide fast query performance due to columnar storage and improved I/O efficiency.
    In addition, data warehouse systems such as Amazon Redshift increase query performance
    by parallelizing queries across multiple nodes and taking advantage of **massively
    parallel processing** (**MPP**).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现代数据仓库使用列式存储来增强查询性能。此类数据仓库的例子包括 Amazon Redshift、Snowflake 和 Google BigQuery。由于列式存储和提高的
    I/O 效率，这些数据仓库提供了快速的查询性能。此外，像 Amazon Redshift 这样的数据仓库系统通过将查询并行化处理，跨多个节点执行，并利用**大规模并行处理**（**MPP**），进一步提高了查询性能。
- en: Columnar storage enhances query performance by storing data in columns rather
    than rows, enabling improved data compression, selective data reading, and faster
    operations. This approach allows for more effective compression as similar data
    is stored sequentially, facilitating faster data retrieval as only necessary columns
    are accessed during queries. It also optimizes CPU cache utilization by loading
    relevant data into memory, enhancing the processing speed. Additionally, columnar
    storage supports massive parallel processing, where multiple processors can work
    on different data segments simultaneously, significantly boosting performance
    for analytical tasks that involve large datasets and require quick aggregation
    and filtering.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列式存储通过将数据按列而非按行存储，增强了查询性能，支持更有效的数据压缩、选择性数据读取和更快的操作。该方法允许更有效的压缩，因为相似的数据按顺序存储，这有助于在查询时仅访问必要的列，从而加速数据检索。它还通过将相关数据加载到内存中，优化了
    CPU 缓存的利用率，提高了处理速度。此外，列式存储支持大规模并行处理，多个处理器可以同时处理不同的数据片段，显著提升了涉及大型数据集并需要快速聚合和过滤的分析任务的性能。
- en: Data warehouse solutions such as Amazon Redshift can process PBs of data and
    provide decoupled compute and storage capabilities to save costs. In addition
    to columnar storage, Redshift uses data encoding, distribution, and zone maps
    to increase query performance. More traditional row-based data warehousing solutions
    include Netezza, Teradata, and Greenplum.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Amazon Redshift 这样的数据仓库解决方案可以处理 PB 级别的数据，并提供解耦的计算和存储能力以节省成本。除了列式存储，Redshift
    还使用数据编码、分布和区域映射来提升查询性能。更传统的行式数据仓库解决方案包括 Netezza、Teradata 和 Greenplum。
- en: Data warehouses lead to the physical separation of data from different applications,
    necessitating data architects to construct new infrastructures around these warehouses.
    The constraints of traditional data warehouses have become more pronounced with
    the growing diversity of enterprise data, including text, IoT data, images, audio,
    and video. Moreover, the advent of ML and **artificial intelligence** (**AI**)
    has brought forth iterative algorithms that demand direct data access and do not
    rely on SQL, thus highlighting the limitations of conventional data warehouse
    models. You will learn more about overcoming these challenges later in this chapter,
    in the *Designing big data architectures* section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库导致不同应用程序的数据物理分离，迫使数据架构师围绕这些仓库构建新的基础设施。随着企业数据的多样性增加，包括文本、物联网数据、图像、音频和视频，传统数据仓库的局限性变得更加明显。此外，机器学习和**人工智能**（**AI**）的出现带来了迭代算法，这些算法需要直接访问数据，而不依赖SQL，从而突显了传统数据仓库模型的局限性。你将在本章后续的*设计大数据架构*部分了解如何克服这些挑战。
- en: NoSQL databases
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NoSQL数据库
- en: NoSQL databases such as DynamoDB, Cassandra, and MongoDB address the scaling
    and performance challenges you often experience with a relational database. As
    the name suggests, NoSQL is a non-relational database. NoSQL databases store data
    without an explicit and structured mechanism to link data from different tables
    (no joins, foreign keys, or normalization enforced).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL数据库，如DynamoDB、Cassandra和MongoDB，解决了你在关系型数据库中常遇到的扩展性和性能问题。顾名思义，NoSQL是一种非关系型数据库。NoSQL数据库存储数据时没有明确和结构化的机制将来自不同表的数据链接起来（没有连接、外键或强制规范化）。
- en: NoSQL utilizes several data models, including columnar, key-value, search, document,
    and graph. NoSQL databases provide scalable performance, high availability, and
    resilience. NoSQL typically does not enforce a strict schema, and every item can
    have an arbitrary number of columns (attributes), meaning one row can have four
    columns. In contrast, another can have ten columns in the same table. The partition
    key is used to retrieve values or documents containing related attributes. NoSQL
    databases are highly distributed and can be replicated. They are durable and don’t
    experience performance issues when highly available.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL利用多种数据模型，包括列式、键值、搜索、文档和图形。NoSQL数据库提供可扩展的性能、高可用性和弹性。NoSQL通常不强制执行严格的模式，每个项可以有任意数量的列（属性），这意味着同一表中的一行可能只有四列，而另一行可能有十列。分区键用于检索包含相关属性的值或文档。NoSQL数据库高度分布式，可以进行复制。它们是持久的，并且在高度可用的情况下不会遇到性能问题。
- en: SQL versus NoSQL databases
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SQL与NoSQL数据库的对比
- en: 'SQL databases have existed for decades, and most are already familiar with
    relational databases. Let’s learn about some significant differences between SQL
    and NoSQL databases:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: SQL数据库已存在数十年，大多数人已经熟悉关系型数据库。让我们了解一下SQL和NoSQL数据库之间的一些显著区别：
- en: '| **Properties** | **SQL Databases** | **NoSQL Databases** |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **属性** | **SQL数据库** | **NoSQL数据库** |'
- en: '| Data model | The relational model normalizes data in SQL databases into tables
    containing rows and columns. A schema includes tables, columns, relationships
    between tables, indexes, and other database elements. | NoSQL databases operate
    without enforcing a fixed schema, offering flexibility in data storage and retrieval.
    They often utilize a partition key to access values from sets of columns. This
    type of database is well-suited for storing semi-structured data, including formats
    like JSON, XML, and various other document types, such as data catalogs and file
    indexes. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 数据模型 | SQL数据库中的关系模型将数据规范化为包含行和列的表。一个模式包括表、列、表之间的关系、索引以及其他数据库元素。 | NoSQL数据库在不强制执行固定模式的情况下运行，提供了在数据存储和检索方面的灵活性。它们通常利用分区键从列集访问值。这种类型的数据库非常适合存储半结构化数据，包括JSON、XML等格式，以及各种其他文档类型，如数据目录和文件索引。
    |'
- en: '| Transaction | SQL-based traditional RDBMSs support and comply with ACID transactional
    data properties. | NoSQL databases sometimes trade certain ACID properties, which
    are characteristic of traditional RDBMSs, in order to facilitate horizontal scaling
    and enhance flexibility in their data models. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 事务 | 基于SQL的传统关系数据库管理系统（RDBMS）支持并符合ACID事务数据属性。 | NoSQL数据库有时会牺牲某些ACID属性，这些属性是传统RDBMS的特征，以便促进横向扩展并增强数据模型的灵活性。
    |'
- en: '| Performance | SQL-based RDBMSs were used to optimize storage when storage
    was expensive and minimize the disk footprint. For traditional RDBMSs, performance
    has mostly relied on the disk. Index creation and table structure modifications
    are required to achieve performance query optimizations. | In NoSQL systems, the
    performance is significantly influenced by factors such as the size of the underlying
    hardware cluster, network latency, and the manner in which the application interacts
    with the database. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 基于 SQL 的关系型数据库（RDBMS）曾用于在存储昂贵时优化存储，并最小化磁盘占用。对于传统的关系型数据库系统，性能主要依赖于磁盘。需要创建索引和修改表结构来实现性能查询优化。
    | 在 NoSQL 系统中，性能受多种因素的显著影响，例如底层硬件集群的规模、网络延迟以及应用程序与数据库交互的方式。 |'
- en: '| Scale | SQL-based RDBMS databases are most straightforward to scale vertically
    with high-configuration hardware. The additional effort requires relational tables
    to span distributed systems, such as performing data sharding. | NoSQL databases
    are engineered to scale out horizontally, utilizing distributed clusters composed
    of cost-effective hardware. This approach is aimed at boosting throughput while
    minimizing any impact on latency. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 扩展性 | 基于 SQL 的关系型数据库（RDBMS）在高配置硬件的支持下，垂直扩展最为直接。额外的工作需要将关系表扩展到分布式系统，例如执行数据分片。
    | NoSQL 数据库旨在水平扩展，利用由经济型硬件组成的分布式集群。该方法旨在提高吞吐量，同时最小化对延迟的影响。 |'
- en: Table 12.1 – SQL versus NoSQL database comparison
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.1 – SQL 与 NoSQL 数据库对比
- en: Depending on your data, various categories of NoSQL data stores exist to solve
    a specific problem. Let’s learn about the types of NoSQL databases.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的数据类型，存在多种类别的 NoSQL 数据存储，用于解决特定问题。让我们了解一下 NoSQL 数据库的类型。
- en: Types of NoSQL databases
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NoSQL 数据库的类型
- en: 'The following are the major NoSQL database types:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是主要的 NoSQL 数据库类型：
- en: '**Columnar databases**: Apache Cassandra and Apache HBase are the popular columnar
    databases. A columnar data store helps you scan a particular column when querying
    the data rather than scanning the entire row. Suppose an item table has ten columns
    with one million rows, and you want to query the number of items available in
    inventory. In that case, the columnar database will apply the query to the item
    quantity column rather than scanning the entire table.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列式数据库**：Apache Cassandra 和 Apache HBase 是常见的列式数据库。列式数据存储可以帮助你在查询数据时扫描特定的列，而不是扫描整行数据。假设一个商品表有十列和一百万行数据，如果你想查询库存中商品的数量，列式数据库会将查询应用到商品数量这一列，而不是扫描整个表。'
- en: '**Document databases**: Some of the most popular document databases are **MongoDB**,
    **Couchbase**, **MarkLogic**, **DynamoDB**, **DocumentDB**, and **Cassandra**.
    You can use a document database to store semi-structured data in JSON and XML
    formats.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档数据库**：一些最流行的文档数据库有 **MongoDB**、**Couchbase**、**MarkLogic**、**DynamoDB**、**DocumentDB**
    和 **Cassandra**。你可以使用文档数据库以 JSON 和 XML 格式存储半结构化数据。'
- en: '**Graph databases**: Popular graph database choices include **Amazon Neptune**,
    **JanusGraph**, **TinkerPop**, **Neo4j**, **OrientDB**, **GraphDB**, and **GraphX**
    in Spark. A graph database stores vertices and links between vertices called **edges**.
    Graphs can be built on both relational and non-relational databases.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图数据库**：流行的图数据库有 **Amazon Neptune**、**JanusGraph**、**TinkerPop**、**Neo4j**、**OrientDB**、**GraphDB**
    和 Spark 中的 **GraphX**。图数据库存储顶点和顶点之间的连接，称为 **边**。图可以建立在关系型和非关系型数据库之上。'
- en: '**In-memory key-value stores**: Some of the most popular in-memory key-value
    stores are Redis and Memcached. They store data in memory for heavy reading applications.
    Any query from an application first goes to an in-memory database, and if the
    data is available in the cache, it doesn’t hit the master database. The in-memory
    database is suitable for storing user-session information, which results in complex
    queries and frequently requests data such as user profiles.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存键值存储**：一些最流行的内存键值存储有 Redis 和 Memcached。它们将数据存储在内存中，以支持重度读取应用。任何来自应用的查询首先会访问内存数据库，如果数据已存在于缓存中，就不会访问主数据库。内存数据库适用于存储用户会话信息，这些信息会涉及复杂的查询和频繁请求数据，例如用户资料。'
- en: NoSQL has many use cases, but you must index all your data to build a search.
    Let’s learn more about search data stores.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: NoSQL 有许多应用场景，但你必须为所有数据建立索引才能进行搜索。让我们来了解一下搜索数据存储。
- en: Search data stores
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 搜索数据存储
- en: The Elasticsearch service is one of the most popular search engines for big
    data use cases like clickstream and log analysis. Search engines work well for
    warm data that can be queried ad hoc across any number of attributes, including
    string tokens.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 服务是最受欢迎的大数据搜索引擎之一，广泛应用于点击流分析和日志分析等大数据场景。搜索引擎非常适合处理可以按需查询的“热数据”，这些数据可以跨多个属性进行查询，包括字符串标记。
- en: Amazon OpenSearch Service provides data search capabilities and the support
    of open source Elasticsearch clusters, including API access. It also provides
    Kibana as a visualization mechanism to search for indexed data stores. AWS manages
    capacity, scaling, and patching of clusters, removing any operational overhead.
    Log search and analysis is a popular big data use case where OpenSearch helps
    you analyze log data from websites, server fleets, IoT sensors, and so on. Various
    applications in industries such as banking, gaming, marketing, application monitoring,
    advertisement technology, fraud detection, recommendations, and IoT utilize OpenSearch
    and Elasticsearch. ML-based search services, such as Amazon Kendra, are also available,
    providing more advanced search capabilities using **natural language processing**
    (**NLP**).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon OpenSearch 服务提供数据搜索功能，并支持开源的 Elasticsearch 集群，包括 API 访问。它还提供 Kibana 作为可视化机制，用于搜索已索引的数据存储。AWS
    负责集群的容量管理、扩展和补丁处理，消除了任何操作上的开销。日志搜索和分析是 OpenSearch 的一个热门大数据应用场景，它帮助你分析来自网站、服务器群集、物联网传感器等的日志数据。银行、游戏、营销、应用监控、广告技术、欺诈检测、推荐系统以及物联网等行业的各种应用都利用
    OpenSearch 和 Elasticsearch。基于机器学习的搜索服务，如 Amazon Kendra，也提供更先进的搜索能力，利用**自然语言处理**（**NLP**）。
- en: Unstructured data stores
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非结构化数据存储
- en: When you look at the requirements for an unstructured data store, Hadoop is
    a perfect choice because it is scalable, extensible, and very flexible. It can
    run on consumer hardware, has a vast ecosystem of tools, and appears cost-effective.
    Hadoop uses a *master-and-child-node* model, where data is distributed between
    multiple child nodes, and the primary node coordinates jobs for running queries
    on data. The Hadoop system is based on MPP, making it fast to perform queries
    on all data types, whether structured or unstructured.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑非结构化数据存储的需求时，Hadoop 是一个完美的选择，因为它可扩展、可扩展且非常灵活。它可以运行在消费者硬件上，拥有庞大的工具生态系统，并且看起来具有成本效益。Hadoop
    使用*主从节点*模型，其中数据在多个子节点之间分配，主节点协调任务以对数据进行查询。Hadoop 系统基于 MPP（大规模并行处理），使得在所有数据类型（无论是结构化还是非结构化数据）上执行查询变得快速。
- en: When a Hadoop cluster is created, each child node created from the server comes
    with a block of the attached disk storage called a local HDFS disk store. You
    can run the query against stored data using common processing frameworks like
    Hive, Pig, and Spark. However, data on the local disk persists only for the life
    of the associated instance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个 Hadoop 集群时，从服务器创建的每个子节点都会附带一块称为本地 HDFS 磁盘存储的磁盘存储块。你可以使用常见的处理框架（如 Hive、Pig
    和 Spark）对存储的数据进行查询。然而，本地磁盘上的数据仅在相关实例的生命周期内存在。
- en: If you use Hadoop’s storage layer (HDFS) to store your data, you are coupling
    storage with compute. Increasing storage space means adding more machines, which
    also increases compute capacity. For maximum flexibility and cost-effectiveness,
    you need to separate compute and storage and scale them independently. Overall,
    object storage is more suited to data lakes to store all kinds of data cost-effectively
    and efficiently. Cloud-based data lakes backed by object storage provide flexibility
    to decouple compute and storage. Let’s learn more about object storage.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Hadoop 的存储层（HDFS）来存储数据，你就将存储和计算结合在一起。增加存储空间意味着需要添加更多的机器，这也增加了计算能力。为了最大化灵活性和成本效益，你需要将计算和存储分开，并独立扩展它们。总体而言，对象存储更适合用来存储数据湖中的各种数据，能够以成本效益高且高效的方式存储数据。由对象存储支持的基于云的数据湖提供灵活性，可以将计算和存储解耦。让我们进一步了解对象存储。
- en: Object storage
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对象存储
- en: Object storage refers to data stored and accessed with units often referred
    to as objects stored in buckets. In object storage, files or objects are not split
    into data blocks, but data and metadata are kept together. There is no limit on
    the number of objects stored in a bucket, and they are accessed using API calls
    (usually through `HTTP`, `GET`, or `PUT`) to read and write to and from buckets.
    Typically, object storage is not mounted as a filesystem on operating systems
    because the latency of API-based file requests and lack of file-level locking
    provide poor performance as a filesystem.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储指的是使用单元（通常称为对象）存储和访问的数据，这些对象存储在桶中。在对象存储中，文件或对象不会被拆分成数据块，而是将数据和元数据一起存储。桶中存储的对象数量没有限制，且它们是通过
    API 调用（通常通过`HTTP`、`GET` 或 `PUT`）来读取和写入桶中的数据。通常，对象存储不会作为操作系统上的文件系统挂载，因为基于 API 的文件请求的延迟以及缺乏文件级锁定会导致作为文件系统的性能较差。
- en: Object storage offers scale and has a flat namespace, reducing management overhead
    and metadata management. Object storage has become more popular with the public
    cloud and is the go-to storage to build a scalable data lake in the cloud. Amazon
    S3, Azure Blob Storage, and Google Cloud Storage in GCP are the most popular object
    storage options.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储提供了扩展性，并且具有平坦的命名空间，减少了管理开销和元数据管理。随着公共云的普及，对象存储变得越来越流行，并且是构建云中可扩展数据湖的首选存储方案。Amazon
    S3、Azure Blob Storage 和 Google Cloud Storage（GCP）是最受欢迎的对象存储选项。
- en: Vector Database (VectorDB)
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量数据库（VectorDB）
- en: '**VectorDB** has become very popular recently due to an increased focus on
    generative AI and ML. Vector data typically refers to high-dimensional data points,
    often used in the context of ML models. For example, an image, text, or audio
    file can be converted into a vector representation (a list of numbers) that captures
    its essential features. These vectors are used in ML tasks such as similarity
    search (finding the most similar items), clustering, or classification. For example,
    if you want to build customer segmentation, vector embeddings can be used to cluster
    customers into different groups based on their purchasing behavior or preferences.
    By analyzing the vector representations of customers’ purchase histories or interactions
    with a website, businesses can identify distinct clusters of similar customers.
    This enables them to tailor marketing strategies, personalize offers, or develop
    targeted products for each specific customer group, enhancing customer satisfaction
    and loyalty.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**VectorDB** 最近因为生成式 AI 和机器学习的关注度提高而变得非常流行。向量数据通常指高维数据点，通常用于机器学习模型的上下文中。例如，图像、文本或音频文件可以被转换为向量表示（一个数字列表），以捕捉其核心特征。这些向量被用于机器学习任务，如相似性搜索（寻找最相似的项目）、聚类或分类。例如，如果你想进行客户细分，可以使用向量嵌入将客户根据其购买行为或偏好聚类成不同的群体。通过分析客户的购买历史或与网站的互动的向量表示，企业可以识别出相似客户的不同群体。这样，企业可以定制营销策略、个性化优惠，或为每个特定客户群体开发有针对性的产品，从而提升客户满意度和忠诚度。'
- en: '**VectorDB**, or vector databases, represent an emerging category in the database
    technology landscape, primarily focused on efficiently handling vector data. This
    data type is often associated with ML, particularly in areas like image recognition,
    NLP, and recommendation systems.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**VectorDB** 或向量数据库是数据库技术领域中新兴的一类，主要聚焦于高效处理向量数据。这种数据类型通常与机器学习相关，尤其是在图像识别、自然语言处理（NLP）和推荐系统等领域。'
- en: A vector database’s core functionality is storing and managing vector data efficiently.
    This involves storing the high-dimensional data points and optimizing the database
    architecture to support quick and efficient querying, often in the form of nearest
    neighbor search.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库的核心功能是高效存储和管理向量数据。这涉及到存储高维数据点，并优化数据库架构，以支持快速高效的查询，通常以最近邻搜索的形式进行。
- en: Advanced vector databases may incorporate ML models directly into the database,
    enabling on-the-fly transformation of raw data (like images or text) into vectors,
    which can then be stored or queried.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 高级向量数据库可能会将机器学习模型直接集成到数据库中，从而实现实时将原始数据（如图像或文本）转换为向量，然后可以存储或查询这些向量。
- en: 'A common use case is finding items similar to a given query item. For instance,
    the database can quickly retrieve images most similar to the query image in an
    image search. Vector databases can power recommendation engines by matching user
    profiles with product vectors to suggest relevant items. They can efficiently
    handle and query large-scale text data transformed into vector space for various
    NLP applications. The following are the pros of **VectorDB**:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的使用案例是找到与给定查询项相似的项目。例如，数据库可以快速检索与查询图像最相似的图像用于图像搜索。向量数据库可以通过将用户资料与产品向量匹配来为推荐引擎提供支持，从而建议相关的商品。它们还可以高效地处理和查询大规模的文本数据，将其转换为向量空间，以支持各种自然语言处理应用。以下是**VectorDB**的优点：
- en: '**Speed and efficiency**: Tailored to handle high-dimensional data, vector
    databases can perform similarity searches much faster than traditional databases.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度和效率**：专门针对处理高维数据优化的向量数据库，比传统数据库执行相似性搜索要快得多。'
- en: '**Scalability**: They are designed to scale with the size of the data, which
    is crucial in ML applications where datasets are often large.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：它们设计为能随着数据量的增长进行扩展，这对于机器学习应用尤为重要，因为数据集通常非常庞大。'
- en: '**Integration with ML/AI pipelines**: Seamless integration with ML workflows,
    allowing direct querying and manipulation of vector data.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与ML/AI管道的集成**：与ML工作流的无缝集成，允许直接查询和操作向量数据。'
- en: 'Let’s look at some of the cons of **VectorDB** as well:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也来看看**VectorDB**的一些缺点：
- en: '**Complexity**: The management and indexing of high-dimensional vector data
    can be complex.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：管理和索引高维向量数据可能会非常复杂。'
- en: '**Resource intensive**: These databases might require significant computational
    resources, especially for large-scale datasets.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源密集型**：这些数据库可能需要大量计算资源，尤其是在处理大规模数据集时。'
- en: '**Emerging technology**: Being relatively new, the ecosystem around vector
    databases might not be as mature as traditional databases, which can be a consideration
    for enterprise adoption.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新兴技术**：由于相对较新，围绕向量数据库的生态系统可能不如传统数据库成熟，这可能是企业采用时需要考虑的因素。'
- en: Vector databases are part of a broader trend towards specialized databases tailored
    for specific types of data and workloads, particularly in the context of ML and
    AI. They represent a significant step in evolving database technology to keep
    pace with data science and analytics advancements. As this technology matures,
    it’s likely to become an integral part of the data infrastructure in organizations
    heavily invested in ML and AI.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库是朝向专门化数据库的一部分趋势，专为特定类型的数据和工作负载设计，尤其是在机器学习和人工智能领域。它们代表了数据库技术发展的重要一步，跟上数据科学和分析进展的步伐。随着这项技术的成熟，它很可能成为重度投资于机器学习和人工智能的组织数据基础设施的核心组成部分。
- en: Blockchain data stores
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区块链数据存储
- en: Blockchain technology, commonly associated with cryptocurrencies, offers a revolutionary
    approach to data management and transaction processing in various sectors beyond
    finance. Blockchain data stores offer a robust mechanism for decentralized verification,
    fundamentally altering how transactions are recorded and validated across various
    sectors. In a blockchain-based land registry system, for instance, every transaction
    involving property sales or purchases is recorded on a shared ledger, instantly
    accessible and verifiable by all network participants. This transparency contrasts
    with traditional centralized systems, where data is managed by a single authority,
    reducing the risk of fraud and enhancing trust among participants.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 区块链技术通常与加密货币相关联，提供了一种在金融以外的多个行业中变革性的数据管理和交易处理方法。区块链数据存储为去中心化验证提供了一种强大的机制，根本改变了交易如何在各个行业中记录和验证。在基于区块链的土地登记系统中，例如，每一笔涉及房地产买卖的交易都会记录在共享的账本上，所有网络参与者可以即时访问和验证。这种透明性与传统的中心化系统形成了对比，后者由单一权威机构管理数据，从而减少了欺诈风险，并增强了参与者之间的信任。
- en: The immutability and security features of blockchain further increase its application
    across industries. In healthcare, for example, blockchain ensures that once patient
    records are entered into the system, they remain unchanged and secure. This immutability
    is vital for medical professionals who depend on accurate historical data for
    treatment decisions. Additionally, the cryptographic security of blockchain protects
    sensitive health information, allowing access only to authorized users and ensuring
    patient privacy. These attributes make blockchain an invaluable tool in sectors
    where data integrity and security are paramount.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 区块链的不可篡改性和安全性特点进一步增加了其在各个行业中的应用。例如，在医疗领域，区块链确保一旦患者记录被录入系统，就保持不变且安全。这种不可篡改性对医疗专业人员至关重要，因为他们依赖于准确的历史数据来做出治疗决策。此外，区块链的加密安全性保护了敏感的健康信息，只允许授权用户访问，从而确保患者隐私。这些特性使区块链在数据完整性和安全性至关重要的行业中成为宝贵的工具。
- en: 'To achieve immutability, blockchain networks play a key role, which is a decentralized
    digital ledger that records transactions across multiple computers in a way that
    ensures the integrity and security of the data. In a blockchain network, transactions
    are grouped into blocks, and each block is linked to the previous one, forming
    a chain. This structure makes it extremely difficult to alter information retroactively
    without the consensus of the network participants. The following are the types
    of blockchain networks:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现不可篡改性，区块链网络发挥了关键作用。区块链是一种去中心化的数字账本，用于记录多个计算机中的交易，从而确保数据的完整性和安全性。在区块链网络中，交易被打包成区块，每个区块都与前一个区块相链接，形成一条链。这种结构使得没有网络参与者的共识，几乎不可能事后修改信息。以下是区块链网络的几种类型：
- en: '**Public blockchain**: Ethereum is often used for **decentralized applications**
    (**DApps**) and smart contracts. Ethereum is open, and anyone can join and participate
    in the network. For instance, a developer might create a DApp for **decentralized
    finance** (**DeFi**) on the Ethereum network, allowing users to engage in financial
    transactions without traditional banks.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公有区块链**：以太坊通常用于**去中心化应用**（**DApps**）和智能合约。以太坊是开放的，任何人都可以加入并参与网络。例如，开发者可能会在以太坊网络上创建一个**去中心化金融**（**DeFi**）的DApp，允许用户进行不依赖传统银行的金融交易。'
- en: '**Private blockchain**: This type of blockchain is restricted and controlled
    by an organization, offering more privacy and control. A pharmaceutical company
    might use a private blockchain to manage its drug development process. Access
    to the blockchain is restricted to company researchers and regulators, ensuring
    sensitive data is kept confidential.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**私有区块链**：这种类型的区块链由一个组织进行限制和控制，提供更多的隐私性和控制权。例如，一家制药公司可能会使用私有区块链来管理其药物研发过程。区块链的访问权限仅限于公司研究人员和监管机构，确保敏感数据保持机密。'
- en: '**Consortium blockchain**: This involves multiple organizations managing a
    blockchain network, balancing decentralization with control. An example would
    be a group of shipping companies forming a consortium to manage a shared blockchain.
    This blockchain could be used to track cargo shipments across the globe, with
    each company maintaining a node on the network.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联盟区块链**：这涉及多个组织共同管理一个区块链网络，平衡去中心化和控制的关系。一个例子是，一群航运公司组成联盟，共同管理一个共享的区块链。这个区块链可以用于追踪全球范围内的货物运输，每家公司在网络中维护一个节点。'
- en: Cloud providers like **Amazon Web Services** (**AWS**) offer blockchain as a
    service, simplifying the setup and management of blockchain networks. Amazon **Quantum
    Ledger Database** (**QLDB**) is an example of a centralized ledger database that
    provides an immutable and cryptographically verifiable record of transactions.
    Popular managed blockchain services include **Amazon Managed Blockchain** (**AMB**),
    **R3 Corda**, **Ethereum**, and **Hyperledger**, catering to various needs from
    financial transactions to supply chain management.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 像**亚马逊网络服务**（**AWS**）这样的云服务提供商提供区块链即服务，简化了区块链网络的设置和管理。亚马逊**量子账本数据库**（**QLDB**）是一个集中的账本数据库的例子，它提供不可篡改和加密验证的交易记录。流行的托管区块链服务包括**亚马逊托管区块链**（**AMB**）、**R3
    Corda**、**以太坊**和**Hyperledger**，满足从金融交易到供应链管理等各种需求。
- en: Streaming data processing used to be a niche technology, but now it’s becoming
    common as every organization wants to get fast insight from real-time data processing.
    Let’s learn more about streaming data stores.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据处理曾经是一个小众技术，但现在它变得越来越普遍，因为每个组织都希望从实时数据处理中获取快速的洞察。让我们了解更多关于流数据存储的信息。
- en: Streaming data stores
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流数据存储
- en: Streaming data has a continuous data flow with no start and end. Lots of data
    from various real-time resources, such as stock trading, autonomous cars, smart
    spaces, social media, e-commerce, gaming, ride apps, and so on, needs to be stored
    and processed quickly. Netflix provides real-time recommendations based on the
    content you are watching, and Lyft uses streaming to connect passengers to a driver
    in real time.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据具有持续的数据流，没有开始和结束。来自各种实时资源的大量数据，如股票交易、自动驾驶汽车、智能空间、社交媒体、电商、游戏、打车应用等，需要快速存储和处理。Netflix根据你正在观看的内容提供实时推荐，Lyft利用流技术将乘客与司机实时连接。
- en: 'Storing and processing streaming data is challenging as there is a continuous
    stream of data coming in, and you cannot predict the storage capacity. Along with
    high volume, streaming data comes with very high velocity, which requires a scalable
    storage system that can store the data and provide the ability to replay it. Data
    streams can become very expensive to maintain and complex to manage over time.
    Popular streaming data storage services are Apache Kafka, Apache Flink, Apache
    Spark Structured Streaming, Apache Samza, and Amazon Kinesis. AWS provides managed
    Kafka, known as Amazon Managed Streaming for Kafka. Let’s learn more details about
    streaming data ingestion and storage technology:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 存储和处理流数据具有挑战性，因为数据是持续不断流入的，且无法预测存储容量。除了数据量大，流数据的速度也非常快，这需要一个可扩展的存储系统来存储数据并提供回放的能力。随着时间推移，数据流可能会变得非常昂贵且管理复杂。流数据存储的流行服务有Apache
    Kafka、Apache Flink、Apache Spark Structured Streaming、Apache Samza和Amazon Kinesis。AWS提供了托管Kafka服务，称为Amazon
    Managed Streaming for Kafka。让我们深入了解流数据摄取和存储技术：
- en: '**Amazon Kinesis**: Amazon Kinesis offers three capabilities. The first, **Kinesis
    Data Streams** (**KDS**), is a place to store a raw data stream to perform any
    downstream processing of the desired records. The second is **Amazon Kinesis Data
    Firehose** (**KDF**), which facilitates transferring these records into common
    analytic environments like Amazon S3, Elasticsearch, Redshift, and Splunk. Firehose
    will automatically buffer up all the records in the stream and flush out to the
    target as a single file or set of records based on either a time or data-size
    threshold that you can configure or whichever is reached first.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Kinesis**：Amazon Kinesis提供了三种能力。第一种是**Kinesis Data Streams**（**KDS**），它是一个存储原始数据流的地方，用于对所需记录进行下游处理。第二种是**Amazon
    Kinesis Data Firehose**（**KDF**），它促进将这些记录传输到常见的分析环境，如Amazon S3、Elasticsearch、Redshift和Splunk。Firehose会自动缓冲所有流中的记录，并根据你配置的时间或数据大小阈值，或第一个达到的条件，将记录作为单个文件或一组记录刷新到目标位置。'
- en: The third is **Kinesis Data Analytics** (**KDA**), which performs analytics
    on stream records using Apache Flink. The output can subsequently flow into further
    streams you create to build an entire serverless streaming pipeline.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三种是**Kinesis Data Analytics**（**KDA**），它使用Apache Flink对流记录进行分析。输出结果随后可以流入你创建的进一步流，以构建一个完整的无服务器流处理管道。
- en: '**Amazon Managed Streaming for Kafka** (**MSK**): MSK is a fully managed, highly
    available, and secure service. Amazon MSK runs applications on Apache Kafka in
    the AWS cloud without needing Apache Kafka infrastructure management expertise.
    Amazon MSK provides a managed Apache Kafka cluster with a ZooKeeper cluster to
    maintain configuration and build a producer/consumer for data ingestion and processing.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Managed Streaming for Kafka**（**MSK**）：MSK是一个完全托管的、高可用且安全的服务。Amazon
    MSK在AWS云中运行基于Apache Kafka的应用程序，无需Apache Kafka基础设施管理的专业知识。Amazon MSK提供一个托管的Apache
    Kafka集群和一个ZooKeeper集群，用于维护配置并构建数据摄取和处理的生产者/消费者。'
- en: '**Apache Flink**: Flink is another open source platform for streaming data
    and batch data processing. Flink consists of a streaming dataflow engine that
    can process bounded and unbounded data streams. A bounded data stream has a defined
    start and end, while an unbounded data stream has a start but no end. Flink can
    perform batch processing on its streaming engine and supports batch optimizations.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Flink**：Flink是另一个开源平台，用于流数据和批量数据处理。Flink由一个流数据处理引擎组成，可以处理有界和无界数据流。一个有界数据流有明确的开始和结束，而无界数据流有开始但没有结束。Flink可以在其流引擎上执行批处理，并支持批优化。'
- en: '**Apache Spark Streaming**: Spark Streaming helps ingest live data streams
    with high throughput and a fault-tolerant, scalable manner. Spark Streaming divides
    the incoming data streams into batches before sending them to the Spark engine
    for processing. Spark Streaming uses DStreams, which are sequences of **resilient
    distributed datasets** (**RDDs**).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka**: Kafka is one of the most popular open source streaming platforms
    that helps you publish and subscribe to a data stream. A Kafka cluster stores
    a recorded stream in a Kafka topic. A producer can publish data in a Kafka topic,
    and consumers can take the output data stream by subscribing to the Kafka topic.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming storage needs to persist a continuous stream of data and provide the
    ability to maintain the order if required. You will learn more about streaming
    architecture in the upcoming section, *Streaming data architecture*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data storage in the cloud
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud data storage is a crucial aspect of modern IT infrastructure, offering
    scalability, flexibility, and cost-effectiveness. The leading cloud service providers
    – AWS, GCP, and Azure – provide various data storage options to cater to different
    needs, from simple file storage to complex databases and data warehousing solutions.
    The following lists the key characteristics of cloud data storage across these
    platforms.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Simple Storage Service (S3)**: This is a highly scalable object storage
    service known for its high data availability, security, and performance. Amazon
    S3 is versatile, perfect for storing any volume of data applicable in various
    scenarios like websites, mobile apps, backup and restoration, archival needs,
    enterprise applications, IoT devices, and big data analytics.'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Elastic Block Store (EBS)**: EBS offers block-level storage volumes
    for use with EC2 instances. It’s particularly suitable for data that demands consistent
    and low-latency performance, such as databases or ERP (Enterprise Resource Planning)
    systems.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Relational Database Service** **(RDS)**: RDS streamlines the setup,
    operation, and scaling of a relational database in the cloud. It offers a cost-effective
    solution with resizable capacity while automating many of the time-consuming tasks
    associated with database administration.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon S3 Glacier**: This service provides secure, durable, and low-cost
    cloud storage for archiving and long-term backup. Amazon S3 Glacier is ideal for
    storing data that is accessed infrequently, offering a solution for long-term
    data retention.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCP:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Cloud Storage**: This offers object storage for companies of all sizes.
    It’s highly scalable and flexible, providing secure and durable storage for high-demand
    applications and workloads.'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Persistent Disk**: This provides block storage for Google Compute Engine
    instances. It offers high-performance SSD and HDD storage that can be attached
    to instances running in Compute Engine or **Google Kubernetes Engine** (**GKE**).'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud SQL**: A fully managed database service that makes it easy to set up,
    maintain, manage, and administer relational databases on Google Cloud.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloud SQL**：一个完全托管的数据库服务，使得在 Google Cloud 上设置、维护、管理和操作关系型数据库变得更加简单。'
- en: '**Google Cloud Bigtable**: A scalable, fully managed NoSQL database service
    for large analytical and operational workloads.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Bigtable**：一个可扩展、完全托管的 NoSQL 数据库服务，适用于大规模的分析和操作工作负载。'
- en: 'Microsoft Azure:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Azure：
- en: '**Azure Blob Storage**: This is Azure’s object storage solution designed for
    the cloud. It excels at storing large amounts of unstructured data, such as text
    or binary data. This includes various types of content like documents, media files,
    backups, and logs, making it highly versatile for a wide range of uses.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Blob 存储**：这是 Azure 的面向云的对象存储解决方案。它在存储大量非结构化数据（如文本或二进制数据）方面表现卓越。它包括各种类型的内容，如文档、媒体文件、备份和日志，使其在广泛的使用场景中具有高度的灵活性。'
- en: '**Azure File Storage**: Offers cloud-based, fully managed file shares that
    are accessible using the standard SMB protocol. This service is particularly useful
    for businesses looking to migrate their existing on-premises file shares to the
    cloud environment.'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure 文件存储**：提供基于云的、完全托管的文件共享，使用标准的 SMB 协议进行访问。此服务对于希望将现有本地文件共享迁移到云环境的企业尤其有用。'
- en: '**Azure SQL Database**: A comprehensive, fully managed relational database
    service in the cloud. It provides the capabilities of SQL Server, but without
    the need for extensive infrastructure and database administration tasks, simplifying
    database management.'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure SQL 数据库**：一个全面的、完全托管的云关系型数据库服务。它提供了 SQL Server 的功能，但无需大量的基础设施和数据库管理任务，从而简化了数据库管理。'
- en: '**Azure Disk Storage**: This delivers high-performance, reliable block storage
    for Azure Virtual Machines. Azure Disk Storage includes both SSD and HDD options,
    catering to a range of requirements from high-speed performance to cost efficiency.'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure 磁盘存储**：这提供了高性能、可靠的块存储，用于 Azure 虚拟机。Azure 磁盘存储包括 SSD 和 HDD 选项，满足从高速性能到成本效益等各种需求。'
- en: Cloud data storage services across these platforms are designed to provide secure,
    scalable, and accessible storage solutions, accommodating various applications
    and use cases. Each service has its specific strengths, making them suitable for
    different performance, scalability, data access, and cost requirements.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 跨这些平台的云数据存储服务旨在提供安全、可扩展且易于访问的存储解决方案，适应各种应用和使用场景。每项服务都有其特定的优势，适用于不同的性能、可扩展性、数据访问和成本要求。
- en: Once you ingest and store data, processing the data in the desired structure
    is essential to visualize and analyze it for business insights. Let’s learn more
    about data processing and transformation.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你采集并存储了数据，按所需结构处理数据是至关重要的，这样才能可视化并分析这些数据，从而得出商业洞察。让我们进一步了解数据处理和转换。
- en: Processing data and performing analytics
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理和执行分析
- en: Data analytics is the process of ingesting, transforming, and visualizing data
    to discover valuable insights for business decision making. Over the previous
    decade, more data has been collected than ever before, and customers are looking
    for greater insights into their data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析是将数据采集、转换和可视化的过程，用于发现有价值的商业决策洞察。在过去的十年里，收集的数据比以往任何时候都多，客户希望能从他们的数据中获得更多的洞察。
- en: These customers also want these insights in the least amount of time, sometimes
    even in real time. They want more ad hoc queries to answer more business questions.
    To answer these questions, customers need more powerful and efficient systems.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这些客户还希望在最短的时间内获得这些洞察，有时甚至是实时的。他们希望能通过更多的即席查询来回答更多的商业问题。为了回答这些问题，客户需要更强大和高效的系统。
- en: Batch processing typically involves querying large amounts of cold data. In
    batch processing, it may take hours to get answers to business questions. For
    example, you may use batch processing to generate a billing report at the end
    of the month. Stream processing in real time typically involves querying small
    amounts of hot data, and it takes only a short amount of time to get answers.
    MapReduce-based systems such as Hadoop are examples of platforms that support
    the batch jobs category, while data warehouses are examples of platforms that
    support the query engine category.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理通常涉及查询大量的冷数据。在批处理过程中，可能需要数小时才能回答业务问题。例如，你可能会使用批处理在月末生成账单报告。实时流处理通常涉及查询少量的热数据，并且只需很短的时间就能得到答案。基于
    MapReduce 的系统（如 Hadoop）是支持批处理作业的平台示例，而数据仓库则是支持查询引擎平台的示例。
- en: Streaming data processing activities ingest a data sequence and incrementally
    update functions in response to each data record. Typically, they ingest continuously
    produced streams of data records, such as metering data, monitoring data, audit
    logs, debugging logs, website clickstreams, and location-tracking events for devices,
    people, and physical goods.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据处理活动摄取数据序列，并根据每条数据记录逐步更新功能。通常，它们会摄取持续产生的数据流记录，如计量数据、监控数据、审计日志、调试日志、网站点击流和设备、人员及实物商品的定位追踪事件。
- en: 'The following diagram illustrates a data lake pipeline for processing, transforming,
    and visualizing data using the AWS cloud tech stack:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个数据湖流水线，利用 AWS 云技术栈来处理、转换和可视化数据：
- en: '![](img/B21336_12_05.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21336_12_05.png)'
- en: 'Figure 12.5: Data lake ETL pipeline for big data processing'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5：用于大数据处理的数据湖 ETL 流水线
- en: Here, the ETL pipeline uses Amazon Athena for ad hoc querying of data stored
    in Amazon S3\. The data ingested from various data sources (for example, web application
    servers) generates log files that persist into S3\. These files are then transformed
    and cleansed into a set form required for meaningful insights using Amazon **Elastic
    MapReduce** (**EMR**) and loaded into Amazon S3\. Amazon EMR provides a managed
    Hadoop server in the cloud to perform data processing using various open source
    technologies such as **Hive**, **Pig**, and **Spark**.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ETL 流水线使用 Amazon Athena 对存储在 Amazon S3 中的数据进行临时查询。来自各种数据源（例如，Web 应用服务器）摄取的数据生成日志文件，这些日志文件会持久化到
    S3 中。然后，这些文件通过使用 Amazon **Elastic MapReduce**（**EMR**）进行转换和清洗，形成一个有意义洞察所需的统一格式，并加载到
    Amazon S3 中。Amazon EMR 提供了一个托管的 Hadoop 服务器，用于在云中使用各种开源技术（如 **Hive**、**Pig** 和
    **Spark**）进行数据处理。
- en: These transformed files are loaded into Amazon Redshift using the `COPY` command
    and visualized using Amazon QuickSight. Using Amazon Athena, you can query the
    data directly from Amazon S3 when the data is stored and after transformation
    (with aggregated datasets). You can visualize the data from Athena in Amazon QuickSight.
    You can easily query these files without changing your existing dataflow.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换后的文件通过 `COPY` 命令加载到 Amazon Redshift 中，并使用 Amazon QuickSight 进行可视化。通过使用 Amazon
    Athena，你可以在数据存储和转换后（包括汇总数据集）直接从 Amazon S3 查询数据。你可以通过 Amazon QuickSight 可视化 Athena
    中的数据。你可以轻松查询这些文件，而无需改变现有的数据流。
- en: Let’s look at some popular tools for data processing.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些流行的数据处理工具。
- en: Technology choices for data processing and analysis
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据处理与分析的技术选择
- en: 'The following are some of the most popular data processing technologies that
    help you to perform transformation and processing for a large amount of data:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最受欢迎的数据处理技术，它们帮助你对大量数据进行转换和处理：
- en: '**Apache Hadoop** uses a distributed processing architecture in which a task
    is mapped to a cluster of commodity servers for processing. Each piece of work
    distributed to the cluster servers can be run or re-run on any server. The cluster
    servers frequently use the HDFS to store data locally for processing. The Hadoop
    framework takes a big job, splits it into discrete tasks, and processes them in
    parallel. It allows for massive scalability across an enormous number of Hadoop
    clusters. It’s also designed for fault tolerance, where each worker node periodically
    reports its status to a primary node, and the primary node can redistribute work
    from a cluster that doesn’t respond positively. Some of the most popular frameworks
    used with Hadoop are **Hive**, **Presto**, **Pig**, and **Spark**.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Spark** is an in-memory processing framework. Apache Spark is a massively
    parallel processing system with different executors that can take apart a Spark
    job and run tasks in parallel. To increase the parallelism of a job, add nodes
    to the cluster. Spark supports batch, interactive, and streaming data sources.
    Spark uses **directed acyclic graphs** (**DAGs**) for all the stages during the
    execution of a job. The DAGs can keep track of your data or lineage transformations
    during the jobs and efficiently minimize the I/O by storing the DataFrames in
    memory. Spark is also partition-aware to avoid network-intensive shuffles.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop User Experience** (**HUE**) enables you to run queries and scripts
    on your cluster through a browser-based **user interface** (**UI**) instead of
    the command line. HUE provides the most common Hadoop components in a UI. It enables
    browser-based viewing and tracking of Hadoop operations. Multiple users can access
    the cluster via HUE’s login portal, and administrators can manage access manually
    or with **Lightweight Directory Access Protocol** (**LDAP**), **Pluggable Authentication
    Modules** (**PAM**), **Simple and Protected GSSAPI Negotiation Mechanism** (**SPNEGO**),
    OpenID, OAuth, and **Security Assertion Markup Language 2.0** (**SAML2**) authentication.
    HUE allows you to view logs in real time and provides a metastore manager to manipulate
    Hive metastore contents.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pig** is typically used to process large amounts of raw data before storing
    it in a structured format (SQL tables). Pig is well suited to ETL operations such
    as data validation, loading, transformation, and combining data from multiple
    sources in multiple formats. In addition to ETL, Pig supports relational operations
    such as nested data, joins, and grouping. Pig scripts can input unstructured and
    semi-structured data (such as web server logs or clickstream logs). In contrast,
    Hive consistently enforces a schema on input data. Pig Latin scripts contain instructions
    on filtering, grouping, and joining data, but Pig is not intended to be a query
    language. Hive is better suited to querying data. The Pig script compiles and
    runs to transform the data based on the instructions in the Pig Latin script.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hive** is an open source data warehouse and query package that runs on top
    of a Hadoop cluster. Being able to use SQL is a skill that helps the team easily
    transition into the big data world. Hive uses a SQL-like language called **Hive
    Query Language** (**HQL**), making it easy to query and process data in a Hadoop
    system. Hive abstracts the complexity of writing programs in a coding language
    like Java to perform analytics jobs.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Presto** is a Hive-like query engine, but it is much faster. It supports
    the **American National Standards Institute** (**ANSI**) SQL standard, which is
    easy to learn and the most popular skill set. Presto supports complex queries,
    joins, and aggregation functions. Unlike Hive or MapReduce, Presto executes queries
    in memory, which reduces latency and improves query performance. You need to be
    careful while selecting the server capacity for Presto, as it needs to have high
    memory. A Presto job will restart in the event of memory spillover.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HBase** is a NoSQL database developed as a part of the open source Hadoop
    project. HBase runs on the HDFS to provide non-relational database capabilities
    for the Hadoop ecosystem. HBase helps to store large quantities of data in a columnar
    format with compression. Also, it provides a fast lookup because large portions
    of the data cache are kept in memory while cluster instance storage is still used.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Zeppelin** is a web-based editor for data analytics built on top of
    the Hadoop system, also known as a Zeppelin notebook. It uses the concept of an
    interpreter for its backend language and allows any language to be plugged into
    Zeppelin. Apache Zeppelin includes some basic charts and pivot charts. It’s very
    flexible in terms of any output from any language backend that can be recognized
    and visualized.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ganglia** is a Hadoop cluster monitoring tool. However, you need to install
    Ganglia on the cluster during launch. The Ganglia UI runs on the primary node,
    which you can see using an SSH tunnel. Ganglia is an open source project designed
    to monitor clusters without impact on their performance. Ganglia can help to inspect
    the performance of the individual servers in your cluster and the performance
    of clusters as a whole.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JupyterHub** is a multi-user Jupyter notebook. Jupyter Notebook is one of
    the most popular tools among data scientists to perform data engineering and ML.
    The JupyterHub notebook server provides each user with a Jupyter Notebook web-based
    IDE. Multiple users can use their Jupyter notebooks simultaneously to write and
    execute code for exploratory data analytics.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing in the cloud
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data processing in the cloud is a fundamental aspect of modern big data and
    analytics strategies. Three major cloud service providers—AWS, GCP, and Azure—offer
    various data processing services, each with unique features and capabilities.
    The following are some unique features of each of them:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS Data Processing Services:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon EMR**: This provides a cloud-native Hadoop environment, supporting
    a wide range of big data frameworks like Apache Spark, Hadoop, HBase, and Presto.
    EMR is ideal for processing large datasets, and it offers flexibility by separating
    compute and storage, allowing for cost-efficient scaling.'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Glue**: This is a fully managed ETL service that simplifies data preparation
    for analytics. It automates the cumbersome data preparation work, generates ETL
    scripts, and facilitates data movement between various AWS services. Glue is particularly
    effective for data cataloging and job scheduling.'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Athena**: A serverless, interactive query service that allows SQL
    queries directly on data stored in Amazon S3\. It is highly useful for ad hoc
    data analysis and BI querying, with no infrastructure management required.'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCP Data Processing Services:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google BigQuery**: This is a fully managed, serverless data warehouse solution
    designed for rapid, cost-efficient SQL querying across extensive datasets. BigQuery
    is particularly geared towards real-time analytics and is capable of handling
    streaming data effectively.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Dataflow**: A fully managed service dedicated to processing data in
    both stream and batch modes. Built on Apache Beam, Cloud Dataflow offers a unified
    programming model, simplifying the development of parallel data processing pipelines.
    It’s adept at handling a range of tasks from complex ETL processes to batch and
    real-time streaming workloads.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Dataprep**: An advanced data service that allows users to visually
    explore, clean, and prepare both structured and unstructured data for analysis.
    Seamlessly integrated with BigQuery and Cloud Dataflow, Cloud Dataprep enhances
    the capabilities of data exploration and transformation.'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azure Data Processing Services:'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure HDInsight**: A fully managed cloud service that makes it easy to process
    massive amounts of data with popular open source frameworks such as Apache, Hadoop,
    Spark, Kafka, and HBase. It suits various scenarios like ETL, data warehousing,
    ML, and IoT.'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Databricks**: A fast, easy, and collaborative Apache Spark-based analytics
    platform. It integrates deeply with other Azure services and provides a unified
    platform for ETL processes, streaming analytics, ML, and data warehousing.'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Synapse Analytics**: This is a comprehensive analytics service that
    merges the capabilities of big data and data warehousing. It provides a cohesive
    experience for ingesting, preparing, managing, and delivering data for instant
    BI and ML applications. Azure Synapse Analytics enables the simultaneous querying
    of both data lakes and databases, streamlining data analysis processes.'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each cloud provider’s data processing services are designed to meet specific
    needs in the data life cycle, from processing and transforming large datasets
    to interactive querying and real-time analytics. This diversity ensures businesses
    can choose the most suitable tools and platforms according to their specific data
    processing requirements and objectives.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and processing are huge topics that warrant a book on their own.
    This section gave a high-level overview of popular and common tools used for data
    processing. There are many more proprietary and open source tools available. As
    a solutions architect, you must be aware of various available tools to make the
    right choice for your organization’s use case.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Business analysts need to create reports and dashboards and perform ad hoc queries
    and analyses to identify data insights. Let’s learn about data visualization in
    the next section.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data insights are used to answer important business questions such as revenue
    by customer, profit by region, or advertising referrals by site, among many others.
    In the big data pipeline, enormous amounts of data are collected from various
    sources. However, it is difficult for companies to find information about inventory
    per region, profitability, and increases in fraudulent account expenses. Some
    of the data you continuously collect for compliance purposes can also be leveraged
    for generating business.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The two significant challenges of BI tools are the cost of implementation and
    the time it takes to implement a solution. Let’s look at some technology choices
    for data visualization.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Technology choices for data visualization
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some of the most popular data visualization platforms, which
    help you prepare reports with data visualization as per your business requirements:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon QuickSight** is a cloud-based BI tool for enterprise-grade data visualizations.
    It comes with a variety of visualization graph presets such as line graphs, pie
    charts, treemaps, heat maps, and histograms. Amazon QuickSight has a data-caching
    engine known as a **Super-fast, Parallel, In-memory Calculation Engine** (**SPICE**),
    which helps render visualizations quickly. You can also perform data preparation
    tasks such as renaming and removing fields, changing data types, and creating
    new calculated fields. QuickSight also provides ML-based visualization insights
    and other ML-based features, such as auto forecast predictions.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana** is an open source data visualization tool for stream data visualization
    and log exploration. Kibana offers close integration with Elasticsearch and uses
    it as a default option to search for data on top of the Elasticsearch service.
    Like other BI tools, Kibana also provides popular visualization charts such as
    histograms, pie charts, and heat maps, and offers built-in geospatial support.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tableau** is one of the most popular BI tools for data visualization. It
    uses a visual query engine, which is a purpose-built engine, to analyze big data
    faster than traditional queries. Tableau offers a drag-and-drop interface and
    the ability to blend data from multiple resources.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spotfire** uses in-memory processing for faster response times, enabling
    extensive datasets from various resources. It allows you to plot your data on
    a geographical map and share it on Twitter. With Spotfire recommendations, it
    inspects your data automatically and suggests how to visualize it best.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaspersoft** enables self-service reporting and analysis. It also offers
    drag-and-drop designer capabilities.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power BI** is a popular BI tool provided by Microsoft. It provides self-service
    analytics with a variety of visualization choices.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization is an essential and massive topic for solutions architects.
    As a solutions architect, you need to be aware of the available tools and make
    the right choice per your business requirements for data visualization.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: You have learned about various data pipeline components, from ingestion, storage,
    and processing, to visualization. In the next section, let’s put them together
    and learn how to orchestrate a big data architecture.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Designing big data architectures
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data solutions are comprised of data ingestion, storage transformation,
    data processing, and visualization in a repeated manner to run daily business
    operations. You can build these workflows using the open source or cloud technologies
    you learned about in previous sections.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to learn which architectural style is right for you by working
    backward from the business use case. You need to understand the end user of your
    big data architecture and create a user persona to understand the requirements
    better. To identify the key personas you are targeting with big data architecture,
    you need to understand some of the following points:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Which teams, units, or departments inside your organization are they a part
    of?
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is their level of data analysis and data engineering proficiency?
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What tools do they typically use?
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you need to cater to the organization’s employees, customers, or partners?
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For your reference, taking an example of a retail store chain analysis, you
    may identify the following personas:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The **product manager** persona, who owns a product line/code but only sees
    turnover for their product.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **store manager** persona, who wants to know the sales turnover and product
    mix for a single store (only able to see their store).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **admin** persona, who wants to have access to all data.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **data analyst**, who wants to access all data with PII data redacted.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **customer retention managers**, who want to understand repeated customer
    traffic.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data scientists** need access to raw and processed data to build recommendations
    and forecasts.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have a clear understanding of your user persona, the next step is
    to identify the business use cases these personas aim to address. Some examples
    include:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer spending trends**: Analyze how many customers are increasing or
    decreasing their spending over time. Characterize these customers based on their
    spending patterns.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Growth categories among higher spenders**: Identify which product or service
    categories are witnessing faster growth among customers who are spending more
    over time.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decline categories among lower spenders**: Determine the categories where
    there is a noticeable decline in engagement among customers who are spending less
    over time.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impact of demographics on spending**: Investigate which demographic factors,
    such as household size, presence of children, or income level, influence customer
    spending habits. Also, assess which demographic factors impact engagement with
    specific product or service categories.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Effectiveness of direct marketing**: Explore whether there is evidence to
    suggest that direct marketing campaigns lead to improved overall customer engagement.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-category impact of direct marketing**: Assess whether direct marketing
    efforts in one category have a positive effect on customer engagement in other
    categories.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While you get details on the use case, the essential aspect of building your
    data architecture is to understand access patterns and data retention, which can
    be analyzed by using the following queries:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: How often do key users and personas run their reports, queries, or models?
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is their expectation for data freshness?
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is their expectation of data granularity?
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What portion of data is most frequently accessed for analysis?
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long do you intend to retain data for analysis?
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At what point can data age out of the data lake environment?
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is always some sensitivity attached when you deal with data. Each country
    and area has its local regulatory compliance requirements, which solutions architects
    need to understand, such as:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: What compliance requirements does your business have?
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you subject to data locality, privacy, or redaction requirements?
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who is authorized to see which records and which attributes in the dataset?
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will you enforce the deletion of records on request?
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where can you store data, for example, local to geolocation, county, or global?
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a data architect, you must also consider the return on investment and how
    it will help overall business decisions. To understand, you may want to go through
    the following points:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: What primary business processes and decisions does your data lake support?
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What level of granularity is required for these decisions?
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the impact of data latency on business decisions?
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you plan to measure success?
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the expected return on the time and material invested?
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, you want to build a data architecture where you can provide flexibility
    to make technology choices, for example, by using the best of cloud-based managed
    services and open source technologies to capitalize on existing skills and investments.
    You want to build big data solutions to use parallelism to achieve high performance
    and scalability. It is best to make sure any components of your big data pipeline
    can scale in or scale out independently so that you can adjust it according to
    different business workloads.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: To utilize the full potential of your solution, you want to provide interoperability
    with existing applications so that components of the big data architecture are
    also used for ML processing and enterprise BI solutions. It will enable you to
    create an integrated solution across data workloads. Let’s learn about some big
    data architecture patterns.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Data lake architecture
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A data lake serves as a centralized repository that accommodates both structured
    and unstructured data, encompassing the diverse data types present in a corporation.
    It has emerged as a solution for transferring all enterprise data into a cost-effective
    storage system, like Amazon S3\. In a data lake, data can be accessed through
    generic APIs and open file formats, including Apache Parquet and **Optimized Row
    Columnar** (**ORC**). This storage method preserves data in its original form,
    utilizing open source file formats, thereby facilitating direct analytics and
    ML applications.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'The data lake is becoming a popular way to store and analyze large volumes
    of data in a centralized repository. Data can be stored as is in its current format,
    and you don’t need to convert data into a predefined schema, which increases the
    data ingestion speed. As illustrated in the following diagram, the data lake is
    a single source of truth for all data in your organization:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21336_12_06.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Object store for data lake'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the benefits of a data lake:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion from various sources**: Data lakes let you store and analyze
    data from multiple sources, such as relational and non-relational databases and
    streams, in one centralized location for a single source of truth. This answers
    questions such as *Why is the data distributed in many places?* and *Where is
    the single source of truth?*'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collecting and efficiently storing data**: A data lake can ingest any data
    structure, including semi-structured and unstructured data, without needing schema.
    This answers questions such as: *How can I ingest data quickly from various sources
    and in multiple formats and store it efficiently at scale?*'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale up with the volume of generated data**: Data lakes allow you to separate
    the storage and compute layers to scale each component separately. This answers
    questions such as: *How can I scale up with the volume of data generated?*'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applying analytics to data from different sources**: With a data lake, you
    can determine the schema on reading and create a centralized data catalog on data
    collected from various resources. This enables you to perform quick ad hoc analysis.
    This answers questions such as: *Can I apply multiple analytics and processing
    frameworks to the same data?*'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It would help if you had an unlimited scalable data storage solution for your
    data lake. Decoupling your processing and storage provides many benefits, including
    the ability to process and analyze the same data with various tools. Although
    this may require an additional step to load your data into the right tool, Amazon
    S3, as your central data store, provides even more benefits than traditional storage
    options. The following diagram provides a view of the data lake using AWS services:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21336_12_07.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Data lake architecture in the AWS platform'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram depicts a data lake using Amazon S3 storage. Data is ingested
    into centralized storage from various resources such as relational databases and
    master data files. In the data lake’s raw layer, all data is kept in its original
    format. This data then undergoes cataloging and transformation via the AWS Glue
    service. AWS Glue is a serverless solution for data cataloging and ETL processes,
    built on the Spark framework within the AWS cloud platform. Here, the AWS Glue
    crawler helps in cataloging data stores. It automatically scans your data sources,
    identifies data formats, and infers schemas, creating and populating a data catalog
    with metadata information. The crawler classifies the data to understand its format
    and structure and creates table definitions in the data catalog, which makes it
    easy to build queries for data analytics. Once transformed, this data is stored
    in the data lake’s processed layer, making it available for various consumption
    purposes.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers can run ad hoc queries using Amazon Athena, a serverless query
    service built on top of managed Presto instances, and use SQL to query the data
    directly from Amazon S3\. Business analysts can use Amazon QuickSight, Tableau,
    or Power BI to build visualizations for business users or load selective data
    in Amazon Redshift to create a data warehouse mart. Finally, data scientists can
    consume this data using Amazon SageMaker to perform ML.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: One tool cannot do everything. You need to use the right tool for the right
    job, and data lakes enable you to build a highly configurable big data architecture
    to meet your specific needs. Business problems must be narrower, deeper, and more
    complex for one tool to solve everything, especially big data and analytics.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: However, with time, organizations realized that data lakes have their limitations.
    As data lakes use cheap storage, organizations store as much of their data as
    they can in data lakes, providing the flexibility of open, direct access to files.
    Quickly, data lakes started becoming **data swamps** due to data quality issues
    and granular data security. However, to address the data lake’s performance and
    quality issues, organizations process a small subset of data in the data lake
    to a downstream data warehouse to use in BI applications for important decisions.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The dual system architecture between a data lake and a data warehouse requires
    continuous data engineering to maintain and process data between these two systems.
    Each step in data processing carries the risk of failures that can compromise
    data quality. Additionally, maintaining consistency between the data lake and
    the data warehouse can be both challenging and expensive. Users face the burden
    of paying double for storage—once for the data stored in the lake and again for
    data replicated in the warehouse. This is in addition to the ongoing costs associated
    with continuous data processing.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: To address the dual-system problem, a new type of architecture called the data
    lakehouse has been discovered. Let’s learn more about lakehouse architecture.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Lakehouse architecture
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The lakehouse architecture has emerged as a solution to bridge the gaps between
    traditional data lakes and data warehouses, integrating the strengths of both.
    This architecture is designed to harness the expansive storage capacity of data
    lakes for ingesting and keeping vast quantities of data in open formats, which
    are essential for analytics. Simultaneously, it aims to provide the ease of SQL-based
    querying and the reliability associated with data warehouses. Key characteristics
    of lakehouse architecture include:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '**Data storage in open-data formats**: Lakehouse architecture stores data in
    open formats, facilitating interoperability and flexibility in data processing
    and analytics.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoupled storage and compute**: It separates storage and computing resources,
    allowing independent scaling and optimization of each, leading to cost efficiency
    and performance improvement.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transactional guarantees**: Ensuring data integrity, lakehouse architecture
    provides transactional guarantees, akin to those in traditional database systems,
    supporting reliable concurrent access and modifications.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for diverse consumption needs**: Designed to cater to a wide range
    of data consumption requirements, lakehouse architecture accommodates different
    data analytics and processing approaches, from batch to real-time streaming.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure and governed**: The architecture emphasizes security and governance,
    ensuring that data access is controlled, and compliance with data privacy regulations
    is maintained.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unified platform**: Lakehouse architecture provides a unified platform for
    various data operations, from ETL processes and ML to BI and reporting, eliminating
    the need for disparate systems.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced query performance**: By leveraging techniques like indexing, caching,
    and data clustering, lakehouse architecture improves query performance, making
    it suitable for complex analytical workloads.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effective scalability**: The architecture offers cost-effective scalability
    options, balancing the need for performance with budgetary constraints, especially
    beneficial for growing data volumes.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible data management**: Lakehouse architecture supports flexible data
    management practices, accommodating evolving data schemas and structures, making
    it ideal for agile and evolving business environments.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lakehouse architecture represents a significant evolution in data management,
    offering a comprehensive, scalable, and efficient approach to handling vast and
    diverse datasets while ensuring data integrity, security, and easy accessibility.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram shows a sample lakehouse architecture using Redshift Spectrum
    for data sharing. Amazon Redshift Spectrum provides the ability to query data
    from the data lake without storing data in the data warehouse. Suppose you were
    already using Amazon Redshift for data warehousing. In that case, you don’t need
    to load all the data into the Amazon Redshift cluster. Still, you can use Spectrum
    to query data directly from the Amazon S3 data lake and combine it with data warehouse
    data.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21336_12_08.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: Lakehouse architecture in the AWS cloud platform using Redshift
    Spectrum'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Data is ingested from an on-premises **enterprise data warehouse** (**EDW**)
    into S3 using the S3 API in the preceding diagram. AWS Glue stores the metadata
    and the credit and loan data individually. Data analysts in the loan department
    would be granted read-only access to the loan data for data access. Similarly,
    credit analysts would be granted read-only access to the credit data. For data
    sharing, if a credit analyst needs access to the loan data, the credit analyst
    can be given the loan data’s read-only schema.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Lakehouse architecture has benefits; however, more is needed for large organizations
    with a complex application landscape driven by geographically separated business
    units. These business units have built data lakes and warehouses as their analytical
    sources. Each business unit may merge multiple internal application data lakes
    to support their business. Centralized enterprise data lakes or data lakehouses
    are challenging to achieve as the pace of change is generally low, and it isn’t
    easy to meet all requirements across different business units. To handle this
    problem, you need domain-oriented decentralized data ownership and architecture.
    That’s where data mesh comes into the picture. Let’s learn more about data mesh
    architecture.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Data mesh architecture
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The major difference between data mesh and data lake architecture is that data
    is intentionally left distributed rather than trying to combine multiple domains
    into a centrally managed data lake. Data mesh provides a pattern that allows a
    large organization to connect multiple data lakes/lakehouses within large enterprises
    and facilitate sharing with partners, academia, and even competitors.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Data mesh represents a significant shift in both architecture and organizational
    approaches toward managing extensive analytical datasets. It is built upon four
    fundamental principles:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain-oriented decentralization of ownership and architecture**: This principle
    emphasizes decentralizing data ownership and architecture decisions to specific
    business domains. It encourages individual domains to take responsibility for
    their data, leading to more tailored and effective data solutions.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data served as a product**: Treating data as a product means it is maintained,
    improved, and presented with the end user in mind. It shifts the focus from data
    as a mere resource to a valuable asset that provides utility and solves user problems.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Federated data governance with centralized audit controls**: This principle
    strikes a balance between decentralized data management and the need for overarching
    governance. It allows for domain-specific data governance while maintaining centralized
    controls for auditing and compliance.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common access that makes data consumable**: Ensuring data is accessible and
    usable across the organization, this principle focuses on creating a common framework
    that enables easy and efficient data consumption.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It encourages data-driven agility and supports domain-local governance through
    a lightweight centralized policy. Data mesh provides better ownership by isolating
    data resources with clear accountability. The core concept of data mesh is to
    feature data domains as nodes in data lake accounts.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'A data producer contributes one or more data products to a central catalog
    in a data mesh account where federated data governance is applied to sharing data
    products, delivering discoverable metadata and audibility. A data consumer searches
    for a catalog and gains access to a data product by accepting a resource share
    via the data mesh pattern. The following is a data mesh architecture in the AWS
    cloud:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21336_12_09.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Data mesh architecture on the AWS cloud platform'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the components implemented to build a data mesh, as shown
    in the preceding diagram:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: The central AWS account is where data products are registered, comprising databases,
    tables, columns, and rows.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access control tags and tag access policies are managed centrally.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It stores data permissions that implement sharing with a consumer. Permissions
    can be direct or based on tags.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applies security and governance policies to producer and consumer accounts and
    their published data products.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a data mesh architecture, you can accelerate the independent delivery of
    the business domain lakehouses. Data mesh increases data security and compliance
    within domains and enables self-service data product creation, discovery, and
    subscription, allowing consumers to access data products transparently. There
    is a growing need to provide fast insight and act quickly based on customer needs,
    which makes streaming data analytics an essential aspect of any business. Let’s
    learn more details about streaming data analytics architecture.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data architecture
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Streaming data, a rapidly expanding segment of data, requires the capability
    to ingest and swiftly process real-time data from a variety of sources. These
    sources include video, audio, application logs, website clickstreams, and IoT
    telemetry data, all aimed at delivering prompt business insights. The typical
    use cases for streaming data follow a consistent pattern:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '**Data generation**: Sources continuously produce data.'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ingestion**: This data is then delivered through an ingestion stage to a
    streaming storage layer.'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stream storage**: In this layer, the incoming data is durably captured and
    made accessible for real-time processing.'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stream processing**: Here, the data residing in the storage layer is processed.
    This processing might involve filtering, aggregating, or analyzing the data as
    it streams.'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data output**: The processed data is then dispatched to a designated destination,
    which could be a database, a data lake, or another storage solution, for further
    use or long-term storage.'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This flow ensures that data is not only captured as it is generated but also
    processed in a timely manner, leading to quicker decision making and more immediate
    business insights.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data architecture is different as it needs to process a continuous
    massive data stream with very high velocity. Often, this data is semi-structured
    and needs a lot of processing to get actionable insights. While designing streaming
    data architecture, you need to quickly scale data storage while getting real-time
    pattern identification from time-series data.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be best to think about the producer who generated a stream of data,
    such as IoT sensors, how to store and process the data using a real-time data
    processing tool, and finally, how to query the data in real time. The following
    diagram shows a streaming data analytics pipeline using a managed service on the
    AWS platform:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21336_12_10.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Streaming data analytics for IoT data'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, data is ingested from the wind farm to understand
    the health and speed of a wind turbine. It’s important to control wind turbines
    in real time to avoid costly repairs in the case of high wind speeds beyond the
    wind turbine’s limit.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: The wind turbine data is ingested into Kinesis Data Streams using AWS IoT Greengrass.
    Kinesis Data Streams can retain the streaming data for up to a year and provide
    replay capability. These are subjected to the fan-out technique to deliver the
    data to multiple resources, where you can message data using Lambda and store
    it in Amazon S3 for further analytics using Amazon Kinesis Firehose.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: You can perform real-time queries on streaming data using simple SQL queries
    with Kinesis Data Analytics for SQL and you can automate a data pipeline to transform
    streaming data in real time using Kinesis Data Analytics for Java Flink and store
    the processed data in Amazon OpenSearch to get data insights. You can also add
    Kibana to OpenSearch to visualize the wind turbine data in real time.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: The preceding solution is data agnostic and easily customizable, enabling customers
    to quickly modify pre-configured defaults and start writing code to include their
    specific business logic.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right big data architecture
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Choosing between data lake, lakehouse, and data mesh architectures depends
    on your specific business requirements, data strategy, and technical capabilities.
    Each architecture offers unique benefits and is suited for different data management
    and analytics scenarios. To aid in making the right choice, the following list
    highlights the benefits, important considerations, and ideal use cases for each
    type of architecture:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '**Data lake architecture**: A data lake is primarily intended for the storage
    of large volumes of raw data in its original format.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Benefits**: It provides high scalability and flexibility in handling various
    data types. It’s cost-effective for storing large amounts of data and can be used
    as a central repository for all organizational data.'
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Considerations**: Without proper governance, data lakes can become unmanageable
    (“data swamps”). They require careful management to ensure data quality and accessibility.'
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use cases**: It is suitable for big data analytics, ML, and situations where
    you need to store and analyze large volumes of diverse data at a low cost. It
    is particularly suitable for situations where there is a requirement to store
    diverse types of data – including structured, semi-structured, and unstructured
    – without having a predetermined schema at the point of data entry.'
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lakehouse architecture**: This combines elements of both data lakes and data
    warehouses.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Benefits**: It aims to provide the low-cost scalability of data lakes with
    the robust schema and performance optimization of data warehouses. It offers a
    unified platform for all types of data processing and analytics, reducing data
    silos. It also supports ACID transactions and schema enforcement, improving data
    reliability and quality.'
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Considerations**: Implementing a lakehouse architecture can be complex, requiring
    integrating various components and ensuring consistency and reliability across
    different workloads.'
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use cases**: It is best for organizations requiring big data processing and
    traditional BI from a single platform. It’s ideal for use cases that need real-time
    analytics and reporting on large and diverse datasets.'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data mesh architecture**: It focuses on decentralizing the data architecture
    and ownership. It treats data as a product, with domain-oriented teams owning
    and providing their data as products to the rest of the organization.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Benefits**: It encourages a more agile and flexible data management and analytics
    approach. It also promotes data democratization, allowing for faster decision
    making and innovation within domains.'
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Considerations**: It requires a cultural shift in how data is managed and
    shared. It demands strong governance and standardization across domains to ensure
    data interoperability and quality.'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use cases**: It is suitable for large organizations with multiple independent
    teams or departments, where different domains produce and consume data.'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some key decision factors:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '**Organizational structure**: Consider whether your organization is centralized
    or decentralized. Data mesh is more suitable for the latter.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data volume and variety**: Data lakes are ideal for massive, diverse datasets,
    while lakehouses provide a more structured environment for such data.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytical needs**: A lakehouse might be the best fit if you need real-time
    analytics combined with big data processing.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Governance and compliance**: Assess your data governance, quality, and compliance
    needs. A lakehouse architecture tends to offer more robust governance mechanisms.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical expertise**: Implementing and managing a data mesh or lakehouse
    architecture requires specific technical expertise and resources.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, the choice depends on aligning the architecture with your business
    goals, technical capabilities, and data strategy. Each architecture has its strengths,
    and the best choice may even be a hybrid approach, depending on your specific
    requirements.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Big data architecture best practices
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You learned about various big data technology and architecture patterns in previous
    sections. Let’s look at the following reference architecture diagram with different
    layers of a data lake architecture to learn best practices.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21336_12_11.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Data lake reference architecture'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram depicts an end-to-end data pipeline in a data lake architecture
    using the AWS cloud platform with the following components:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: AWS Direct Connect will set up a high-speed network connection between the on-premises
    data center and AWS to migrate data. If you have large volumes of archive data,
    using the AWS Snow family to move it offline is better.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data ingestion layer with various components to ingest streaming data using
    Amazon Kinesis, relational data using AWS **Data Migration Service** (**DMS**),
    secure file transfer using AWS Transfer for **Secure Shell File Transfer Protocol**
    (**SFTP**), and AWS DataSync to update data files between cloud and on-premises
    systems.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized data storage for all data using Amazon S3, where data storage has
    multiple layers to store raw data, processed data, and archive data.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift is a cloud-native data warehouse solution with Redshift Spectrum
    to support lakehouse architecture.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ad hoc query functionality using Amazon Athena.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick ETL pipeline based on Spark using AWS Glue.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon EMR will re-utilize existing Hadoop scripts and other Apache Hadoop frameworks.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Lake Formation to build comprehensive data cataloging and granular access
    control at the data lake level.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AI/ML extension with Amazon SageMaker.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other components include Amazon **Key Management Service** (**KMS**) for data
    encryption, Amazon **Identity and Access Management** (**IAM**) for access control,
    Amazon Macie for PII data detection to adhere to data compliance such as **Payment
    Card Industry Data Security Standard** (**PCI DSS**), CloudWatch to monitor the
    operation, and CloudTrail to audit the data lake activities.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to validate your big data architecture using the following criteria:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Security:'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify data and define corresponding data protection policies using resource-based
    access control.
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a strong identity foundation using user permission and **single sign-on**
    (**SSO**).
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable environment and data traceability for audit purposes.
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply security at all layers and protect data in transit and at rest using SSL
    and encryption at all layers.
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep people away from data, such as locking down write access to production
    datasets.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reliability:'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enforce data hygiene using automated data profiling using data cataloging.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage the life cycle of data assets, transitioning, and expiration using data
    tiering between the data warehouse and data lake.
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preserve data lineage by maintaining the history of data movement through the
    data catalog.
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Design resiliency for analytics pipelines and monitor system SLAs with automated
    recovery of ETL job failures.
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance efficiency:'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use data profiling to improve performance with data validation and to build
    a sanitization layer.
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuously optimize data storage, such as using data compression with a Parquet
    format, data partition, file size optimization, and so on.
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cost optimization:'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adopt a consumption model and determine whether you need an ad hoc or fast query
    pattern.
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete out-of-use data; define data retention rules and delete or archive data
    out of the retention period.
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decouple compute and storage with a data lake-based solution.
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement migration efficiency using different migration strategies for various
    data sources and volumes.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use managed and application-level services to reduce the cost of ownership.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operational excellence:'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform operations as code using tools such as CloudFormation, Terraform, and
    Ansible.
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automate operations such as building an orchestration layer with Step Functions
    or Apache Airflow.
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Anticipate failure in advance by continuously monitoring and automating the
    recovery of ETL job failures.
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Measure the health of your workload.
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the preceding checklist as a guide to validate your big data architecture.
    Data engineering is a vast topic that warrants multiple books to cover each topic
    in depth.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned about various components of data engineering with
    a popular architecture pattern, which will help you get started and explore the
    topic in more depth.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-440
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the big data architecture and components
    for a big data pipeline design. You learned about data ingestion and various technology
    choices available to collect batch and stream data for processing. As the cloud
    is central to storing the vast amounts of data produced today, you learned about
    the various services available to ingest data in the AWS cloud ecosystem.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Data storage is one of the central points of handling big data. You learned
    about various kinds of data stores, including structured and unstructured data,
    NoSQL, and data warehousing, with the appropriate technology choices associated
    with each. You learned about cloud data storage from popular public cloud providers.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Once you collect and store data, you need to transform it to get insights into
    that data and visualize your business requirements. You learned about data processing
    architecture and technology choices to choose open source and cloud-based data
    processing tools per your data requirements. These tools help you get data insights
    and visualizations per the nature of your data and organizational needs.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: You learned about various big data architecture patterns, including data lake,
    lakehouse, data mesh, streaming data architecture, reference architecture, and
    how to choose the right architecture for your data needs. Finally, you learned
    big data architecture best practices by combining all your learning in the reference
    architecture.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: As you collect more data, it’s always beneficial to get future insights, which
    can be exceptionally beneficial for business. You often need ML to predict future
    outcomes based on historical data. In the next chapter, let’s learn more about
    ML and how to make your data architecture future-proof.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace to ask questions and interact with the authors
    and other solution architecture professionals: [https://packt.link/SAHandbook](Chapter_12.xhtml)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code930022060277868125.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
