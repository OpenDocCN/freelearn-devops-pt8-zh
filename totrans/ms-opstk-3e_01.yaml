- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Revisiting OpenStack – Design Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “I have found you have got to look back at the old things and see them in a
    new light.”
  prefs: []
  type: TYPE_NORMAL
- en: '- John Coltrane'
  prefs: []
  type: TYPE_NORMAL
- en: Since the last edition of the *Mastering OpenStack* series, the OpenStack community
    has kept the momentum by growing and innovating around its ecosystem. At the time
    of writing this edition, the OpenStack design has been going through different
    cycles of improvements by including new projects and providing seamless integration
    with existing systems to respond to organization demands and custom features.
    Since the first *Austin* release back in 2010, the innovation kept opening new
    opportunities for companies and quickly adopting a stable private cloud setup
    to stay ahead in the market. The challenges of handling a scalable infrastructure
    have been felt by big and medium players who joined the OpenStack evolvement.
    Today, as the first page of this edition is being written, 14 years and 28 releases
    have already passed through different OpenStack releases, from *Austin* back in
    the day to the latest *Dalmatian* release. That is a full circle back to the beginning
    of the alphabet. A lot of experiences and insights have been revealed that have
    boosted the OpenStack community and made it the world’s leading open source cloud
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Numbers show how successfully OpenStack has been adopted by mid to large enterprises.
    As per the annual *OpenStack User Survey* results in the OpenStack blog in November
    2022, it was declared that more than 40 million cores are running in production
    across 300 deployments empowered by OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenInfra community has been committing enormously to hardening the most
    stable versions of OpenStack software. Starting from the *Antelope* release that
    was launched in March 2023 and, more recently, the *Dalmatian* release in October
    2024, organizations have quickly upgraded, and the good news is a more stable
    and reliable version than ever. The secret sauce of this great achievement is
    that the community has switched gears on stabilizing basic OpenStack services,
    including compute, storage, and network services, increasing the cadence of release
    testing of each service extensively and thus leaving neither possible gaps nor
    potential vulnerabilities that could raise bugs in deployments in production.
    On the other side of the story, by going through the map of each OpenStack release,
    you might notice some extension projects appearing and disappearing. Community
    contributors did the right thing, discounting non-stable features that would hinder
    the maturity of the OpenStack private setup. Today, the release is exposing basic
    services in addition to more projects and features in a stable upstream. The other
    bright side of the story is that the OpenStack ecosystem has always been one step
    ahead to adopt new trends of market technologies that include containerization,
    serverless, big data analysis, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Today, either starting a new OpenStack adventure or turning a new page for a
    new update would bring great cost savings, flexibility in IT handling compared
    to old-school virtualization alternatives, and an efficient solution to hyperscale
    with minimum concerns.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack’s state-of-the-art documentation and resources can be found more abundantly
    than ever on the internet. There are plenty of different options to design and
    deploy a complete OpenStack environment. On the other hand, the paradox of choices
    between different ways of designing and running a full ecosystem can be challenging
    even for experienced administrators, architects, and developers. That could be
    phrased differently as *too much can be too little* ! The adoption of OpenStack
    can be a struggle and has always been a challenge, from design phases to deployments
    and operational days. As stated previously, you can think of many reasons for
    this, but mainly, being such a versatile software can make it overwhelming!
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will go through the updated pieces of the OpenStack
    ecosystem and divide and conquer together with a step-by-step approach to design,
    deploy, and operate a scalable OpenStack environment that would respond to your
    needs and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: To address the challenges of the OpenStack ecosystem complexity and allow newcomers
    to enjoy this cloud journey, we will define a state vision for a private cloud
    adoption strategy throughout the first chapter. Without a well-crafted basic knowledge
    of this ecosystem, it would be more difficult to make later decisions on empowering
    running clusters in production when production days start knocking on the door.
    A very attractive citation from Robert Waterman, an expert on business management
    practices, is *“A strategy is necessary because the future* *is unpredictable.”*
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to a complex system such as the OpenStack ecosystem, setting up
    the right resources is required as we cannot predict with 100% accuracy. OpenStack
    is designed with much more flexibility and a loosely coupled architecture. This
    way, it is up to us to use those key elements to make capacity decisions considering
    short- and long-term goals for growth and availability and respond to new workloads
    based on existing resources that we aim to apply in the near and long term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our OpenStack journey will continue in this edition by covering the following
    topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting and highlighting the latest updates of the OpenStack core ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying the logical architecture based on the latest releases introduced
    from the *Antelope* release
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drafting a first physical design to ensure a seamless deployment in later stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing for a large-scale OpenStack environment through capacity planning
    as part of our cloud strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack – a plethora of innovations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It might be confusing to understand the state of the art of innovation when
    it comes to OpenStack software offerings. The rise of this open source project
    has been unique since its birth 14 years ago. Through its different releases,
    the OpenStack community has kept developing what a private cloud solution can
    offer based on ongoing technology trends. When the first releases of Kubernetes
    (container orchestration technology) were made public, subsequent OpenStack releases
    quickly dedicated a cycle to include services around the OpenStack ecosystem that
    would facilitate container management out of the box. One of the OpenStack success
    stories is undoubtedly its vision of staying one step ahead. There is a clear
    reason why OpenStack remains the fourth largest open source community worldwide:
    trust in its main core services. Since the first days of OpenStack, mid and large
    enterprises have invested and keep contributing to its software to boost its major
    core services. As we will see later, compute, network, identity, image, and storage
    services are counted as the most basic and core components of the OpenStack ecosystem.
    Within each new release, more enhancements and extensive development have been
    done to bring a more alive version of each of them. That brings more contributors
    to see services more alive, taking more workloads, addressing more demand, and
    unblocking enterprises to take advantage of a more agile and flexible infrastructure.
    More big players have joined the cloud era from the early days, such as IBM, Red
    Hat, HP, Rackspace, eBay, and more, to name but a few. Each of them associates
    different goals and projects from different expertise, such as bringing new features,
    plugins, bug fixing, and so on. Some of those contributors have built their own
    dedicated private cloud platform based on OpenStack and keep contributing to the
    open source world, making it a win-win deal.'
  prefs: []
  type: TYPE_NORMAL
- en: Another key aspect of OpenStack novelty is the speed of extending its capabilities
    from a wider range of services similar to public major cloud providers such as
    **Amazon Web Services** ( **AWS** ), **Microsoft** **Azure** , and **Google Cloud
    Platform** ( **GCP** ). Starting from traditional **Infrastructure as a Service**
    ( **IaaS** ), the OpenStack ecosystem has tailored the next levels by bringing
    managed services and offering **Platform as a Service** ( **PaaS** ) as well as
    **Software as a Service** ( **SaaS** ) models. By skimming the surface of the
    speed of OpenStack growth, we might find out that the core services have reached
    a level of maturity that could unlock gazillions of doors of innovative features
    and solutions. On top of those services, more PaaS environments can be offered,
    such as databases, big data, container services, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The other side of the success of the OpenStack growth is the nature of the software
    itself. Written mostly in Python, since the first days of its development, the
    OpenStack software has been designed to offer a rich **application programming
    interface** ( **API** ). This was a game changer by enabling automation in almost
    everything and everywhere. Hence, each contributor can use the domain of expertise
    and seamlessly integrate new features into the software ecosystem, thanks to the
    nature of API design.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack’s APIs have been of considerable significance in adopting a hybrid
    cloud approach. As we will see in [*Chapter 11*](B21716_11.xhtml#_idTextAnchor230)
    , *A Hybrid Cloud Hyperscale Use Case – Scaling a Kubernetes Workload* , OpenStack
    offers the **Elastic Compute Cloud** ( **EC2** ) API, a compatible API to interact
    with the AWS world.
  prefs: []
  type: TYPE_NORMAL
- en: The innovation and continuous growth of the OpenStack ecosystem are without
    a doubt a success story thanks to the community’s consistency and, most importantly,
    the development of OpenStack naturally with the modular design. Nowadays, it is
    possible to build an OpenStack private cloud for one or several purposes that
    can serve web applications for public hosting workloads, managed databases, big
    data, high-performance computing (HPC), or even several optimized containerized
    environments for rapid application development. Most importantly, getting acquainted
    with core and latest service updates is the must-have key to unlocking the cloud
    journey.
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks Of OpenStack – the control plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenStack project has been launched to solve the IaaS paradigm. By adopting
    the pay-as-you-use model, underlying resources, including compute, networking,
    and storage, are exposed as pools and reserved on demand securely per the user’s
    request. Throughout the development of the OpenStack ecosystem (considered in
    other literature as the *cloud operating system* ), more open source projects
    have joined the emerging cloud software life cycle to extend its capabilities.
    As mentioned previously, the secret sauce of such project development comes from
    the natural API design, which consistently facilitates communication between services.
    The versatile number of services could sidetrack newcomers in understanding where
    and how fundamental parts of the ecosystem work to enable a well-designed architecture
    for custom needs and requirements. The following sections will iterate through
    what are considered the *core services* of OpenStack, to begin with. It is essential
    to get acquainted with each of these services as each OpenStack release, coming
    with new projects or services, relies on them. Being strategic in mastering core
    components and their capabilities will allow easier deployment of your future
    private cloud setup and, hence, extend its capabilities with less wasted time
    and effort. The following table shows the core services that will be tackled in
    the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service** | **Code name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Compute | Nova | Manages the **virtual machine** ( **VM** ) life cycle |'
  prefs: []
  type: TYPE_TB
- en: '| Network | Neutron | Manages network environment per project |'
  prefs: []
  type: TYPE_TB
- en: '| Identity | Keystone | Provides authentication and authorization information
    service |'
  prefs: []
  type: TYPE_TB
- en: '| Block storage | Cinder | Manages the VM disks and snapshots life cycle |'
  prefs: []
  type: TYPE_TB
- en: '| Object Storage | Swift | Accessible REST API storage for object data types
    such as images and media files |'
  prefs: []
  type: TYPE_TB
- en: '| Image | Glance | Manages the VM image life cycle |'
  prefs: []
  type: TYPE_TB
- en: '| Dashboard | Horizon | OpenStack frontend web interface |'
  prefs: []
  type: TYPE_TB
- en: '| File sharing | Manila | Manages projects across shared filesystems |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling | Placement | Helps to track provider’s resource inventories and
    usage |'
  prefs: []
  type: TYPE_TB
- en: '| Telemetry | Ceilometer | Provides data collection service for resource tracking
    and billing |'
  prefs: []
  type: TYPE_TB
- en: '| Alarming | Aodh | Triggers actions and alarms based on collected metrics
    and configured rules |'
  prefs: []
  type: TYPE_TB
- en: Table 1.1 – OpenStack core services
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is still possible to deploy an OpenStack environment without object storage
    and a dashboard. Some distributions come by default with both services enabled.
    For example, spinning up a VM does not necessarily require object storage. Additionally,
    the OpenStack **command-line interface** ( **CLI** ) supports all operations for
    each installation that can be directed without the OpenStack dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Keystone – the authentication and authorization service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The keywords *authentication* and *authorization* are the main functions of
    this identity service. **Keystone** is part of the control plane of the OpenStack
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Security has been brought over from the earliest releases, and it became a necessity
    to dive further into the **AAA** (short for **Authentication, Authorization, Accounting**
    ) service in OpenStack. Each request made to contact an OpenStack service must
    be validated by Keystone (via the identity API). The API response will return
    an authentication token to be used against the requested service ( API call).
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, the Keystone workflow might seem simpler than we thought. On
    the other hand, when we scale hundreds of hosts to fulfill thousands of requests
    in a short time, we should consider how to make sure that Keystone, as a critical
    service, is fully operational. That will be discussed in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: What has been brought from old OpenStack releases to the latest stable ones
    within the identity service is mainly the version of the API. Formerly, when installing
    the identity service, it was possible to keep running with version 2; starting
    from the *Grizzly* release, version 3 has been launched and can be installed.
    Today, the identity API version 2 has been deprecated in favor of newer releases.
    We will consider version 3 and above (at the writing of this book, version 3.14
    is the latest within the *Ussuri* OpenStack release).
  prefs: []
  type: TYPE_NORMAL
- en: But what can an identity API offer? As mentioned previously, the design of all
    communications between OpenStack services is made via APIs (later, we will skim
    the surface for interaction even with non-OpenStack services via APIs). Consider
    that a failed simple authentication request might reflect a service failure from
    the end user’s perspective. A good practice is to make sure, on the first iteration,
    that there are no issues with Keystone requests when troubleshooting a failing
    service request. Although the previous editions of this book did not dive deep
    into the Keystone workflow, we will revisit it in more detail in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane –* *Shared Servi ces* .
  prefs: []
  type: TYPE_NORMAL
- en: Nova – the compute service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Austin* , the first OpenStack release, brought the first version of the **Nova**
    project to spin up an instance and started dealing with compute management through
    a rich API. Unlike the aforementioned identity service, Nova has evolved through
    different releases to accommodate more amazing functions but at the cost of its
    complexity compared to other core services. The Nova service design has not changed
    a lot from the *Grizzly* release. On the other hand, it is still required to reiterate
    through the different Nova pieces that form the compute service engine in the
    OpenStack ecosystem.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of capabilities brought by Nova in the latest OpenStack releases,
    which will be detailed in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125) , *OpenStack
    Compute – Compute Capacity* *and Flavors* .
  prefs: []
  type: TYPE_NORMAL
- en: The **nova-api** service interacts with user API calls that manage compute instances.
    It communicates with other components of the compute service over a message bus.
  prefs: []
  type: TYPE_NORMAL
- en: The **nova-scheduler** service listens to the new instance request on the message
    bus. The job of this service is to select the best compute node for the new instance.
  prefs: []
  type: TYPE_NORMAL
- en: The **nova-compute** service is the process responsible for starting and terminating
    VMs. This service runs on the compute nodes and listens for new requests over
    the message bus.
  prefs: []
  type: TYPE_NORMAL
- en: The **nova-conductor** service handles database access calls from the compute
    nodes to limit the risk of database access by an attacker via a compromis ed host.
  prefs: []
  type: TYPE_NORMAL
- en: Placement – the scheduling service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever it comes to a new system improvement or logical addition, the OpenStack
    community will create, move, or place different services or components to make
    that happen. One of the major updates of the OpenStack ecosystem since the *Newton*
    release is the introduction of the **Placement** API service. Before the latter
    release, users struggled to identify the counter on different sets of resource
    providers, such as computation, networking, and storage allocated pools. That
    is where it is useful to dedicate a separate service, hence an API well connected
    with other services (such as Nova), to track those resource providers and keep
    an eye on their usage. The Placement service acts mainly as a resource inventory.
    Not only that, but the most pertinent addition has also made the process of filtering
    (via **nova-scheduler** ) more fine-grained. In some other glossaries, you might
    find the word *prefiltering* as a reference for the new Placement service. This
    step is the preparation of available compute nodes by starting a filtering process
    before dealing with the scheduler based on some configurable specs and traits.
    The Placement service will be demonstrated in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity* *and Flavors* .
  prefs: []
  type: TYPE_NORMAL
- en: Glance – the imaging service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To launch an instance in OpenStack via the Nova service, an instance image will
    be required to successfully accomplish VM provisioning. **Glance** is one of the
    core services that deals not only with instance images (as a template source image)
    but also the snapshots that can be created from an instance. When it comes to
    the question of where Glance will store images and snapshots, the answer can be
    found in its latest supported driver configurations. As with many other OpenStack
    services, a number of storage options can be used to back the storage of images
    by Glance. Again, if we think about the OpenStack infrastructure extension in
    the short and long term, we must think about which backend storage Glance should
    use. The OpenStack community has developed the most commonly used storage backend
    either within proper OpenStack-supported storage services such as *Swift Object
    Storage* and *Cinder Block Storage* or a third-party extension such as *Ceph*
    storage based on **RADOS Block Device** ( **RBD** ), VMware storage, and even
    AWS **Simple Storage Service** ( **S3** ) storage.
  prefs: []
  type: TYPE_NORMAL
- en: This variety of backend storage options might increase the paradox of architectural
    choices, but it raises the question of which needs we would address from business
    and capacity perspectives. As an example, Glance configuration enables multiple
    storage backends in the same configuration layout that can be customized to instruct
    the imaging service to use an existing block storage pool to attach images directly
    and reduce the time of waiting to download from scratch. A massive Ceph infrastructure
    with dozens of object storage nodes can be leveraged if they are operational by
    dedicating a Ceph pool for production images, making use of existing resources
    granted by the stable driver integration between Glance and the Ceph backend.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand your requirements before selecting any type of
    backend, as not all backends will support the same features. For example, within
    the *Antelope* release, the OpenStack Glance team included a new feature in the
    block storage backend – *Cinder* – by allowing the extension on the running attached
    volumes. The Ceph storage extension will be discussed in more detail in [*Chapter
    5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage – Block, Object, and*
    *File Shares* .
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, more interesting use cases would leverage Swift as a storage backend
    for Glance snapshots and templates. That can be considered a safe internal backup
    scenario. Going further, increasing your **fault tolerance** ( **FT** ) domain
    can cross public cloud services such as the AWS S3 backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most impressive progressions in the Glance service is the wider
    list of supported images. The *Antelope* release came with at least 11 supported
    formats, including RAW, QCOW2, VDI, VHD, ISO, OVA, PLOOP, and Docker, and great
    additional formats dealing with Amazon public cloud compatibility are AKI, AMI,
    and ARI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Glance supported backends](img/B21716_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Glance supported backends
  prefs: []
  type: TYPE_NORMAL
- en: Swift has been one of the most preferred options to store Glance images in large
    deployments, which will be highlighted in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Swift – the object storage service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Together with Nova, **Swift** came with the first-ever OpenStack release, *Austin*
    , back in 2010. Object storage has added great value to enterprises that resolve
    several storage challenges, unlike the traditional persistent storage design.
    Swift itself has been a revolutionary service at a time when cloud technologies
    just started to warm up. As per the nature of the object storage model, objects
    are stored in a flat hierarchy. Most of the main usage of Swift in a given OpenStack
    deployment is to perform archiving and backups. Many services around the OpenStack
    ecosystem that deal with storage are compatible with the Swift storage backend
    either for direct storage, backup purposes, or even both.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will unleash a few of the latest additions on the Swift service in [*Chapter
    5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage – Block, Object, and
    File Shares* , a brief revisit of object storage common specs are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Developed with no **single point of failure** ( **SPOF** ) by design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposes the HTTP REST API for common object management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly scalable and a good fit for workloads demanding boosted performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed for eventual consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly available by design and can scale easily horizontally through inexpensive
    hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at the next storage optio n: the Cinder service.'
  prefs: []
  type: TYPE_NORMAL
- en: Cinder – the block storage service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other storage offering in the OpenStack ecosystem and counted as a core
    service is the block storage service named **Cinder** . Cinder has been developed
    to maintain independent life cycle operations of instance volumes (as virtual
    disks). The Cinder API provides an exhaustive list of different volume and snapshot
    operations, including create, delete, attach, detach, extend, clone, create images
    from volumes, create volumes from images, and create volumes from snapshots. With
    recent OpenStack releases, more operational capabilities have been developed to
    support backups, restoration , and volume migration.
  prefs: []
  type: TYPE_NORMAL
- en: Manila – the shared filesystems service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: File-sharing solutions have been an emerging storage option for enterprises.
    The need to have a collaborative central storage brain for access facility and
    simplicity of management has grown in the latest decade in IT infrastructure either
    on-premises or in cloud environments. The OpenStack community did not miss the
    opportunity since the *Liberty* release, introducing the **Manila Distributed
    and Shared File Systems** solution. Coming from the same logical workflow as Cinder,
    Manila has not been included as a core OpenStack project due to its limited capabilities.
    As per the *Kilo* release until *the latest releases* , Manila had proven itself
    as a stable component within the OpenStack infrastructure, allowing enterprises
    to gain access to self-service file-sharing for different hungry resources and
    clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, several storage vendors have kept the momentum of contributing
    to the Manila service by developing more drivers to support their storage backend
    hardware. Following the same steps as Cinder, we might find an exhaustive list
    of supported file share drivers by Manila, including CephFS, LVM, Hadoop HDFS,
    EMC, IBM, and many others. A full list of supported drivers can be found on the
    Manila OpenStack web page: [https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html](https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html)
    . More interesting updates have been included in the latest releases for the Manila
    service, including security, backup share integration, and increased level of
    accessibility, which will be discussed more in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Storage – Block , Object, and* *File Shares* .'
  prefs: []
  type: TYPE_NORMAL
- en: Neutron – the networking service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous citation might be arguable if we consider **nova-network** still
    being used! On the other hand, since the *Grizzly* release, the **Neutron** project
    has become a more **Networking as a Service** ( **NaaS** ) pillar in the OpenStack
    ecosystem. That makes sense due to the approach taken by the community to provide
    an independent networking service and not embedded, as is the case with the Nova
    compute service. The transition from legacy **nova-network** to Neutron has been
    tougher for companies willing to migrate from the old days to the new network
    era led by Neutron. Although we will keep the light on Neutron in the next parts
    of the book, a section will be dedicated in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* , to discuss
    the more advanced features of Neutron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The motivation to adopt Neutron is fairly obvious considering the rise of this
    project to solve many **nova-network** limitations and open new networking capabilities.
    Compared to **nova-network** , which provides basic networking in the OpenStack
    ecosystem (mainly limited to interaction with the compute service), Neutron brings
    several aspects and features that can summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Simplified self-service for projects to provision ports, subnets, networks,
    and router objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced networking features can be deployed in no time, such as firewalls,
    **virtual private networks** ( **VPNs** ), and load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More support for complex network topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vast ways of third-party network solution integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A standalone service making architecture less complex to support **high** **availability**
    ( **HA** )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective service for all sizes of deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the emergence of hardware and software based on **software-defined networking**
    ( **SDN** ) solutions, the OpenStack Neutron team has taken the lead to engage
    more support on the SDN part, resulting in the development of Neutron drivers
    to integrate some SDN famous implementations such as OpenContrail and VMware NSX.
  prefs: []
  type: TYPE_NORMAL
- en: Since 2013, within each OpenStack release, the Neutron project has been considered
    one of the services with the most number of commits and feature development. [*Chapter
    6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking – Connectivity and
    Managed Service Options* , will iterate in depth about more Neutron capabilities
    and connect the pieces with SDNs within th e latest OpenStack releases.
  prefs: []
  type: TYPE_NORMAL
- en: Ceilometer – the telemetry service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The telemetry service was introduced in the OpenStack ecosystem in the *Havana*
    release. Code-named **Ceilometer** , the telemetry service has emerged with a
    pertinent mission: record, gather, and monitor resource utilization metrics across
    the OpenStack infrastructure. Unlike older OpenStack releases, the latest Ceilometer
    versions include only metric collection features, leaving alarming and notification
    functions to anoth er dedicated project code-named Aodh.'
  prefs: []
  type: TYPE_NORMAL
- en: Aodh – the alerting service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of the monitoring services chain, enabling users to trigger custom notifications
    based on telemetry events has become a required addition to keeping an eye on
    deployed project resources. As mentioned in the *Ceilometer – the telemetry service*
    section, the alarming feature has been separated from Ceilometer, and **Aodh**
    is dedicated to firing alarms based on configured rules and set thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Ceilometer is designed to send collected metric data to other destinations,
    including a non-OpenStack open source project named *Gnocchi* . The Gnocchi project
    formulated the time-series **Database as a Service** ( **DBaaS** ) term. In this
    case, a dedicated database interface is no lo nger required to store and query
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Horizon – the dashboard service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a **graphical user interface** ( **GUI** ) to operate your OpenStack
    environment would definitely make your administrative tasks much simpler. Surprisingly,
    the OpenStack first releases did not bring a GUI initiative to fire a few clicks
    from a dashboard to assist in managing the infrastructure resources. It was only
    in the *Essex* release that **Horizon** was born to accommodate the first core
    project’s operations, such as the creation of object containers, operating a life
    cycle of instances, managing images, and a basic layout on project users’ management,
    formerly named tenant users. The need to include more control from the dashboard
    has been felt with the extensive inclusion of more services, and, within the latest
    releases, we might notice that the Horizon experience is much more sophisticated.
    Built with the Django framework and supporting most of the OpenStack APIs, the
    majority of the core services’ resources can be operated through Horizon. On the
    other hand, more advanced operations such as customizing networking configuration
    can be fired only using the OpenStack CLI. Additionally, do not expect that Horizon
    will automatically include an extra service beyond the core ones. It is not a
    limitation, but, for modular design reasons, installing a new service would require
    the addition of a new Horizon module associated with it and defined as a *panel*
    . Horizon, with a good reference of the CLI, would be sufficient to administer
    your cloud if running a small setup. Talking about the wider infrastructure, with
    dozens of projects and hundreds of resources, administering this size of setup
    might not be ideal using Horizon. Agile thinking would require more automation
    and scripting on a large-scale size. In [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right Way (DevSecOps)* , we will discuss
    in detail the automation of the OpenStack setup and how to use the same approach
    to manage r esources within project resources themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Non-OpenStack services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As part of the OpenStack control plane, we might find, as per any software
    architecture, other essential core components that all OpenStack services rely
    on: **Advanced Messaging Queuing Protocol** ( **AMQP** ) and database services.'
  prefs: []
  type: TYPE_NORMAL
- en: Passing the message – AMQP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AMQP, the messaging queue service, is one of the other ingredients for making
    a modular architecture design in the OpenStack ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: For each request hitting a specific OpenStack service API, the message queue
    server will make sure to communicate messages between all engaged components (processes)
    to accomplish the request asynchronously. A variety of open source AMQP software
    can be used throughout your OpenStack deployment if you are biased about using
    one of them; it can just be simply specified during the setup to have it runn
    ing and communicating with different components. RabbitMQ, Qpid, and ZeroMQ are
    supported message queue solutions; RabbitMQ is the most used one. Being an expert
    in AMQP operations is not a requirement for a successful OpenStack operation day,
    but a few main points should be taken into consideration when dealing with a message
    queue in OpenStack. We will see in later chapters how OpenStack services depend
    on the message queue from an architectural perspective, giving importance to such
    services running on the highest possible availability. The other aspect, having
    some adequate familiarity with how messaging concepts work, would be more than
    helpful for troubleshooting days. [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane – Shared Services* , and [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , will highlight in sequence
    the internal workflow of AMQP in OpenStack and how to maximize its availability.
  prefs: []
  type: TYPE_NORMAL
- en: Storing the state – the database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The other fundamental OpenStack control plane service is the database. Similar
    to the AMQP service, each OpenStack service will require a database connection
    to store its different states. The database in the OpenStack ecosystem defines
    persistent storage that can be consulted and updated by all services upon each
    request. The database topic from an architectural perspective has been a *hot*
    topic mostly due to security concerns. As mentioned earlier, in the *Nova – the
    compute service* section, there is a tendency from that learning curve to reduce
    any possible insecure access to the database or services running in different
    hosts that could potentially present a security risk. The other side of the database
    sensitivity topic, similar to the AMQP service, is the availability dilemma. Besides
    the risk of failures, OpenStack architects and administrators will need to reiterate
    through the other traditional database anomaly once it starts growing: performance.
    [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane – Shared
    Services* , and [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly
    Available Cloud – Meeting the SLA* , will cover how to ensure a healthy start
    of the database for larger OpenStack deployments. [*Chapter 9*](B21716_09.xhtml#_idTextAnchor204)
    , *Benchmarking the Infrastructure – Evaluating Resource Capacity and Optimization*
    , will bring the motivation behind the database bench marking routine for a better
    state of performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Other services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The latest OpenStack releases have demonstrated goodwill to OpenStack users
    to deploy and operate an OpenStack private cloud infrastructure not only with
    very stable core services but also with additional ones. Compared to older releases,
    several incubated projects have been in on-off mode, making the decision to adopt
    some of them in your existing deployment insecure. Different insights from different
    project initiatives have sculpted more confident software within the latest OpenStack
    releases ready to support more features and services in your cloud environment.
    Some of the extra stable services are listed in the following table in the order
    of integration through different OpenStack cycle releases:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service** | **Release** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Heat | Havana | Orchestration PaaS extension on top of the IaaS provided
    by OpenStack. Heat abstracts cloud resources into code – also known as **domain-specific
    language** ( **DSL** ). That comes when cloud operators leverage the power of
    code templating to automate things, reduce human error, and increase the agility
    of resource management. Heat uses **Heat Orchestration Templates** ( **HOTs**
    ), in YAML or JSON format, allowing users to define the whole infrastructure to
    run their workloads from code. |'
  prefs: []
  type: TYPE_TB
- en: '| Trove | Icehouse | DBaaS allows users to automate the provisioning of relational
    and non-relational scalable databases with the minimum overhead of database tasks
    such as patching, maintenance, and backup through provided automation. |'
  prefs: []
  type: TYPE_TB
- en: '| Sahara | Juno (incubated in Icehouse) | **Elastic Data Processing as a Service**
    ( **EDPaaS** ) provides a means of orchestrating structured and unstructured data
    processing and analysis software and infrastructure on top of OpenStack, such
    as Hadoop and Spark clusters. |'
  prefs: []
  type: TYPE_TB
- en: '| Ironic | Kilo | **Bare Metal as a Service** ( **BMaaS** ) allows users to
    provision infrastructure directly on a physical machine, thus no hypervisor layer
    is involved. |'
  prefs: []
  type: TYPE_TB
- en: '| Murano | Kilo | **Application as a Service** ( **AaaS** ) enables developers
    to speed up application deployment and publish into an application catalog that
    can be browsable and ready for deployment in an automated fashion. |'
  prefs: []
  type: TYPE_TB
- en: '| Designate | Liberty | **DNS as a Service** ( **DNSaaS** ) handles **Domain
    Name Service** ( **DNS** ) entries such as DNS records and zones. |'
  prefs: []
  type: TYPE_TB
- en: '| Barbican | Liberty | **Secret Manager as a Service** ( **SMaaS** ) is designed
    to centrally store different types of secrets, such as passwords, keys, and certificates,
    to be used by other OpenStack services in a secure way. |'
  prefs: []
  type: TYPE_TB
- en: '| Zaqar | Liberty | **Messaging as a Service** ( **MSaaS** ) allows users to
    provision and manage multi-tenant messaging and notification queues. |'
  prefs: []
  type: TYPE_TB
- en: '| Magnum | Liberty | **Container as a Service** ( **CaaS** ) orchestrates a
    set of containers through a supported **container orchestration engine** ( **COE**
    ) such as Docker Swarm, Kubernetes, and Mesos. |'
  prefs: []
  type: TYPE_TB
- en: '| Octavia | Liberty | **Load Balancer as a Service** ( **LBaaS** ) provides
    a load balancing function. Octavia can be counted as an enterprise-class load
    balancer solution. This service has become an attractive feature that can be used
    much more easily compared to the Neutron LBaaS driver. |'
  prefs: []
  type: TYPE_TB
- en: Table 1.2 – Additional OpenStack services
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing the core and incubated OpenStack services, we can now walk through
    different functional requirements that would construct what a private cloud can
    offer as services.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before building a picture of your future OpenStack environment, it is necessary
    to have business goals that align with the outcome of the OpenStack solution.
    As we have seen briefly in the previous sections with the latest updates in the
    OpenStack glossary, you might feel how overwhelming it could be with newly developed
    services and features. Obviously, the new image of OpenStack looks more robust
    than ever, with many hands of **Anything as a Service** ( **XaaS** ) opportunities.
    The array of choices might prevent architects and operators from starting a proper
    OpenStack infrastructure exercise if the scope of your business is not defined
    from the first day of drafting the design. As with existing major public providers
    such as AWS, Azure, or GCP, the OpenStack deployment should be considered as a
    way of investment in the long run. Expecting operational and cost savings can
    be felt only at later stages. Being strategic is essential to take advantage of
    the OpenStack solution if used properly from the early days. Hundreds of enterprises
    have switched gears to use OpenStack in their environment, but not all of them
    had a successful journey and soon dropped it. For this reason, understanding in
    which use case your business will fall ensures a safe start to your OpenStack
    journey.
  prefs: []
  type: TYPE_NORMAL
- en: Since 2010, OpenStack usage has taken different facets whereby companies have
    invested in the OpenStack ecosystem, followed the milestone generation, and adopted
    it. After 14 years, we can see a plethora of deployments where OpenStack shines
    with a respectful history of success. The next sections will broadly list a few
    common use cases of implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Application development accelerator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The modern **software development life cycle** ( **SDLC** ) takes a more mature
    direction by enhancing the stages of its release and automating the whole chain,
    from simple code commits to different types of testing. Although **Continuous
    Integration/Continuous Delivery** ( **CI/CD** ) methods and tools emerged slightly
    before the rise of the cloud era, delivering a well-tested software product has
    faced a new challenge: *resource limitations* . Although the wording *limitation*
    is not exactly the term that would describe this challenge, from a developer’s
    perspective, it means there are no available resources to continue or perform
    specific unit, smoke, or integration testing. Preproduction environments are even
    more challenging to validate in all stages if a business will not allow the product
    to go live without provisioning exactly the same environment as in production
    for the sake of testing. Resource instrumentation and virtualization have resolved
    this issue, but only partially. With a lack of automation and an engine of operation,
    the whole chain still does not save costs and reduce the **time to market** (
    **TTM** ). That is where OpenStack has been engaged for several hundred enterprises:
    accelerate product releases by running their CI/CD pipelines in a multi-tenant
    OpenStack environm ent.'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud application enabler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are planning to provide what most developers dream of (developing from
    scratch!), then you are in the right corner. Whether the developed application
    is a web pet store, online reservation assistant, or flight reservation engine,
    wrapping the application in its proper infrastructure can be done in a fast and
    clean way using Heat, for example. As the application will be defined as *code*
    in a HOT file, you will not need to worry about the application configuration
    consistency as well as its degree of scalability if autoscaling is declared in
    the template. As OpenStack services will take care of all the automation overhead,
    application owners can focus on the next level of the application business to
    find more spots for improvements rather than wasting effort on resource management
    and operation overhead.
  prefs: []
  type: TYPE_NORMAL
- en: The other trait of using OpenStack, especially within the survey from the *Kilo*
    release, is the trending cloud-ready application. OpenStack enables a fast application
    publishing process in different ways. As container technologies adoption increases,
    OpenStack has included container services to support a few famous container orchestration
    engines, such as Docker Swarm and Kubernetes, via the OpenStack Magnum service.
    76% of users in the *User Survey* from the *Juno* release showed interest in the
    OpenStack and containerization marriage. A well-known example of a vast OpenStack
    environment is CERN. Based on OpenStack, CERN is running over 300,000 cores, with
    a big chunk of it being Kubernetes workloads (over 500 clusters) within the Magnum
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More details on the conducted OpenStack survey can be found at [https://www.openstack.org/user-survey/2022-user-survey-report](https://www.openstack.org/user-survey/2022-user-survey-report)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The Murano application catalog service is another OpenStack service that facilitates
    application publishing in no time based on contai ners.
  prefs: []
  type: TYPE_NORMAL
- en: HPC supporter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might be confusing to mix the *HPC* term in the context of OpenStack. Eventually,
    OpenStack does not access the hardware to configure compute power and increase
    hardware performance. On the other hand, it enables designing your commodity hardware
    in a way to grow horizontally when hitting more compute, storage, and network
    demand. In the HPC context, the main focus is on augmenting the underlying hardware
    capacity, and the cloud will secure the multi-tenancy to access resources in the
    most optimized way.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, in the early days of OpenStack when only Nova and Swift existed,
    the **National Aeronautics and Space Administration** ( **NASA** ) and Rackspace
    were the initiators of the first OpenStack production environment running HPC.
    More research organizations have since adopted the OpenStack ecosystem to deliver
    compute and storage at a large scale to researchers and scientists, but again,
    selecting the hardware layout and planning ahead is still required to feel the
    added value of OpenStack either via virtualized infrastructure, bare metal, or
    both. OpenStack enables all methods of orchestration for HPC environment depl
    oyments.
  prefs: []
  type: TYPE_NORMAL
- en: Network functions virtualization moderator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Only a few enterprises in the telecommunication industry realized the opportunities
    of virtualizing network services in the early days. Following the same steps as
    the server virtualization experience, enterprises started investing in the new
    trend of running their edge network services on standard machines and providing
    functions such as routers, proxies, load balancers, and firewalls to consumers
    as VMs. This became a major asset in responding quickly to network changes and
    feature demands. **Network functions virtualization** ( **NFV** ) relies heavily
    on compute power that can run on commodity hardware and virtual environments,
    whereas in the traditional way, physical appliances running each network function
    must exist. In the OpenStack context, NFV will need only a compute service to
    expose different functions. As per the *Kilo User Survey* , more than 12% of organizations
    use OpenStack in the networking and telecommunication industry. As Telco adopts
    OpenStack for such a purpose, Nova will be your assistant but with a strategic
    eye on compute resources and placement to make i t happen.
  prefs: []
  type: TYPE_NORMAL
- en: Big data facilitator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Volkswagen AG and BMW have shared their feedback on using OpenStack for big
    data and analytics, and the outcome was valuable assistance of the ecosystem orchestrating
    all the different services and resources to accomplish some large data analytics
    tasks. Before the *Juno* release and the unleashing of the Sahara – EDPaaS – project,
    big data resources could be orchestrated using HOTs to provision a cluster in
    no time. Sahara was unleashed to simplify the deployment of such resources in
    more granular and ready-to-go ways, considering the type of the data processing
    framework, versions, size of the cluster, desired topology, and so on. The big
    data scope in OpenStack can be wider when thinking out loud by using some advanced
    features of the OpenStack services, such as storage and networking. Companies
    keep showing interest in automating their **Extract-Transform-Load** ( **ETL**
    ) pipelines internally by leveraging the OpenStack big data service. If you consider
    your business to be bound to a Hadoop provider, for example, make sure that performance
    is a key specification for your underlying hardware that No va will use.
  prefs: []
  type: TYPE_NORMAL
- en: Private cloud service provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Private cloud environments could be the first main motivation when reading the
    definition of OpenStack for the first time in the old releases’ documentation.
    Being private will limit the services offered to end users within the perimeter
    of your organization or, technically, *behind* *the firewall* .
  prefs: []
  type: TYPE_NORMAL
- en: The term *private cloud* might sum up all the use cases previously. The only
    difference is that previous OpenStack implementations can stay isolated within
    your perimeter or serve on -demand needs.
  prefs: []
  type: TYPE_NORMAL
- en: Public cloud service provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Going public! Why not? Back in the early 2000s, Amazon internally launched
    a private elastic compute cloud to enable infrastructure agility for developers
    and users running the giant online e-commerce website and other services selling
    millions of items worldwide. **Simple Queue Service** ( **SQS** ) was launched
    and exposed to the public as the first cloud service, then later S3, followed
    by the EC2 service. The idea started with object storage named S3 and the EC2
    service. Originally, NASA and Rackspace initiated and immensely contributed to
    software code to create software operating a private cloud initially for elastic
    computation and object storage capabilities. That is how Nova and Swift came to
    be. The first release attracted early-bird companies seeking innovations and started
    contributing to the new open source and soon cultivating more experience on market
    models for XaaS products. Rackspace launched its *public cloud* version offering
    compute and object storage services initially. This was followed by many others
    adopting a public cloud powered by OpenStack, such as Open Telekom Cloud, Cleura,
    VEXXHOST, and many more. Lately, it feels like the world runs on OpenStack with
    the hashtag **#RunOnOpenStack** . With its widespread implementation with big
    names around the world, the OpenStack community has initiated the concept of *OpenStack
    public cloud passports* . The idea behind it is to act as a major public cloud
    provider by providing OpenStack services in different regions and locations worldwide.
    The program ensures a massive hub of collaboration between different OpenStack
    public cloud providers, enabling users to roam between more than 60 **availability
    zones** ( **AZs** ) worldwide. The [openstack.org](http://openstack.org) website
    keeps updating the public cloud locations that can be filtered through the OpenStack
    public cloud marketplace page: [https://www.openstack.org/marketpla ce/public-clouds/](https://www.openstack.org/marketplace/public-clouds/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Picking up the pieces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Independent of which use case your future OpenStack deployment will start with,
    mastering the art of its ecosystem is a vital step before investigating any further
    design and architecture options. OpenStack covers more than 1 million lines of
    Python code! At this stage, there is no need to worry about learning how the code
    works: far more important is to take a reverse engineering approach: understanding
    the ecosystem interactions. Simply understanding the interrelation of core services
    gives you the keys to the kingdom of the art of design. Starting with the basics,
    the foundational components will open more doors to the next ones if we master
    the core ones correctly. So far, we have revisited core services of OpenStack
    that exist with each release and had a brief scan of the incubated projects, particularly
    within the latest releases. Next, we will select a common workflow that illustrates
    the different steps and pieces to spawn a VM. This type of workflow will demonstrate
    how different OpenStack core services interact and will involve almost all the
    components.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The listing of which services are counted as core ones might differ between
    different resources and documentation. Telemetry, scheduling, and dashboard services
    could be considered optional services. The current assumption is based on real-world
    deployment that leverages the usage of those services from the start of the journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first iteration demonstrates a high level of the building blocks of different
    OpenStack core services that include compute, imaging, identity, block storage,
    monitoring, scheduling, dashboard, and networking services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – High-level services interaction spawning instance workflow](img/B21716_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – High-level services interaction spawning instance workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to take note of the main blocks before jumping into the granular
    presentation of the spawning process as illustrated in the previous diagram. By
    providing the Horizon dashboard or via the CLI (similarly, via APIs), an authentication
    process will be fired first by challenging the user request with login credentials
    that will be handled by the identity service, Keystone. Once validated (an authentication
    token will be provided for the whole session and subsequent requests), the request
    will reach the Nova API to take the next steps. Nova will interact with each of
    the following services counted as *mandatory* to successfully spawn an instance:'
  prefs: []
  type: TYPE_NORMAL
- en: The imaging service, Glance, queries an image to start the instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The networking service, Neutron, acquires networking resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The block storage service, Cinder, allocates storage volumes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scheduling service, Placement, allocates available compute provider resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the whole chain of interactions passes through the API of each component.
    For each inter-component request, the core database will be consulted for read
    and object updates. As per spawning an instance, a record of a new VM instance
    will be created in the instance and used for reference with other services in
    the subsequent steps. As per the instance request, Nova will interact with the
    queuing message service as well – for example, to request an instance launch (for
    the Nova compute process).
  prefs: []
  type: TYPE_NORMAL
- en: As part of this workflow, any API endpoint, scheduling, imaging, identity, and
    shared components such as database and queuing services are counted as part of
    the OpenStack control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The control plane and data plane are ways of conceptual separation in complex
    system architectures. The terms will be defined in more detail in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane –* *Shared Services* .
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s zoom in and explore the workflow in a more granular view so that
    we can build a whole spawning instance picture. As demonstrated in the previous
    part of the chapter, each OpenStack service includes subcomponents working together
    to serve a specific request. Obviously, the API part exists in each service. Remember
    that is the *gate* of each module in the OpenStack ecosystem. Each exposed API
    service (providing an HTTP-based RESTful API) reaches other components and, subsequently,
    subcomponents via the queuing message service bus, as shown in the following workflow
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Low-level services interaction spawning instance workflow](img/B21716_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Low-level services interaction spawning instance workflow
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the identity service, Keystone should be aware of the available
    services in the architecture setup to authorize any further interaction. Keystone
    holds a service catalog containing all API endpoints of different services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous diagram illustrates each step by including different components
    and their respective subcomponents:'
  prefs: []
  type: TYPE_NORMAL
- en: A REST API call will be initiated by using Horizon or the CLI and reaches the
    identity service Keystone endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keystone challenges the request, gets the user credentials (a form of username
    and password), and validates the authentication via its API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The validated authentication generates an authentication token (an **auth**
    token) back to the requester user. The **auth** token will be cached for the specific
    user to be used for subsequent API REST calls between the rest of the services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As our request is a *compute* one, the next station that will be reached is
    the compute service API, **nova-api** . Again, the form of the request is a REST
    API, forwarded by Keystone from its catalog. You can think of the catalog as a
    *service map* where Keystone locates each service via the registered endpoint.
    In this case, the REST API encapsulated with the **auth** token will be sent to
    the compute service endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The compute API, the **nova-api** subcomponent, checks the **auth** token coming
    with a request reaching the compute endpoint. At this stage, the **auth** token
    will be validated in return by the identity service, Keystone. The purpose of
    this secondary interaction with Keystone is to inform the compute API in the request
    which permissions and roles users will have and for how long the **auth** token
    will stay alive before generating a new one (noted as the token expiry period).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once validated, the first database interaction will take place by **nova-api**
    , which creates a new VM object that specifies the different parameters in the
    request. Bear in mind that each service will have its own logical database to
    update the state of different resources. In the current scenario, the Nova service
    holds its dedicated database schema for different state updates and read and write
    operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we come to the next interaction of another subcomponent of the Nova service,
    **nova-scheduler** , to specify which compute node will host the new instance.
    The **nova-api** subcomponent will contact the scheduler with such a request via
    the communication hub: the messaging queue service. This nature of the operation
    is processed via a **Remote Procedure Call** ( **RPC** ) call, noted as **rpc.call**
    , which will be published in the message queue and remain there till **nova-scheduler**
    picks it up.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-scheduler** subcomponent takes the lead on fetching the most convenient
    compute node to host the new instance. In our highlighted basic core design, the
    Placement service will be engaged in the process of selecting the best compute
    node(s).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-scheduler** subcomponent reaches **placement-api** to query its compute
    resource provider. Note that the Placement service is still considered a new service
    as an extension of the other services for tracking their resources, such as compute,
    IP pools, and storage, expressed as resource providers. Since the *Stein* release,
    it has been extracted from the Nova ecosystem and runs on its own. In our workflow,
    we can consider the usage of Placement in the *prefiltering* stage before the
    final selection at the Nova scheduling level. The Placement service also exposes
    its own endpoint in the Keystone catalog. Hence, it will validate the **auth**
    token via the identity service, which updates the token header and returns it
    to the **placement-api** process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The selection of hosts is determined via the Placement service by querying its
    own inventory database and, at the first iteration, returns a selection of compute
    nodes and any traits associated with them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By applying its filtering and weighing parameters at the returned result by
    the **placement-api** component, the **nova-scheduler** process will come up with
    the right compute node and reach **placement-api** in the second iteration to
    claim the resources for the instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **placement-api** component adjusts its state records in the database, and
    **nova-scheduler** updates the state of the VM object in its database with the
    compute node ID and its hostname and then submits a message in the form of **rpc.cast**
    to the messaging hub. This message will be picked up by **nova-compute** to launch
    the instance in the designated hypervisor host ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To communicate with the next subcomponent, **nova-conductor** , **nova-compute**
    will publish the next **rpc.call** call in the message queue to start the launch
    of the instance with details for preparation, such as CPU, disk, and RAM specs.
    Note that **nova-compute** can be running in different process occurrences in
    different compute nodes. Only one particular **nova-compute** occurrence will
    publish **rpc.call** to **nova-conductor** based on the compute node chosen in
    *step 8* .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we recall the **nova-conductor** addition in the Nova service architecture,
    the main purpose of this service is to play a *gate role* between the **nova-compute**
    process and the database to minimize the database exposure and hence reduce its
    attack blast radius. As we are aiming for a large OpenStack deployment, the **nova-compute**
    component will be detached from the controller node (holding the main control
    plane services) and will be dedicated to different physical hypervisor nodes.
    If one of the compute nodes has been compromised, spreading that security issue
    to the database will be more difficult due to the logical and physical separation
    when processes interact with the database. The **nova-conductor** component will
    be the one interacting directly with the Nova database to read the instance details,
    such as compute node ID, its requested flavors, memory, CPU, and disk claims.
    The state will be reported back to the messaging queue service by an RPC publish
    call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **nova-compute** component picks up the published message and subsequently
    reaches the next service component by sending a RESTful API to the image service,
    Glance: the **glance-api** process. The request contains a form of **GET API**
    request for the image details encapsulated in the requested image ID information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Glance will provide the requested image ID through its image-registered URL.
    But first, as we are reaching out to another service in the ecosystem, Glance
    will contact the identity service, Keystone, to validate the **auth** token again.
    Assuming the token session has not expired yet, Keystone will validate the token,
    and in the same way as Nova in *step 5* , the identity service will check the
    user role and permissions before it updates the token header and sends it back
    to the Image API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Glance component is considered much simpler than Nova, as there is no interaction
    with a message queue in this step. The **glance-api** component simply checks
    the Glance database (running its own dedicated database schema), retrieves the
    metadata of the requested image ID, and reports back to **nova-compute** with
    the image URL format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-compute** component reaches the image store and starts loading the
    provided image via the returned URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now come to the next service to interact with: networking. The **nova-compute**
    component sends a RESTful API request to the Neutron service, via **neutron-server**
    . As discussed earlier, Neutron (by naming convention in its own architecture)
    does not involve an explicit subcomponent name with the **api** suffix. The **neutron-server**
    subcomponent is the *API gate* that will deal with all API requests. Running the
    same identity cycle and assuming that the token has not expired yet, **neutron-server**
    will forward the API request to Keystone, where it gets validated, updates the
    token header with different roles and permissions, and sends it back to the **neutron-server**
    process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **neutron-server** component checks the networking item’s status and respectively
    creates the requested resources encapsulated in the network parameters’ API call.
    Note that the Neutron service varies between different types of requests and hence
    exposes an array of workflows depending on the request complexity. We will cover
    other workflow scenarios dealing with other network controllers, agents, and plugins,
    such as **Open vSwitch** ( **OVS** ), in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* . For example,
    once a port is created for an instance, **neutron-server** will publish an RPC
    call in the message queue reaching out to the **Dynamic Host Configuration Protocol**
    ( **DHCP** ) agent. The **neutron-dhcp-agent** process will invoke its associated
    DHCP driver to reload its host’s file entries via the **dnsmasq** process. Once
    the instance is booted, **dnsmasq** is ready to pick up the instance request and
    send a DHCP offer for the final instance network configuration. This scenario
    also assumes a network and subnet are created where the instance will be attached,
    and that is specified in the instance creation request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once networking parameters are acquired for the new instance request, **nova-compute**
    reaches the database and updates the VM object network state record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last service to accomplish the instance creation is to reach out to the
    block storage service. Following the same way as previous services, **nova-compute**
    sends its RESTful API request to the Cinder API, the **cinder-api** subcomponent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In turn, the Cinder API process will get to the identity service where Keystone
    validates the **auth** token. Assuming that it has not expired, Keystone returns
    the updated token header once validated with the roles and permissions set for
    the requester.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As with Nova and Neutron, Cinder has more subcomponents that would take a few
    steps further before the final admission of the request back to the **nova-compute**
    process. Cinder involves its own scheduler as well by publishing an RPC call to
    the messaging queue service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **cinder-scheduler** subcomponent picks up the request and locates available
    resources for a new volume candidate list based on the requested volume specifications
    (volume size and type). Note that Cinder also provides another process, **cinder-volume**
    , that interacts with **cinder-scheduler** whenever a new read or write request
    is initiated within a specific storage provider. In this case, **cinder-volume**
    will be interfacing with backend driver methods to generate a candidate list that
    will be posted in the message queue and picked up later by the **cinder-scheduler**
    subcomponent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **cinder-scheduler** subcomponent updates the state of the volume ID with
    its associated returned metadata in the Cinder database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **cinder-api** component picks up the published message from **cinder-volume**
    and responds to **nova-compute** via a RESTful call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-compute** component receives the created volume metadata in the API
    request. By default, if no other specific hypervisor virtualization driver is
    being configured, Nova will execute the **libvirt** daemon and proceed by creating
    the instance on the designated compute node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the final stage of the instance spawning workflow, the created volume should
    be mapped by the **cinder-volume** process. That is when the virtualization driver,
    in this case, **libvirt** , will make the volume available to the instance by
    firing a **mount** operation. The way it is performed depends on the storage provider
    and protocol, but a common case is passing the volume path to the hypervisor,
    which will mount it to the instance as a virtual block device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The last piece of our initial workflow is the ingestion of the instance metrics
    in the Ceilometer service. It is vital to watch what has been spawned by checking
    the status of the created instance. Ceilometer provides two ways of collecting
    data metrics for instances: *polling agents* or *notification agents* . Collected
    metrics will pass through a Ceilometer pipeline transformation for further manipulation
    and prepare this data to be published. The latest releases come with an array
    of choices to store the published data in the Ceilometer database, which can be
    accessed through the Ceilometer API, Gnocchi, or even a simple file store.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zooming in on the communication between the different core components is essential
    before drafting a first design layout of your private cloud, which will be detailed
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Architecting the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying the initial use case of your OpenStack environment is a major key
    to a successful start of the cloud journey. With a vast array of features, especially
    within the latest releases, the cloud management platform has become more mature
    than ever, but that might be overwhelming. Sidetracking the purpose of your OpenStack
    implementation would increase management complexity and potentially put your business
    at risk if such investment is not carefully accompanied by a vision. For this
    purpose, drafting your design into different iterations will help you avoid being
    blocked by *the paradox of choices* and unleash a clear short-term vision toward
    the longer one. As the first design iteration, we will simply identify our conceptual
    OpenStack model, followed by the logical architecture. Once drafted, we will walk
    through the practical implementation and put some numbers together to reflect
    our first deplo yment picture in the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Drafting the conceptual design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the latest launching instance workflow, we have settled on the core
    services that should exist in our first OpenStack implementation. Naturally, any
    services that will be extending our basic setup later, such as PaaS services,
    will rely on the services underway that construct mainly our IaaS layer. During
    the next design phases, we will consider a generic OpenStack use case that would
    be suitable for internal usage as a private cloud to support organizations with
    elastic IaaS resources, boosting cloud application development, enabling data
    analysis, and helping with public hosting workloads. Beyond the core services,
    other mature ones will be included in this design iteration, as summarized in
    the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cloud service** | **Service role** |'
  prefs: []
  type: TYPE_TB
- en: '| Compute – Nova | Manages VM life cycleUI-enabled |'
  prefs: []
  type: TYPE_TB
- en: '| Imaging – Glance | Manages VM image filesUI-enabled |'
  prefs: []
  type: TYPE_TB
- en: '| Object storage – Swift | Manages objects for persistent storageUI-enabled
    |'
  prefs: []
  type: TYPE_TB
- en: '| Block storage – Cinder | Manages VM volumes for persistent storageUI-enabled
    |'
  prefs: []
  type: TYPE_TB
- en: '| Networking – Neutron | Manages network L2 and L3 resourcesUI-enabled |'
  prefs: []
  type: TYPE_TB
- en: '| Data metrics – Ceilometer | Collects resource data metrics for monitoringUI-enabled
    |'
  prefs: []
  type: TYPE_TB
- en: '| File share – Manila | Manages scale-out file share systemsUI-enabled |'
  prefs: []
  type: TYPE_TB
- en: '| Identity – Keystone | Authentication and authorization |'
  prefs: []
  type: TYPE_TB
- en: '| Dashboard – Horizon | GUI |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling – Placement | Prefiltering for resource providers and traits |'
  prefs: []
  type: TYPE_TB
- en: Table 1.3 – Initial OpenStack services
  prefs: []
  type: TYPE_NORMAL
- en: 'Summing up those services into one conceptual diagram will captivate our next
    logical design iteration and narrow down the distribution of the services across
    different cluster roles of the OpenStack implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – OpenStack services interaction workflow](img/B21716_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – OpenStack services interaction workflow
  prefs: []
  type: TYPE_NORMAL
- en: The previous diagram illustrates a high-level presentation of our first OpenStack
    architecture draft that counts several services to host a variety of workloads
    for end users. We can proceed with our design by developing a logical layout in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Drafting the logical design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logical design iteration is a vital step that should be carried out with
    more attention. More brainstorming points will be raised at each level and within
    each service of the design to formulate an initial draft that can be implemented
    with more confidence. The other key consideration is to run our services for both
    the control and data planes in a minimum setup of HA. That can be empowered by
    clustering the controller and compute nodes in the subsequent iterations. While
    working on the cloud roadmap, it is essential to reflect on the growth aspect.
    As your business needs are identified, the first logical design should be mapped
    with the first explored requirements. Trying OpenStack even with the minimum installation
    can be helpful to familiarize yourself with its basic ecosystem components. On
    the other hand, a major consideration of the first deployment for future successful
    experiences is to think of *growth* . As mentioned earlier, OpenStack is developed
    with modular software architecture; reflecting the same approach in the logical
    design followed by the physical one will absolutely make your cloud shine. Consider
    the following guidelines to begin with:'
  prefs: []
  type: TYPE_NORMAL
- en: Define roles for the OpenStack system in both the control and data planes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolate roles starting with the minimum number of nodes and sufficient hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think ahead of the possibility of component services failing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use software that the cloud operations team is most familiar with, such as queuing
    message and database common services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throughout the first iteration of our logical design, an incremental approach
    will be taken, starting with the following roles:'
  prefs: []
  type: TYPE_NORMAL
- en: A pair of controller nodes running the control plane, including the database
    and queuing message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One compute node running the hypervisor layer based on **Kernel-based Virtual**
    **Machine** ( **KVM** ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQL with the MariaDB engine and RabbitMQ will be chosen as the database and
    messaging queue services, respectively. Most of the OpenStack-documented implementations
    use MySQL and RabbitMQ as almost a religious matter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy and Pacemaker will be running in each controller node for HA, load balancing,
    and clustering. Database HA will be implemented through Galera multi-master replication.
    RabbitMQ instances will run on their native clustering mode based on queue mirroring
    across both cloud controller hosts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The first logical design should not be overwhelming as it is not a final draft
    ready for production yet. [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running
    a Highly Available Cloud – Meeting the SLA* , has been dedicated to going through
    each layer of the future implemented control and data planes in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first logical design proposal can be drafted by using the previously listed
    software tools and the cloud controller and compute roles, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – High-level logical design of OpenStack components for deployment](img/B21716_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – High-level logical design of OpenStack components for deployment
  prefs: []
  type: TYPE_NORMAL
- en: Once the logical setup of different components and roles has been drafted, we
    will need to identify how the OpenStack nodes should conne ct.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting the dots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another essential aspect during the logical design phase is to provide a consistent
    networking layout between the different roles of each OpenStack entity. There
    are gazillions of ways to connect different pieces in an OpenStack ecosystem;
    the following layout demonstrates a network segmentation approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Network segmentation OpenStack nodes](img/B21716_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Network segmentation OpenStack nodes
  prefs: []
  type: TYPE_NORMAL
- en: The isolation of different types of networks will increase security access through
    networking segmentation. The trade-off is the extra complexity, but security comes
    first. The other opinion on this choice is also the performance question, as dedicating
    a separate segment for each type of traffic would save bandwidth significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous diagram illustrates four types of networks denoted with the following
    conventions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**External network** : Connecting to the outside world. OpenStack APIs can
    be reachable from the public as well as private backbone within a global organization
    network. The external network will provide routable IP addresses. This type of
    network will be part of the data plane as it will expose or direct traffic instances
    within the underlying infrastructure. It is essential to keep security configuration
    tied at this level by fronting the network with load balancer devices or appliances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Management network** : As part of the control plane, this provides interconnectivity
    between all the various nodes in the OpenStack environment. Services such as the
    database and queuing message will be plugged into this network to be reached by
    the compute nodes. The management network does not expose anything to the outside
    world and should be seen as a private or internal network (you might find the
    naming convention of *API network* in other sources).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tenant network** : As users will require their virtual networks, the tenant
    network (referred to as a *guest* or *overlay network* in other sources) will
    be dedicated to handling instance traffic. There are a variety of options for
    this type of network that could be attached to SDN capabilities in the network
    node. [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking
    – Connectivity and Managed Service Options* , will highlight the scope of virtual
    overlay networking in more detail. The tenant network can be considered as part
    of the data plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage network** : As part of the data plane, the storage network will connect
    compute, and storage cluster nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Instances can use a direct physical network instead of virtual ones empowered
    by Neutron through SDN, which would assign floating IPs to access the outside
    world. This model is referred to as a *provider network* that would connect network
    and compute. More details will be highlighted in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed* *Service Options* .
  prefs: []
  type: TYPE_NORMAL
- en: Enumerating the different types of networks earlier will help the selection
    and reservation of different ports and network cards of each host per role in
    the long-term setup. It is still possible to combine more than one network in
    the same physical connection. However, the continuous expansion of the underlying
    infrastructure will hit the limit of the physical capacity of the combined segments,
    leaving a variety of issues with bottlenecks and network performance anomalies.
    Planning and preparing in advance will save an immense amount of time and effort
    in modifying the networking layout once network performance starts boiling. If
    a network node is dedicated in the next iteration, for example, the controller
    node will not need to connect to the tenant and exter nal networks.
  prefs: []
  type: TYPE_NORMAL
- en: Drafting the physical design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next section will assign values to our parameters defined in the previous
    logical design. However, understanding a few key concepts and practices beforehand
    will save plenty of effort and costs. Let’s start with a capacity-plannin g demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for capacity planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Capacity planning comes at every stage of a new infrastructure project, either
    within in-house hosting or for the development of a simple application. The cosmos
    of capacity-planning practices is based on forecasting and predicting how many
    resources an IT infrastructure would require to respond to business needs. In
    the OpenStack context, once your business case has been defined, the process of
    capacity analysis is narrowed to a specific set of resources that should exist.
    If you are planning to host a mixture of generic web hosting applications and
    data analysis workloads, there are certain considerations to be planned in terms
    of hardware sizing and the technology that will run the workloads. NFV would require
    more attention when selecting the hardware as it can be a performance eater.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building the capacity planning for the OpenStack case can be summarized as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Operate with elasticity** : Be able to respond and pull more ubiquitous compute
    resources when needed in the case of a failure or load increase using automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expect to fail** : Underlying resources should be ready to be replaced immediately
    without the need to spend time on fixing and reconfiguration efforts during incidents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Track for growth** : Available capacity over the course of production can
    be variable. As we are in an on-demand model, the expected growth is not forcibly
    linear. Regularly track the usage of your underlying infrastructure to plot your
    cloud usage and update the capacity roadmap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another key foundation that is highly appreciated by large infrastructure management
    is the adoption of **Information Technology Infrastructure Library** ( **ITIL**
    ) practices. By reflecting on the IT infrastructure that will run the OpenStack
    cloud environment, ITIL methodologies will definitely refine a strategic process
    to identify a complete cycle of your capacity management under the ITIL service
    design umbrella. If your organization has already rolled out ITIL practices, feel
    free to reuse and apply them in the cloud journey. From a **Cloud Service Provider**
    ( **CSP** ) perspective, having a tactical approach to managing the underlying
    IT infrastructure to align with business needs and user demand and take full control
    of the financial aspect is a *must-have* .
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: ITIL is a framework presented with a set of practices and methodologies to standardize,
    manage, and optimize IT services offered in a given business. ITIL has evolved
    into four different versions. All the versions emphasize common core concepts
    around the business service, such as the service design pillar targeting capacity
    management.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping the land
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a first iteration, it would be ideal to dig into a few parameters that would
    generate some numbers to start the journey. In order to estimate our hardware
    and configurations, we can think of a reverse approach. Instead of looking at
    how many instances the first deployment could accommodate, we should see this
    from the end user’s eyes: *Which instance flavors could be offered to run which
    specific workloads?* An instance flavor is a set of specifications defined as
    a characteristics template in the compute service, including the number of vCPU
    cores, amount of RAM (including swap), and root disk (including ephemeral size).'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More advanced flavor specs can be customized and created in OpenStack to deploy
    instances that require specific workloads by scheduling the Nova service to use
    a set of compute nodes, such as support of certain CPU architecture or intensive
    workloads for HPC.
  prefs: []
  type: TYPE_NORMAL
- en: A simple approach is to spread the population of compute resources into an average
    flavor model that could run generic workloads. As a start, an instance flavor
    could be suitable for testing tenant environments with a couple of vCPUs and 1024
    MB of RAM capacity. Defining the baseline flavor model will enable us to determine
    the next ones by doubling the size of capacity in the next iteration. Keep in
    mind that there are gazillions of ways to define the set of flavors, but, most
    importantly, identifying the starting one will help to classify the next sizing
    in the series.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your captured business needs, hence the type(s) of workload(s),
    you will have a clearer picture of how much hardware capacity will be required
    in each compute node you plan to invest in. With every combination of flavors,
    putting the density of resources in the compute box will help to measure whether
    any wasted room has been left behind so that it can be used within a new flavor.
    For example, if a compute node could support 40 medium and 10 small-sized instances
    and still have some room left, create a new flavor with the gapped size of 1 vCPU
    and 512 MB of RAM to be added to the compute node flavor catalog.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following use case will consider a business initiative to run instances
    with versatile types of generic workloads that include instance flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Flavor** | **vCPU** | **RAM (MB)** | **Disk (GB)** |'
  prefs: []
  type: TYPE_TB
- en: '| Tiny | 1 | 512 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Small | 1 | 1024 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Medium | 2 | 2048 | 40 |'
  prefs: []
  type: TYPE_TB
- en: '| Large | 4 | 4096 | 80 |'
  prefs: []
  type: TYPE_TB
- en: Table 1.4 – Instance flavors list
  prefs: []
  type: TYPE_NORMAL
- en: The first capacity-planning roadmap will assume 200 VMs as a starting point
    with the first set of compute nodes. The following sections will conduct an estimation
    for different hardware specs, including CPU, RAM, storage, and network for each
    compute node.
  prefs: []
  type: TYPE_NORMAL
- en: CPU estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A proper calculation of the compute power will depend significantly on the
    technology and model supported by the hardware. For this reason, we will define
    a list of hardware assumptions for our use case as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: GHz per physical core = 2.6 GHz
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical core hyperthreading support = use factor 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GHz per VM (AVG compute units) = 2 GHz
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GHz per VM (MAX compute units) = 16 GHz
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intel Xeon E5-2648L v2 core CPU = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU sockets per server = 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU overcommit ratio = 16:1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oversubscription: Disabled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyperthreading: Disabled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operating system overhead: 20%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack compute allows the overcommitment of resources for CPU and RAM. The
    technique of overcommitment aims to maximize the usage of resources by leveraging
    sharing between VMs running in the same hypervisor machine. For example, running
    16 vCPUs per 1 physical CPU in a hypervisor host is denoted as a CPU overcommitment
    of 16:1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the number of virtual cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(number of VMs x number of GHz per VM) / number of GHz* *per core*'
  prefs: []
  type: TYPE_NORMAL
- en: '**(200 * 2) / 2.6 =** **153.8 46**'
  prefs: []
  type: TYPE_NORMAL
- en: A rounded decimal estimation will result in 154 vCPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take into account the operating system overhead running in the compute
    node of 20%, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**154 + ((154 * 20)/100) =** **184.4**'
  prefs: []
  type: TYPE_NORMAL
- en: A rounded decimal estimation will be 185 vCPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding the overcommitment ratio 16:1 parameter to estimate the actual physical
    CPU on the compute node can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**185/16 =** **11.5625**'
  prefs: []
  type: TYPE_NORMAL
- en: A rounded decimal estimation will result in 12 CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: As a starting point, a compute node of 12 CPU cores would host 200 instances
    with a *small* model flavo r.
  prefs: []
  type: TYPE_NORMAL
- en: Memory estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following assumptions would be required to align with the previous CPU
    estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: 1024 MB of RAM per instance for the small flavor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8 GB of RAM maximum dynamic allocations per VM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute nodes supporting slots of 2, 4, 8, and 16 GB sticks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAM overcommit ratio of 1:1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating system overhead of 20%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For 200 VMs with small-sized flavor instances, a RAM estimation is given as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**200 * 1024 MB =** **200 GB**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding the 20% operating system overhead gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**200 + ((200 * 20)/100) =** **240 GB**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A 1:1 overcommitment ratio will determine the actual RAM size needed per compute
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '**240/1 =** **240 GB**'
  prefs: []
  type: TYPE_NORMAL
- en: That would require an amount of 240 GB of RAM in our compute node to accommodate
    200 small flavor instances.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The default Nova compute configuration comes with an overcommitment value of
    1:1.5. Be aware that, unlike CPU overcommitment behavior, memory overcommitment
    can affect the instance performance if not enough room for the swap memory is
    planned in advance. As a good practice, with 1:1.5, configure the swap memory
    to be at least double what is provid ed.
  prefs: []
  type: TYPE_NORMAL
- en: Storage estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As per storage, physical compute nodes should acquire enough storage capacity
    with a multitude of options by providing the instance’s root disk from the compute
    nodes’ physical disks themselves or via attached storage devices.
  prefs: []
  type: TYPE_NORMAL
- en: Considering 200 VMs and a small size flavor instance of 20 GB root disk, a compute
    node should acquire an amount of *200 * 40 =* *800 GB* .
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the operating system requirement for disk space and other factors
    with caching and swapping configuration, an estimation of between 900 GB and 1
    TB of storage will grant a secured disk allocation for all instances (think of
    swap and extra caching disk operation overheads).
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect that can be considered for more flavor customization is the storage
    type specs. That was not mentioned in the previous flavor catalog table, but,
    as part of the capacity management exercise, when the business changes the scope
    of the nature of the workload, extra classes of flavors can be added to expand
    that list, such as high-performance storage and **input/output operations per
    second** ( **IOPS** ) specs. Storage devices have various specifications to deal
    with certain use cases and patterns; for highly intensive workloads that require
    fast reads and writes, **Solid-State Drive** ( **SSD** ) disks should be considered
    in the hardware list, followed by a new custom flavor presenting the SSD specifica
    tions.
  prefs: []
  type: TYPE_NORMAL
- en: Networking estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Networking capacity planning comes in two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Network topology, and switching and routing layers to provide sufficient IP
    addressing of the underlying cloud layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instances and overlay networking functions running on top of the infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first pillar would require a consistent network layout that would reflect
    which node would acquire a private IP, a public one, or both. Obviously, when
    putting the nodes together, as per network design, the first step is to assign
    each host its private IP from a given private IP pool. Components that would require
    access to the outside world, as defined in the logical draft, would use public
    IPs. That can be granted via frontend devices or appliances such as load balancers
    or routers via **Source Network Address Translation** ( **SNAT** ) and **Destination
    Network Address Translation** ( **DNAT** ) mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second pillar captures a few factors of network capacity estimation targeting
    *undercloud* resources, which includes mainly the following assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum bandwidth of 50 Mbits/sec per virtual interface per instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association of one floating IP per instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association of one floating IP per NFV overlay function – virtual routers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10% reserved floating IP for future use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another essential aspect that should be considered when planning for networking
    is the network interfaces that will be configured in each compute node. As we
    covered a high-level design in our logical architecture draft, a few networks
    will be created and assigned to each interface in each host. This architecture
    will involve switching L2 configuration, typically via **Virtual Local Area Network**
    ( **VLAN** ) aggregation. Once basic configurations are in place, running some
    benchmarking tools is necessary to gather a few metrics on the bandwidth capacity
    in each switch port and the compute host interface. That will help to get at-a-glance
    network performance information for the hop between the compute host and the main
    physical switch port. As the physical bandwidth will be shared between different
    virtual network instance interfaces, the first value of the switch port and compute
    node physical interface should be acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: The next consideration is taking into account three network interfaces cabled
    to each compute node. One of the network interfaces attached to the tenant network
    (associated with the VLAN in a dedicated switch port) will be cabled using a 10
    GB physical link that will serve 200 VMs, giving 50 MBits/sec for each.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: L2-specific vendors’ network performance varies from one device to another.
    Make sure to measure network traffic and supported **Maximum Transmission Unit**
    ( **MTU** ) sizes when designing your network segmentation and VLAN configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Physical network failure and performance should be highlighted as early as
    possible instead of waiting to be alerted. Including whole configurations and
    resetting cabling and interfaces when running in production does not sound like
    the most brilliant approach. One of the most popular networking techniques that
    would save cost and performance ahead is **network bonding** . Bonding supports
    two different modes: *active-backup* and *active-active* . The first will keep
    running only one network interface as active while the others are in the backup
    state, whereas the second involves the *link aggregation* concept – **Link Aggregation
    Control Protocol** ( **LACP** ) – where traffic will be load balanced through
    different interfaces.'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid network performance bottleneck surprises, the same approach can be
    taken with physical switches. A common well-known technology, **Multi-Chassis
    Link Aggregation** ( **MLAG** ), would transform several physical switches into
    one logical one, allowing FT and exposing the best-effort bandwidth to the connected
    node’s ports.
  prefs: []
  type: TYPE_NORMAL
- en: Empowering the network node’s performance without taking care of what capabilities
    the physical switching could provide would not guarantee a promising experience.
    Gathering the physical network capacity metrics in all the various layers is vital
    to avoid hitting unexpected performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second pillar of our initial network capacity planning is the floating
    IP pool. As Neutron will be our network master in the OpenStack ecosystem, we
    are expecting the network node to interact with different network resources, such
    as instances, virtual routers, and load balancers, without mentioning advanced
    SDN configurations that would overwhelm our initial estimations. Floating IPs
    are publicly routable (public IPs are typically what you get from an **Internet
    Service Provider** ( **ISP** )). A floating IP request either from an instance
    or a network function resource such as a router or load balancer should not fail.
    Thus, we will estimate the pool as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '200 instances per compute node: 200 floating IPs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 tenants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10 routers per tenant: 200 floating IPs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10 load balancers per tenant: 200 floating IPs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding 10% for future use will generate a pool of 660 floating IPs.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing the land
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous capacity-planning exercise considers a mono-compute density prototype
    targeting a *small* flavor. The capacity-planning study could involve more than
    one prototype of distributing the compute density, depending on your strategic
    business needs. The target layout can be extended to involve more flavors, supported
    in either heterogeneous or homogeneous compute density form. The **nova-scheduler**
    component can be configured, as we highlighted in a previous section, with advanced
    scheduling that works hand in hand with the Placement service to find the most
    optimal compute node. The same resource estimation for vCPU, RAM, and root disk
    can be followed to iterate through the next flavor to determine the next set of
    compute node groups. We will cover the filtering mechanisms to respond to compute
    requests in the most adequate way in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity* *and Flavors* .
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in the following diagram, one way to set future boxes in the
    most predictable fashion is to assign each available compute node a homogenous
    flavor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Compute node placements per flavor and workload trait](img/B21716_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Compute node placements per flavor and workload trait
  prefs: []
  type: TYPE_NORMAL
- en: As per the scheduling configuration together with the defined traits in the
    Placement service, a Nova request will reach the right available compute node
    to accommodate a specific workload. This model layout would classify compute nodes
    per types of workloads defined in your business requirements, so the compute nodes’
    capacity would be prepared in advance to expect the initial forecasted demand.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being strategic is a fundamental key to making the right decisions when it comes
    to how to architect a complex ecosystem such as OpenStack. This chapter should
    lower the entry barrier to start an effective plan that meets your organization’s
    needs. The approach taken should help you identify different stages to start your
    cloud journey. There is no exact rule of thumb on how to design an operational
    OpenStack environment but templating the design patterns for each core service
    and sticking to the initially collected requirements will definitely enhance the
    journey. This chapter went through the newest updates on core services in the
    OpenStack ecosystem within *Antelope* and later releases, and a few more projects
    were considered to be offered once the private cloud is up and running. From an
    architecture perspective, this chapter should be revisited during the next stages
    to align and update your design draft in each step. As we used an iterative and
    incremental approach for our future OpenStack cloud environment, the next chapter
    will take you to the next deployment stage from what we have in draft, spiced
    up with best practices for the setup process.
  prefs: []
  type: TYPE_NORMAL
