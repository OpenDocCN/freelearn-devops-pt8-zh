- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Revisiting OpenStack – Design Considerations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重访 OpenStack – 设计考虑因素
- en: “I have found you have got to look back at the old things and see them in a
    new light.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “我发现你必须回顾过去的事物，并从新的角度看待它们。”
- en: '- John Coltrane'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '- 约翰·科尔特兰'
- en: Since the last edition of the *Mastering OpenStack* series, the OpenStack community
    has kept the momentum by growing and innovating around its ecosystem. At the time
    of writing this edition, the OpenStack design has been going through different
    cycles of improvements by including new projects and providing seamless integration
    with existing systems to respond to organization demands and custom features.
    Since the first *Austin* release back in 2010, the innovation kept opening new
    opportunities for companies and quickly adopting a stable private cloud setup
    to stay ahead in the market. The challenges of handling a scalable infrastructure
    have been felt by big and medium players who joined the OpenStack evolvement.
    Today, as the first page of this edition is being written, 14 years and 28 releases
    have already passed through different OpenStack releases, from *Austin* back in
    the day to the latest *Dalmatian* release. That is a full circle back to the beginning
    of the alphabet. A lot of experiences and insights have been revealed that have
    boosted the OpenStack community and made it the world’s leading open source cloud
    platform.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自从*精通 OpenStack*系列的上一个版本发布以来，OpenStack 社区一直保持着势头，围绕其生态系统不断增长和创新。在撰写本版本时，OpenStack
    设计经历了不同周期的改进，通过引入新项目并与现有系统提供无缝集成，以应对组织需求和定制功能。自2010年首次发布的*Austin*版本以来，创新不断为公司开辟了新的机会，并迅速采用稳定的私有云设置，以在市场中保持领先地位。大公司和中型企业在加入
    OpenStack 的演变过程中，感受到了处理可扩展基础设施的挑战。如今，在本版的第一页正在书写之际，14年和28个版本已经穿越了不同的 OpenStack
    发布，从最初的*Austin*版本到最新的*Dalmatian*版本。这是一个回到字母表开头的完整循环。许多经验和见解已经揭示出来，推动了 OpenStack
    社区的发展，使其成为全球领先的开源云平台。
- en: Numbers show how successfully OpenStack has been adopted by mid to large enterprises.
    As per the annual *OpenStack User Survey* results in the OpenStack blog in November
    2022, it was declared that more than 40 million cores are running in production
    across 300 deployments empowered by OpenStack.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据显示 OpenStack 已经成功地被中型和大型企业广泛采用。根据2022年11月在 OpenStack 博客上发布的年度*OpenStack 用户调查*结果，报告称，超过4000万个核心正在生产环境中运行，支持
    OpenStack 的300个部署。
- en: The OpenInfra community has been committing enormously to hardening the most
    stable versions of OpenStack software. Starting from the *Antelope* release that
    was launched in March 2023 and, more recently, the *Dalmatian* release in October
    2024, organizations have quickly upgraded, and the good news is a more stable
    and reliable version than ever. The secret sauce of this great achievement is
    that the community has switched gears on stabilizing basic OpenStack services,
    including compute, storage, and network services, increasing the cadence of release
    testing of each service extensively and thus leaving neither possible gaps nor
    potential vulnerabilities that could raise bugs in deployments in production.
    On the other side of the story, by going through the map of each OpenStack release,
    you might notice some extension projects appearing and disappearing. Community
    contributors did the right thing, discounting non-stable features that would hinder
    the maturity of the OpenStack private setup. Today, the release is exposing basic
    services in addition to more projects and features in a stable upstream. The other
    bright side of the story is that the OpenStack ecosystem has always been one step
    ahead to adopt new trends of market technologies that include containerization,
    serverless, big data analysis, and so on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: OpenInfra 社区在加强 OpenStack 软件最稳定版本方面做出了巨大贡献。从2023年3月发布的*Antelope*版本开始，到最近的2024年10月发布的*Dalmatian*版本，组织们迅速完成了升级，值得庆幸的是，这是一个比以往任何时候都更稳定可靠的版本。这一巨大成就的秘诀在于，社区已经改变了稳定
    OpenStack 基础服务（包括计算、存储和网络服务）的工作方式，极大地提高了每个服务发布测试的频率，从而避免了可能的漏洞和潜在的安全问题，这些问题可能会在生产环境部署时引发故障。从故事的另一面来看，通过分析每个
    OpenStack 版本的图谱，你可能会注意到一些扩展项目的出现和消失。社区贡献者做得对，摒弃了那些不稳定的特性，以免影响 OpenStack 私有部署的成熟度。今天，发布的版本暴露了基本服务，并在稳定的上游增加了更多的项目和功能。这个故事的另一个亮点是，OpenStack
    生态系统始终领先一步，及时采用市场技术的新趋势，包括容器化、无服务器架构、大数据分析等。
- en: Today, either starting a new OpenStack adventure or turning a new page for a
    new update would bring great cost savings, flexibility in IT handling compared
    to old-school virtualization alternatives, and an efficient solution to hyperscale
    with minimum concerns.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，无论是开始新的 OpenStack 探险，还是翻开新的一页来进行更新，都能带来巨大的成本节约，相比传统的虚拟化替代方案，提供更高的 IT 灵活性，并在最小的担忧下实现高效的超大规模解决方案。
- en: OpenStack’s state-of-the-art documentation and resources can be found more abundantly
    than ever on the internet. There are plenty of different options to design and
    deploy a complete OpenStack environment. On the other hand, the paradox of choices
    between different ways of designing and running a full ecosystem can be challenging
    even for experienced administrators, architects, and developers. That could be
    phrased differently as *too much can be too little* ! The adoption of OpenStack
    can be a struggle and has always been a challenge, from design phases to deployments
    and operational days. As stated previously, you can think of many reasons for
    this, but mainly, being such a versatile software can make it overwhelming!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 最先进的文档和资源现在比以往任何时候都更容易在互联网上找到。设计和部署一个完整的 OpenStack 环境有许多不同的选择。另一方面，关于不同设计和运行完整生态系统的选择之间的矛盾，即便对于有经验的管理员、架构师和开发者来说，也是一个挑战。这可以用*选择太多反而太少*来形容！OpenStack
    的采用一直是一个挑战，从设计阶段到部署和运维阶段。正如前面所说，你可以想到很多原因，但最主要的原因是，这款软件功能如此多样，容易让人感到不知所措！
- en: Throughout this book, we will go through the updated pieces of the OpenStack
    ecosystem and divide and conquer together with a step-by-step approach to design,
    deploy, and operate a scalable OpenStack environment that would respond to your
    needs and requirements.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将通过逐步的方式，带你了解 OpenStack 生态系统的更新部分，并通过分步实施的方式，设计、部署和运营一个可扩展的 OpenStack 环境，满足你的需求和要求。
- en: To address the challenges of the OpenStack ecosystem complexity and allow newcomers
    to enjoy this cloud journey, we will define a state vision for a private cloud
    adoption strategy throughout the first chapter. Without a well-crafted basic knowledge
    of this ecosystem, it would be more difficult to make later decisions on empowering
    running clusters in production when production days start knocking on the door.
    A very attractive citation from Robert Waterman, an expert on business management
    practices, is *“A strategy is necessary because the future* *is unpredictable.”*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 OpenStack 生态系统复杂性带来的挑战，并让新手能够享受这一云计算旅程，我们将在第一章中定义一个私有云采用策略的愿景。如果没有一个精心设计的基础知识框架，将很难在生产阶段开始时做出有关如何支持生产集群的决策。正如商业管理实践专家罗伯特·沃特曼所说，*“战略是必要的，因为未来*
    *是不可预测的。”*
- en: When it comes to a complex system such as the OpenStack ecosystem, setting up
    the right resources is required as we cannot predict with 100% accuracy. OpenStack
    is designed with much more flexibility and a loosely coupled architecture. This
    way, it is up to us to use those key elements to make capacity decisions considering
    short- and long-term goals for growth and availability and respond to new workloads
    based on existing resources that we aim to apply in the near and long term.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像 OpenStack 生态系统这样的复杂系统，设置正确的资源是必要的，因为我们无法做到 100% 准确预测。OpenStack 设计时具有更大的灵活性和松耦合架构。这样，关键在于我们如何利用这些元素做出容量决策，考虑短期和长期的增长及可用性目标，并根据我们在短期和长期内打算应用的现有资源来响应新的工作负载。
- en: 'Our OpenStack journey will continue in this edition by covering the following
    topics in this chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 OpenStack 之旅将在本版本中继续，涵盖以下主题：
- en: Revisiting and highlighting the latest updates of the OpenStack core ecosystem
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾并突出展示 OpenStack 核心生态系统的最新更新
- en: Demystifying the logical architecture based on the latest releases introduced
    from the *Antelope* release
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解密基于最新发布版本的逻辑架构，特别是从 *Antelope* 发布版开始引入的内容
- en: Drafting a first physical design to ensure a seamless deployment in later stages
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起草初步的物理设计，以确保后续阶段的无缝部署
- en: Preparing for a large-scale OpenStack environment through capacity planning
    as part of our cloud strategy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过容量规划为大规模 OpenStack 环境做准备，作为我们云策略的一部分
- en: OpenStack – a plethora of innovations
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack – 无数创新
- en: 'It might be confusing to understand the state of the art of innovation when
    it comes to OpenStack software offerings. The rise of this open source project
    has been unique since its birth 14 years ago. Through its different releases,
    the OpenStack community has kept developing what a private cloud solution can
    offer based on ongoing technology trends. When the first releases of Kubernetes
    (container orchestration technology) were made public, subsequent OpenStack releases
    quickly dedicated a cycle to include services around the OpenStack ecosystem that
    would facilitate container management out of the box. One of the OpenStack success
    stories is undoubtedly its vision of staying one step ahead. There is a clear
    reason why OpenStack remains the fourth largest open source community worldwide:
    trust in its main core services. Since the first days of OpenStack, mid and large
    enterprises have invested and keep contributing to its software to boost its major
    core services. As we will see later, compute, network, identity, image, and storage
    services are counted as the most basic and core components of the OpenStack ecosystem.
    Within each new release, more enhancements and extensive development have been
    done to bring a more alive version of each of them. That brings more contributors
    to see services more alive, taking more workloads, addressing more demand, and
    unblocking enterprises to take advantage of a more agile and flexible infrastructure.
    More big players have joined the cloud era from the early days, such as IBM, Red
    Hat, HP, Rackspace, eBay, and more, to name but a few. Each of them associates
    different goals and projects from different expertise, such as bringing new features,
    plugins, bug fixing, and so on. Some of those contributors have built their own
    dedicated private cloud platform based on OpenStack and keep contributing to the
    open source world, making it a win-win deal.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Another key aspect of OpenStack novelty is the speed of extending its capabilities
    from a wider range of services similar to public major cloud providers such as
    **Amazon Web Services** ( **AWS** ), **Microsoft** **Azure** , and **Google Cloud
    Platform** ( **GCP** ). Starting from traditional **Infrastructure as a Service**
    ( **IaaS** ), the OpenStack ecosystem has tailored the next levels by bringing
    managed services and offering **Platform as a Service** ( **PaaS** ) as well as
    **Software as a Service** ( **SaaS** ) models. By skimming the surface of the
    speed of OpenStack growth, we might find out that the core services have reached
    a level of maturity that could unlock gazillions of doors of innovative features
    and solutions. On top of those services, more PaaS environments can be offered,
    such as databases, big data, container services, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The other side of the success of the OpenStack growth is the nature of the software
    itself. Written mostly in Python, since the first days of its development, the
    OpenStack software has been designed to offer a rich **application programming
    interface** ( **API** ). This was a game changer by enabling automation in almost
    everything and everywhere. Hence, each contributor can use the domain of expertise
    and seamlessly integrate new features into the software ecosystem, thanks to the
    nature of API design.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack增长成功的另一面是软件本身的特性。自开发初期以来，OpenStack软件大部分是用Python编写的，旨在提供丰富的**应用程序编程接口**（**API**）。这是一项改变游戏规则的创新，几乎在任何地方都能够实现自动化。因此，每个贡献者可以利用自己的专业领域，利用API设计的特性，顺利地将新功能集成到软件生态系统中。
- en: OpenStack’s APIs have been of considerable significance in adopting a hybrid
    cloud approach. As we will see in [*Chapter 11*](B21716_11.xhtml#_idTextAnchor230)
    , *A Hybrid Cloud Hyperscale Use Case – Scaling a Kubernetes Workload* , OpenStack
    offers the **Elastic Compute Cloud** ( **EC2** ) API, a compatible API to interact
    with the AWS world.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack的API在采用混合云方法中具有重要意义。正如我们在[*第11章*](B21716_11.xhtml#_idTextAnchor230)《混合云超大规模用例——扩展Kubernetes工作负载》中所看到的，OpenStack提供了**弹性计算云**（**EC2**）API，这是与AWS世界兼容的API。
- en: The innovation and continuous growth of the OpenStack ecosystem are without
    a doubt a success story thanks to the community’s consistency and, most importantly,
    the development of OpenStack naturally with the modular design. Nowadays, it is
    possible to build an OpenStack private cloud for one or several purposes that
    can serve web applications for public hosting workloads, managed databases, big
    data, high-performance computing (HPC), or even several optimized containerized
    environments for rapid application development. Most importantly, getting acquainted
    with core and latest service updates is the must-have key to unlocking the cloud
    journey.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack生态系统的创新和持续增长无疑是一个成功的故事，这得益于社区的一致性，最重要的是，OpenStack凭借模块化设计自然发展。如今，构建一个OpenStack私有云已经可以用于一个或多个目的，它可以为公共托管工作负载、托管数据库、大数据、高性能计算（HPC），甚至多个优化的容器化环境提供Web应用程序，满足快速应用开发的需求。最重要的是，了解核心和最新服务的更新是开启云计算之旅的必备钥匙。
- en: Building blocks Of OpenStack – the control plane
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack的构建模块——控制平面
- en: 'The OpenStack project has been launched to solve the IaaS paradigm. By adopting
    the pay-as-you-use model, underlying resources, including compute, networking,
    and storage, are exposed as pools and reserved on demand securely per the user’s
    request. Throughout the development of the OpenStack ecosystem (considered in
    other literature as the *cloud operating system* ), more open source projects
    have joined the emerging cloud software life cycle to extend its capabilities.
    As mentioned previously, the secret sauce of such project development comes from
    the natural API design, which consistently facilitates communication between services.
    The versatile number of services could sidetrack newcomers in understanding where
    and how fundamental parts of the ecosystem work to enable a well-designed architecture
    for custom needs and requirements. The following sections will iterate through
    what are considered the *core services* of OpenStack, to begin with. It is essential
    to get acquainted with each of these services as each OpenStack release, coming
    with new projects or services, relies on them. Being strategic in mastering core
    components and their capabilities will allow easier deployment of your future
    private cloud setup and, hence, extend its capabilities with less wasted time
    and effort. The following table shows the core services that will be tackled in
    the following sections:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack项目的启动是为了解决IaaS范式问题。通过采用按需付费模式，底层资源，包括计算、网络和存储，作为池子被按需安全地暴露并根据用户需求进行预留。在OpenStack生态系统的发展过程中（在其他文献中被称为*云操作系统*），更多的开源项目加入了新兴的云软件生命周期，扩展了其功能。正如前面所提到的，这种项目开发的秘诀在于自然的API设计，它始终促进了服务之间的通信。大量的服务可能会让新手困惑，不知道生态系统中的基本部分如何工作，从而启用为定制需求和要求设计的良好架构。接下来的章节将逐步介绍OpenStack的*核心服务*，作为开始。了解这些服务中的每一个至关重要，因为每个OpenStack版本都会引入新的项目或服务，并依赖于它们。战略性地掌握核心组件及其功能，将使您更容易部署未来的私有云设置，从而以更少的时间和精力扩展其能力。下表展示了接下来章节将要讨论的核心服务：
- en: '| **Service** | **Code name** | **Description** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **服务** | **代号** | **描述** |'
- en: '| --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Compute | Nova | Manages the **virtual machine** ( **VM** ) life cycle |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 计算 | Nova | 管理 **虚拟机**（**VM**）生命周期 |'
- en: '| Network | Neutron | Manages network environment per project |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | Neutron | 管理每个项目的网络环境 |'
- en: '| Identity | Keystone | Provides authentication and authorization information
    service |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 身份 | Keystone | 提供身份验证与授权信息服务 |'
- en: '| Block storage | Cinder | Manages the VM disks and snapshots life cycle |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 块存储 | Cinder | 管理 VM 磁盘和快照生命周期 |'
- en: '| Object Storage | Swift | Accessible REST API storage for object data types
    such as images and media files |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 对象存储 | Swift | 可访问的 REST API 存储，用于存储图像、媒体文件等对象数据类型 |'
- en: '| Image | Glance | Manages the VM image life cycle |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 镜像 | Glance | 管理 VM 镜像生命周期 |'
- en: '| Dashboard | Horizon | OpenStack frontend web interface |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 仪表盘 | Horizon | OpenStack 前端 Web 界面 |'
- en: '| File sharing | Manila | Manages projects across shared filesystems |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 文件共享 | Manila | 管理跨共享文件系统的项目 |'
- en: '| Scheduling | Placement | Helps to track provider’s resource inventories and
    usage |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 调度 | 放置 | 帮助跟踪提供商的资源库存和使用情况 |'
- en: '| Telemetry | Ceilometer | Provides data collection service for resource tracking
    and billing |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 遥测 | Ceilometer | 提供资源跟踪和计费的数据收集服务 |'
- en: '| Alarming | Aodh | Triggers actions and alarms based on collected metrics
    and configured rules |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 报警 | Aodh | 基于收集的指标和配置的规则触发动作和报警 |'
- en: Table 1.1 – OpenStack core services
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1.1 – OpenStack 核心服务
- en: Important note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is still possible to deploy an OpenStack environment without object storage
    and a dashboard. Some distributions come by default with both services enabled.
    For example, spinning up a VM does not necessarily require object storage. Additionally,
    the OpenStack **command-line interface** ( **CLI** ) supports all operations for
    each installation that can be directed without the OpenStack dashboard.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然可以在没有对象存储和仪表盘的情况下部署 OpenStack 环境。某些发行版默认启用了这两项服务。例如，启动一个虚拟机不一定需要对象存储。此外，OpenStack
    **命令行界面**（**CLI**）支持每个安装的所有操作，并且可以无需 OpenStack 仪表盘进行指令。
- en: Keystone – the authentication and authorization service
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keystone – 身份验证与授权服务
- en: The keywords *authentication* and *authorization* are the main functions of
    this identity service. **Keystone** is part of the control plane of the OpenStack
    ecosystem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 关键字 *身份验证* 和 *授权* 是此身份服务的主要功能。**Keystone** 是 OpenStack 生态系统控制平面的一部分。
- en: Security has been brought over from the earliest releases, and it became a necessity
    to dive further into the **AAA** (short for **Authentication, Authorization, Accounting**
    ) service in OpenStack. Each request made to contact an OpenStack service must
    be validated by Keystone (via the identity API). The API response will return
    an authentication token to be used against the requested service ( API call).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性从最早的版本开始被引入，并且深入研究 OpenStack 中的 **AAA**（**身份验证、授权、计费**）服务成为了必需。每一个联系 OpenStack
    服务的请求都必须由 Keystone 验证（通过身份 API）。API 响应将返回一个身份验证令牌，用于访问请求的服务（API 调用）。
- en: At this stage, the Keystone workflow might seem simpler than we thought. On
    the other hand, when we scale hundreds of hosts to fulfill thousands of requests
    in a short time, we should consider how to make sure that Keystone, as a critical
    service, is fully operational. That will be discussed in later chapters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，Keystone 工作流可能看起来比我们想象的简单。另一方面，当我们扩展数百个主机以在短时间内处理成千上万的请求时，我们应考虑如何确保 Keystone
    作为关键服务能够完全正常运行。这将在后面的章节中讨论。
- en: What has been brought from old OpenStack releases to the latest stable ones
    within the identity service is mainly the version of the API. Formerly, when installing
    the identity service, it was possible to keep running with version 2; starting
    from the *Grizzly* release, version 3 has been launched and can be installed.
    Today, the identity API version 2 has been deprecated in favor of newer releases.
    We will consider version 3 and above (at the writing of this book, version 3.14
    is the latest within the *Ussuri* OpenStack release).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从旧版 OpenStack 版本到最新稳定版，在身份服务中的主要变化是 API 的版本。以前，在安装身份服务时，可以继续使用版本 2；从 *Grizzly*
    版本开始，推出了版本 3，并且可以安装。今天，身份 API 版本 2 已被弃用，取而代之的是更新的版本。我们将考虑版本 3 及以上（截至本书撰写时，版本 3.14
    是 *Ussuri* OpenStack 版本中的最新版本）。
- en: But what can an identity API offer? As mentioned previously, the design of all
    communications between OpenStack services is made via APIs (later, we will skim
    the surface for interaction even with non-OpenStack services via APIs). Consider
    that a failed simple authentication request might reflect a service failure from
    the end user’s perspective. A good practice is to make sure, on the first iteration,
    that there are no issues with Keystone requests when troubleshooting a failing
    service request. Although the previous editions of this book did not dive deep
    into the Keystone workflow, we will revisit it in more detail in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane –* *Shared Servi ces* .
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 那么身份 API 能提供什么呢？如前所述，所有 OpenStack 服务之间的通信设计都是通过 API 实现的（稍后，我们将简单介绍如何通过 API 与非
    OpenStack 服务进行交互）。考虑到一个失败的简单身份验证请求可能会从最终用户的角度反映出服务故障。一个好的做法是，在首次排查故障时，确保 Keystone
    请求没有问题，特别是在服务请求失败时。尽管本书的前几个版本没有深入探讨 Keystone 工作流，但我们将在 [*第3章*](B21716_03.xhtml#_idTextAnchor108)
    中以更详细的方式重新审视它，*OpenStack 控制平面 - * *共享服务*。
- en: Nova – the compute service
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nova – 计算服务
- en: '*Austin* , the first OpenStack release, brought the first version of the **Nova**
    project to spin up an instance and started dealing with compute management through
    a rich API. Unlike the aforementioned identity service, Nova has evolved through
    different releases to accommodate more amazing functions but at the cost of its
    complexity compared to other core services. The Nova service design has not changed
    a lot from the *Grizzly* release. On the other hand, it is still required to reiterate
    through the different Nova pieces that form the compute service engine in the
    OpenStack ecosystem.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*Austin*，OpenStack 的第一个版本，推出了 **Nova** 项目的第一个版本，用于启动实例，并开始通过丰富的 API 处理计算管理。与前述的身份服务不同，Nova
    在不同版本中不断发展，增加了更多惊人的功能，但相较于其他核心服务，它的复杂性也有所提高。Nova 服务的设计自 *Grizzly* 版本以来变化不大。另一方面，仍然需要反复讨论构成
    OpenStack 生态系统中计算服务引擎的不同 Nova 组件。'
- en: Important note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There are plenty of capabilities brought by Nova in the latest OpenStack releases,
    which will be detailed in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125) , *OpenStack
    Compute – Compute Capacity* *and Flavors* .
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新的 OpenStack 版本中，Nova 带来了许多新功能，详细内容请参见 [*第4章*](B21716_04.xhtml#_idTextAnchor125)，*OpenStack
    计算 - 计算能力* *与规格*。
- en: The **nova-api** service interacts with user API calls that manage compute instances.
    It communicates with other components of the compute service over a message bus.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-api** 服务与用户 API 调用进行交互，管理计算实例。它通过消息总线与计算服务的其他组件进行通信。'
- en: The **nova-scheduler** service listens to the new instance request on the message
    bus. The job of this service is to select the best compute node for the new instance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-scheduler** 服务监听来自消息总线的新实例请求。该服务的任务是为新实例选择最佳计算节点。'
- en: The **nova-compute** service is the process responsible for starting and terminating
    VMs. This service runs on the compute nodes and listens for new requests over
    the message bus.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-compute** 服务是负责启动和终止虚拟机的进程。该服务运行在计算节点上，并通过消息总线监听新的请求。'
- en: The **nova-conductor** service handles database access calls from the compute
    nodes to limit the risk of database access by an attacker via a compromis ed host.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-conductor** 服务处理计算节点的数据库访问请求，以限制攻击者通过被妥协的主机对数据库的访问风险。'
- en: Placement – the scheduling service
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Placement – 调度服务
- en: Whenever it comes to a new system improvement or logical addition, the OpenStack
    community will create, move, or place different services or components to make
    that happen. One of the major updates of the OpenStack ecosystem since the *Newton*
    release is the introduction of the **Placement** API service. Before the latter
    release, users struggled to identify the counter on different sets of resource
    providers, such as computation, networking, and storage allocated pools. That
    is where it is useful to dedicate a separate service, hence an API well connected
    with other services (such as Nova), to track those resource providers and keep
    an eye on their usage. The Placement service acts mainly as a resource inventory.
    Not only that, but the most pertinent addition has also made the process of filtering
    (via **nova-scheduler** ) more fine-grained. In some other glossaries, you might
    find the word *prefiltering* as a reference for the new Placement service. This
    step is the preparation of available compute nodes by starting a filtering process
    before dealing with the scheduler based on some configurable specs and traits.
    The Placement service will be demonstrated in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity* *and Flavors* .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每当涉及到新的系统改进或逻辑添加时，OpenStack 社区会创建、移动或部署不同的服务或组件来实现这一目标。自 *Newton* 版本以来，OpenStack
    生态系统的一个重要更新是引入了 **Placement** API 服务。在此版本之前，用户在识别不同资源提供者（如计算、网络和存储分配池）上的计数器时遇到困难。正是在这里，专门为此创建一个独立的服务，进而通过与其他服务（如
    Nova）的良好连接的 API 来跟踪这些资源提供者并监控其使用情况，就显得尤为重要。Placement 服务主要作为资源清单进行工作。不仅如此，最重要的改进还使得过滤过程（通过
    **nova-scheduler** ）更加精细。在一些其他术语中，你可能会看到 *预过滤* 这个词作为对新 Placement 服务的引用。此步骤是在处理调度器之前，根据一些可配置的规格和特性，启动过滤过程以准备可用的计算节点。Placement
    服务将在 [*第 4 章*](B21716_04.xhtml#_idTextAnchor125) ，*OpenStack 计算 – 计算容量* *与规格*
    中演示。
- en: Glance – the imaging service
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Glance – 镜像服务
- en: To launch an instance in OpenStack via the Nova service, an instance image will
    be required to successfully accomplish VM provisioning. **Glance** is one of the
    core services that deals not only with instance images (as a template source image)
    but also the snapshots that can be created from an instance. When it comes to
    the question of where Glance will store images and snapshots, the answer can be
    found in its latest supported driver configurations. As with many other OpenStack
    services, a number of storage options can be used to back the storage of images
    by Glance. Again, if we think about the OpenStack infrastructure extension in
    the short and long term, we must think about which backend storage Glance should
    use. The OpenStack community has developed the most commonly used storage backend
    either within proper OpenStack-supported storage services such as *Swift Object
    Storage* and *Cinder Block Storage* or a third-party extension such as *Ceph*
    storage based on **RADOS Block Device** ( **RBD** ), VMware storage, and even
    AWS **Simple Storage Service** ( **S3** ) storage.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过 Nova 服务在 OpenStack 中启动一个实例，成功完成虚拟机配置需要一个实例镜像。**Glance** 是一个核心服务，它不仅处理实例镜像（作为模板源镜像），还处理可以从实例创建的快照。当涉及到
    Glance 存储镜像和快照的位置问题时，答案可以在其最新支持的驱动配置中找到。与许多其他 OpenStack 服务一样，可以使用多种存储选项来支持 Glance
    的镜像存储。再次提醒，如果我们从短期和长期来看待 OpenStack 基础设施扩展，我们必须考虑 Glance 应该使用哪种后端存储。OpenStack 社区已经开发了最常用的存储后端，无论是在适当的
    OpenStack 支持的存储服务中，如 *Swift 对象存储* 和 *Cinder 块存储*，还是基于 **RADOS 块设备**（**RBD**）的第三方扩展，如
    *Ceph* 存储、VMware 存储，甚至是 AWS **简单存储服务**（**S3**）存储。
- en: This variety of backend storage options might increase the paradox of architectural
    choices, but it raises the question of which needs we would address from business
    and capacity perspectives. As an example, Glance configuration enables multiple
    storage backends in the same configuration layout that can be customized to instruct
    the imaging service to use an existing block storage pool to attach images directly
    and reduce the time of waiting to download from scratch. A massive Ceph infrastructure
    with dozens of object storage nodes can be leveraged if they are operational by
    dedicating a Ceph pool for production images, making use of existing resources
    granted by the stable driver integration between Glance and the Ceph backend.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样化的存储后端选项可能增加架构选择的矛盾，但它提出了一个问题：我们从业务和容量角度应该解决哪些需求。例如，Glance 配置允许在相同的配置布局中启用多个存储后端，可以定制指示映像服务使用现有的块存储池来直接附加图像，减少从头开始下载的等待时间。如果
    Ceph 基础设施庞大，并且拥有数十个对象存储节点，可以通过将一个 Ceph 存储池专用于生产图像，利用 Glance 与 Ceph 后端之间稳定的驱动程序集成所提供的现有资源来提高其使用效率。
- en: Important note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is important to understand your requirements before selecting any type of
    backend, as not all backends will support the same features. For example, within
    the *Antelope* release, the OpenStack Glance team included a new feature in the
    block storage backend – *Cinder* – by allowing the extension on the running attached
    volumes. The Ceph storage extension will be discussed in more detail in [*Chapter
    5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage – Block, Object, and*
    *File Shares* .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择任何类型的后端之前，理解您的需求非常重要，因为并非所有后端都支持相同的功能。例如，在 *Antelope* 版本中，OpenStack Glance
    团队在块存储后端 *Cinder* 中加入了一个新特性——允许扩展运行中的附加卷。Ceph 存储扩展将在 [*第 5 章*](B21716_05.xhtml#_idTextAnchor146)
    中详细讨论， *OpenStack 存储 – 块存储、对象存储和* *文件共享*。
- en: Conversely, more interesting use cases would leverage Swift as a storage backend
    for Glance snapshots and templates. That can be considered a safe internal backup
    scenario. Going further, increasing your **fault tolerance** ( **FT** ) domain
    can cross public cloud services such as the AWS S3 backend.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，更多有趣的使用案例将利用 Swift 作为 Glance 快照和模板的存储后端。这可以被认为是一种安全的内部备份场景。进一步说，增加您的 **容错能力**（**FT**）领域可以跨越公共云服务，比如
    AWS S3 后端。
- en: 'One of the most impressive progressions in the Glance service is the wider
    list of supported images. The *Antelope* release came with at least 11 supported
    formats, including RAW, QCOW2, VDI, VHD, ISO, OVA, PLOOP, and Docker, and great
    additional formats dealing with Amazon public cloud compatibility are AKI, AMI,
    and ARI:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Glance 服务中最令人印象深刻的进展之一是支持的图像格式列表的扩展。*Antelope* 版本支持至少 11 种格式，包括 RAW、QCOW2、VDI、VHD、ISO、OVA、PLOOP
    和 Docker，另外还增加了处理亚马逊公共云兼容性的格式，分别是 AKI、AMI 和 ARI：
- en: '![Figure 1.1 – Glance supported backends](img/B21716_01_01.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Glance 支持的后端](img/B21716_01_01.jpg)'
- en: Figure 1.1 – Glance supported backends
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Glance 支持的后端
- en: Swift has been one of the most preferred options to store Glance images in large
    deployments, which will be highlighted in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Swift 一直是大型部署中存储 Glance 图像的最优选项之一，下一节将重点介绍这一点。
- en: Swift – the object storage service
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swift – 对象存储服务
- en: Together with Nova, **Swift** came with the first-ever OpenStack release, *Austin*
    , back in 2010. Object storage has added great value to enterprises that resolve
    several storage challenges, unlike the traditional persistent storage design.
    Swift itself has been a revolutionary service at a time when cloud technologies
    just started to warm up. As per the nature of the object storage model, objects
    are stored in a flat hierarchy. Most of the main usage of Swift in a given OpenStack
    deployment is to perform archiving and backups. Many services around the OpenStack
    ecosystem that deal with storage are compatible with the Swift storage backend
    either for direct storage, backup purposes, or even both.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Nova 一起，**Swift** 在 2010 年推出了首个 OpenStack 版本 *Austin*。对象存储为解决多个存储挑战的企业提供了巨大价值，区别于传统的持久化存储设计。Swift
    本身是一个革命性的服务，尤其在云技术刚刚起步的时期。根据对象存储模型的特点，对象以平面层级结构存储。在特定的 OpenStack 部署中，Swift 的主要用途是进行归档和备份。许多与
    OpenStack 生态系统中存储相关的服务，都与 Swift 存储后端兼容，无论是直接存储、备份目的，还是两者兼有。
- en: 'As we will unleash a few of the latest additions on the Swift service in [*Chapter
    5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage – Block, Object, and
    File Shares* , a brief revisit of object storage common specs are listed as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Developed with no **single point of failure** ( **SPOF** ) by design
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposes the HTTP REST API for common object management
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly scalable and a good fit for workloads demanding boosted performance
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designed for eventual consistency
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly available by design and can scale easily horizontally through inexpensive
    hardware
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at the next storage optio n: the Cinder service.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Cinder – the block storage service
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other storage offering in the OpenStack ecosystem and counted as a core
    service is the block storage service named **Cinder** . Cinder has been developed
    to maintain independent life cycle operations of instance volumes (as virtual
    disks). The Cinder API provides an exhaustive list of different volume and snapshot
    operations, including create, delete, attach, detach, extend, clone, create images
    from volumes, create volumes from images, and create volumes from snapshots. With
    recent OpenStack releases, more operational capabilities have been developed to
    support backups, restoration , and volume migration.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Manila – the shared filesystems service
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: File-sharing solutions have been an emerging storage option for enterprises.
    The need to have a collaborative central storage brain for access facility and
    simplicity of management has grown in the latest decade in IT infrastructure either
    on-premises or in cloud environments. The OpenStack community did not miss the
    opportunity since the *Liberty* release, introducing the **Manila Distributed
    and Shared File Systems** solution. Coming from the same logical workflow as Cinder,
    Manila has not been included as a core OpenStack project due to its limited capabilities.
    As per the *Kilo* release until *the latest releases* , Manila had proven itself
    as a stable component within the OpenStack infrastructure, allowing enterprises
    to gain access to self-service file-sharing for different hungry resources and
    clients.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, several storage vendors have kept the momentum of contributing
    to the Manila service by developing more drivers to support their storage backend
    hardware. Following the same steps as Cinder, we might find an exhaustive list
    of supported file share drivers by Manila, including CephFS, LVM, Hadoop HDFS,
    EMC, IBM, and many others. A full list of supported drivers can be found on the
    Manila OpenStack web page: [https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html](https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html)
    . More interesting updates have been included in the latest releases for the Manila
    service, including security, backup share integration, and increased level of
    accessibility, which will be discussed more in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Storage – Block , Object, and* *File Shares* .'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Neutron – the networking service
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous citation might be arguable if we consider **nova-network** still
    being used! On the other hand, since the *Grizzly* release, the **Neutron** project
    has become a more **Networking as a Service** ( **NaaS** ) pillar in the OpenStack
    ecosystem. That makes sense due to the approach taken by the community to provide
    an independent networking service and not embedded, as is the case with the Nova
    compute service. The transition from legacy **nova-network** to Neutron has been
    tougher for companies willing to migrate from the old days to the new network
    era led by Neutron. Although we will keep the light on Neutron in the next parts
    of the book, a section will be dedicated in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* , to discuss
    the more advanced features of Neutron.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'The motivation to adopt Neutron is fairly obvious considering the rise of this
    project to solve many **nova-network** limitations and open new networking capabilities.
    Compared to **nova-network** , which provides basic networking in the OpenStack
    ecosystem (mainly limited to interaction with the compute service), Neutron brings
    several aspects and features that can summarized as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Simplified self-service for projects to provision ports, subnets, networks,
    and router objects
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced networking features can be deployed in no time, such as firewalls,
    **virtual private networks** ( **VPNs** ), and load balancers
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More support for complex network topologies
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vast ways of third-party network solution integration
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A standalone service making architecture less complex to support **high** **availability**
    ( **HA** )
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective service for all sizes of deployments
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the emergence of hardware and software based on **software-defined networking**
    ( **SDN** ) solutions, the OpenStack Neutron team has taken the lead to engage
    more support on the SDN part, resulting in the development of Neutron drivers
    to integrate some SDN famous implementations such as OpenContrail and VMware NSX.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Since 2013, within each OpenStack release, the Neutron project has been considered
    one of the services with the most number of commits and feature development. [*Chapter
    6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking – Connectivity and
    Managed Service Options* , will iterate in depth about more Neutron capabilities
    and connect the pieces with SDNs within th e latest OpenStack releases.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Ceilometer – the telemetry service
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The telemetry service was introduced in the OpenStack ecosystem in the *Havana*
    release. Code-named **Ceilometer** , the telemetry service has emerged with a
    pertinent mission: record, gather, and monitor resource utilization metrics across
    the OpenStack infrastructure. Unlike older OpenStack releases, the latest Ceilometer
    versions include only metric collection features, leaving alarming and notification
    functions to anoth er dedicated project code-named Aodh.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Aodh – the alerting service
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As part of the monitoring services chain, enabling users to trigger custom notifications
    based on telemetry events has become a required addition to keeping an eye on
    deployed project resources. As mentioned in the *Ceilometer – the telemetry service*
    section, the alarming feature has been separated from Ceilometer, and **Aodh**
    is dedicated to firing alarms based on configured rules and set thresholds.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Ceilometer is designed to send collected metric data to other destinations,
    including a non-OpenStack open source project named *Gnocchi* . The Gnocchi project
    formulated the time-series **Database as a Service** ( **DBaaS** ) term. In this
    case, a dedicated database interface is no lo nger required to store and query
    metrics.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Horizon – the dashboard service
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a **graphical user interface** ( **GUI** ) to operate your OpenStack
    environment would definitely make your administrative tasks much simpler. Surprisingly,
    the OpenStack first releases did not bring a GUI initiative to fire a few clicks
    from a dashboard to assist in managing the infrastructure resources. It was only
    in the *Essex* release that **Horizon** was born to accommodate the first core
    project’s operations, such as the creation of object containers, operating a life
    cycle of instances, managing images, and a basic layout on project users’ management,
    formerly named tenant users. The need to include more control from the dashboard
    has been felt with the extensive inclusion of more services, and, within the latest
    releases, we might notice that the Horizon experience is much more sophisticated.
    Built with the Django framework and supporting most of the OpenStack APIs, the
    majority of the core services’ resources can be operated through Horizon. On the
    other hand, more advanced operations such as customizing networking configuration
    can be fired only using the OpenStack CLI. Additionally, do not expect that Horizon
    will automatically include an extra service beyond the core ones. It is not a
    limitation, but, for modular design reasons, installing a new service would require
    the addition of a new Horizon module associated with it and defined as a *panel*
    . Horizon, with a good reference of the CLI, would be sufficient to administer
    your cloud if running a small setup. Talking about the wider infrastructure, with
    dozens of projects and hundreds of resources, administering this size of setup
    might not be ideal using Horizon. Agile thinking would require more automation
    and scripting on a large-scale size. In [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right Way (DevSecOps)* , we will discuss
    in detail the automation of the OpenStack setup and how to use the same approach
    to manage r esources within project resources themselves.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Non-OpenStack services
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As part of the OpenStack control plane, we might find, as per any software
    architecture, other essential core components that all OpenStack services rely
    on: **Advanced Messaging Queuing Protocol** ( **AMQP** ) and database services.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Passing the message – AMQP
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AMQP, the messaging queue service, is one of the other ingredients for making
    a modular architecture design in the OpenStack ecosystem.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: For each request hitting a specific OpenStack service API, the message queue
    server will make sure to communicate messages between all engaged components (processes)
    to accomplish the request asynchronously. A variety of open source AMQP software
    can be used throughout your OpenStack deployment if you are biased about using
    one of them; it can just be simply specified during the setup to have it runn
    ing and communicating with different components. RabbitMQ, Qpid, and ZeroMQ are
    supported message queue solutions; RabbitMQ is the most used one. Being an expert
    in AMQP operations is not a requirement for a successful OpenStack operation day,
    but a few main points should be taken into consideration when dealing with a message
    queue in OpenStack. We will see in later chapters how OpenStack services depend
    on the message queue from an architectural perspective, giving importance to such
    services running on the highest possible availability. The other aspect, having
    some adequate familiarity with how messaging concepts work, would be more than
    helpful for troubleshooting days. [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane – Shared Services* , and [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , will highlight in sequence
    the internal workflow of AMQP in OpenStack and how to maximize its availability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Storing the state – the database
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The other fundamental OpenStack control plane service is the database. Similar
    to the AMQP service, each OpenStack service will require a database connection
    to store its different states. The database in the OpenStack ecosystem defines
    persistent storage that can be consulted and updated by all services upon each
    request. The database topic from an architectural perspective has been a *hot*
    topic mostly due to security concerns. As mentioned earlier, in the *Nova – the
    compute service* section, there is a tendency from that learning curve to reduce
    any possible insecure access to the database or services running in different
    hosts that could potentially present a security risk. The other side of the database
    sensitivity topic, similar to the AMQP service, is the availability dilemma. Besides
    the risk of failures, OpenStack architects and administrators will need to reiterate
    through the other traditional database anomaly once it starts growing: performance.
    [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane – Shared
    Services* , and [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly
    Available Cloud – Meeting the SLA* , will cover how to ensure a healthy start
    of the database for larger OpenStack deployments. [*Chapter 9*](B21716_09.xhtml#_idTextAnchor204)
    , *Benchmarking the Infrastructure – Evaluating Resource Capacity and Optimization*
    , will bring the motivation behind the database bench marking routine for a better
    state of performance.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Other services
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The latest OpenStack releases have demonstrated goodwill to OpenStack users
    to deploy and operate an OpenStack private cloud infrastructure not only with
    very stable core services but also with additional ones. Compared to older releases,
    several incubated projects have been in on-off mode, making the decision to adopt
    some of them in your existing deployment insecure. Different insights from different
    project initiatives have sculpted more confident software within the latest OpenStack
    releases ready to support more features and services in your cloud environment.
    Some of the extra stable services are listed in the following table in the order
    of integration through different OpenStack cycle releases:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service** | **Release** | **Description** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| Heat | Havana | Orchestration PaaS extension on top of the IaaS provided
    by OpenStack. Heat abstracts cloud resources into code – also known as **domain-specific
    language** ( **DSL** ). That comes when cloud operators leverage the power of
    code templating to automate things, reduce human error, and increase the agility
    of resource management. Heat uses **Heat Orchestration Templates** ( **HOTs**
    ), in YAML or JSON format, allowing users to define the whole infrastructure to
    run their workloads from code. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| Trove | Icehouse | DBaaS allows users to automate the provisioning of relational
    and non-relational scalable databases with the minimum overhead of database tasks
    such as patching, maintenance, and backup through provided automation. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| Sahara | Juno (incubated in Icehouse) | **Elastic Data Processing as a Service**
    ( **EDPaaS** ) provides a means of orchestrating structured and unstructured data
    processing and analysis software and infrastructure on top of OpenStack, such
    as Hadoop and Spark clusters. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| Ironic | Kilo | **Bare Metal as a Service** ( **BMaaS** ) allows users to
    provision infrastructure directly on a physical machine, thus no hypervisor layer
    is involved. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| Murano | Kilo | **Application as a Service** ( **AaaS** ) enables developers
    to speed up application deployment and publish into an application catalog that
    can be browsable and ready for deployment in an automated fashion. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Designate | Liberty | **DNS as a Service** ( **DNSaaS** ) handles **Domain
    Name Service** ( **DNS** ) entries such as DNS records and zones. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Barbican | Liberty | **Secret Manager as a Service** ( **SMaaS** ) is designed
    to centrally store different types of secrets, such as passwords, keys, and certificates,
    to be used by other OpenStack services in a secure way. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Zaqar | Liberty | **Messaging as a Service** ( **MSaaS** ) allows users to
    provision and manage multi-tenant messaging and notification queues. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Magnum | Liberty | **Container as a Service** ( **CaaS** ) orchestrates a
    set of containers through a supported **container orchestration engine** ( **COE**
    ) such as Docker Swarm, Kubernetes, and Mesos. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| Octavia | Liberty | **Load Balancer as a Service** ( **LBaaS** ) provides
    a load balancing function. Octavia can be counted as an enterprise-class load
    balancer solution. This service has become an attractive feature that can be used
    much more easily compared to the Neutron LBaaS driver. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: Table 1.2 – Additional OpenStack services
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing the core and incubated OpenStack services, we can now walk through
    different functional requirements that would construct what a private cloud can
    offer as services.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the motivation
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before building a picture of your future OpenStack environment, it is necessary
    to have business goals that align with the outcome of the OpenStack solution.
    As we have seen briefly in the previous sections with the latest updates in the
    OpenStack glossary, you might feel how overwhelming it could be with newly developed
    services and features. Obviously, the new image of OpenStack looks more robust
    than ever, with many hands of **Anything as a Service** ( **XaaS** ) opportunities.
    The array of choices might prevent architects and operators from starting a proper
    OpenStack infrastructure exercise if the scope of your business is not defined
    from the first day of drafting the design. As with existing major public providers
    such as AWS, Azure, or GCP, the OpenStack deployment should be considered as a
    way of investment in the long run. Expecting operational and cost savings can
    be felt only at later stages. Being strategic is essential to take advantage of
    the OpenStack solution if used properly from the early days. Hundreds of enterprises
    have switched gears to use OpenStack in their environment, but not all of them
    had a successful journey and soon dropped it. For this reason, understanding in
    which use case your business will fall ensures a safe start to your OpenStack
    journey.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Since 2010, OpenStack usage has taken different facets whereby companies have
    invested in the OpenStack ecosystem, followed the milestone generation, and adopted
    it. After 14 years, we can see a plethora of deployments where OpenStack shines
    with a respectful history of success. The next sections will broadly list a few
    common use cases of implementations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Application development accelerator
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The modern **software development life cycle** ( **SDLC** ) takes a more mature
    direction by enhancing the stages of its release and automating the whole chain,
    from simple code commits to different types of testing. Although **Continuous
    Integration/Continuous Delivery** ( **CI/CD** ) methods and tools emerged slightly
    before the rise of the cloud era, delivering a well-tested software product has
    faced a new challenge: *resource limitations* . Although the wording *limitation*
    is not exactly the term that would describe this challenge, from a developer’s
    perspective, it means there are no available resources to continue or perform
    specific unit, smoke, or integration testing. Preproduction environments are even
    more challenging to validate in all stages if a business will not allow the product
    to go live without provisioning exactly the same environment as in production
    for the sake of testing. Resource instrumentation and virtualization have resolved
    this issue, but only partially. With a lack of automation and an engine of operation,
    the whole chain still does not save costs and reduce the **time to market** (
    **TTM** ). That is where OpenStack has been engaged for several hundred enterprises:
    accelerate product releases by running their CI/CD pipelines in a multi-tenant
    OpenStack environm ent.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Cloud application enabler
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are planning to provide what most developers dream of (developing from
    scratch!), then you are in the right corner. Whether the developed application
    is a web pet store, online reservation assistant, or flight reservation engine,
    wrapping the application in its proper infrastructure can be done in a fast and
    clean way using Heat, for example. As the application will be defined as *code*
    in a HOT file, you will not need to worry about the application configuration
    consistency as well as its degree of scalability if autoscaling is declared in
    the template. As OpenStack services will take care of all the automation overhead,
    application owners can focus on the next level of the application business to
    find more spots for improvements rather than wasting effort on resource management
    and operation overhead.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The other trait of using OpenStack, especially within the survey from the *Kilo*
    release, is the trending cloud-ready application. OpenStack enables a fast application
    publishing process in different ways. As container technologies adoption increases,
    OpenStack has included container services to support a few famous container orchestration
    engines, such as Docker Swarm and Kubernetes, via the OpenStack Magnum service.
    76% of users in the *User Survey* from the *Juno* release showed interest in the
    OpenStack and containerization marriage. A well-known example of a vast OpenStack
    environment is CERN. Based on OpenStack, CERN is running over 300,000 cores, with
    a big chunk of it being Kubernetes workloads (over 500 clusters) within the Magnum
    space.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: More details on the conducted OpenStack survey can be found at [https://www.openstack.org/user-survey/2022-user-survey-report](https://www.openstack.org/user-survey/2022-user-survey-report)
    .
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The Murano application catalog service is another OpenStack service that facilitates
    application publishing in no time based on contai ners.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: HPC supporter
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might be confusing to mix the *HPC* term in the context of OpenStack. Eventually,
    OpenStack does not access the hardware to configure compute power and increase
    hardware performance. On the other hand, it enables designing your commodity hardware
    in a way to grow horizontally when hitting more compute, storage, and network
    demand. In the HPC context, the main focus is on augmenting the underlying hardware
    capacity, and the cloud will secure the multi-tenancy to access resources in the
    most optimized way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, in the early days of OpenStack when only Nova and Swift existed,
    the **National Aeronautics and Space Administration** ( **NASA** ) and Rackspace
    were the initiators of the first OpenStack production environment running HPC.
    More research organizations have since adopted the OpenStack ecosystem to deliver
    compute and storage at a large scale to researchers and scientists, but again,
    selecting the hardware layout and planning ahead is still required to feel the
    added value of OpenStack either via virtualized infrastructure, bare metal, or
    both. OpenStack enables all methods of orchestration for HPC environment depl
    oyments.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Network functions virtualization moderator
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Only a few enterprises in the telecommunication industry realized the opportunities
    of virtualizing network services in the early days. Following the same steps as
    the server virtualization experience, enterprises started investing in the new
    trend of running their edge network services on standard machines and providing
    functions such as routers, proxies, load balancers, and firewalls to consumers
    as VMs. This became a major asset in responding quickly to network changes and
    feature demands. **Network functions virtualization** ( **NFV** ) relies heavily
    on compute power that can run on commodity hardware and virtual environments,
    whereas in the traditional way, physical appliances running each network function
    must exist. In the OpenStack context, NFV will need only a compute service to
    expose different functions. As per the *Kilo User Survey* , more than 12% of organizations
    use OpenStack in the networking and telecommunication industry. As Telco adopts
    OpenStack for such a purpose, Nova will be your assistant but with a strategic
    eye on compute resources and placement to make i t happen.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Big data facilitator
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Volkswagen AG and BMW have shared their feedback on using OpenStack for big
    data and analytics, and the outcome was valuable assistance of the ecosystem orchestrating
    all the different services and resources to accomplish some large data analytics
    tasks. Before the *Juno* release and the unleashing of the Sahara – EDPaaS – project,
    big data resources could be orchestrated using HOTs to provision a cluster in
    no time. Sahara was unleashed to simplify the deployment of such resources in
    more granular and ready-to-go ways, considering the type of the data processing
    framework, versions, size of the cluster, desired topology, and so on. The big
    data scope in OpenStack can be wider when thinking out loud by using some advanced
    features of the OpenStack services, such as storage and networking. Companies
    keep showing interest in automating their **Extract-Transform-Load** ( **ETL**
    ) pipelines internally by leveraging the OpenStack big data service. If you consider
    your business to be bound to a Hadoop provider, for example, make sure that performance
    is a key specification for your underlying hardware that No va will use.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Private cloud service provider
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Private cloud environments could be the first main motivation when reading the
    definition of OpenStack for the first time in the old releases’ documentation.
    Being private will limit the services offered to end users within the perimeter
    of your organization or, technically, *behind* *the firewall* .
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The term *private cloud* might sum up all the use cases previously. The only
    difference is that previous OpenStack implementations can stay isolated within
    your perimeter or serve on -demand needs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Public cloud service provider
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Going public! Why not? Back in the early 2000s, Amazon internally launched
    a private elastic compute cloud to enable infrastructure agility for developers
    and users running the giant online e-commerce website and other services selling
    millions of items worldwide. **Simple Queue Service** ( **SQS** ) was launched
    and exposed to the public as the first cloud service, then later S3, followed
    by the EC2 service. The idea started with object storage named S3 and the EC2
    service. Originally, NASA and Rackspace initiated and immensely contributed to
    software code to create software operating a private cloud initially for elastic
    computation and object storage capabilities. That is how Nova and Swift came to
    be. The first release attracted early-bird companies seeking innovations and started
    contributing to the new open source and soon cultivating more experience on market
    models for XaaS products. Rackspace launched its *public cloud* version offering
    compute and object storage services initially. This was followed by many others
    adopting a public cloud powered by OpenStack, such as Open Telekom Cloud, Cleura,
    VEXXHOST, and many more. Lately, it feels like the world runs on OpenStack with
    the hashtag **#RunOnOpenStack** . With its widespread implementation with big
    names around the world, the OpenStack community has initiated the concept of *OpenStack
    public cloud passports* . The idea behind it is to act as a major public cloud
    provider by providing OpenStack services in different regions and locations worldwide.
    The program ensures a massive hub of collaboration between different OpenStack
    public cloud providers, enabling users to roam between more than 60 **availability
    zones** ( **AZs** ) worldwide. The [openstack.org](http://openstack.org) website
    keeps updating the public cloud locations that can be filtered through the OpenStack
    public cloud marketplace page: [https://www.openstack.org/marketpla ce/public-clouds/](https://www.openstack.org/marketplace/public-clouds/)
    .'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Picking up the pieces
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Independent of which use case your future OpenStack deployment will start with,
    mastering the art of its ecosystem is a vital step before investigating any further
    design and architecture options. OpenStack covers more than 1 million lines of
    Python code! At this stage, there is no need to worry about learning how the code
    works: far more important is to take a reverse engineering approach: understanding
    the ecosystem interactions. Simply understanding the interrelation of core services
    gives you the keys to the kingdom of the art of design. Starting with the basics,
    the foundational components will open more doors to the next ones if we master
    the core ones correctly. So far, we have revisited core services of OpenStack
    that exist with each release and had a brief scan of the incubated projects, particularly
    within the latest releases. Next, we will select a common workflow that illustrates
    the different steps and pieces to spawn a VM. This type of workflow will demonstrate
    how different OpenStack core services interact and will involve almost all the
    components.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The listing of which services are counted as core ones might differ between
    different resources and documentation. Telemetry, scheduling, and dashboard services
    could be considered optional services. The current assumption is based on real-world
    deployment that leverages the usage of those services from the start of the journey.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The first iteration demonstrates a high level of the building blocks of different
    OpenStack core services that include compute, imaging, identity, block storage,
    monitoring, scheduling, dashboard, and networking services:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – High-level services interaction spawning instance workflow](img/B21716_01_02.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – High-level services interaction spawning instance workflow
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to take note of the main blocks before jumping into the granular
    presentation of the spawning process as illustrated in the previous diagram. By
    providing the Horizon dashboard or via the CLI (similarly, via APIs), an authentication
    process will be fired first by challenging the user request with login credentials
    that will be handled by the identity service, Keystone. Once validated (an authentication
    token will be provided for the whole session and subsequent requests), the request
    will reach the Nova API to take the next steps. Nova will interact with each of
    the following services counted as *mandatory* to successfully spawn an instance:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: The imaging service, Glance, queries an image to start the instance.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The networking service, Neutron, acquires networking resources.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The block storage service, Cinder, allocates storage volumes.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scheduling service, Placement, allocates available compute provider resources.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the whole chain of interactions passes through the API of each component.
    For each inter-component request, the core database will be consulted for read
    and object updates. As per spawning an instance, a record of a new VM instance
    will be created in the instance and used for reference with other services in
    the subsequent steps. As per the instance request, Nova will interact with the
    queuing message service as well – for example, to request an instance launch (for
    the Nova compute process).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: As part of this workflow, any API endpoint, scheduling, imaging, identity, and
    shared components such as database and queuing services are counted as part of
    the OpenStack control plane.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The control plane and data plane are ways of conceptual separation in complex
    system architectures. The terms will be defined in more detail in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane –* *Shared Services* .
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s zoom in and explore the workflow in a more granular view so that
    we can build a whole spawning instance picture. As demonstrated in the previous
    part of the chapter, each OpenStack service includes subcomponents working together
    to serve a specific request. Obviously, the API part exists in each service. Remember
    that is the *gate* of each module in the OpenStack ecosystem. Each exposed API
    service (providing an HTTP-based RESTful API) reaches other components and, subsequently,
    subcomponents via the queuing message service bus, as shown in the following workflow
    diagram:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Low-level services interaction spawning instance workflow](img/B21716_01_03.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Low-level services interaction spawning instance workflow
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the identity service, Keystone should be aware of the available
    services in the architecture setup to authorize any further interaction. Keystone
    holds a service catalog containing all API endpoints of different services.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous diagram illustrates each step by including different components
    and their respective subcomponents:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: A REST API call will be initiated by using Horizon or the CLI and reaches the
    identity service Keystone endpoint.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keystone challenges the request, gets the user credentials (a form of username
    and password), and validates the authentication via its API.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The validated authentication generates an authentication token (an **auth**
    token) back to the requester user. The **auth** token will be cached for the specific
    user to be used for subsequent API REST calls between the rest of the services.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As our request is a *compute* one, the next station that will be reached is
    the compute service API, **nova-api** . Again, the form of the request is a REST
    API, forwarded by Keystone from its catalog. You can think of the catalog as a
    *service map* where Keystone locates each service via the registered endpoint.
    In this case, the REST API encapsulated with the **auth** token will be sent to
    the compute service endpoint.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The compute API, the **nova-api** subcomponent, checks the **auth** token coming
    with a request reaching the compute endpoint. At this stage, the **auth** token
    will be validated in return by the identity service, Keystone. The purpose of
    this secondary interaction with Keystone is to inform the compute API in the request
    which permissions and roles users will have and for how long the **auth** token
    will stay alive before generating a new one (noted as the token expiry period).
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once validated, the first database interaction will take place by **nova-api**
    , which creates a new VM object that specifies the different parameters in the
    request. Bear in mind that each service will have its own logical database to
    update the state of different resources. In the current scenario, the Nova service
    holds its dedicated database schema for different state updates and read and write
    operations.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we come to the next interaction of another subcomponent of the Nova service,
    **nova-scheduler** , to specify which compute node will host the new instance.
    The **nova-api** subcomponent will contact the scheduler with such a request via
    the communication hub: the messaging queue service. This nature of the operation
    is processed via a **Remote Procedure Call** ( **RPC** ) call, noted as **rpc.call**
    , which will be published in the message queue and remain there till **nova-scheduler**
    picks it up.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-scheduler** subcomponent takes the lead on fetching the most convenient
    compute node to host the new instance. In our highlighted basic core design, the
    Placement service will be engaged in the process of selecting the best compute
    node(s).
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-scheduler** subcomponent reaches **placement-api** to query its compute
    resource provider. Note that the Placement service is still considered a new service
    as an extension of the other services for tracking their resources, such as compute,
    IP pools, and storage, expressed as resource providers. Since the *Stein* release,
    it has been extracted from the Nova ecosystem and runs on its own. In our workflow,
    we can consider the usage of Placement in the *prefiltering* stage before the
    final selection at the Nova scheduling level. The Placement service also exposes
    its own endpoint in the Keystone catalog. Hence, it will validate the **auth**
    token via the identity service, which updates the token header and returns it
    to the **placement-api** process.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The selection of hosts is determined via the Placement service by querying its
    own inventory database and, at the first iteration, returns a selection of compute
    nodes and any traits associated with them.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By applying its filtering and weighing parameters at the returned result by
    the **placement-api** component, the **nova-scheduler** process will come up with
    the right compute node and reach **placement-api** in the second iteration to
    claim the resources for the instance.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **placement-api** component adjusts its state records in the database, and
    **nova-scheduler** updates the state of the VM object in its database with the
    compute node ID and its hostname and then submits a message in the form of **rpc.cast**
    to the messaging hub. This message will be picked up by **nova-compute** to launch
    the instance in the designated hypervisor host ID.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To communicate with the next subcomponent, **nova-conductor** , **nova-compute**
    will publish the next **rpc.call** call in the message queue to start the launch
    of the instance with details for preparation, such as CPU, disk, and RAM specs.
    Note that **nova-compute** can be running in different process occurrences in
    different compute nodes. Only one particular **nova-compute** occurrence will
    publish **rpc.call** to **nova-conductor** based on the compute node chosen in
    *step 8* .
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we recall the **nova-conductor** addition in the Nova service architecture,
    the main purpose of this service is to play a *gate role* between the **nova-compute**
    process and the database to minimize the database exposure and hence reduce its
    attack blast radius. As we are aiming for a large OpenStack deployment, the **nova-compute**
    component will be detached from the controller node (holding the main control
    plane services) and will be dedicated to different physical hypervisor nodes.
    If one of the compute nodes has been compromised, spreading that security issue
    to the database will be more difficult due to the logical and physical separation
    when processes interact with the database. The **nova-conductor** component will
    be the one interacting directly with the Nova database to read the instance details,
    such as compute node ID, its requested flavors, memory, CPU, and disk claims.
    The state will be reported back to the messaging queue service by an RPC publish
    call.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **nova-compute** component picks up the published message and subsequently
    reaches the next service component by sending a RESTful API to the image service,
    Glance: the **glance-api** process. The request contains a form of **GET API**
    request for the image details encapsulated in the requested image ID information.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Glance will provide the requested image ID through its image-registered URL.
    But first, as we are reaching out to another service in the ecosystem, Glance
    will contact the identity service, Keystone, to validate the **auth** token again.
    Assuming the token session has not expired yet, Keystone will validate the token,
    and in the same way as Nova in *step 5* , the identity service will check the
    user role and permissions before it updates the token header and sends it back
    to the Image API.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Glance component is considered much simpler than Nova, as there is no interaction
    with a message queue in this step. The **glance-api** component simply checks
    the Glance database (running its own dedicated database schema), retrieves the
    metadata of the requested image ID, and reports back to **nova-compute** with
    the image URL format.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-compute** component reaches the image store and starts loading the
    provided image via the returned URL.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now come to the next service to interact with: networking. The **nova-compute**
    component sends a RESTful API request to the Neutron service, via **neutron-server**
    . As discussed earlier, Neutron (by naming convention in its own architecture)
    does not involve an explicit subcomponent name with the **api** suffix. The **neutron-server**
    subcomponent is the *API gate* that will deal with all API requests. Running the
    same identity cycle and assuming that the token has not expired yet, **neutron-server**
    will forward the API request to Keystone, where it gets validated, updates the
    token header with different roles and permissions, and sends it back to the **neutron-server**
    process.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **neutron-server** component checks the networking item’s status and respectively
    creates the requested resources encapsulated in the network parameters’ API call.
    Note that the Neutron service varies between different types of requests and hence
    exposes an array of workflows depending on the request complexity. We will cover
    other workflow scenarios dealing with other network controllers, agents, and plugins,
    such as **Open vSwitch** ( **OVS** ), in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* . For example,
    once a port is created for an instance, **neutron-server** will publish an RPC
    call in the message queue reaching out to the **Dynamic Host Configuration Protocol**
    ( **DHCP** ) agent. The **neutron-dhcp-agent** process will invoke its associated
    DHCP driver to reload its host’s file entries via the **dnsmasq** process. Once
    the instance is booted, **dnsmasq** is ready to pick up the instance request and
    send a DHCP offer for the final instance network configuration. This scenario
    also assumes a network and subnet are created where the instance will be attached,
    and that is specified in the instance creation request.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once networking parameters are acquired for the new instance request, **nova-compute**
    reaches the database and updates the VM object network state record.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last service to accomplish the instance creation is to reach out to the
    block storage service. Following the same way as previous services, **nova-compute**
    sends its RESTful API request to the Cinder API, the **cinder-api** subcomponent.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In turn, the Cinder API process will get to the identity service where Keystone
    validates the **auth** token. Assuming that it has not expired, Keystone returns
    the updated token header once validated with the roles and permissions set for
    the requester.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As with Nova and Neutron, Cinder has more subcomponents that would take a few
    steps further before the final admission of the request back to the **nova-compute**
    process. Cinder involves its own scheduler as well by publishing an RPC call to
    the messaging queue service.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **cinder-scheduler** subcomponent picks up the request and locates available
    resources for a new volume candidate list based on the requested volume specifications
    (volume size and type). Note that Cinder also provides another process, **cinder-volume**
    , that interacts with **cinder-scheduler** whenever a new read or write request
    is initiated within a specific storage provider. In this case, **cinder-volume**
    will be interfacing with backend driver methods to generate a candidate list that
    will be posted in the message queue and picked up later by the **cinder-scheduler**
    subcomponent.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **cinder-scheduler** subcomponent updates the state of the volume ID with
    its associated returned metadata in the Cinder database.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **cinder-api** component picks up the published message from **cinder-volume**
    and responds to **nova-compute** via a RESTful call.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **nova-compute** component receives the created volume metadata in the API
    request. By default, if no other specific hypervisor virtualization driver is
    being configured, Nova will execute the **libvirt** daemon and proceed by creating
    the instance on the designated compute node.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the final stage of the instance spawning workflow, the created volume should
    be mapped by the **cinder-volume** process. That is when the virtualization driver,
    in this case, **libvirt** , will make the volume available to the instance by
    firing a **mount** operation. The way it is performed depends on the storage provider
    and protocol, but a common case is passing the volume path to the hypervisor,
    which will mount it to the instance as a virtual block device.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The last piece of our initial workflow is the ingestion of the instance metrics
    in the Ceilometer service. It is vital to watch what has been spawned by checking
    the status of the created instance. Ceilometer provides two ways of collecting
    data metrics for instances: *polling agents* or *notification agents* . Collected
    metrics will pass through a Ceilometer pipeline transformation for further manipulation
    and prepare this data to be published. The latest releases come with an array
    of choices to store the published data in the Ceilometer database, which can be
    accessed through the Ceilometer API, Gnocchi, or even a simple file store.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zooming in on the communication between the different core components is essential
    before drafting a first design layout of your private cloud, which will be detailed
    in the next section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Architecting the cloud
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying the initial use case of your OpenStack environment is a major key
    to a successful start of the cloud journey. With a vast array of features, especially
    within the latest releases, the cloud management platform has become more mature
    than ever, but that might be overwhelming. Sidetracking the purpose of your OpenStack
    implementation would increase management complexity and potentially put your business
    at risk if such investment is not carefully accompanied by a vision. For this
    purpose, drafting your design into different iterations will help you avoid being
    blocked by *the paradox of choices* and unleash a clear short-term vision toward
    the longer one. As the first design iteration, we will simply identify our conceptual
    OpenStack model, followed by the logical architecture. Once drafted, we will walk
    through the practical implementation and put some numbers together to reflect
    our first deplo yment picture in the hardware.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Drafting the conceptual design
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the latest launching instance workflow, we have settled on the core
    services that should exist in our first OpenStack implementation. Naturally, any
    services that will be extending our basic setup later, such as PaaS services,
    will rely on the services underway that construct mainly our IaaS layer. During
    the next design phases, we will consider a generic OpenStack use case that would
    be suitable for internal usage as a private cloud to support organizations with
    elastic IaaS resources, boosting cloud application development, enabling data
    analysis, and helping with public hosting workloads. Beyond the core services,
    other mature ones will be included in this design iteration, as summarized in
    the following table:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cloud service** | **Service role** |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Compute – Nova | Manages VM life cycleUI-enabled |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Imaging – Glance | Manages VM image filesUI-enabled |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| Object storage – Swift | Manages objects for persistent storageUI-enabled
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| Block storage – Cinder | Manages VM volumes for persistent storageUI-enabled
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| Networking – Neutron | Manages network L2 and L3 resourcesUI-enabled |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| Data metrics – Ceilometer | Collects resource data metrics for monitoringUI-enabled
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| File share – Manila | Manages scale-out file share systemsUI-enabled |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Identity – Keystone | Authentication and authorization |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Dashboard – Horizon | GUI |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| Scheduling – Placement | Prefiltering for resource providers and traits |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: Table 1.3 – Initial OpenStack services
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Summing up those services into one conceptual diagram will captivate our next
    logical design iteration and narrow down the distribution of the services across
    different cluster roles of the OpenStack implementation:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – OpenStack services interaction workflow](img/B21716_01_04.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – OpenStack services interaction workflow
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The previous diagram illustrates a high-level presentation of our first OpenStack
    architecture draft that counts several services to host a variety of workloads
    for end users. We can proceed with our design by developing a logical layout in
    the next section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Drafting the logical design
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logical design iteration is a vital step that should be carried out with
    more attention. More brainstorming points will be raised at each level and within
    each service of the design to formulate an initial draft that can be implemented
    with more confidence. The other key consideration is to run our services for both
    the control and data planes in a minimum setup of HA. That can be empowered by
    clustering the controller and compute nodes in the subsequent iterations. While
    working on the cloud roadmap, it is essential to reflect on the growth aspect.
    As your business needs are identified, the first logical design should be mapped
    with the first explored requirements. Trying OpenStack even with the minimum installation
    can be helpful to familiarize yourself with its basic ecosystem components. On
    the other hand, a major consideration of the first deployment for future successful
    experiences is to think of *growth* . As mentioned earlier, OpenStack is developed
    with modular software architecture; reflecting the same approach in the logical
    design followed by the physical one will absolutely make your cloud shine. Consider
    the following guidelines to begin with:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Define roles for the OpenStack system in both the control and data planes
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolate roles starting with the minimum number of nodes and sufficient hardware
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think ahead of the possibility of component services failing
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use software that the cloud operations team is most familiar with, such as queuing
    message and database common services
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throughout the first iteration of our logical design, an incremental approach
    will be taken, starting with the following roles:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: A pair of controller nodes running the control plane, including the database
    and queuing message.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One compute node running the hypervisor layer based on **Kernel-based Virtual**
    **Machine** ( **KVM** ).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQL with the MariaDB engine and RabbitMQ will be chosen as the database and
    messaging queue services, respectively. Most of the OpenStack-documented implementations
    use MySQL and RabbitMQ as almost a religious matter.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy and Pacemaker will be running in each controller node for HA, load balancing,
    and clustering. Database HA will be implemented through Galera multi-master replication.
    RabbitMQ instances will run on their native clustering mode based on queue mirroring
    across both cloud controller hosts.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The first logical design should not be overwhelming as it is not a final draft
    ready for production yet. [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running
    a Highly Available Cloud – Meeting the SLA* , has been dedicated to going through
    each layer of the future implemented control and data planes in detail.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'The first logical design proposal can be drafted by using the previously listed
    software tools and the cloud controller and compute roles, as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – High-level logical design of OpenStack components for deployment](img/B21716_01_05.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – High-level logical design of OpenStack components for deployment
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Once the logical setup of different components and roles has been drafted, we
    will need to identify how the OpenStack nodes should conne ct.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Connecting the dots
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another essential aspect during the logical design phase is to provide a consistent
    networking layout between the different roles of each OpenStack entity. There
    are gazillions of ways to connect different pieces in an OpenStack ecosystem;
    the following layout demonstrates a network segmentation approach:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Network segmentation OpenStack nodes](img/B21716_01_06.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Network segmentation OpenStack nodes
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The isolation of different types of networks will increase security access through
    networking segmentation. The trade-off is the extra complexity, but security comes
    first. The other opinion on this choice is also the performance question, as dedicating
    a separate segment for each type of traffic would save bandwidth significantly.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous diagram illustrates four types of networks denoted with the following
    conventions:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '**External network** : Connecting to the outside world. OpenStack APIs can
    be reachable from the public as well as private backbone within a global organization
    network. The external network will provide routable IP addresses. This type of
    network will be part of the data plane as it will expose or direct traffic instances
    within the underlying infrastructure. It is essential to keep security configuration
    tied at this level by fronting the network with load balancer devices or appliances.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Management network** : As part of the control plane, this provides interconnectivity
    between all the various nodes in the OpenStack environment. Services such as the
    database and queuing message will be plugged into this network to be reached by
    the compute nodes. The management network does not expose anything to the outside
    world and should be seen as a private or internal network (you might find the
    naming convention of *API network* in other sources).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tenant network** : As users will require their virtual networks, the tenant
    network (referred to as a *guest* or *overlay network* in other sources) will
    be dedicated to handling instance traffic. There are a variety of options for
    this type of network that could be attached to SDN capabilities in the network
    node. [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking
    – Connectivity and Managed Service Options* , will highlight the scope of virtual
    overlay networking in more detail. The tenant network can be considered as part
    of the data plane.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage network** : As part of the data plane, the storage network will connect
    compute, and storage cluster nodes.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Instances can use a direct physical network instead of virtual ones empowered
    by Neutron through SDN, which would assign floating IPs to access the outside
    world. This model is referred to as a *provider network* that would connect network
    and compute. More details will be highlighted in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed* *Service Options* .
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Enumerating the different types of networks earlier will help the selection
    and reservation of different ports and network cards of each host per role in
    the long-term setup. It is still possible to combine more than one network in
    the same physical connection. However, the continuous expansion of the underlying
    infrastructure will hit the limit of the physical capacity of the combined segments,
    leaving a variety of issues with bottlenecks and network performance anomalies.
    Planning and preparing in advance will save an immense amount of time and effort
    in modifying the networking layout once network performance starts boiling. If
    a network node is dedicated in the next iteration, for example, the controller
    node will not need to connect to the tenant and exter nal networks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Drafting the physical design
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next section will assign values to our parameters defined in the previous
    logical design. However, understanding a few key concepts and practices beforehand
    will save plenty of effort and costs. Let’s start with a capacity-plannin g demonstration.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for capacity planning
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Capacity planning comes at every stage of a new infrastructure project, either
    within in-house hosting or for the development of a simple application. The cosmos
    of capacity-planning practices is based on forecasting and predicting how many
    resources an IT infrastructure would require to respond to business needs. In
    the OpenStack context, once your business case has been defined, the process of
    capacity analysis is narrowed to a specific set of resources that should exist.
    If you are planning to host a mixture of generic web hosting applications and
    data analysis workloads, there are certain considerations to be planned in terms
    of hardware sizing and the technology that will run the workloads. NFV would require
    more attention when selecting the hardware as it can be a performance eater.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Building the capacity planning for the OpenStack case can be summarized as
    follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '**Operate with elasticity** : Be able to respond and pull more ubiquitous compute
    resources when needed in the case of a failure or load increase using automation.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expect to fail** : Underlying resources should be ready to be replaced immediately
    without the need to spend time on fixing and reconfiguration efforts during incidents.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Track for growth** : Available capacity over the course of production can
    be variable. As we are in an on-demand model, the expected growth is not forcibly
    linear. Regularly track the usage of your underlying infrastructure to plot your
    cloud usage and update the capacity roadmap.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another key foundation that is highly appreciated by large infrastructure management
    is the adoption of **Information Technology Infrastructure Library** ( **ITIL**
    ) practices. By reflecting on the IT infrastructure that will run the OpenStack
    cloud environment, ITIL methodologies will definitely refine a strategic process
    to identify a complete cycle of your capacity management under the ITIL service
    design umbrella. If your organization has already rolled out ITIL practices, feel
    free to reuse and apply them in the cloud journey. From a **Cloud Service Provider**
    ( **CSP** ) perspective, having a tactical approach to managing the underlying
    IT infrastructure to align with business needs and user demand and take full control
    of the financial aspect is a *must-have* .
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: ITIL is a framework presented with a set of practices and methodologies to standardize,
    manage, and optimize IT services offered in a given business. ITIL has evolved
    into four different versions. All the versions emphasize common core concepts
    around the business service, such as the service design pillar targeting capacity
    management.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Mapping the land
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a first iteration, it would be ideal to dig into a few parameters that would
    generate some numbers to start the journey. In order to estimate our hardware
    and configurations, we can think of a reverse approach. Instead of looking at
    how many instances the first deployment could accommodate, we should see this
    from the end user’s eyes: *Which instance flavors could be offered to run which
    specific workloads?* An instance flavor is a set of specifications defined as
    a characteristics template in the compute service, including the number of vCPU
    cores, amount of RAM (including swap), and root disk (including ephemeral size).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: More advanced flavor specs can be customized and created in OpenStack to deploy
    instances that require specific workloads by scheduling the Nova service to use
    a set of compute nodes, such as support of certain CPU architecture or intensive
    workloads for HPC.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: A simple approach is to spread the population of compute resources into an average
    flavor model that could run generic workloads. As a start, an instance flavor
    could be suitable for testing tenant environments with a couple of vCPUs and 1024
    MB of RAM capacity. Defining the baseline flavor model will enable us to determine
    the next ones by doubling the size of capacity in the next iteration. Keep in
    mind that there are gazillions of ways to define the set of flavors, but, most
    importantly, identifying the starting one will help to classify the next sizing
    in the series.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your captured business needs, hence the type(s) of workload(s),
    you will have a clearer picture of how much hardware capacity will be required
    in each compute node you plan to invest in. With every combination of flavors,
    putting the density of resources in the compute box will help to measure whether
    any wasted room has been left behind so that it can be used within a new flavor.
    For example, if a compute node could support 40 medium and 10 small-sized instances
    and still have some room left, create a new flavor with the gapped size of 1 vCPU
    and 512 MB of RAM to be added to the compute node flavor catalog.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The following use case will consider a business initiative to run instances
    with versatile types of generic workloads that include instance flavors:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '| **Flavor** | **vCPU** | **RAM (MB)** | **Disk (GB)** |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| Tiny | 1 | 512 | 10 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| Small | 1 | 1024 | 20 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| Medium | 2 | 2048 | 40 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| Large | 4 | 4096 | 80 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: Table 1.4 – Instance flavors list
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The first capacity-planning roadmap will assume 200 VMs as a starting point
    with the first set of compute nodes. The following sections will conduct an estimation
    for different hardware specs, including CPU, RAM, storage, and network for each
    compute node.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: CPU estimation
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A proper calculation of the compute power will depend significantly on the
    technology and model supported by the hardware. For this reason, we will define
    a list of hardware assumptions for our use case as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: GHz per physical core = 2.6 GHz
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical core hyperthreading support = use factor 2
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GHz per VM (AVG compute units) = 2 GHz
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GHz per VM (MAX compute units) = 16 GHz
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intel Xeon E5-2648L v2 core CPU = 10
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU sockets per server = 2
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU overcommit ratio = 16:1
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oversubscription: Disabled'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyperthreading: Disabled'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operating system overhead: 20%'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack compute allows the overcommitment of resources for CPU and RAM. The
    technique of overcommitment aims to maximize the usage of resources by leveraging
    sharing between VMs running in the same hypervisor machine. For example, running
    16 vCPUs per 1 physical CPU in a hypervisor host is denoted as a CPU overcommitment
    of 16:1.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the number of virtual cores:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '*(number of VMs x number of GHz per VM) / number of GHz* *per core*'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '**(200 * 2) / 2.6 =** **153.8 46**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: A rounded decimal estimation will result in 154 vCPU cores.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take into account the operating system overhead running in the compute
    node of 20%, as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '**154 + ((154 * 20)/100) =** **184.4**'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: A rounded decimal estimation will be 185 vCPUs.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding the overcommitment ratio 16:1 parameter to estimate the actual physical
    CPU on the compute node can be calculated as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '**185/16 =** **11.5625**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: A rounded decimal estimation will result in 12 CPU cores.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: As a starting point, a compute node of 12 CPU cores would host 200 instances
    with a *small* model flavo r.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Memory estimation
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following assumptions would be required to align with the previous CPU
    estimation:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 1024 MB of RAM per instance for the small flavor
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8 GB of RAM maximum dynamic allocations per VM
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute nodes supporting slots of 2, 4, 8, and 16 GB sticks
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAM overcommit ratio of 1:1
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating system overhead of 20%
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For 200 VMs with small-sized flavor instances, a RAM estimation is given as
    follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '**200 * 1024 MB =** **200 GB**'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding the 20% operating system overhead gives us the following:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '**200 + ((200 * 20)/100) =** **240 GB**'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'A 1:1 overcommitment ratio will determine the actual RAM size needed per compute
    node:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '**240/1 =** **240 GB**'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: That would require an amount of 240 GB of RAM in our compute node to accommodate
    200 small flavor instances.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: The default Nova compute configuration comes with an overcommitment value of
    1:1.5. Be aware that, unlike CPU overcommitment behavior, memory overcommitment
    can affect the instance performance if not enough room for the swap memory is
    planned in advance. As a good practice, with 1:1.5, configure the swap memory
    to be at least double what is provid ed.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Storage estimation
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As per storage, physical compute nodes should acquire enough storage capacity
    with a multitude of options by providing the instance’s root disk from the compute
    nodes’ physical disks themselves or via attached storage devices.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Considering 200 VMs and a small size flavor instance of 20 GB root disk, a compute
    node should acquire an amount of *200 * 40 =* *800 GB* .
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the operating system requirement for disk space and other factors
    with caching and swapping configuration, an estimation of between 900 GB and 1
    TB of storage will grant a secured disk allocation for all instances (think of
    swap and extra caching disk operation overheads).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect that can be considered for more flavor customization is the storage
    type specs. That was not mentioned in the previous flavor catalog table, but,
    as part of the capacity management exercise, when the business changes the scope
    of the nature of the workload, extra classes of flavors can be added to expand
    that list, such as high-performance storage and **input/output operations per
    second** ( **IOPS** ) specs. Storage devices have various specifications to deal
    with certain use cases and patterns; for highly intensive workloads that require
    fast reads and writes, **Solid-State Drive** ( **SSD** ) disks should be considered
    in the hardware list, followed by a new custom flavor presenting the SSD specifica
    tions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Networking estimation
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Networking capacity planning comes in two categories:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Network topology, and switching and routing layers to provide sufficient IP
    addressing of the underlying cloud layer
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instances and overlay networking functions running on top of the infrastructure
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first pillar would require a consistent network layout that would reflect
    which node would acquire a private IP, a public one, or both. Obviously, when
    putting the nodes together, as per network design, the first step is to assign
    each host its private IP from a given private IP pool. Components that would require
    access to the outside world, as defined in the logical draft, would use public
    IPs. That can be granted via frontend devices or appliances such as load balancers
    or routers via **Source Network Address Translation** ( **SNAT** ) and **Destination
    Network Address Translation** ( **DNAT** ) mechanisms.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'The second pillar captures a few factors of network capacity estimation targeting
    *undercloud* resources, which includes mainly the following assumptions:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Minimum bandwidth of 50 Mbits/sec per virtual interface per instance
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association of one floating IP per instance
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association of one floating IP per NFV overlay function – virtual routers
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10% reserved floating IP for future use
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another essential aspect that should be considered when planning for networking
    is the network interfaces that will be configured in each compute node. As we
    covered a high-level design in our logical architecture draft, a few networks
    will be created and assigned to each interface in each host. This architecture
    will involve switching L2 configuration, typically via **Virtual Local Area Network**
    ( **VLAN** ) aggregation. Once basic configurations are in place, running some
    benchmarking tools is necessary to gather a few metrics on the bandwidth capacity
    in each switch port and the compute host interface. That will help to get at-a-glance
    network performance information for the hop between the compute host and the main
    physical switch port. As the physical bandwidth will be shared between different
    virtual network instance interfaces, the first value of the switch port and compute
    node physical interface should be acceptable.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The next consideration is taking into account three network interfaces cabled
    to each compute node. One of the network interfaces attached to the tenant network
    (associated with the VLAN in a dedicated switch port) will be cabled using a 10
    GB physical link that will serve 200 VMs, giving 50 MBits/sec for each.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: L2-specific vendors’ network performance varies from one device to another.
    Make sure to measure network traffic and supported **Maximum Transmission Unit**
    ( **MTU** ) sizes when designing your network segmentation and VLAN configurations.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Physical network failure and performance should be highlighted as early as
    possible instead of waiting to be alerted. Including whole configurations and
    resetting cabling and interfaces when running in production does not sound like
    the most brilliant approach. One of the most popular networking techniques that
    would save cost and performance ahead is **network bonding** . Bonding supports
    two different modes: *active-backup* and *active-active* . The first will keep
    running only one network interface as active while the others are in the backup
    state, whereas the second involves the *link aggregation* concept – **Link Aggregation
    Control Protocol** ( **LACP** ) – where traffic will be load balanced through
    different interfaces.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: To avoid network performance bottleneck surprises, the same approach can be
    taken with physical switches. A common well-known technology, **Multi-Chassis
    Link Aggregation** ( **MLAG** ), would transform several physical switches into
    one logical one, allowing FT and exposing the best-effort bandwidth to the connected
    node’s ports.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Empowering the network node’s performance without taking care of what capabilities
    the physical switching could provide would not guarantee a promising experience.
    Gathering the physical network capacity metrics in all the various layers is vital
    to avoid hitting unexpected performance bottlenecks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'The second pillar of our initial network capacity planning is the floating
    IP pool. As Neutron will be our network master in the OpenStack ecosystem, we
    are expecting the network node to interact with different network resources, such
    as instances, virtual routers, and load balancers, without mentioning advanced
    SDN configurations that would overwhelm our initial estimations. Floating IPs
    are publicly routable (public IPs are typically what you get from an **Internet
    Service Provider** ( **ISP** )). A floating IP request either from an instance
    or a network function resource such as a router or load balancer should not fail.
    Thus, we will estimate the pool as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '200 instances per compute node: 200 floating IPs'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 tenants
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10 routers per tenant: 200 floating IPs'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10 load balancers per tenant: 200 floating IPs'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding 10% for future use will generate a pool of 660 floating IPs.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Mixing the land
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous capacity-planning exercise considers a mono-compute density prototype
    targeting a *small* flavor. The capacity-planning study could involve more than
    one prototype of distributing the compute density, depending on your strategic
    business needs. The target layout can be extended to involve more flavors, supported
    in either heterogeneous or homogeneous compute density form. The **nova-scheduler**
    component can be configured, as we highlighted in a previous section, with advanced
    scheduling that works hand in hand with the Placement service to find the most
    optimal compute node. The same resource estimation for vCPU, RAM, and root disk
    can be followed to iterate through the next flavor to determine the next set of
    compute node groups. We will cover the filtering mechanisms to respond to compute
    requests in the most adequate way in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity* *and Flavors* .
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in the following diagram, one way to set future boxes in the
    most predictable fashion is to assign each available compute node a homogenous
    flavor:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Compute node placements per flavor and workload trait](img/B21716_01_07.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Compute node placements per flavor and workload trait
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: As per the scheduling configuration together with the defined traits in the
    Placement service, a Nova request will reach the right available compute node
    to accommodate a specific workload. This model layout would classify compute nodes
    per types of workloads defined in your business requirements, so the compute nodes’
    capacity would be prepared in advance to expect the initial forecasted demand.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being strategic is a fundamental key to making the right decisions when it comes
    to how to architect a complex ecosystem such as OpenStack. This chapter should
    lower the entry barrier to start an effective plan that meets your organization’s
    needs. The approach taken should help you identify different stages to start your
    cloud journey. There is no exact rule of thumb on how to design an operational
    OpenStack environment but templating the design patterns for each core service
    and sticking to the initially collected requirements will definitely enhance the
    journey. This chapter went through the newest updates on core services in the
    OpenStack ecosystem within *Antelope* and later releases, and a few more projects
    were considered to be offered once the private cloud is up and running. From an
    architecture perspective, this chapter should be revisited during the next stages
    to align and update your design draft in each step. As we used an iterative and
    incremental approach for our future OpenStack cloud environment, the next chapter
    will take you to the next deployment stage from what we have in draft, spiced
    up with best practices for the setup process.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
