- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Benchmarking the Infrastructure – Evaluating Resource Capacity and Optimization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试基础设施——评估资源容量和优化
- en: “Save for a rainy day.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “为将来做准备。”
- en: – Aesop
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: —— 伊索
- en: One of the major challenges when operating a growing infrastructure is keeping
    different services up and running within the expected defined **Service Level
    Agreements** ( **SLAs** ). During the course of production, unexpected issues
    might arise, even though a proper monitoring solution has been put in place to
    act proactively when certain situations are detected, such as scaling cluster
    nodes up and down to accommodate more tenant workloads. In a complex environment
    such as OpenStack, hitting system performance limits is considered a common issue
    due to its distributed and loose architecture. The more services join the ecosystem,
    the more performance issues will be raised. You might have a monitoring dashboard
    where all services are showing green but cloud tenant users are facing issues
    with spawning a **Virtual Machine** ( **VM** ) after several failed requests.
    Logging, as we covered in the previous chapter, could help with deep diving into
    the root cause but not with tracing such reluctant events leading to user frustration
    due to undetected performance issues. For this reason, a best practice is to keep
    profiling the cloud environment in each minor or major ecosystem. One of the most
    highly recommended approaches is to include a benchmarking stage in your CI/CD
    pipeline for each change. Another way is to conduct a profiler cycle on each OpenStack
    environment software or hardware update and collect performance metrics to compare
    to the existing SLA. The OpenStack cloud environment does not come with infinite
    resources, and knowing your limits in advance is essential before making business
    and operational decisions to extend your service catalogs to cloud users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 操作一个不断增长的基础设施时，主要挑战之一就是确保不同服务在预期的**服务级别协议**（**SLA**）内保持运行。即使已经设置了适当的监控解决方案，以便在检测到某些情况时采取主动措施，例如根据需要扩展或缩减集群节点以容纳更多租户工作负载，生产过程中仍然可能出现意外问题。在像
    OpenStack 这样的复杂环境中，由于其分布式和松散的架构，达到系统性能的极限被认为是常见问题。随着更多服务加入生态系统，性能问题也会不断增加。你可能有一个监控仪表板，显示所有服务状态为绿色，但云租户用户在多次请求失败后仍无法启动**虚拟机**（**VM**）。正如我们在上一章中讨论的，日志记录有助于深入分析根本原因，但无法追踪导致用户因未发现性能问题而产生的沮丧情绪。因此，最佳实践是持续对每个小版本或大版本的生态系统进行分析。最推荐的方法之一是在每次更改时将基准测试阶段纳入你的
    CI/CD 流水线。另一种方法是在每次 OpenStack 环境的软件或硬件更新时进行性能分析周期，并收集性能指标与现有的 SLA 进行比较。OpenStack
    云环境并非资源无限，提前了解你的限制对于在做出扩展服务目录的业务和运营决策之前是至关重要的。
- en: 'In this chapter, we will go through different options for running benchmarking
    exercises in OpenStack, detecting potential bottlenecks, and working on recommendations
    to avoid performance issues in production. The following topics will be covered:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍在 OpenStack 中运行基准测试的不同选项，检测潜在的瓶颈，并针对避免生产环境中性能问题提出建议。以下主题将被涵盖：
- en: Improve database performance in OpenStack using caching
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缓存提升 OpenStack 中的数据库性能
- en: Benchmark OpenStack and identify the source of performance bottlenecks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试 OpenStack 并识别性能瓶颈的来源
- en: Optimize our cloud control and data plane using Watcher
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Watcher 优化我们的云控制平面和数据平面
- en: Learn how to trace requests traveling across OpenStack services to identify
    bottlenecks and improve performance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何追踪跨 OpenStack 服务传递的请求，识别瓶颈并提高性能
- en: Empowering the database
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赋能数据库
- en: Databases in OpenStack are considered one of the most critical shared infrastructure
    services that require additional attention. From a high-availability perspective,
    a multi-master cluster based on **Galera MariaDB** would satisfy the requirement
    for a resilient database setup would does not fully guarantee high-performing
    write or read transactions. The database performance becomes an issue when the
    cloud environment keeps growing without measuring the new load. Databases in OpenStack
    can grow massively and result in large tables. Each read or write request and
    API call increases the load, leading to common scenarios encountered with databases,
    such as database inconsistency. A tenant could fire an API request to disassociate
    a network interface from an instance but the record in the database would remain
    unchanged. A quick fix is to log in to the database and change the record manually,
    which can be an error-prone process. The other common pattern is **multiple writes
    concurrency** , whereby two services assign the same resource ID based on an incoherent
    status attribute. For example, a new instance might be spawned and may be unable
    to associate floating IP resources that have been, at the same time, dissociated
    from a terminated instance. Relational databases typically face performance challenges,
    and generally, cloud operators utilize database administrators to keep their large
    and complex database environments. Each request in an OpenStack environment would
    reach a database for a read or write, and as we have learned in previous chapters,
    most OpenStack services interact with the database to fulfill a request, ideally
    within an acceptable response time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 中的数据库被视为最关键的共享基础设施服务之一，需要额外注意。从高可用性的角度来看，基于**Galera MariaDB**的多主集群将满足对弹性数据库设置的要求，但并不能完全保证高性能的写入或读取事务。当云环境不断增长而未测量新负载时，数据库性能会成为一个问题。OpenStack
    中的数据库可能会大规模增长，导致表格庞大。每次读取或写入请求和 API 调用都会增加负载，从而导致数据库不一致等常见场景。租户可能会发出 API 请求以将网络接口与实例解绑，但数据库中的记录仍然未更改。快速修复方法是登录数据库并手动更改记录，这可能是一个容易出错的过程。另一个常见模式是**多写并发**，即两个服务基于不一致的状态属性分配相同的资源
    ID。例如，可能会生成新实例，并且可能无法将已从终止实例中解除关联的浮动 IP 资源关联起来。关系数据库通常面临性能挑战，通常，云操作员利用数据库管理员来管理其庞大且复杂的数据库环境。在
    OpenStack 环境中，每个请求都会到达数据库进行读取或写入，正如我们在前几章中学到的那样，大多数 OpenStack 服务与数据库交互以完成请求，理想情况下在可接受的响应时间内。
- en: Aside from possible ways to improve an OpenStack database setup through hardware
    upscaling and upgrades, it is important to keep track of generic metrics of the
    database, such as the number of reads or writes, input/output storage trends,
    and error rates. That will help you conclude approximately when the database will
    present a bottleneck. If the software and configuration tweaks won’t help bypass
    the surge, a hardware upgrade will be more convenient to ensure that there is
    less trouble in the next production cycle.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过硬件升级和升级可能改善 OpenStack 数据库设置的可能方法之外，还重要的是跟踪数据库的通用指标，例如读取或写入次数、输入/输出存储趋势和错误率。这将帮助您大致确定数据库何时会出现瓶颈。如果软件和配置调整无法帮助避免突增，硬件升级将更为方便，以确保下一个生产周期中少些麻烦。
- en: Running with caching
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行与缓存
- en: One of the fastest ways to ensure an optimally performing database is to look
    at the hardware running the cluster. The nature of physical disks has a great
    influence on their capability to perform a certain number of operations. For example,
    using **Solid State Devices** ( **SSDs** ) for database nodes can be very beneficial
    in improving the access time and transfer data speed, as well as the input/output
    wait factors. The latest generations will consider Flash storage devices that
    improve read/write operations and are capable of handling high operation concurrency
    rates. On the other hand, there are better and more cost-effective ways to design
    a well-performing database solution, such as by reducing the disk input/output
    activities on the database side via caching.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据库性能最优的最快方式之一是查看运行集群的硬件。物理磁盘的特性对其执行一定数量操作的能力有很大影响。例如，使用**固态硬盘**（**SSDs**）作为数据库节点对提高访问时间、传输数据速度以及输入/输出等待因素非常有利。最新一代的设备会考虑闪存存储设备，这些设备能提高读/写操作，并且能够处理高并发操作速率。另一方面，也有更好且更具成本效益的方式来设计高性能数据库解决方案，比如通过缓存减少数据库侧的磁盘输入/输出活动。
- en: Caching happens at every step along the way, from the servers to the browsers
    of end users. From the end user’s perspective, caching can minimize unresponsive
    statuses when passing queries all the way to the database. Additionally, caching
    might be suitable for moving a long queue of database queries entirely outside
    of the database server. In such cases, you are better off looking at an external
    caching solution, such as **Memcached** or **Redis** . By exposing a memory server,
    the OpenStack database servers can benefit from a caching layer for Horizon to
    store OpenStack service data. One important consideration is that the caching
    layer does not store data. Once, for example, a Memcached instance restarts, the
    data will be lost. Memcached is a typically adopted caching solution in OpenStack
    that runs in any type of configuration. Large-scale OpenStack environments run
    the caching layer in dedicated cluster nodes to double the performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存发生在每一个步骤中，从服务器到最终用户的浏览器。从最终用户的角度来看，缓存能够最小化查询传递到数据库时的无响应状态。此外，缓存可能适用于将长队列的数据库查询完全移出数据库服务器。在这种情况下，您最好使用外部缓存解决方案，如**Memcached**或**Redis**。通过暴露内存服务器，OpenStack
    数据库服务器可以受益于 Horizon 缓存层来存储 OpenStack 服务数据。一个重要的考虑因素是，缓存层不存储数据。例如，一旦 Memcached
    实例重新启动，数据将丢失。Memcached 是 OpenStack 中常用的缓存解决方案，可以在任何类型的配置中运行。大规模的 OpenStack 环境会将缓存层运行在专用的集群节点上，以提高性能。
- en: 'A typical Memcached setup only requires the usage of hardware with fewer CPU
    specifications in contrast to database requirements. The following diagram depicts
    how Memcached is used in the OpenStack environment:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 Memcached 配置只需要使用比数据库需求更低 CPU 规格的硬件。下图展示了 Memcached 在 OpenStack 环境中的使用方式：
- en: '![Figure 9.1 – The Memcached integration in OpenStack](img/B21716_09_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – Memcached 在 OpenStack 中的集成](img/B21716_09_01.jpg)'
- en: Figure 9.1 – The Memcached integration in OpenStack
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – Memcached 在 OpenStack 中的集成
- en: This workflow diagram exposes a **write-through caching mechanism** by getting
    data that is stored in Memcached while it performs a read to the MySQL database.
    In the next section, we will deploy a Memcached layer in an existing OpenStack
    environment using **kolla-ansible** .
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该工作流程图通过获取存储在 Memcached 中的数据，同时对 MySQL 数据库执行读取操作，展示了一种**写透缓存机制**。在下一节中，我们将使用**kolla-ansible**在现有的
    OpenStack 环境中部署 Memcached 层。
- en: Deploying caching
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署缓存
- en: Before we start deploying our Memcached instance in the existing OpenStack environment,
    it is essential to iterate through common workflows in OpenStack that would engage
    caching operations. When firing an instance creation request, several API requests
    are generated and aligned to reach specific service endpoints. This process involves
    requests from Horizon to Nova to launch the API request, Glance to fetch an image,
    Cinder to attach volumes, Neutron to assign network ports, and so on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始部署 Memcached 实例到现有的 OpenStack 环境之前，必须遍历 OpenStack 中常见的涉及缓存操作的工作流程。当触发实例创建请求时，会生成多个
    API 请求并与特定的服务端点对齐。此过程涉及 Horizon 到 Nova 发起 API 请求，Glance 获取镜像，Cinder 附加卷，Neutron
    分配网络端口等。
- en: Upon each request, Keystone checks its records in the database to verify the
    validity of internal tokens.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每次请求时，Keystone 都会检查其数据库中的记录，以验证内部令牌的有效性。
- en: With a large number of similar API requests, Keystone would consume more CPU
    resources for the database to fetch those tokens and validate them accordingly.
    That would increase latency in order to fulfill new incoming requests and lead
    to longer delays in lookups through the database table caused by expired tokens.
    Caching can drastically reduce the load on the database by not saving the tokens
    in the database and instead using the caching layer. Keystone will save all its
    token records in a Memcached instance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大量类似的API请求，Keystone将消耗更多的CPU资源以从数据库获取这些令牌并相应地验证它们。这会增加履行新进入请求的延迟，并导致由于过期令牌导致的数据库表查找的延迟增加。缓存可以通过不在数据库中保存令牌而使用缓存层来显著减少数据库负载。Keystone将在Memcached实例中保存其所有令牌记录。
- en: 'In the following exercise, we will deploy a Memcached container as part of
    the control plane nodes. As recommended previously, the Memcached layer can be
    run on a dedicated server of even clusters for the highest performance and would
    require clustering configuration separately, which falls outside of the scope
    of this book. Update the **multi_packtpub_prod** file to assign the **memcached**
    role to cloud controller nodes, as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将作为控制平面节点的一部分部署一个Memcached容器。如前所述，Memcached层可以在专用服务器上运行，甚至可以集群以获得最高性能，并且需要单独进行集群配置，这超出了本书的范围。更新**multi_packtpub_prod**文件以将**memcached**角色分配给云控制器节点，如下所示：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next step is to enable the Memcached service in the **globals.yml** file:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是在**globals.yml**文件中启用Memcached服务：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, run the pipeline and make sure that the **memcached** container is
    created and up and running by executing the following command line on the controller
    node:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请在控制节点上执行以下命令来运行管道，并确保**memcached**容器已创建并正在运行：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is the output we get:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的输出：
- en: '![Figure 9.2 – Listing the Memcached kolla container](img/B21716_09_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 列出Memcached kolla容器](img/B21716_09_02.jpg)'
- en: Figure 9.2 – Listing the Memcached kolla container
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 列出Memcached kolla容器
- en: 'Make sure that Keystone has been reconfigured to use caching, which can be
    checked in the Keystone configuration file in the cloud controller node:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 确保Keystone已重新配置为使用缓存，可以在云控制器节点的Keystone配置文件中进行检查：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Further checks can be performed on the Memcached instance by watching the **get_hits**
    value increase when firing Keystone API calls, as depicted here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行Keystone API调用时，可以通过观察**get_hits**值的增加来进一步检查Memcached实例，如下所示：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 9.3 – Listing Memcached get_hits stats](img/B21716_09_03.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 列出Memcached的get_hits统计信息](img/B21716_09_03.jpg)'
- en: Figure 9.3 – Listing Memcached get_hits stats
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 列出Memcached的get_hits统计信息
- en: 'A few bounced usage statistic values are useful to verify the current behavior
    of the Keystone caching mechanism in real time, such as the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实时验证Keystone缓存机制的当前行为非常有用，例如以下内容：
- en: '**accepting_conns** : This is the number of accepted connections to the Memcached
    server. Any newly added service configured to use Memcached as a cache backend
    will increase its value by 1.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**accepting_conns**：这是连接到Memcached服务器的已接受连接数。任何新添加的服务配置为使用Memcached作为缓存后端都会增加其值1。'
- en: '**bytes** : This is the number of bytes used for caching items in real time.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bytes**：这是实时缓存项目使用的字节数。'
- en: '**bytes_read** : This is the number of incoming bytes to the Memcached server.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bytes_read**：这是传入到Memcached服务器的字节数。'
- en: '**bytes_written** : This is the number of outgoing bytes from the Memcached
    server.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bytes_written**：这是从Memcached服务器传出的字节数。'
- en: '**cmd_get** : This is the number of get commands received by the Memcached
    server.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cmd_get**：这是Memcached服务器接收的获取命令的次数。'
- en: '**cmd_set** : This is the number of set commands processed by the Memcached
    server.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cmd_set**：这是Memcached服务器处理的设置命令的次数。'
- en: '**get_hits** : This is the number of successful cache hits (get requests).
    The hit rate can be obtained by dividing **get_hits** by the **cmd_get** value
    and generating the percentage.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**get_hits**：这是成功的缓存命中（获取请求）的次数。可以通过将**get_hits**除以**cmd_get**值并生成百分比来获取命中率。'
- en: '**get_misses** : This is the number of failed cache hits ( get requests).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**get_misses**：这是失败的缓存命中（获取请求）的次数。'
- en: Most of the OpenStack services can take advantage of the caching layer, so Keystone
    can use Memcached servers to store the tokens for each service request instead
    of caching them by default in-process. Once enabled, **kolla-ansible** will roll
    out the caching layer and adjust the services configuration to use the Memcached
    layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 OpenStack 服务都可以利用缓存层，因此 Keystone 可以使用 Memcached 服务器存储每个服务请求的令牌，而不是默认在进程内缓存它们。一旦启用，**kolla-ansible**
    将推出缓存层并调整服务配置以使用 Memcached 层。
- en: 'As Memcached is deployed through the cloud controller nodes, we can make sure
    that its clients are handled by the HAProxy load balancer and use multiple Memcached
    instances in TCP mode. We can simply edit the **enable_haproxy_memcached** variable
    in the **kolla-ansible/ansible/group_vars/all.yml** file, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Memcached 是通过云控制器节点部署的，我们可以确保其客户端由 HAProxy 负载均衡器处理，并使用多实例的 Memcached 进行 TCP
    模式。我们可以简单地编辑 **kolla-ansible/ansible/group_vars/all.yml** 文件中的 **enable_haproxy_memcached**
    变量，如下所示：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After running the pipeline, the Memcached cluster will be listed in HAProxy
    and served through its virtual IP.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行管道后，Memcached 集群将会在 HAProxy 中列出，并通过其虚拟 IP 提供服务。
- en: 'We will need to tell Nova services that we already have multiple Memcached
    instances running in three different cloud controller nodes. When **cc01.os**
    becomes unavailable, **cc02.os** takes over, and so on. We will set the following
    directive in each controller and compute node in the **/** **etc/nova/nova.conf**
    file:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要告诉 Nova 服务，我们已经在三个不同的云控制器节点上运行了多个 Memcached 实例。当 **cc01.os** 不可用时，**cc02.os**
    会接管，依此类推。我们将在每个控制节点和计算节点的 **/etc/nova/nova.conf** 文件中设置以下指令：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Memcached can also be beneficial for our dashboard. We can tell Horizon to
    use Memcached for the Django web caching. It just needs to point to the virtual
    IP, considering a scalable cloud controller setup. The dashboard includes the
    **CACHES** settings, which we need to edit or add. On your cloud controller nodes,
    edit the **/etc/openstackdashboard/local_settings.py** file like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Memcached 对我们的仪表盘也有益处。我们可以告诉 Horizon 使用 Memcached 来进行 Django 的 web 缓存。只需要指向虚拟
    IP，就可以考虑可扩展的云控制器设置。仪表盘包括 **CACHES** 设置，我们需要编辑或添加它。在你的云控制器节点上，编辑 **/etc/openstackdashboard/local_settings.py**
    文件，内容如下：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Optionally, the next configuration snippet can be added to each HAProxy instance
    to boost a scalable Django dashboard, which will now use a scalable Memcached
    setup:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，可以将下一个配置片段添加到每个 HAProxy 实例中，以增强可扩展的 Django 仪表盘，该仪表盘现在将使用可扩展的 Memcached 设置：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Reload the HAProxy configuration by firing the following command line:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下命令行来重新加载 HAProxy 配置：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are a gazillion use cases where managing the performance of databases
    is important; it is a large topic and demands more database expertise to find
    anomalies and tackle them immediately. Memcached is one way to handle massive
    bursts of read operations, but that may not be enough. Database performance can
    be more complicated to tackle when it grows and there are few tools available
    to automate database performance checks and remediation. OpenStack comes with
    some tooling around its core ecosystem that supports cloud operators to benchmark,
    observe, and make architectural decisions to improve service performance. In the
    next section, we will dive into the art of OpenStack benchmarking using automated
    tooling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理数据库性能的使用场景中，有成千上万的情况是非常重要的；这是一个庞大的话题，需要更多的数据库专业知识来发现异常并立即处理。Memcached 是处理大量读取操作激增的一种方式，但这可能还不足够。当数据库性能变得更复杂，且很少有工具可以自动化数据库性能检查和修复时，问题会更加棘手。OpenStack
    提供了一些工具，围绕其核心生态系统，支持云操作员进行基准测试、监控和做出架构决策，以改善服务性能。在下一部分，我们将深入探讨使用自动化工具进行 OpenStack
    基准测试的技巧。
- en: Benchmarking the cloud
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云基准测试
- en: Scaling up hardware resources is the standard way to address capacity and performance
    limits. With the help of a monitoring system, cloud operators can react proactively
    to add more resources in a defined window of time to accommodate the additional
    load. However, monitoring systems are not enough to better know our limits. In
    distributed computing systems, every circulated request incurs a performance hit.
    In the OpenStack world, a load of API requests can be complicated to trace and
    develop an approximate measurement of how much a part or service can handle.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展硬件资源是解决容量和性能限制的标准方法。在监控系统的帮助下，云操作员可以主动反应，在定义的时间窗口内增加更多资源以适应额外的负载。然而，监控系统不足以更好地了解我们的极限。在分布式计算系统中，每个循环请求都会带来性能损耗。在OpenStack世界中，大量的API请求负载可能很难追踪，并且很难大致测量某个部分或服务能承载多少负载。
- en: From the early stages of the cloud journey, cloud operators should define and
    develop a strategic approach to measure their cloud limits and performance metrics.
    However, the challenging part is the absence of efficient tools that could be
    integrated into the life cycle of cloud deployments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从云计算之旅的初期开始，云操作员应定义并开发战略性的方法来衡量他们的云极限和性能指标。然而，挑战在于缺乏高效的工具，这些工具能够集成到云部署的生命周期中。
- en: 'To address this gap of performance measurement, one key factor is to benchmark
    the private cloud setup under load at scale for the control and data planes separately.
    It is heartening to know that with the great success of OpenStack, more benchmarking
    tools are being developed around its ecosystem and for each plane. In the next
    section, we will explore a sophisticated benchmarking tool to measure the control
    plane: the **Rally** tool.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补性能测量的这一空白，一个关键因素是对私有云的负载进行基准测试，分别对控制平面和数据平面进行测试。值得高兴的是，随着OpenStack的巨大成功，更多的基准测试工具正在围绕其生态系统以及每个平面进行开发。在下一节中，我们将探索一个复杂的基准测试工具来衡量控制平面：**Rally**工具。
- en: Rally in action
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rally的实际应用
- en: '**kolla-ansible** does not support Rally out of the box for installation as
    Rally is not originally native or part of the OpenStack ecosystem. Installing
    Rally can be done in various ways, and its package installation is available for
    many Linux distributions. We will keep our installation platform-agnostic by using
    containers, but this is not a must.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible**不支持开箱即用的Rally安装，因为Rally最初并不是OpenStack生态系统的一部分。安装Rally有多种方式，其安装包适用于许多Linux发行版。我们将通过使用容器来保持平台独立性，但这并非必须。'
- en: Important note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can still integrate the Rally service in the **kolla-ansible** code. You
    will need to build your own Rally container, push it to your own private repository,
    and write the Ansible role.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你仍然可以将Rally服务集成到**kolla-ansible**代码中。你需要构建自己的Rally容器，将其推送到自己的私有仓库，并编写Ansible角色。
- en: 'For the next installation exercise, we will use the latest Docker Rally image
    from Docker Hub that comes with the Rally service and different plugins for OpenStack.
    In one of the cloud controller nodes, pull the Rally container by running the
    following command line in a new directory:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个安装练习中，我们将使用来自Docker Hub的最新Docker Rally镜像，该镜像包含Rally服务及不同的OpenStack插件。在其中一台云控制节点上，通过在新目录中运行以下命令来拉取Rally容器：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The pulled image version of Rally is 2.3.0, which is the latest version at
    the time of writing. Database, configuration, and records of Rally can be stored
    by creating a Docker volume as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取的Rally镜像版本为2.3.0，这是写作时的最新版本。Rally的数据库、配置和记录可以通过创建一个Docker卷来存储，方法如下：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next command step is simply running the Rally Docker container by indicating
    the path where Rally will persist data under the **/** **home/rally/.rally** directory:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步命令只是通过指明Rally将在哪个路径下持久化数据，运行Rally Docker容器，该路径为**/home/rally/.rally**目录：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Important note
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Rally default configuration places the **'rally.sqlite'** Rally database
    under the **/home/rally/.rally** directory. This default configuration can overridden
    by updating the available options located in the **/** **etc/rally/rally.conf**
    file.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Rally的默认配置将**'rally.sqlite'**数据库放在**/home/rally/.rally**目录下。可以通过更新**/etc/rally/rally.conf**文件中的可用选项来覆盖此默认配置。
- en: We will need to register our OpenStack environment with Rally by providing a
    deployment file in which OpenStack environment variables, such as admin credentials,
    are required.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过提供一个部署文件来注册我们的OpenStack环境到Rally，其中需要包括OpenStack环境变量，如管理员凭据等。
- en: Important note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Different variables of the admin username, password, and tenant are generated
    in the **/** **etc/kolla/clouds.yaml** file.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: admin 用户名、密码和租户的不同变量在**/****etc/kolla/clouds.yaml**文件中生成。
- en: 'In the Rally container Bash prompt, create a **deployment.json** file and make
    sure that the different variables are assigned values from the **/** **etc/kolla/clouds.yaml**
    file:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Rally 容器的 Bash 提示符下，创建一个**deployment.json**文件，并确保不同的变量值从**/****etc/kolla/clouds.yaml**文件中获取：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using the Rally client command line, create a deployment using the previously
    created file:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Rally 客户端命令行，通过之前创建的文件创建一个部署：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 9.4 – Listing the Rally deployment status](img/B21716_09_04.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 列出 Rally 部署状态](img/B21716_09_04.jpg)'
- en: Figure 9.4 – Listing the Rally deployment status
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 列出 Rally 部署状态
- en: 'Source the generated **openrc** file located under **~/.rally** :'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 载入生成的**openrc**文件，该文件位于**~/.rally**：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Verify the availability of the OpenStack deployment using the deployment check
    command, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用部署检查命令验证 OpenStack 部署的可用性，如下所示：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is the output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 9.5 – Listing the OpenStack services statuses](img/B21716_09_05.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 列出 OpenStack 服务状态](img/B21716_09_05.jpg)'
- en: Figure 9.5 – Listing the OpenStack services statuses
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 列出 OpenStack 服务状态
- en: 'Now that we have a Rally server installed and properly configured to talk to
    OpenStack APIs, it is time for cloud benchmarking. By default, you may find numerous
    benchmarking scenarios under **/rally/sample/tasks/scenarios** for all OpenStack
    services, including other incubated projects such as Murano and Sahara. We will
    concentrate on benchmarking our existing running OpenStack services. Before starting
    our first benchmark test, it would be great to shine the spotlight on how Rally
    works in the first place. Scenarios in Rally are performed based on tasks. A task
    can include a set of running benchmarks against the OpenStack cloud written in
    sample JSON or YAML file format. The former file generally has the following structure:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装并正确配置了 Rally 服务器，以便与 OpenStack APIs 进行交互，是时候进行云端基准测试了。默认情况下，您可以在 **/rally/sample/tasks/scenarios**
    中找到大量针对所有 OpenStack 服务的基准测试场景，包括其他孵化项目，如 Murano 和 Sahara。我们将集中精力对现有运行中的 OpenStack
    服务进行基准测试。在开始我们的第一个基准测试之前，了解一下 Rally 是如何工作的会非常有帮助。Rally 中的场景是基于任务执行的。一个任务可以包括一组针对
    OpenStack 云的运行基准测试，这些基准测试以 JSON 或 YAML 文件格式编写。前者文件通常具有以下结构：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Each stanza block is defined as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每个区块的定义如下：
- en: '**ScenarioClass.scenario_method** : This defines the name of the benchmark
    scenario.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ScenarioClass.scenario_method**：这定义了基准测试场景的名称。'
- en: '**args** : Every method corresponding to a specific class scenario can be customized
    by passing parameters before launching the benchmark.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**args**：每个对应特定类场景的方法可以通过传递参数来定制，在启动基准测试之前。'
- en: '**runner** : This defines the workload frequency type and the order of the
    benchmarking scenarios. The runner stanza can support different types, as follows:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**runner**：这定义了工作负载频率类型和基准测试场景的顺序。runner 区块可以支持以下不同类型：'
- en: '**constant** : This involves running the scenario for a fixed number of times.
    For example, a scenario can be run 10 times in the total test period.'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**constant**：这涉及运行场景一定次数。例如，一个场景可以在总测试周期内运行 10 次。'
- en: '**constant_for_duration** : This involves running the scenario for a fixed
    number of times until a certain point in time.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**constant_for_duration**：这涉及在固定次数内运行场景，直到达到某个时间点。'
- en: '**periodic** : This involves defining a certain period to run two consecutive
    benchmark scenarios.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**periodic**：这涉及定义一个特定周期来运行两个连续的基准测试场景。'
- en: '**serial** : This involves running the scenario for a fixed number of times
    in a single benchmark thread.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**serial**：这涉及在单一基准测试线程中运行场景一定次数。'
- en: '**context** : This defines the environment type in which our benchmark scenario(s)
    can run. Usually, the concept of context defines how many tenants and active users
    will be associated with a given OpenStack project. It can also specify a quota
    per tenant or user within a certain granted role.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**context**：这定义了我们基准测试场景可以运行的环境类型。通常，context 的概念定义了与给定 OpenStack 项目关联的租户数量和活跃用户数量。它还可以指定每个租户或用户在特定角色中的配额。'
- en: '**sla** : This is very useful for identifying the overall scenario average
    success rate of the benchmark.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sla**：这对于识别基准测试的整体场景平均成功率非常有用。'
- en: If you are hoping to find a convenient benchmarking scenario that will reveal
    more significant results from your current OpenStack deployment, you’ll have to
    keep looking for a real use case that is more specific to cloud operators. For
    example, Rally can help developers easily run synthetic workloads such as VM provisioning
    and destroy instances for a limited period. However, the case seems to be more
    complicated for cloud operators. Such results generated from workloads are more
    high-level but allow you to identify bottlenecks in the cloud.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望找到一个便捷的基准测试场景，以揭示当前OpenStack部署中更显著的结果，你将需要继续寻找一个更加专门面向云运营商的实际用例。例如，Rally可以帮助开发人员轻松地运行合成工作负载，如虚拟机的提供和销毁实例，且只持续有限的时间。然而，对云运营商来说，这种情况似乎更为复杂。从工作负载生成的结果通常更为高层次，但可以帮助你识别云中的瓶颈。
- en: 'Let’s consider a real-world example: companies have several applications that
    need to be deployed in different usage patterns. If we have multiple concurrent
    instances of an application for QA/dev, they will be deployed in different versions
    of this application on the cloud several times per day. Let’s take the use case
    of a large deployment, where there is a set number of teams running a bunch of
    standard stack applications and each application will contain a lot of VMs that
    need to be deployed at certain times in a day. Such workload requirements are
    translated to OpenStack terms as follows: we will have *M* number of users provisioning
    *N* number of VMs within a specific flavor and time period in a concurrent way.
    As we know, OpenStack is not a monolithic structure; it is a distributed system
    with different daemons and services talking to each other. If we decompose a use
    case of provisioning an instance to the primitives, it helps us to understand
    where we spend most of the time during the VM provisioning phase and build records
    of historical data. For example, by running the same benchmark several times but
    changing the parameters of database configuration or enabling Glance. In the next
    subsection, we will use the same benchmarking approach to run against the Keystone
    service.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个实际的例子：公司有多个应用程序需要根据不同的使用模式进行部署。如果我们有多个并发的应用程序实例用于QA/dev，它们将在云端的不同版本中一天被部署多次。以大规模部署为例，设定有若干团队运行一组标准堆栈应用程序，每个应用程序将包含大量需要在一天中的特定时间进行部署的虚拟机。这种工作负载需求转化为OpenStack术语如下：我们将有*M*数量的用户，在特定的口味和时间段内并发地提供*N*数量的虚拟机。如我们所知，OpenStack不是一个单体结构，它是一个分布式系统，具有不同的守护进程和服务相互通信。如果我们将提供实例的用例分解成原子操作，这有助于我们理解在虚拟机提供阶段我们花费最多时间的地方，并建立历史数据记录。例如，通过多次运行相同的基准测试，但更改数据库配置的参数或启用Glance。在下一小节中，我们将采用相同的基准测试方法来测试Keystone服务。
- en: Keystone under stress
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keystone压力测试
- en: Our example scenario is a benchmarking test based on the Rally method named
    **KeystoneBasic.authenticate_user_and_validate_token** . The scenario is intended
    to measure the time to fetch and validate tokens issued by Keystone when authenticating
    users under a specific load. An important note for the sake of the demonstration
    is that the load test will be applied on a specific Keystone configuration that
    does not support the **WSGI** module for the Apache web server.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例场景是基于Rally方法的基准测试，名为**KeystoneBasic.authenticate_user_and_validate_token**。该场景旨在测量在特定负载下，Keystone在认证用户时获取和验证令牌所需的时间。为了演示，重要的注意事项是负载测试将应用于一个不支持**WSGI**模块的特定Keystone配置，该配置用于Apache
    Web服务器。
- en: Important note
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The latest version of **kolla-ansible** comes with Keystone configured with
    the WSGI ( **mod_wsgi** ) module enabled by default for Apache. You can disable
    the module manually in the Keystone configuration file or create a new Kolla container
    to test the first Rally scenario. Make sure to perform the load exercise in a
    separate environment to not break the Keystone configuration and hence other services.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible**的最新版本默认启用了WSGI（**mod_wsgi**）模块配置，用于Apache。你可以在Keystone配置文件中手动禁用该模块，或者创建一个新的Kolla容器来测试第一个Rally场景。请确保在单独的环境中执行负载测试，以免破坏Keystone配置，从而影响其他服务。'
- en: 'Let’s create a new file named **perf_keystone_pp.yaml** . The content of the
    file task looks as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为**perf_keystone_pp.yaml**的新文件。该文件的任务内容如下：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The sample scenario will create a constant load of a Keystone scenario authenticating
    users and validating tokens 50 times without pausing by creating 5 different tenants
    with 10 users in each. Note that in each iteration, 50 scenarios will run at the
    same time in concurrency mode to simulate multiple user access. The **sla** section
    defines a condition where if one authentication attempt fails, then the task will
    be aborted.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 示例场景将创建一个恒定负载的 Keystone 场景，验证用户身份并验证令牌 50 次，且不中断，通过创建 5 个不同的租户，每个租户包含 10 个用户。请注意，在每次迭代中，50
    个场景将同时在并发模式下运行，以模拟多个用户的访问。**sla** 部分定义了一个条件，如果某次身份验证尝试失败，任务将会中止。
- en: 'Let’s run the previous benchmark using the Rally command line, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Rally 命令行运行之前的基准测试，如下所示：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that this time, we added a new option to our **abort-on-sla-failure**
    command. This is a very useful argument if you are running such a benchmark scenario
    in a real OpenStack production environment. Rally generates a heavy workload,
    which might cause performance troubles in the existing cloud. Thus, we tell Rally
    to stop the load at a certain moment when the **sla** conditions are met. The
    output of our executed task is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这次我们为 **abort-on-sla-failure** 命令添加了一个新选项。如果您在实际的 OpenStack 生产环境中运行此类基准场景，这是一个非常有用的参数。Rally
    生成了一个重负载，这可能会导致现有云环境中的性能问题。因此，我们告诉 Rally 在满足 **sla** 条件时，在某个时刻停止负载。我们执行的任务输出如下：
- en: '![Figure 9.6 – Listing Rally task stats](img/B21716_09_06.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 列出 Rally 任务统计信息](img/B21716_09_06.jpg)'
- en: Figure 9.6 – Listing Rally task stats
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 列出 Rally 任务统计信息
- en: 'The Rally benchmark results show that the scenario test has run 50 times and
    is completed with a 100% success rate. To dive into more details, we can visualize
    the HTML report using the generated Rally task ID by running the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Rally 基准测试结果显示，场景测试已运行 50 次，并且以 100% 的成功率完成。要查看更详细的信息，我们可以通过运行以下命令，使用生成的 Rally
    任务 ID 来可视化 HTML 报告：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our first test iteration benchmark involves a simple SLA condition that was
    met during the Rally task:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一次测试迭代基准涉及在 Rally 任务期间满足的简单 SLA 条件：
- en: '![Figure 9.7 – A Rally SLA failure_rate HTML report](img/B21716_09_07.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 一个 Rally SLA failure_rate HTML 报告](img/B21716_09_07.jpg)'
- en: Figure 9.7 – A Rally SLA failure_rate HTML report
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 一个 Rally SLA failure_rate HTML 报告
- en: 'From the same report dashboard, in the **Overview** tab, a second pertinent
    chart, **Load Profile** , illustrates how many iterations were running in parallel
    during the Rally task:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从相同的报告仪表板中，在**概述**标签页中，第二个相关图表，**负载配置文件**，展示了在 Rally 任务期间有多少个迭代是并行运行的：
- en: '![Figure 9.8 – The Load Profile report for the Keystone scenario, generated
    by Rally](img/B21716_09_08.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – Rally 生成的 Keystone 场景的负载配置文件报告](img/B21716_09_08.jpg)'
- en: Figure 9.8 – The Load Profile report for the Keystone scenario, generated by
    Rally
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – Rally 生成的 Keystone 场景的负载配置文件报告
- en: 'The **Load Profile** graph can be used to illustrate the variation of running
    iterations simultaneously over the workload period. This information is useful
    for learning about the system behavior at certain peaks and planning how much
    load can be supported at any given time. More details are provided in the second
    tab, **Details** , where we can find **Atomic Action Durations** charts showing,
    in our case, two actions – **keystone_v2.fetch_token** and **keystone_v2.validate_token**
    :'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**负载配置文件** 图表可以用来说明在工作负载周期中同时运行的迭代变化情况。这些信息对于了解系统在某些峰值时的行为非常有用，并且有助于规划在任何特定时刻系统能够承载的负载量。更多详细信息可以在第二个标签页
    **详细信息** 中找到，在那里我们可以看到**原子操作时长**图表，在我们的案例中，展示了两项操作——**keystone_v2.fetch_token**
    和 **keystone_v2.validate_token**：'
- en: '![Figure 9.9 – An Atomic Action Durations graph for Keystone steps, generated
    by Rally](img/B21716_09_09.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – 一个 Keystone 步骤的原子操作时长图表，由 Rally 生成](img/B21716_09_09.jpg)'
- en: Figure 9.9 – An Atomic Action Durations graph for Keystone steps, generated
    by Rally
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 一个 Keystone 步骤的原子操作时长图表，由 Rally 生成
- en: The chart helps with seeing the variation of the scenario for each action and
    how the duration is affected and changed throughout the execution of iterations.
    As we can see, both actions do not have the same duration as fetching and validating
    tokens are two different operations. If our test case failed in terms of SLA conditions,
    such as a very long duration for scenario execution, we can use this chart to
    drive a granular analysis of which action the bottleneck occurred on.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表有助于查看每个操作的场景变化，以及迭代执行过程中持续时间的变化。如我们所见，由于获取和验证令牌是两个不同的操作，这两个操作的持续时间并不相同。如果我们的测试用例在SLA条件方面失败，例如场景执行时间过长，我们可以使用此图表进行详细分析，确定瓶颈出现在了哪个操作上。
- en: 'We can adjust our success criteria parameters a bit in a second iteration for
    a stricter SLA to visualize a more realistic scenario. For example, we can modify
    our **sla** section as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在第二次迭代中稍微调整成功标准参数，以实现更严格的SLA，从而可视化更现实的场景。例如，我们可以按如下方式修改**sla**部分：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The new **sla** section defines five conditions:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 新的**sla**部分定义了五个条件：
- en: '**max_avg_duration** : If the maximum average duration of an authentication
    is longer than five seconds, the task will be aborted.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_avg_duration** ：如果认证的最大平均时长超过五秒，任务将终止。'
- en: '**max_seconds_per_iteration** : If the maximum duration of an authentication
    request is longer than five seconds, the task will be aborted'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_seconds_per_iteration** ：如果认证请求的最大时长超过五秒，任务将终止。'
- en: '**failure_rate** : If more than one authentication fails, the task will be
    aborted defined by the parameter **max** .'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**failure_rate** ：如果多次认证失败，任务将根据**max**参数终止。'
- en: '**performance_degradation** : If the difference between the maximum and minimum
    duration of completed iterations is more than 50 percent, the task will be aborted.
    The maximum value is defined by **max_degradation** parameter.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**performance_degradation** ：如果已完成迭代的最大时长与最小时长之间的差异超过50%，任务将终止。最大值由**max_degradation**参数定义。'
- en: ': **outlier** : The outlier limits the number of long-running iterations to
    a value of **1** defined by the **max** parameter.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ': **outlier** ：离群值限制长时间运行的迭代次数为**1**，由**max**参数定义。'
- en: 'Once the Rally scenario has been modified, rerun the task as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦修改了Rally场景，按照以下方式重新运行任务：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s check our charts again by generating a new report with a different name
    so we can compare the difference in the results from the previous iteration:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过生成一个不同名称的新报告再次检查我们的图表，以便比较与上次迭代结果的差异：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Figure 9.10 – An SLA for the Keystone load profile, generated by Rally](img/B21716_09_10.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – 由Rally生成的Keystone负载配置文件的SLA](img/B21716_09_10.jpg)'
- en: Figure 9.10 – An SLA for the Keystone load profile, generated by Rally
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 由Rally生成的Keystone负载配置文件的SLA
- en: 'During the test, Rally detected a maximum value for the iteration of **11.27**
    seconds, which does not comply with our SLA requirement:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试过程中，Rally检测到迭代的最大值为**11.27**秒，这不符合我们的SLA要求：
- en: '![Figure 9.11 – A stacked overview of Keystone actions’ durations](img/B21716_09_11.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图9.11 – Keystone操作持续时间的堆叠概览](img/B21716_09_11.jpg)'
- en: Figure 9.11 – A stacked overview of Keystone actions’ durations
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – Keystone操作持续时间的堆叠概览
- en: With the new SLA conditions, Rally execution stopped at the sixth iteration.
    The demanded average time of authenticating a user and validating the token was
    not met and hence, this will affect the overall scenario execution duration time.
    The next goal is to compete against that value and decrease it to below five seconds.
    Our benchmark test showed that authenticating and validating user tokens at a
    certain peak of workload would not achieve our SLA requirements. Moreover, the
    time spent authenticating one user increases and might be timed out as concurrency
    levels hit a specific threshold. This performance challenge can be tweaked by
    revisiting our Keystone setup. We can refer to an advanced Keystone design pattern
    that empowers our identity service performance within the OpenStack environment.
    As many OpenStack components are developed to support the **eventlet-based** process,
    the Keystone component can run in different ways by supporting the multi-threading
    process at the cost of our cloud controller’s CPU power. One recommendation is
    to deploy Keystone in an Nginx server under WSGI or an Apache server with the
    **mod_wsgi** module enabled.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的SLA条件下，Rally执行在第六次迭代时停止。验证用户并验证令牌的平均时间未能满足要求，因此，这将影响整体场景的执行时长。下一步目标是与该值竞争，并将其降到五秒以下。我们的基准测试表明，在某一工作负载峰值时，验证用户令牌的操作无法达到我们的SLA要求。此外，身份验证一个用户所花费的时间会增加，并且当并发水平达到某个阈值时，可能会超时。这个性能挑战可以通过重新审视我们的Keystone设置来调整。我们可以参考一种先进的Keystone设计模式，在OpenStack环境中增强我们的身份服务性能。由于许多OpenStack组件支持**eventlet-based**进程，Keystone组件可以通过支持多线程处理来以不同方式运行，这将消耗我们云控制器的CPU资源。一个建议是在Nginx服务器下通过WSGI或在启用了**mod_wsgi**模块的Apache服务器中部署Keystone。
- en: Fronting our Keystone instance with a web server will bring facilities for handling
    parallel HTTP connections and advanced features to proxy authentication requests
    to our identity instance in a multi-threaded-based process mode. By default, the
    community **kolla-ansible** code comes with the WSGI module enabled that will
    be used for this exercise. The WSGI Keystone configuration is provided in the
    **wsgi-keystone.conf.j2** template located under the **/ansible/roles/keystone/templates**
    directory.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Web服务器作为前端来处理Keystone实例，可以提供并行HTTP连接的处理功能，并且通过代理身份验证请求到我们的身份实例，支持多线程处理模式。默认情况下，社区版**kolla-ansible**代码启用了WSGI模块，将用于本次实验。WSGI
    Keystone配置在**/ansible/roles/keystone/templates**目录下的**wsgi-keystone.conf.j2**模板中提供。
- en: 'The following snippet shows a basic WSGI configuration in Keystone with the
    **VirtualHost** option:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了在Keystone中使用**VirtualHost**选项的基本WSGI配置：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the WSGI process group defines the number of threads to be run by
    the Keystone user ( **threads=1** ) and processes ( **keystone_api_workers** ).
    We can increase the number of processed requests by adjusting the number for processes
    defined in the **keystone_api_workers** option and threads in **WSGIDaemonProcessdirective**
    :'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，WSGI进程组定义了Keystone用户将运行的线程数（**threads=1**）和进程数（**keystone_api_workers**）。我们可以通过调整**keystone_api_workers**选项中定义的进程数和**WSGIDaemonProcessdirective**中的线程数来增加处理请求的数量：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we have introduced a configuration change, run the pipeline to pull and run
    the new configurations to be reflected in the web server.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们进行了配置更改，请运行管道以拉取并运行新配置，并使其在Web服务器中生效。
- en: 'Now we have Keystone backed by a web server and empowered by multi-threaded
    process mode by means of the WSGI module. We have already defined the process
    daemons and threads that will help trace the limit of our hardware and Keystone
    capabilities against the same scenario by running it once again:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个由Web服务器支持，并通过WSGI模块实现多线程处理模式的Keystone。我们已经定义了将帮助我们通过再次运行相同场景来追踪硬件和Keystone性能限制的进程守护程序和线程：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Demonstrating the last Keystone settings, our cloud controller should be running
    more **apache2** processes and hence using more CPU power. That can be illustrated
    in the new performance results for authentication and validation duration change:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过展示上一个Keystone设置，我们的云控制器应该会运行更多的**apache2**进程，从而使用更多的CPU资源。这可以通过身份验证和验证时长变化的新性能结果来说明：
- en: '[PRE27]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Figure 9.12 – The Keystone SLA upon web server threads increasing](img/B21716_09_12.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图9.12 – Web服务器线程增加时Keystone的SLA](img/B21716_09_12.jpg)'
- en: Figure 9.12 – The Keystone SLA upon web server threads increasing
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – Web服务器线程增加时Keystone的SLA
- en: 'Now we have reached our goal by reducing the max number of seconds per iteration
    to below five seconds (4.19 seconds). As we have achieved a *green* SLA, we can
    analyze our new Keystone boost configuration from the **Load** **Profile** chart:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过将每次迭代的最大秒数减少到五秒以下（4.19秒）达到了我们的目标。由于我们已经达成了 *绿色* SLA，我们可以从 **负载** **配置文件**
    图表中分析我们新的 Keystone 提升配置：
- en: "![Figure 9.13 – \uFEFFKeystone load profile after increasing number of web\
    \ server threads](img/B21716_09_13.jpg)"
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13 – 增加 web 服务器线程数后 Keystone 负载配置文件](img/B21716_09_13.jpg)'
- en: Figure 9.13 – Keystone load profile after increasing number of web server threads
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 增加 web 服务器线程数后 Keystone 负载配置文件
- en: Compared to previous iterations, simultaneous requests can be handled by a pool
    of threads, so Keystone is able to face concurrent iterations during the workload
    timeline. Although the currency level was set higher, we can notice that the maximum
    value of the load testing performed was only **24** . That confirms that our thread
    and process settings for WSGI were heading in the right direction, leaving more
    free slots for more concurrency and less time for processing per iteration.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的迭代相比，多个请求可以通过线程池进行处理，因此 Keystone 能够在工作负载时间线期间处理并发迭代。尽管设置的并发水平较高，但我们可以注意到，负载测试的最大值仅为
    **24**。这证明我们为 WSGI 设置的线程和进程配置走在正确的方向上，为更多的并发留出了更多空闲插槽，同时每次迭代的处理时间也更短。
- en: There is more that can be done with Rally, but the scenarios described are sophisticated
    enough to start our benchmarking journey in OpenStack. Rally is also a pluggable
    platform that allows operators to create and customize their benchmarking scenarios
    to meet certain special use cases. Benchmarking is helpful to get insights into
    how your OpenStack is performing in regard to the defined SLA. However, when performance
    anomalies are detected, benchmarking cannot help directly to remediate the issue.
    For this reason, the profiling practice covered in the next section should be
    taken into consideration.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Rally 还有更多可以做的事情，但上述场景已经足够复杂，可以作为我们在 OpenStack 中开始基准测试的起点。Rally 也是一个可插拔平台，允许操作员创建和定制他们的基准测试场景，以满足某些特定的使用案例。基准测试有助于了解
    OpenStack 在定义的 SLA 下的表现。然而，当检测到性能异常时，基准测试不能直接帮助修复问题。因此，下一节中介绍的分析实践应该考虑到。
- en: Profiling the cloud
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云端分析
- en: The OpenStack ecosystem is composed of multiple services that connect with each
    other to fulfill a request. In some cases, a request can be processed extremely
    slowly and fail. If the rate of request failures keeps increasing, cloud operators
    must investigate and understand the root cause of the issue. Monitoring, debugging,
    and logging tools can partially be the answer to such scenarios, but they lack
    a request flow mechanism. A tiny but great tool has been developed in the OpenStack
    ecosystem named **OSProfiler** , which focuses on service tracing. OSProfiler
    provides a view of requests as they travel through different OpenStack services
    and compiles data to be visualized in a timeline graph. Cloud operators can determine
    bottlenecks in the OpenStack deployment and improve its performance by comparing
    trace sets with different conditions. The OSProfiler tool is capable of providing
    valuable insights and tracing data that captures the response time of APIs, databases,
    drivers, and RPC calls. Cloud operators can store the tracing information in persistent
    storage for further analysis, such as Redis, Elasticsearch, a simple file, or
    MongoDB.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 生态系统由多个服务组成，这些服务彼此连接以完成请求。在某些情况下，请求可能会非常慢，甚至失败。如果请求失败的频率不断增加，云操作员必须调查并了解问题的根本原因。监控、调试和日志工具在某些情况下可以部分解决这些问题，但它们缺少请求流机制。在
    OpenStack 生态系统中有一个小巧而强大的工具，名为 **OSProfiler**，它专注于服务追踪。OSProfiler 提供了请求在不同 OpenStack
    服务中流动的视图，并将数据编译成时间轴图表进行可视化。云操作员可以通过比较不同条件下的追踪集，确定 OpenStack 部署中的瓶颈并提升其性能。OSProfiler
    工具能够提供有价值的洞察和追踪数据，捕捉 API、数据库、驱动程序和 RPC 调用的响应时间。云操作员可以将追踪信息存储在持久存储中以便进一步分析，例如 Redis、Elasticsearch、简单文件或
    MongoDB。
- en: In a complex system such as OpenStack, tracing where a request is spent most
    of the time is extremely helpful to troubleshoot faster, identify bottlenecks,
    and prevent the issue from happening again. Using OSProfiler, cloud operators
    can figure out the reason, for example, why launching a VM request takes ages.
    OSProfiler is capable of showing the services involved in such requests, as well
    as their dependencies. Operators can conclude which service presents the bottleneck
    and how to improve the response time.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 OpenStack 这样的复杂系统中，追踪请求的大部分时间花费在哪里，对更快地排除故障、识别瓶颈并防止问题再次发生非常有帮助。使用 OSProfiler，云操作员可以找出原因，例如，为什么启动虚拟机请求需要很长时间。OSProfiler
    能显示与这些请求相关的服务及其依赖关系。操作员可以得出哪个服务存在瓶颈，并了解如何改进响应时间。
- en: The next section will illustrate how to install and run OSProfiler in the existing
    OpenStack environment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将说明如何在现有的 OpenStack 环境中安装和运行 OSProfiler。
- en: Profiler in action
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Profiler 在运行
- en: 'Since the Antelope version, OSProfiler can trace all the OpenStack core services
    in addition to other projects. The OpenStack community aims to use OSProfiler
    for all OpenStack projects without exception due to its light and powerful tracing
    capabilities. The **kolla-ansible** infrastructure already supports OSProfiler,
    which can be installed easily. OSProfiler can be enabled by configuring the following
    settings in the **globals.yml** file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Antelope 版本开始，OSProfiler 除了其他项目外，还可以追踪所有 OpenStack 核心服务。OpenStack 社区的目标是将
    OSProfiler 应用于所有 OpenStack 项目，毫无例外，因为它具有轻量且强大的追踪能力。**kolla-ansible** 基础设施已经支持
    OSProfiler，并且可以轻松安装。可以通过在**globals.yml**文件中配置以下设置来启用 OSProfiler：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that OSProfiler uses Elasticsearch as a backend to store the tracing data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OSProfiler 使用 Elasticsearch 作为后端来存储追踪数据。
- en: Important note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: If **enable_elasticsearch** is set to **"no"** , Kolla will install and use
    Redis as a default backend for OSProfiler.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果**enable_elasticsearch**设置为**"no"**，Kolla 将安装并使用 Redis 作为 OSProfiler 的默认后端。
- en: OSProfiler does not need to be installed on a dedicated host. At the time of
    writing, OpenStack **kolla-ansible** supports OSProfiler for Keystone, Nova, Glance,
    Cinder, Neutron, Placement, Swift, Heat, Trove, Senlin, and Vitrage.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: OSProfiler 不需要安装在专用主机上。写作时，OpenStack **kolla-ansible** 支持 Keystone、Nova、Glance、Cinder、Neutron、Placement、Swift、Heat、Trove、Senlin
    和 Vitrage 等服务的 OSProfiler。
- en: 'Once the **globals.yml** file is updated and pushed, run the pipeline and the
    OSProfiler library should be installed. Before starting a profiling exercise,
    we will need to retrieve the generated OSProfiler secret referenced by the **osprofiler_secret**
    key in the **/etc/kolla/passwords.yml** file. The **osprofiler_secret** key will
    be used to create profiler UUIDs with OpenStack client command lines for supported
    services. The next example illustrates a tracing exercise of a Glance API call,
    as demonstrated in the following steps:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦更新并推送了**globals.yml**文件，运行管道，OSProfiler 库应该已经安装。在开始进行性能分析之前，我们需要从**/etc/kolla/passwords.yml**文件中获取由**osprofiler_secret**键引用的生成的
    OSProfiler 密钥。该**osprofiler_secret**键将用于通过 OpenStack 客户端命令行为支持的服务创建 profiler UUID。以下示例说明了一个
    Glance API 调用的追踪操作，具体步骤如下：
- en: 'Use the official OpenStack Python client running in a cloud controller node
    to generate a tracing output:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在云控制器节点上运行的官方 OpenStack Python 客户端来生成追踪输出：
- en: '[PRE29]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is the output we get:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们得到的输出：
- en: '![Figure 9.14 – OSProfiler image list](img/B21716_09_14.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – OSProfiler 图像列表](img/B21716_09_14.jpg)'
- en: Figure 9.14 – OSProfiler image list
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – OSProfiler 图像列表
- en: '**<OSPROFILER_SECRET>** is the retrieved OSProfiler secret retrieved from the
    **/etc/kolla/passwords.yml** file referenced in the **osprofiler_secret** parameter.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**<OSPROFILER_SECRET>** 是从**/etc/kolla/passwords.yml**文件中检索到的 OSProfiler 密钥，参考了**osprofiler_secret**参数。'
- en: Important note
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is possible to customize the **OSPROFILER_SECRET** value for each supported
    service by configuring the **hmac_keys** setting in each configuration file that
    corresponds to that service.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过配置每个对应服务的**hmac_keys**设置，可以为每个支持的服务自定义**OSPROFILER_SECRET**值。
- en: 'The command line returns an OSProfiler tracing command line, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行返回一个 OSProfiler 追踪命令行，如下所示：
- en: '![Figure 9.15 – An OSProfiler trace command output](img/B21716_09_15.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.15 – OSProfiler 追踪命令输出](img/B21716_09_15.jpg)'
- en: Figure 9.15 – An OSProfiler trace command output
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – OSProfiler 追踪命令输出
- en: 'Use the **osprofiler** command line to print the tracing graph in HTML format.
    The retrieved tracing data can be stored locally in the created Elasticsearch
    instance:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**osprofiler**命令行打印HTML格式的追踪图。检索到的追踪数据可以存储在创建的Elasticsearch实例中：
- en: '[PRE30]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: An HTML report should be available locally under the **/** **tmp** directory.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HTML报告应该可以在本地的**/****tmp**目录下找到。
- en: 'Visualize the tracing results by accessing the generated **image_perf.html**
    file in a browser:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在浏览器中访问生成的**image_perf.html**文件来可视化追踪结果：
- en: '![Figure 9.16 – An example of OSProfiler tracing results](img/B21716_09_16.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16 – OSProfiler追踪结果示例](img/B21716_09_16.jpg)'
- en: Figure 9.16 – An example of OSProfiler tracing results
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 – OSProfiler追踪结果示例
- en: The generated HTML report exposes, for each request made at the time of processing
    of each call, the nature of the service call (API, database, etc.) and its corresponding
    project. The **Levels** column corresponds to a traced point that includes a low
    level of detailed information in JSON format.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的HTML报告会展示每次处理调用时的请求信息，包括服务调用的性质（如API、数据库等）及其相应的项目。**Levels**列对应一个追踪点，包含以JSON格式呈现的低级详细信息。
- en: Tracing is a great practice to debug at a low level what cannot be detected
    in your monitoring system or overlooked in the log data. Both profiling and benchmarking
    provide sufficient information about cloud performance and the trend of resource
    utilization. As the cloud infrastructure keeps receiving new tenants, more hardware
    demands will increase. If this is not watched and analyzed from the early days,
    the cost can rise and the risks will be out of control. Operators should be provided
    with strategies and tooling to automate budget control and management. One of
    the best approaches with limited resources is to seek optimization, which will
    be covered in the next section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪是一种很好的实践，可以在低级别调试监控系统中无法检测到或日志数据中被忽略的问题。性能分析和基准测试提供了足够的信息来了解云的性能和资源利用趋势。随着云基础设施不断接纳新租户，硬件需求会增加。如果从一开始没有进行监控和分析，成本可能会上升，风险也将失控。应该为运营商提供策略和工具，自动化预算控制和管理。对于资源有限的情况，最好的方法之一就是寻求优化，这将在下一节中讨论。
- en: Watching the cloud
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察云
- en: Once your cloud environment has started to expand and more hardware resources
    are deployed, cloud operators should find ways to optimize costs. There are niche
    use cases, such as optimizing the placement of VMs (VMs migrate between hosts
    in case of imbalance detection).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的云环境开始扩展，并且更多的硬件资源被部署，云运营商应该寻找优化成本的方法。有些特定的使用案例，比如优化虚拟机（VM）的位置（在检测到不平衡时，虚拟机会在主机之间迁移）。
- en: 'The lack of tooling and efficient processes to conduct a resource optimization
    exercise presents a big challenge to private cloud operators. Traditionally, OpenStack
    administrators needed to manually grab historical metrics on resource usage, analyze
    them in regular sprints, and make decisions based on the collected data. However,
    manual processing in a large OpenStack deployment can be error prone. To automate
    such a process, the OpenStack community has come up with an evolving incubated
    project code named **Watcher** . The main objective of the Watcher project is
    to enable new ways for cloud operators to reduce the cloud’s **Total Cost of Ownership**
    ( **TCO** ). Watcher is designed to monitor, analyze, and execute optimization
    tasks based on predefined goals. The following diagram illustrates how Watcher’s
    continuous optimization loop is designed through a set of steps:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏工具和高效的流程来进行资源优化演练是私有云运营商面临的一大挑战。传统上，OpenStack管理员需要手动获取资源使用的历史指标，在定期的冲刺中分析这些指标，并根据收集的数据做出决策。然而，在大型OpenStack部署中，手动处理容易出错。为了自动化这一过程，OpenStack社区提出了一个名为**Watcher**的不断发展的孵化项目。Watcher项目的主要目标是为云运营商提供新的方法来减少云的**拥有总成本**（**TCO**）。Watcher被设计用来基于预定义目标监控、分析并执行优化任务。下图展示了Watcher如何通过一系列步骤设计其持续优化循环：
- en: '![Figure 9.17 – OpenStack’s Watcher steps workflow](img/B21716_09_17.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17 – OpenStack的Watcher步骤工作流](img/B21716_09_17.jpg)'
- en: Figure 9.17 – OpenStack’s Watcher steps workflow
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – OpenStack的Watcher步骤工作流
- en: It starts with the **Monitor** state, where data metrics are collected from
    various data sources, such as the type of CPU, memory utilization, and energy
    consumption. Then, the collected information is analyzed and aggregated. Watcher
    then engages a profiler component that concludes a few patterns and uses that
    to predict VM resource utilization. An optimization plan will be created by an
    **Optimizer** component by taking a set of goals and constraints as inputs, for
    example, by creating some collocation rules such as affinity and anti-affinity
    rules in the Nova scheduler. Watcher will make scheduling decisions based on such
    inputs and can leverage some of the other constraints defined by other projects.
    The next stage runs the **Planner** component, so Watcher builds action items,
    for example, if a VM needs to be migrated from one host to another. In the Plan
    phase, Watcher puts a set of steps that can be acted upon and ordered to run serially
    or in parallel. Finally, Watcher executes the action plan and applies the optimal
    state of the infrastructure as specified in the defined goals.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 它从**Monitor**状态开始，在此状态下，来自各种数据源的数据指标被收集，例如 CPU 类型、内存使用率和能耗。然后，收集到的信息会被分析和汇总。Watcher
    随后会启动一个分析器组件，该组件会总结出一些模式，并使用这些模式来预测虚拟机资源的使用情况。一个优化计划将由**Optimizer**组件创建，该组件以一组目标和约束为输入，例如，通过在
    Nova 调度器中创建一些亲和性和反亲和性规则。Watcher 会根据这些输入做出调度决策，并可以利用其他项目定义的一些约束。下一阶段运行**Planner**组件，因此
    Watcher 会构建行动项目，例如，如果虚拟机需要从一个主机迁移到另一个主机。在计划阶段，Watcher 会列出一组可以执行的步骤，并按顺序或并行执行。最后，Watcher
    执行行动计划，并根据定义的目标应用基础设施的最佳状态。
- en: The next section will go through the installation of the Watcher project in
    the existing OpenStack environment using **kolla-ansible** and demonstrate its
    usage.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将介绍如何使用 **kolla-ansible** 在现有 OpenStack 环境中安装 Watcher 项目，并演示其使用方法。
- en: Watcher in action
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Watcher 运行中
- en: 'The latest **kolla-ansible** code releases come with the Watcher playbook ready
    for installation. Keystone, Nova, and Ceilometer must be running before installing
    Watcher. In the following setup, all Watcher components, including **watcher-api**
    , **watcher-engine** , and **watcher-applier** , will be part of the cloud controller
    node group. To install **watcher** components, configure the **multi_packtpub_prod**
    inventory file by adding the following section:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 **kolla-ansible** 代码发布版本已经包含了用于安装 Watcher 的 Playbook。在安装 Watcher 之前，Keystone、Nova
    和 Ceilometer 必须已经在运行。以下设置中，所有 Watcher 组件，包括 **watcher-api**、**watcher-engine**
    和 **watcher-applier**，将成为云控制器节点组的一部分。要安装 **watcher** 组件，请通过添加以下部分来配置 **multi_packtpub_prod**
    库存文件：
- en: '[PRE31]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, enable the Watcher service in the **globals.yml** file by adjusting the
    following configuration line:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过调整以下配置行，在**globals.yml**文件中启用 Watcher 服务：
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The Watcher service comes with a Horizon plugin that will be visible on the
    Horizon dashboard once installed. Commit the changes and run the pipeline. Once
    finished, check the new Docker containers for the Watcher service by running the
    following command line in any of the controller nodes:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Watcher 服务带有一个 Horizon 插件，一旦安装，它将在 Horizon 仪表盘上可见。提交更改并运行管道。完成后，通过在任何控制节点上运行以下命令，检查
    Watcher 服务的新 Docker 容器：
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here is the output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![Figure 9.18 – A Watcher deployment check](img/B21716_09_18.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.18 – Watcher 部署检查](img/B21716_09_18.jpg)'
- en: Figure 9.18 – A Watcher deployment check
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – Watcher 部署检查
- en: 'Before starting to use the Watcher service, it is important to highlight the
    different steps of a Watcher workflow:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用 Watcher 服务之前，重要的是要强调 Watcher 工作流的不同步骤：
- en: Create an optimization goal and associate it with a strategy.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个优化目标并将其与策略关联。
- en: Create an audit template that is associated with the optimization goal.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个与优化目标关联的审计模板。
- en: Create an audit that will be triggered by the audit template.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个由审计模板触发的审计。
- en: Generate an action plan by the created audit.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过创建的审计生成行动计划。
- en: Take action either manually or in an automatic fashion.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采取手动或自动的行动。
- en: In the following part of this section, we will demonstrate a predefined Watcher
    strategy, which is referred to as the **VM Workload Consolidation Strategy** .
    The goal of such a strategy is to consolidate a running workload in the cloud
    and optimize the number of resources that run it.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的接下来的部分，我们将演示一个预定义的 Watcher 策略，称为**VM 工作负载合并策略**。该策略的目标是将云中正在运行的工作负载进行合并，并优化运行该工作负载所需的资源数量。
- en: 'To quickly check the predefined list of goals in Watcher, run the following
    command line and take note of the **Server Consolidation** goal **UUID** or **Name**
    in the output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速检查Watcher中预定义目标列表，请运行以下命令行并记下输出中的**服务器合并**目标**UUID**或**名称**：
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 9.19 – Watcher optimization goal listing](img/B21716_09_19.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – Watcher优化目标列表](img/B21716_09_19.jpg)'
- en: Figure 9.19 – Watcher optimization goal listing
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – Watcher优化目标列表
- en: 'Next, obtain the list of available strategies for the **Server Consolidation**
    goal using the listed goal **UUID** or **Name** from the previous output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用上一步输出中列出的目标**UUID**或**名称**获取**服务器合并**目标的可用策略列表：
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We get the following output now:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到以下输出：
- en: '![Figure 9.20 – A Watcher optimization strategy listing](img/B21716_09_20.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.20 – Watcher优化策略列表](img/B21716_09_20.jpg)'
- en: Figure 9.20 – A Watcher optimization strategy listing
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 – Watcher优化策略列表
- en: 'From the returned list of strategies, we will use the **vm_workload_consolidation**
    strategy to minimize the number of hosts running a workload with respect to resource
    capacity constraints. Before creating the audit template, we can check the distribution
    of the instances across different hypervisor machines before applying the selected
    optimization strategy in the Watcher panel:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从返回的策略列表中，我们将使用**vm_workload_consolidation**策略，以减少运行在与资源容量约束相关的工作负载的主机数量。在创建审计模板之前，在Watcher面板中应用所选的优化策略前，我们可以检查不同虚拟化管理程序机器上实例的分布：
- en: '![Figure 9.21 – The instance distribution state before the optimization is
    applied](img/B21716_09_21.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.21 – 优化应用前实例分布状态](img/B21716_09_21.jpg)'
- en: Figure 9.21 – The instance distribution state before the optimization is applied
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 – 优化应用前实例分布状态
- en: The **Hypervisor** list is composed of three compute nodes and the specific
    distribution of instances spread across them.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟化管理程序**列表包括三个计算节点和它们之间实例的特定分布。'
- en: 'In the next step, create an audit template associated with the selected goal
    and the chosen strategy by running the following command line:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过运行以下命令行创建与所选目标和策略关联的审计模板：
- en: '[PRE36]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is the output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 9.22 – Watcher optimization audit template creation](img/B21716_09_22.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.22 – Watcher优化审计模板创建](img/B21716_09_22.jpg)'
- en: Figure 9.22 – Watcher optimization audit template creation
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – Watcher优化审计模板创建
- en: 'Next, run an audit from the created audit template, as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行从创建的审计模板中的审计，如下所示：
- en: '[PRE37]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 9.23 – Watcher optimization audit creation](img/B21716_09_23.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.23 – Watcher优化审计创建](img/B21716_09_23.jpg)'
- en: Figure 9.23 – Watcher optimization audit creation
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – Watcher优化审计创建
- en: Note that the audit creation may take longer to complete. The **State** field
    shows **PENDING** . During that state, the audit request is treated by the Watcher
    decision engine, which passes the requested audit to the next state. If the Watcher
    decision engine finds at least one optimization option that can be applied within
    the set of the audited resources (compute nodes for the context of the current
    goal and strategy), the audit state will change to **SUCCEEDED** .
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，审计创建可能需要更长时间才能完成。**状态**字段显示**PENDING**。在此状态下，审计请求由Watcher决策引擎处理，Watcher决策引擎将审计传递到下一个状态。如果Watcher决策引擎在当前目标和策略的上下文中找到至少一个可应用的优化选项，则审计状态将更改为**SUCCEEDED**。
- en: 'Use the **optimize audit show** command with the generated audit UUID and verify
    whether the status of the audit creation is completed:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**optimize audit show**命令，并使用生成的审计UUID验证审计创建的状态是否已完成：
- en: '[PRE38]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is the output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 9.24 – Watcher optimization audit validation](img/B21716_09_24.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.24 – Watcher优化审计验证](img/B21716_09_24.jpg)'
- en: Figure 9.24 – Watcher optimization audit validation
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24 – Watcher优化审计验证
- en: For each created audit, the optimization service generates an action plan. An
    action plan defines which tasks need to be executed to achieve the goal set at
    the start. Action plans use advanced algorithms and return some pertinent information,
    such as **efficacy indicators** (reflects an improvement score based on the generated
    audit solution) and global efficacy (general scoring for the evaluated action
    plan).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个创建的审计，优化服务会生成一个行动计划。行动计划定义了为实现开始时设定的目标需要执行的任务。行动计划使用先进的算法并返回一些相关信息，如**效能指标**（基于生成的审计解决方案反映的改进得分）和全局效能（评估的行动计划的总体得分）。
- en: 'Once the audit creation status changes to **SUCCEEDED** , run the following
    command line with the audit UUID to retrieve the action plan generated by the
    audit:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦审计创建状态更改为**成功**，请使用审计的 UUID 运行以下命令行，以检索审计生成的行动计划：
- en: '[PRE39]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 9.25 – Watcher optimization action plan listing](img/B21716_09_25.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.25 – Watcher 优化行动计划列表](img/B21716_09_25.jpg)'
- en: Figure 9.25 – Watcher optimization action plan listing
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25 – Watcher 优化行动计划列表
- en: Note in the previous output that the Watcher planner has created an action plan
    that is waiting for operator validation. The **RECOMMENDED** state is the last
    step performed by the Planner component. From the **Global efficacy** information,
    the **watcher-planner** process has concluded that around 33% of compute nodes
    can be pulled back while maintaining the same amount of running workloads.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的输出中，Watcher 计划器已经创建了一个等待操作员验证的行动计划。**推荐**状态是计划器组件执行的最后一步。从**全局效能**信息来看，**watcher-planner**进程得出结论，约
    33% 的计算节点可以在保持相同运行负载的情况下被撤回。
- en: 'The returned action plan line is not sufficient to go through all the different
    action items. There is a separate command line for that purpose. Before applying
    any action plan, an operator should review the associated actions as follows.
    Run the following command line with the action plan UUID generated in the previous
    step:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的行动计划行不足以列出所有不同的行动项。为此，存在单独的命令行。在应用任何行动计划之前，操作员应按如下方式查看相关的行动项。使用前一步生成的行动计划
    UUID 运行以下命令行：
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The action plan lists all items that will be actioned. The following list is
    a truncated version for a longer output that indicates the change of the Nova
    service’s state in the first line. The following lines are the planned action
    based on the first item that will be executed serially. Each following action
    item is a recommendation of the Watcher planner to migrate a set of instances
    once the parent action (the Nova service’s state) is updated:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 行动计划列出了所有将被执行的项目。以下列表是一个简化版本，表示在第一行中 Nova 服务状态的变化。接下来的行是基于第一项计划的行动，将按顺序执行。每个后续的行动项是
    Watcher 计划器的推荐，旨在更新父行动（Nova 服务状态）后迁移一组实例：
- en: '![Figure 9.26 – Watcher optimization action listing](img/B21716_09_26.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.26 – Watcher 优化动作列表](img/B21716_09_26.jpg)'
- en: Figure 9.26 – Watcher optimization action listing
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26 – Watcher 优化行动列表
- en: 'Once reviewed, an operator can go ahead and run the **watcher-applier** process
    to execute the action plan by running the following command line with the action
    plan UUID:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 审核完成后，操作员可以继续运行 **watcher-applier** 进程，通过运行以下命令行并使用行动计划 UUID 来执行该行动计划：
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output can be quite large due to the values assigned to action plan indicators
    ( **Efficacy indicators** and **Global efficacy** embedded JSON values). A simple
    truncated version can be visualized as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分配给行动计划指标的值（**效能指标**和**全局效能**嵌入的 JSON 值），输出可能非常大。可以将其可视化为一个简化的截断版本，如下所示：
- en: '![Figure 9.27 – Watcher optimization action start](img/B21716_09_27.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.27 – Watcher 优化行动开始](img/B21716_09_27.jpg)'
- en: Figure 9.27 – Watcher optimization action start
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.27 – Watcher 优化行动开始
- en: 'Most importantly, note the **PENDING** state of the action plan execution.
    That is expected to keep running for a longer time due to the migration of a set
    of instance tasks in the background. The **Efficacy indicators** and **Global
    efficacy** fields simply reflect a description of the recommended action plan
    that can be summarized as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，请注意行动计划执行的**待处理**状态。由于后台迁移一组实例任务，这一状态预计会持续较长时间。**效能指标**和**全局效能**字段仅反映推荐的行动计划描述，概要如下：
- en: '**Efficacy indicators** : This refers to the number of compute nodes in which
    the **Applier** component is performing optimization actions. In our example,
    we are dealing with three nodes. As recommended, the audit aims to release one
    compute node from the three. Additionally, the indicator keeps counting each migrated
    instance reported by the Nova service when accomplished. The **instance_migrations_count**
    recommendation indicates that the number of instance migrations to be performed
    is **2** .'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效果指标**：指的是**Applier** 组件执行优化操作的计算节点数量。在我们的示例中，我们处理的是三个节点。根据推荐，审核旨在从这三个节点中释放一个计算节点。此外，当
    Nova 服务报告完成迁移时，该指标会继续计算每个迁移的实例。**instance_migrations_count** 推荐表明要执行的实例迁移数量为**2**。'
- en: '**Global efficacy** : This is the ratio of nodes aimed to be released once
    all action items are executed. The ratio is obtained by dividing the number of
    released compute nodes by the total number of compute nodes in the audit scope.
    In our case, the ratio will be approximately 33%.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局效果**：这是指在执行所有操作项后，计划释放的节点比例。该比例通过将已释放的计算节点数除以审核范围内的总计算节点数来获得。在我们的例子中，比例大约为33%。'
- en: 'You can run the previous command line several times and notice the change of
    the action plan execution state from **PENDING** to **ONGOING** . Once all migration
    tasks are performed, the state of the action plan should be updated to **SUCCEEDED**
    with the execution summary in the **Efficacy indicators** and **Global** **efficacy**
    fields:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以多次运行之前的命令行，并注意到行动计划执行状态从**PENDING**变为**ONGOING**的变化。一旦所有迁移任务完成，行动计划的状态应该更新为**SUCCEEDED**，并在**效果指标**和**全局效果**字段中显示执行摘要：
- en: '[PRE42]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 9.28 – Watcher optimization action plan start](img/B21716_09_28.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.28 – Watcher 优化行动计划开始](img/B21716_09_28.jpg)'
- en: Figure 9.28 – Watcher optimization action plan start
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28 – Watcher 优化行动计划开始
- en: 'By checking the Watcher dashboard, we can notice the change in the instance
    counts across the listed hypervisors where **cn01.os** becomes completely free.
    Watcher has determined that a given workload can run in two compute nodes instead
    of three, leaving one hypervisor machine’s resources free, as shown:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查 Watcher 仪表板，我们可以注意到列出超管的实例数量发生了变化，其中**cn01.os**变得完全空闲。Watcher 已确定，给定的工作负载可以在两个计算节点上运行，而不是三个，从而释放了一个超管机器的资源，如下所示：
- en: '![Figure 9.29 – The instance distribution state after optimization is applied](img/B21716_09_29.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.29 – 应用优化后的实例分布状态](img/B21716_09_29.jpg)'
- en: Figure 9.29 – The instance distribution state after optimization is applied
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.29 – 应用优化后的实例分布状态
- en: With this optimization achievement, cloud operators should get more free resources
    to route requests to for future workloads. Predefined goals are a great start
    to performing optimization exercises. Operators can define custom strategies and
    goals for different cases based on workload needs as well as the scheduling mechanism
    that has already been configured. From a big-picture perspective, a well-defined
    filtering and scheduling configuration is beneficial not only to tackle customization
    on compute resources and allocation but also to help the Watcher planner find
    the best optimization options. That will save operational overhead and a good
    amount of costs.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一优化成果，云运营商应该能够获取更多的空闲资源，以便将请求路由到未来的工作负载。预定义的目标是执行优化操作的一个良好开端。运营商可以根据工作负载需求以及已经配置的调度机制，为不同的情况定义自定义策略和目标。从大局观来看，精心设计的过滤和调度配置不仅有助于应对计算资源和分配的定制化，还能帮助
    Watcher 规划者找到最佳的优化方案。这将节省运营开销并大幅降低成本。
- en: Summary
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we brought our OpenStack setup to the next level by highlighting
    a few advanced settings that leverage its performance, such as the database. You
    should now understand the necessity for undergoing rigorous and effective testing
    of the cloud platform.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过突出了几项利用 OpenStack 性能的高级设置（例如数据库），将我们的 OpenStack 设置提升到了一个新的水平。你现在应该理解，对云平台进行严格且有效的测试是必要的。
- en: There is a learning curve in the art of benchmarking, which gives insight into
    all facets of the components running in the OpenStack environment, including system
    hardware and software resources. The chapter highlighted a tiny component, OSProfiler,
    that can be installed easily in OpenStack to trace and debug requests traveling
    across the OpenStack services. With a complete view of requests, operators can
    collect more data and generate a detailed service map that can be used for further
    performance analytics. The last part of the chapter drew upon best practices relating
    to resource optimization and automating recommendations when opportunities exist
    to reduce costs and improve performance. With Watcher, operators will not need
    to seek a third-party solution unless more functionalities are required. Benchmarking,
    profiling, and optimization practices should be performed periodically. Although
    the chapter did not cover how to automate those practices, creating dedicated
    pipelines for each benchmarking, profiling, and optimization process is highly
    recommended.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试的艺术有一定的学习曲线，它可以深入了解在 OpenStack 环境中运行的各个组件的各个方面，包括系统硬件和软件资源。本章重点介绍了一个小组件——OSProfiler，它可以轻松安装在
    OpenStack 中，用于跟踪和调试在 OpenStack 服务之间传递的请求。通过对请求的全面视图，运维人员可以收集更多数据并生成详细的服务地图，进一步用于性能分析。本章的最后部分讨论了与资源优化相关的最佳实践，并在有机会减少成本和提升性能时，自动化推荐方案。借助
    Watcher，运维人员无需寻找第三方解决方案，除非需要更多功能。基准测试、性能分析和优化实践应该定期执行。虽然本章没有讲解如何自动化这些实践，但强烈推荐为每个基准测试、性能分析和优化过程创建专门的管道。
- en: This chapter closes the second part of this book. In the next part, we will
    look at some recent and modern approaches that leverage the benefits of private
    and public clouds. In the next chapter, we will go through some common cloud hybrid
    design patterns where OpenStack continues to shine.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了本书的第二部分。在接下来的部分，我们将探讨一些现代方法，这些方法利用了私有云和公有云的优势。在下一章中，我们将讨论一些常见的云混合设计模式，OpenStack
    在这些模式中继续发挥作用。
- en: 'Part 3: Extending the OpenStack Cloud'
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：扩展 OpenStack 云
- en: The final part of the book will crystallize a trendy topic covering the adoption
    of a hybrid cloud setup. Using OpenStack as a private cloud environment joined
    with a public cloud provider, this part of the book will demonstrate how to leverage
    both cloud models to spread workloads in the most efficient and cost-saving ways.
    Empowered with microservices design patterns and containerization technology,
    this part will explore different tools and ways to run a workload based on Kubernetes
    across a running private cloud based on OpenStack and a public cloud based on
    AWS.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的最后部分将具体讲解一个趋势话题——混合云架构的采用。通过将 OpenStack 作为私有云环境，并与公有云提供商连接，本书这一部分将展示如何利用这两种云模型以最有效、最节省成本的方式分配工作负载。借助微服务设计模式和容器化技术，本部分将探讨在基于
    OpenStack 的私有云和基于 AWS 的公有云上运行 Kubernetes 工作负载的不同工具和方法。
- en: 'This part has the following chapters:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 10*](B21716_10.xhtml#_idTextAnchor217) , *OpenStack Hybrid Cloud
    – Design Patterns*'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 10 章*](B21716_10.xhtml#_idTextAnchor217)，*OpenStack 混合云——设计模式*'
- en: '[*Chapter 11*](B21716_11.xhtml#_idTextAnchor230) , *A Hybrid Cloud Hyperscale
    Use Case – Scaling a Kubernetes Workload*'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第 11 章*](B21716_11.xhtml#_idTextAnchor230)，*混合云超大规模用例——扩展 Kubernetes 工作负载*'
