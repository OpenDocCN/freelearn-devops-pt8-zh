- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring and Logging – Remediating Proactively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “To accept something on mere presumption and, likewise, to fail to investigate
    it may cover over, blind, and lead astray.”
  prefs: []
  type: TYPE_NORMAL
- en: – Abu Nasr Al Farabi
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the underlying infrastructure of the private cloud is an essential
    operational activity that requires consistent tools and automated ways to achieve
    real-time metrics collection and alerting mechanisms. Among the different OpenStack
    releases, a variety of open-source and commercial tools exist to support the monitoring
    of different pieces of the OpenStack ecosystem. In the latest releases, we can
    find more emerging tools with successful monitoring stories, offering more simplicity
    and easier integration into the OpenStack ecosystem. Collecting metrics data is
    one part of OpenStack’s operational tasks; the other part is the log data. Each
    OpenStack and shared service generates a load of log data with different logging
    levels, which, combined, would become a complicated task to manually parse and
    find anomalies when issues occur. Having a robust logging pipeline is a requirement
    to satisfy one pillar of the OpenStack operational journey. This chapter will
    combine both monitoring and logging practices with recent emerging tools that
    can be integrated into a running OpenStack environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Prometheus as a solution for the OpenStack infrastructure and workload
    monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and integrating Prometheus in the current OpenStack deployment and
    starting to use a separate monitoring stack to expose OpenStack metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring, deploying, and building dashboards using Grafana as a central observability
    tool for data representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an efficient and centralized logging solution using OpenSearch to ship,
    parse, and build custom search queries for collected logs, enabling further analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack infrastructure monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Beyond the workloads running in OpenStack tenants, cloud operators must ensure
    that they have the necessary metrics to ensure OpenStack services’ availability,
    performance, and usage status. Monitoring data should involve mainly two different
    categories of the OpenStack infrastructure services, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud software** : This includes OpenStack APIs, schedulers, databases, message
    busses, and load-balancing services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Under cloud** : This includes running operating systems, storage, and network
    hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will explore Prometheus, a well-integrated and
    adopted monitoring and alerting toolkit for large-scale deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus in a nutshell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Prometheus** allows you to collect and store metrics as time-series data.
    Prometheus is considered a great fit for large deployment, as it provides exporters
    with components that ship various amounts of metrics to a Prometheus server. Additionally,
    the tool includes an alerting mechanism to manage alerts and notifications based
    on configured thresholds. Unlike other tools, such as Zabbix or Nagios, Prometheus
    offers flexible ways to feed metrics and data collected in its format. This is
    performed by third-party exporters that provide Prometheus-compatible metrics
    with less effort to develop separate converters for each system of the software
    framework. Some of the official exporters are maintained in Prometheus’s GitHub
    repository and can be found at [https://github.com/prometheus](https://github.com/prometheus)
    . In the OpenStack world, a vast number of metrics can be exported to Prometheus
    via the OpenStack Prometheus exporter, with a full list at [https://github.com/openstack-exporter/openstack-exporter](https://github.com/openstack-exporter/openstack-exporter)
    . As shown in the following diagram, Prometheus also has a discovery service that
    can be used by the OpenStack APIs to explore and generate data metrics for the
    Prometheus server. This is mainly useful for reporting metrics for virtual machines
    – for example, when utilizing the service discovery on the Nova API service. As
    soon as metrics are collected and stored by Prometheus, a single pane of glass
    should exist to consolidate and organize all data in comprehensible dashboards.
    Grafana is a great open-source tool for metrics visualization that can be configured
    to use Prometheus as its data source.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A Prometheus integration and monitoring stack in OpenStack](img/B21716_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A Prometheus integration and monitoring stack in OpenStack
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, Prometheus will be deployed and integrated as the
    main monitoring tool in the OpenStack environment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying infrastructure monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adopting the Prometheus and Grafana pair as a central monitoring and observability
    solution for the OpenStack infrastructure is a quick win for cloud operators,
    due to the well-tested and integrated monitoring solution in the latest OpenStack
    releases. Kolla Ansible promotes and supports Prometheus and Grafana integration
    in the latest code releases. An additional monitoring host can be dedicated to
    run the Prometheus server and, optionally, Grafana. This can be achieved through
    the following configuration in the infrastructure as code settings:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the **multi_packtpub_prod** inventory file by adding the new monitoring
    host, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the Prometheus service to the monitoring host group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the built-in Prometheus alert manager service to the monitoring host group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Include the OpenStack Prometheus exporter in the monitoring host group. This
    will create a dedicated service container to automatically pull formatted Prometheus
    data from the OpenStack services’ metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the Prometheus Node Exporter role to run in each host. The Node Exporter
    provides a wide range of Linux operating system and hardware metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prometheus provides a dedicated exporter for the **MariaDB database** . The
    MySQL server metrics exposed through the exporter provide an extended list of
    data metrics available for scraping. The Prometheus MySQL exporter can be added
    to the **mariadb** role running in the cloud controller nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, Prometheus can be configured to scrape the load balancing metrics
    and stats running HAProxy via its dedicated exporter running in the load balancer
    nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Another available exporter that exposes both host and instance metrics is the
    **Prometheus Libvirt exporter** . This exporter collects data from the **Libvirt
    API** and exposes useful metrics for compute nodes and running instances. The
    **libvirt** exporter can be installed in the compute nodes as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For a broader monitoring setup across the OpenStack infrastructure, consider
    keeping an eye on the status of the containers running in each node. The Prometheus
    **cAdvisor** (short for **Container Advisor** ) exporter is designed to collect,
    analyze, aggregate, and export container performance and metrics data. The **cAdvisor**
    exporter will run across all OpenStack nodes as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to enable and install the Prometheus service using **kolla-ansible**
    . At the time of writing this edition, **kolla-ansible** will install the latest
    version of the Prometheus server after 2.0 by adding the following variable to
    the **globals.yml** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For data exposure and visualization, enable Grafana in the **globals.yml**
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**kolla-ansible** allows custom settings for the monitoring layer for both
    the Prometheus and Grafana services. Once both services are installed, you will
    need to provide a username and password for access. By default, **kolla-ansible**
    installs Prometheus using basic authentication via HTTP, with an **admin** user
    and a Grafana user, **grafana** . Both default variables are assigned to the **/kolla-ansible/ansible/group_vars/all.yml**
    file as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For custom user access settings, you can override the default usernames by
    adjusting the variables directly from the **all.yml** file or in the **prometheus_basic_auth_users**
    configuration section in the **/kolla-ansible/ansible/roles/prometheus/defaults/main.yml**
    file, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'User passwords for both Prometheus and Grafana can be reconfigured in the **/etc/kolla/passwords.yml**
    file, where all service secrets are defined. Both passwords are defined with the
    following variables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The other option is to create new users using either custom passwords or the
    default ones, without overriding the existing ones, by extending the **prometheus_basic_auth_users_extra**
    variable in the **/kolla-ansible/ansible/roles/prometheus/defaults/main.yml**
    file, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Another great metrics-scraping way is to configure a custom Prometheus run
    by listing a set of startup parameters in the **prometheus_cmdline_extras** variable,
    in the **globals.yml** file. For example, Prometheus can be configured to retain
    metrics data up to a specific size, via the **--storage.tsdb.retention.size**
    parameter. In the following listing, a customized configuration is defined to
    limit the maximum number of simultaneous connections on the Prometheus dashboard
    to **30** connections, adjust the log severity level to include **error** (the
    default severity level is **info** ), increase the frequency of resending an alert
    to the Prometheus alert manager every **30** seconds, and retain metrics data
    for **30** days in storage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A full list of the Prometheus startup parameters can be found at [https://prometheus.io/docs/prometheus/latest/command-line/prometheus/](https://prometheus.io/docs/prometheus/latest/command-line/prometheus/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Run a pipeline to deploy the Prometheus server, Grafana, Prometheus cAdvisor,
    and relevant exporter components. Once the deployment pipeline is completed, verify
    the newly deployed containers by running the following command line in one of
    the controller nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Listing the Prometheus and Grafana Kolla containers](img/B21716_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Listing the Prometheus and Grafana Kolla containers
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will check out the Prometheus-collected metrics
    and build insightful dashboards using Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the monitoring data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The latest **kolla-ansible** deployment enables the Prometheus server to start
    collecting different metrics available for scraping that can be checked on the
    Prometheus dashboard. The main Prometheus dashboard can be accessed by pointing
    to the IP address of the monitoring host: **http://host_monitoring_ip/prometheus**
    , where **host_monitoring_ip** is the IP address of the monitoring host running
    the Prometheus server.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The Prometheus dashboard interface](img/B21716_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – The Prometheus dashboard interface
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus collects an exhaustive list of metrics from each installed exporter
    across all defined hosts in the inventory. It uses **labels** to define each metric’s
    target endpoint. For example, once the OpenStack exporter is installed, Prometheus
    starts collecting OpenStack-related metrics that can be checked by pointing to
    **http://host_monitoring_ip:9090/targets** , where **host_monitoring_ip** is the
    IP address of the monitoring host running the Prometheus server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Collected metrics will be visible in the Prometheus dashboard as soon as the
    exporter service in each node sends metrics data for scraping. It might take a
    while to visualize all the metrics in the Prometheus dashboard. For example, the
    following listing shows that the initial OpenStack service exported metrics through
    **http://host_monitoring_ip:9100/metrics** , where **host_monitoring_ip** is the
    IP address of the monitoring host running the Prometheus server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – A listing of the collected OpenStack service metrics in Prometheus](img/B21716_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – A listing of the collected OpenStack service metrics in Prometheus
  prefs: []
  type: TYPE_NORMAL
- en: 'The Node Exporter service installed on each OpenStack physical machine to gather
    host metrics, such as memory usage and stats, can be checked through the metrics
    listing at the same Prometheus dashboard URL, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – A listing of the collected memory metrics and stats in Prometheus](img/B21716_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – A listing of the collected memory metrics and stats in Prometheus
  prefs: []
  type: TYPE_NORMAL
- en: With more nodes joining the OpenStack infrastructure, the number of collected
    metrics could be doubled, which would affect the monitoring host performance and
    disk capacity. For this reason, it is highly encouraged to use more than one Prometheus
    server instance to handle more metrics export and scraping jobs. Prometheus is
    designed to scale well in large deployments, with gazillions of resource types
    and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Browsing hundreds of metrics on the Prometheus dashboard to follow up on hundreds
    of service statuses and related usage information can be cumbersome. Prometheus
    can be very helpful with most essential operational tasks, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics data collection configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Management and configuration of alerts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting metrics export issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus is a highly configurable tool and provides a native command-line
    interface to customize metrics collection and scraping settings, which are beyond
    the scope of this book. We will use the exported metrics data by Prometheus and
    visualize it in a simple Grafana dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: The latest **kolla-ansible** run should enable the Grafana service with the
    default dashboard and configure Prometheus as a data source. To validate the Prometheus
    data source, point to **http://host_monitoring_ip:3000** , where **host_monitoring_ip**
    is the IP address of the monitoring host running the Grafana instance. Provide
    the username and password to access the Grafana dashboard configured in the previous
    steps. If you did not customize the Grafana access credentials, use the username
    and password pair as **admin** / **admin** , respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once logged in, navigate in the dashboard to the settings icon | **Configuration**
    | **Data sources** , and make sure that the Prometheus data source is added. Verify
    that the data source is functional, and no errors are reported. Click on the **Save
    & test** button, which should display that the data source configuration is functional,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Adding a Prometheus data source in the Grafana dashboard](img/B21716_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Adding a Prometheus data source in the Grafana dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the data source is checked, building dashboards in Grafana is straightforward
    and can be done either by creating freestyle dashboards or by importing dashboards
    from the Grafana website catalog. [https://grafana.com/](https://grafana.com/)
    exposes different dashboards for each Prometheus exporter, identified by a dashboard
    ID that can be simply added to the local Grafana server. You can also import the
    dashboard in the JSON format and copy and paste it into the Grafana import section.
    In the next wizard, we will import Grafana dashboards for the OpenStack and Node
    Exporters using the **dashboard import IDs** option. To do so, simply navigate
    to the Grafana home page, then click on the **+** icon and select **Import** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Adding a dashboard in Grafana](img/B21716_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Adding a dashboard in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the OpenStack metrics exporter, navigate to the OpenStack Grafana
    dashboard at [https://grafana.com/grafana/dashboards/9701-openstack-dashboard/](https://grafana.com/grafana/dashboards/9701-openstack-dashboard/)
    and copy the ID of the dashboard, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Obtaining the dashboard ID](img/B21716_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Obtaining the dashboard ID
  prefs: []
  type: TYPE_NORMAL
- en: 'Paste the copied dashboard ID into the Grafana dashboard **Import** page. The
    dashboard ID should be **9701** :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Importing the dashboard ID into Grafana](img/B21716_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Importing the dashboard ID into Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'The next page of the import wizard will guide you through the settings of the
    imported dashboard. Enter the name of the dashboard of your choice, and make sure
    that the **Prometheus** data source is selected, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Configuring the dashboard with Prometheus as the data source](img/B21716_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Configuring the dashboard with Prometheus as the data source
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the import wizard has completed, the imported Grafana dashboard for the
    OpenStack exporter metrics will display a variety of graphs and metrics, such
    as a generic overview of a resource’s usage per OpenStack service. Some highlights
    of the services metrics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The current existing Keystone users, groups, and projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutron resources such as the floating IPs, virtual networks, and security groups
    deployed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of running instances by the Nova service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current count of Glance images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volumes and snapshots managed by the Cinder service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall usage of the CPU, RAM, and disk resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several dashboards can be displayed, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – The imported generic dashboard with Prometheus as the data
    source](img/B21716_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – The imported generic dashboard with Prometheus as the data source
  prefs: []
  type: TYPE_NORMAL
- en: The imported dashboard has more insights into each service and its relative
    subcomponents, such as API services and agents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – The imported dashboards for each OpenStack API service status](img/B21716_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – The imported dashboards for each OpenStack API service status
  prefs: []
  type: TYPE_NORMAL
- en: Another useful dashboard should exist to monitor the hosts themselves running
    the cloud environment, via the Prometheus Node Exporter. The Grafana template
    dashboard can be found at [https://grafana.com/grafana/dashboards/1860-node-exporter-full/](https://grafana.com/grafana/dashboards/1860-node-exporter-full/)
    , with the dashboard ID **1860** . Follow the same import instructions performed
    for the OpenStack exporter dashboard to make sure the data source selected is
    Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once imported, a new Grafana namespace will be created, with Prometheus as
    the data source. The collected data reflects granular and insightful information
    about the operating system and hardware metrics collected from each OpenStack
    host, using the installed Node Exporter service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – A consolidated dashboard for OpenStack](img/B21716_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – A consolidated dashboard for OpenStack
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can import ready-to-go Grafana dashboards for each installed
    Prometheus exporter. The database metrics dashboard can be found at [https://grafana.com/grafana/dashboards/14057-mysql/](https://grafana.com/grafana/dashboards/14057-mysql/)
    , and HAProxy is available at [https://grafana.com/grafana/dashboards/2428-haproxy/](https://grafana.com/grafana/dashboards/2428-haproxy/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: As far as data collected from the hosts running OpenStack services is concerned,
    the Prometheus Node Exporter service supports proactively identifying possible
    system anomalies. The imported dashboard consolidates all hosts’ metrics in one
    single pane of glass that can be filtered by one or a group of hosts, offering
    a more custom visualization experience.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will move on to keep track of the workloads running on top of the OpenStack
    private cloud.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack workload monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next remaining part of the operational task of cloud operators is to provide
    monitoring support to the running workloads of the OpenStack tenants. Operators
    should also monitor some generic metrics of the provisioned resources by tenants
    to anticipate quota limitations and gain insightful analysis of the infrastructure
    usage. This can be achieved by using the same toolsets covered in the previous
    section. Workload monitoring focuses more on the resources deployed by each tenant
    – more precisely, the Nova instances. Prometheus is capable of querying the Nova
    API, and along with its labeling configuration settings, you should be able to
    filter and organize a list of thousands of instances into a set of scraped targets.
    In the previous section, the Prometheus Libvirt exporter was deployed in each
    compute node (see *Step 8* of the *Deploying infrastructure monitoring* section).
    Thus, there is no need to reconfigure any part of the infrastructure hosts. Instances
    metrics should be available in the Prometheus server, but identifying any reported
    instances with additional information is optional. Prometheus can be configured
    to discover instances in OpenStack by applying exporter configuration and the
    **relabel_configs** settings, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The tag-relabeling configuration helps to pull the metadata of discovered instances
    that matches a regular expression in Prometheus. In the previous example, the
    **__meta_openstack_(.+)** regular expression is applied and added for each metric
    received for all different source labels. This is useful to visualize tenant resources
    with more specific information, such as the instance name, IP address, and instance
    status, as shown in the following Prometheus server endpoint dashboard example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – A listing of instance metrics in the Prometheus dashboard](img/B21716_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – A listing of instance metrics in the Prometheus dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**kolla-ansible** ’s default Prometheus configuration does not include a full
    set of labels for each exporter type. The main configuration of Prometheus for
    all exporters can be found in the **kolla-ansible/ansible/roles/prometheus/templates/prometheus.yml.j2**
    file. One way to include a custom relabeling configuration is to edit the latter
    file and amend each exporter section settings. The recommended way is to create
    a new custom configuration file – for example, one named **custom.yml** under
    the new config directory, such as **/etc/kolla/config/prometheus/prometheus.yml.d/**
    , and apply the new additional settings by running the pipeline. **kolla-ansible**
    will merge the additional configurations in the running Prometheus server, but
    bear in mind that that will require a short restart of the server to load the
    new configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the instance metrics in Grafana would require only building corresponding
    dashboards. Similar to how observability is performed for the host metrics, the
    Node Exporter and Libvirt template dashboards combine instance-specific metrics
    to build custom tabs for instances’ resource usage per compute node. This includes
    an overview of the total resource consumption, such as the CPU, RAM, and allocated
    volumes. At the time of writing this edition, importing Grafana dashboards for
    virtual machine metrics using the Prometheus discovery service is not available.
  prefs: []
  type: TYPE_NORMAL
- en: A Grafana dashboard for instance metrics from [grafana.com](http://grafana.com)
    can be found at [https://grafana.com/grafana/dashboards/15330-openstack-instance-metrics/](https://grafana.com/grafana/dashboards/15330-openstack-instance-metrics/)
    with the dashboard ID **15330** . The dashboard cannot be used without a fully
    running **telemetry OpenStack** service (composed of **Ceilometer** and **Gnocchi**
    ), which will be covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry and charging back
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenStack provides telemetry for tenant environments with the Ceilometer service.
    The Ceilometer project aims to support cloud operators to collect metrics, further
    assisting and understanding resource utilization. This enables us to address infrastructure
    scalability issues and update resource planning for expansion and growth before
    hitting the resource limits. Since the Queens OpenStack release, the telemetry
    module has been extended with additional components that work in tandem with Ceilometer,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Aodh** : An alarming service that acts upon collected metrics data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gnocchi** : A data store and collection of time series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Panko** : A data store for events data generated by Ceilometer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this edition, the **Panko** module has been deprecated
    and is no longer maintained by the OpenStack community.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will highlight the telemetry stack in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The Ceilometer anatomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Ceilometer project originally aimed (until the Folsom release) to collect
    resources’ usage metrics from cloud tenants and transform them into billable items
    for invoicing purposes. The continuous development of the Ceilometer project has
    enabled additional monitoring and alarming capabilities. They have been split
    into their own modules since the Liberty release.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the following diagram, Ceilometer relies on agents to collect, process,
    store, and retrieve data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – A general overview of the Ceilometer architecture](img/B21716_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – A general overview of the Ceilometer architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Each data workflow is actioned by a dedicated agent that can be listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Polling agents** : These regularly poll each OpenStack infrastructure service
    to formulate measurements via API calls. A compute polling agent gathers statistics
    from instances running in a compute node and polls them to the message queue.
    The central agent runs in a central management server in OpenStack, such as a
    cloud controller. It polls statistics of resources other than instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notification agents** : These listen periodically on the message queue bus,
    collect new notification messages set by different OpenStack services, and translate
    them into metrics before pushing them back to the appropriate message queue bus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collector agents** : These monitor the message queue, gather samples, and
    collect the metering messages generated by either polling or notification agents.
    Therefore, the new metering messages will be recorded in a backend storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API service** : This exposes a standard API that provides access to the internal
    Ceilometer database, if enabled, to query metering data against it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gathering data is mainly performed by agents using a pipeline mechanism. Internally,
    they periodically send requests for sample objects that reflect a certain meter.
    Every sample request will be forwarded to the pipeline. Once passed to the pipeline,
    meters can be manipulated by several transformer types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accumulator** : This accumulates multi-values and sends them in a batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregator** : This aggregates multi-values into one arithmetic. This includes
    arithmetic functions to compute the percentage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rate of change** : This identifies trends by deriving another meter from
    the previous data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit conversion** : This gives the type of unit conversion to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once manipulated and transformed, a meter might follow its path via one of
    the multiple publisher types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Notifier** : This is the meter data pushed over a reliable messaging queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rpc** : This is the synchronous RPC meter data publisher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**udp** : This is the meter data sent over the UDP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**file** : This is the meter data sent into a file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The aforementioned components are illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Metrics data transformation and publishing through metric pipelines](img/B21716_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Metrics data transformation and publishing through metric pipelines
  prefs: []
  type: TYPE_NORMAL
- en: Data collected can be stored in different types of databases supported by Ceilometer.
    Originally, MongoDB was a widely adopted data store for the telemetry metric storage.
    Due to MongoDB’s scalability and performance issues, Gnocchi has been evaluated
    as a well-suited time series database for Ceilometer, which will be covered in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The Gnocchi data store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the Train OpenStack release, **Gnocchi** has been considered the default
    and official supported storage service for Ceilometer. Given the performance and
    scalability issues of former metrics data store options, the introduction of Gnocchi
    as a time series database has solved main bottlenecks issues when processing and
    storing metrics data in large-scale environments. Data samples are no longer written
    directly into a database. Rather, they are converted into Gnocchi elements and
    posted afterward on its native API. Aggregated data is recorded in a time series.
    Each converted sample presents a data point that has a timestamp and measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hallmark of data optimization using Gnocchi is indexing resources and their
    associated attributes. As a result, data searches are quicker. As mentioned previously,
    Gnocchi offers a scalable metric storage design by utilizing metric data storage
    in a scalable storage system, such as **Swift** and **Ceph** . As shown in the
    next figure, Gnocchi can be configured to aggregate and store measures in Swift
    or Ceph as a storage backend, using its built-in storage drivers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – The Gnocchi data store multi-backend support](img/B21716_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – The Gnocchi data store multi-backend support
  prefs: []
  type: TYPE_NORMAL
- en: Data is processed by **metricd** for aggregation and storage. **metricd** handles
    flushing metrics marked for deletion. Gnocchi provides a compatible **statsd**
    protocol ( **gnocchi-statsd** ) that listens to the incoming metrics. **statsd**
    will be used later for Gnocchi deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting with Aodh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the Liberty release, the Ceilometer alarm module has been forked and renamed
    to **Aodh** , which takes the lead in triggering alarms based on custom rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, a significant difference can be noticed
    when comparing the current alarm service to Ceilometer’s preceding one – the ability
    to scale horizontally when hitting more load. Using the same messaging queue,
    Aodh exposes an event listener that catches new notifications and provides an
    instant response time, with zero latency. Aodh listeners rely on predefined alarms
    that will trigger instantly based on events and configured measures. In this case,
    reacting against auto-scaling conditions when using an orchestration service (e.g.,
    **Heat** ) becomes very useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – An Aodh alarm overview in OpenStack](img/B21716_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – An Aodh alarm overview in OpenStack
  prefs: []
  type: TYPE_NORMAL
- en: Aodh is constructed mainly by an API server ( **aodh-api** ) that provides access
    to the data store. The Aodh alarm evaluator ( **aodh-evaluator** ) triggers alarms,
    based on the collected statistical trends crossing a threshold within a certain
    period. Alarms are triggered by an Aodh notification listener ( **aodh-listener**
    ), based on rules against events reported by the notification agents. The Aodh
    alarm notifier ( **aodh-notifier** ) enables alarm settings based on the threshold
    for a collection of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Aodh notification supports both event- and threshold-based alarms. In the latest
    OpenStack releases, Aodh queries measurements by default from the Gnocchi data
    store.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the telemetry service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The current **kolla-ansible** repository supports a full deployment of the
    telemetry service in OpenStack. This includes the Ceilometer, Aodh, and Gnocchi
    services. As we have already deployed the Ceilometer service in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane – Shared Services* , we will walk through a detailed
    configuration of the Ansible roles for the telemetry stack:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure the Ceilometer service is added to run it as part of the cloud controller
    nodes if it is not enabled by adding it to the **multi_packtpub_prod** inventory
    file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the Ceilometer central polling and notification agents to the cloud controller
    group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the ceilometer polling agent for the compute nodes group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the Gnocchi service as part of the cloud controller nodes group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign the Gnocchi components, including the API, **statsd** (the **gnocchi-statsd**
    daemon), and **metricd** (the **gnocchi-metricd** daemon) services, to run as
    part of the Gnocchi instance in the cloud controller nodes group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the Aodh alarm service as part of the cloud controller nodes group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign the Aodh components, including the API, evaluator, listener, and **notifier**
    services, to run as part of the Aodh instance in the cloud controller nodes group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, make sure the Ceilometer service is enabled in the **globals.yml** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enable the Gnocchi service in the **globals.yml** file and, optionally, the
    **statsd** protocol support that will install and run the **gnocchi-statd** daemon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last configuration in the **globals.yml** file is to enable the Aodh alerting
    service by setting its corresponding variable to **yes** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the deployment pipeline and make sure that no errors have been reported
    when installing the telemetry service. Verify that the telemetry service containers
    are up and running in one of the cloud controller nodes using the following command
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the output we get:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.19 – A listing of the deployed telemetry Kolla containers](img/B21716_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – A listing of the deployed telemetry Kolla containers
  prefs: []
  type: TYPE_NORMAL
- en: Once the trio of Ceilometer, Gnocchi, and Aodh are joined to the OpenStack deployment,
    you should have a fully functional telemetry service. Navigating through Ceilometer
    metrics is straightforward and can be used to configure alarms via Aodh. Metrics
    and alarms are essential elements to tackle daily monitoring operations. However,
    even these would not be sufficient without the addition of service logs, which
    we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Grasping centralized logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second essential part of operating an OpenStack cloud environment is to
    embrace the inspection of events in the infrastructure through logging information.
    With dozens of deployed services involved in the OpenStack setup, cloud operators
    should enable logging in each service, running at least within the control plane
    for troubleshooting, analysis, and even creating custom metrics for alerting.
    Depending on the OpenStack deployment tool and configuration settings used, the
    standard services in Linux/Unix systems write their logs in the **/var/log/**
    directory and subdirectories.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the tons of log data generated, parsing and processing it all presents
    a hindrance to extracting meaningful information or troubleshooting unexpected
    issues, which take a longer time to solve. To overcome such a challenge, your
    log environment must evolve to become centralized. A good option is to start flowing
    logs in a dedicated **rsyslog** server. You might put in so much data that your
    log server starts starving for larger storage capacity. Furthermore, archiving
    the aforementioned data will not be useful when you need to extract information
    for a particular context. Additionally, correlating log data that has different
    formats (taking into consideration the RabbitMQ and MySQL logs) with generated
    events might even be impossible. So, what we need at this point is a set of quality
    requirements for a good OpenStack logging experience, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A parsing capability for log metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast indexing of log data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comprehensible presentation of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics constructed from custom log data aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generated logs in OpenStack are structured, making them suitable for processing,
    and can be parsed by several tools, including the open-source **ELK** stack, which
    is composed of **Elasticsearch** , **Logstash** , and **Kibana** , or even by
    commercial third-party tools, such as Datadog and Splunk. ELK is the most adopted
    tool as a logging pipeline in large OpenStack deployments. Most OpenStack deployment
    tools, other than **kolla-ansible** , are supported by an ELK stack installation
    out of the box. Although the ELK stack is still a valid option to manage logs
    in OpenStack, the latest OpenStack releases have shifted gears to adopt a forked
    open-source Elasticsearch solution, and **OpenSearch** was announced as the new
    way to handle logging in the next OpenStack operations.
  prefs: []
  type: TYPE_NORMAL
- en: OpenSearch under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenSearch is an open-source software offered under the Apache version 2.0
    license. It has become a very popular search engine and log analytics suite, due
    to its large community and active commits, and originated from the Elasticsearch
    project. The OpenSearch suite includes a search engine, data store, and dashboards
    for visualizations, which originated from Kibana. The OpenSearch architecture
    is based on a distributed design, where different components can run in different
    cluster nodes. The following diagram shows an overview of a logging solution based
    on OpenSearch for OpenStack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – An overview of the OpenSearch logging cluster in an OpenStack
    environment](img/B21716_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – An overview of the OpenSearch logging cluster in an OpenStack
    environment
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the previous diagram, a typical OpenSearch environment for mid
    to large deployments consists of a variety of cluster node roles, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The cluster manager node** : This keeps track of the OpenSearch formation
    cluster state, including node state changes (i.e., joiners and leavers) and health,
    indexing management, and shard allocation. For high availability reasoning, it
    is recommended to run at least two manager nodes for production use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The coordinator node** : This receives requests initiated from a dashboard
    or client libraries and delegates them to the right shards on the data nodes,
    and then fetches, aggregates, and returns the data results to the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The data source node** : This represents the worker horses of the OpenSearch
    cluster that store the data and perform different data-related operations, including
    indexing, aggregation, and searching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Other types, including the **master-eligible** and **ingest** nodes, can form
    part of an OpenSearch formation cluster. Any node that is not marked as a master
    is designated with a master-eligible role. Ingest nodes are recommended for heavy
    data ingestion pipelines and to offload indexing workloads from data nodes.
  prefs: []
  type: TYPE_NORMAL
- en: OpenSearch is highly extensible, and several types of plugins can be installed
    for an enhanced search experience, security features, machine learning, and performance
    analysis. The available plugins are listed at [https://opensearch.org/docs/latest/install-and-configure/plugins/#available-plugins](https://opensearch.org/docs/latest/install-and-configure/plugins/#available-plugins)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The OpenSearch solution comes with a distributed architecture, enabling you
    to scale for large search and analytics operations. Niche features are supported
    by the OpenSearch core search service, making it easy to run log analytics, data
    ingestion, and observability capabilities. Additionally, performance has become
    a valid reason to adopt OpenSearch, due to additional ways to handle shards across
    multiple nodes that speed up the search operation. The amount of data generated
    by different OpenStack and common infrastructure services can be immense and can
    be fed into an OpenSearch cluster formation. By providing a minimum size of an
    OpenSearch cluster, cloud operators can start exploring a rich search and visualization
    experience. This enables you to create visualizations to spot the malfunctioning
    parts of complex and large systems that are hard to detect with basic metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying OpenSearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the time of writing this edition, **kolla-ansible** supports OpenSearch
    installation. The community recommends the adoption of the OpenSearch option,
    which can be deployed in a few steps. It is highly recommended to dedicate a logging
    cluster that runs different OpenSearch node types, as covered in the previous
    section. Other considerations should be taken for each OpenSearch node requirement
    so that you can tailor your logging cluster, based on the required performance.
    For example, data nodes would require more disk space and could be upgraded with
    fast disks with higher IOPS, such as **solid-state drives** ( **SSDs** ). Master
    and coordinating nodes are more demanding computers that can be deployed with
    higher CPU power. For the next setup, we will run a simple instance of the OpenSearch
    service as part of the control node:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assign the OpenSearch engine and **dashboards** roles to the control node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the following variable in the **globals.yml** file to install OpenSearch
    as a central logging service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By default, Kolla deploys the OpenSearch service to listen on port **9200**
    . When deployed in the controller nodes, OpenSearch Dashboards should be accessible
    via the reserved VIP assigned to Keepalived to run the cloud controller clusters.
    Optionally, you can assign an FQDN and populate the hostname internally across
    all OpenStack nodes. The dashboards can be accessed with a browser internally
    on port **5601** and the configured **kolla_internal_fqdn** variable set in the
    **globals.yml** file that points to the **kolla_internal_vip_address** variable.
    Make sure that at least the **kolla_internal_vip_address** variable is set to
    the VIP address or mapped DNS endpoint name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'OpenStack dashboards are created with the default **''opensearch''** username,
    and the password can be edited in the **kolla-ansible/etc/kolla/passwords.yml**
    file by setting the following variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Other advanced and useful configurations that can be performed in advance to
    address common OpenSearch operation settings are the log retention policies, which
    are critical when dealing with disk-size management, data rotation, and the life
    cycle. OpenSearch comes with an **Index State Management** plugin to manage data
    rotation through the definition of log retention policies. Make sure that is enabled
    in the Ansible OpenSearch role defined in the **ansible/roles/opensearch/defaults/main.yml**
    file by checking the following variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once enabled, OpenSearch provides an indices data life cycle by defining the
    soft and hard retention periods. The soft retention period is defined by the **opensearch_soft_retention_period_days**
    variable in the **ansible/roles/opensearch/defaults/main.yml** file where indices
    are closed, no longer active, and staged for deletion, but they still occupy space
    on the disk and can be reopened. The hard retention period is the duration after
    which indices are deleted permanently and no longer occupy a space on the disk.
    They are defined by the **opensearch_hard_retention_period_days** variable in
    the **ansible/roles/opensearch/defaults/main.yml** file. The default soft and
    hard duration periods are set to **30** and **60** , respectively. That can be
    edited either by creating new variables in the **globals.yml** file and referencing
    them in the OpenSearch playbook, or directly in the **ansible/roles/opensearch/defaults/main.yml**
    file by changing the soft duration to **45** and the hard one to **90** days:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the pipeline and ensure that the output does not contain errors in the
    job configuration. After completion, run the following command line to validate
    the additional Kolla containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Listing of OpenSearch Kolla containers](img/B21716_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – Listing of OpenSearch Kolla containers
  prefs: []
  type: TYPE_NORMAL
- en: 'Point to the OpenSearch Dashboards URL from a browser at **http://10.0.0.47:5601**
    . Enter the **''opensearch''** username and configured password to start a log
    search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.22 – The OpenSearch login page](img/B21716_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – The OpenSearch login page
  prefs: []
  type: TYPE_NORMAL
- en: 'To start searching logs after the first login, an index pattern must be created
    to pull data from the logs’ sources. The index pattern points to a specific index
    that can pull log data from a specific date, for example. From the **OpenSearch
    Dashboards** panel, click on the **Index Patterns** button, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Creating index patterns in the OpenSearch dashboard](img/B21716_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 – Creating index patterns in the OpenSearch dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, specify the index pattern settings by applying a filter to pull and aggregate
    data. In the current example, the **timestamp** filter will be applied. By default,
    OpenSearch associates a unique identifier to each index pattern, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Applying a filter for the index pattern](img/B21716_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 – Applying a filter for the index pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'Once created, the index pattern will apply the filter to the source data and
    visualize the field’s mapped core type, as saved by OpenSearch. For example, the
    created index pattern references a set of fields that are visible in the **Discover**
    tab on the upper side of the dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Listing the field selection for the index pattern](img/B21716_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 – Listing the field selection for the index pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'Logs can be filtered using the search bar in the **Discover** panel, which
    lists all the associated events ending with a time frame, as shown in the following
    figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.26 – Applying time frames for logs filtering](img/B21716_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.26 – Applying time frames for logs filtering
  prefs: []
  type: TYPE_NORMAL
- en: 'Further filtering can be applied using the search filters bar. In the following
    example, an added filter is applied to pull all data with an **ERROR** value in
    the **log_level** field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.27 – Applying a log_level filter for log filtering](img/B21716_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.27 – Applying a log_level filter for log filtering
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly monitor the trend of the pulled data with a specific filter, it
    is important to use the power of OpenSearch’s visualization capability. With a
    rich variety of supported dashboard objects, operators can build different types
    of visualizations for each search-filtered query. To create a visualization, point
    to the **Visualize** tab and click on the **Create Visualization** button. From
    the returned list, the **Vertical Bar Chart** visualization type is selected to
    expose the collected error occurrences in the last hour from the last saved query
    search. Once created, a new graph is generated that can be broken up into more
    specific fields, referred to as **Buckets** . Data graph aggregation can be performed
    by adding, for example, a bucket in the created graph settings and selecting the
    **X-axis** option. As shown in the next example, the data aggregation is fixed
    with the **Date Histogram** type, with the default **@** **timestamp** field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28 – Data aggregation using OpenSearch buckets](img/B21716_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.28 – Data aggregation using OpenSearch buckets
  prefs: []
  type: TYPE_NORMAL
- en: 'Once updated, a generic overview of the error rate across all pulled log data
    can be visualized in a single pane of glass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.29 – The log visualization dashboard based on the applied filters](img/B21716_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.29 – The log visualization dashboard based on the applied filters
  prefs: []
  type: TYPE_NORMAL
- en: OpenSearch dashboards are highly customizable so that more data aggregation
    can be performed for more granular data presentation. For example, the previous
    graph representation can be saved in a dedicated service state dashboard, and
    then you can add a new visualization from the same graph by exposing the errors
    for each service.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered two essential practices for operating an OpenStack environment,
    exploring the possible ways to monitor and log. Both deployed monitoring and logging
    stacks can vary from one setup to another, depending on the operator’s experience
    and their familiarity with a specific tool, or some other third-party solutions.
    One of the best practices is to always monitor what is happening under and on
    top of the cloud platform, providing the necessary ways to capture metrics and
    collect data before it reaches a higher severity level and you have to start dealing
    with incidents. Prometheus, as demonstrated throughout this chapter, provides
    a multitude of configurations, and with the simple integration of exporters, cloud
    operators can easily ship metrics in an automated fashion. A single pane, Grafana,
    centralizes all the metrics in comprehensive dashboards, and operators can closely
    observe different system elements’ trends. The OpenStack telemetry service, commonly
    referred to as the Ceilometer, Aodh, and Gnocchi trio, was covered and deployed
    to start charging and tracking resource usage for tenants. At the end of the chapter,
    the logging pipeline challenge was explored, as well as a widely adopted solution,
    OpenSearch, that enables the consolidation of the immense amount of generated
    logs in each corner of OpenStack, where you can explore suspicious events to remediate
    them proactively. Logs provide valuable information if combined in a robust pipeline
    for analysis, and cloud operators can identify anomalies that are hard to detect
    with simple metrics. Early detection of signs of failure can help to evaluate
    cloud infrastructure performance and provide feedback, enhancing an environment’s
    services to keep up with the promised SLA.
  prefs: []
  type: TYPE_NORMAL
- en: Besides setting a logging pipeline, the next chapter will cover how to conduct
    benchmarking to gain more insights into the OpenStack infrastructure and evaluating
    the private cloud capacity.
  prefs: []
  type: TYPE_NORMAL
