- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Monitoring and Logging – Remediating Proactively
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控与日志记录 – 主动修复
- en: “To accept something on mere presumption and, likewise, to fail to investigate
    it may cover over, blind, and lead astray.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “仅仅接受某事物的假设，或者同样地，未加调查地忽视它，可能会掩盖事实、让人盲目，并误导他人。”
- en: – Abu Nasr Al Farabi
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 阿布·纳萨尔·法拉比
- en: Monitoring the underlying infrastructure of the private cloud is an essential
    operational activity that requires consistent tools and automated ways to achieve
    real-time metrics collection and alerting mechanisms. Among the different OpenStack
    releases, a variety of open-source and commercial tools exist to support the monitoring
    of different pieces of the OpenStack ecosystem. In the latest releases, we can
    find more emerging tools with successful monitoring stories, offering more simplicity
    and easier integration into the OpenStack ecosystem. Collecting metrics data is
    one part of OpenStack’s operational tasks; the other part is the log data. Each
    OpenStack and shared service generates a load of log data with different logging
    levels, which, combined, would become a complicated task to manually parse and
    find anomalies when issues occur. Having a robust logging pipeline is a requirement
    to satisfy one pillar of the OpenStack operational journey. This chapter will
    combine both monitoring and logging practices with recent emerging tools that
    can be integrated into a running OpenStack environment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 监控私有云的底层基础设施是一个至关重要的操作活动，要求使用一致的工具和自动化方式来实现实时的指标收集和告警机制。在不同的 OpenStack 版本中，存在多种开源和商业工具来支持监控
    OpenStack 生态系统中的不同组件。在最新版本中，我们可以找到更多成功的监控工具，它们提供了更简便的操作方式，且更容易集成到 OpenStack 生态系统中。收集指标数据是
    OpenStack 操作任务的一部分；另外一部分是日志数据。每个 OpenStack 和共享服务都会生成大量具有不同日志级别的日志数据，所有这些数据加起来会变得非常复杂，在发生问题时，手动解析和查找异常会变得非常困难。拥有一个强大的日志管道是满足
    OpenStack 操作旅程其中一项需求的关键。本章将结合监控和日志记录的实践，并介绍能够集成到运行中的 OpenStack 环境中的最新工具。
- en: 'We will discuss the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Introducing Prometheus as a solution for the OpenStack infrastructure and workload
    monitoring
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入 Prometheus 作为 OpenStack 基础设施和工作负载监控的解决方案
- en: Deploying and integrating Prometheus in the current OpenStack deployment and
    starting to use a separate monitoring stack to expose OpenStack metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前的 OpenStack 部署中部署和集成 Prometheus，并开始使用单独的监控堆栈来暴露 OpenStack 的指标
- en: Exploring, deploying, and building dashboards using Grafana as a central observability
    tool for data representation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Grafana 作为中央可观测性工具来构建、部署并探索仪表板，以便进行数据展示
- en: Building an efficient and centralized logging solution using OpenSearch to ship,
    parse, and build custom search queries for collected logs, enabling further analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 OpenSearch 构建高效且集中的日志解决方案，通过该解决方案传输、解析并构建自定义搜索查询，以便收集的日志进行进一步分析
- en: OpenStack infrastructure monitoring
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 基础设施监控
- en: 'Beyond the workloads running in OpenStack tenants, cloud operators must ensure
    that they have the necessary metrics to ensure OpenStack services’ availability,
    performance, and usage status. Monitoring data should involve mainly two different
    categories of the OpenStack infrastructure services, as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了运行在 OpenStack 租户中的工作负载外，云操作员还必须确保他们具备必要的指标来确保 OpenStack 服务的可用性、性能和使用状态。监控数据主要涉及
    OpenStack 基础设施服务的两大类，如下所示：
- en: '**Cloud software** : This includes OpenStack APIs, schedulers, databases, message
    busses, and load-balancing services.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云软件**：包括 OpenStack API、调度程序、数据库、消息总线和负载均衡服务。'
- en: '**Under cloud** : This includes running operating systems, storage, and network
    hardware.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云下**：包括运行的操作系统、存储和网络硬件。'
- en: In the following section, we will explore Prometheus, a well-integrated and
    adopted monitoring and alerting toolkit for large-scale deployment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨 Prometheus，这是一款已广泛集成和采用的监控与告警工具，适用于大规模部署。
- en: Prometheus in a nutshell
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prometheus 简介
- en: '**Prometheus** allows you to collect and store metrics as time-series data.
    Prometheus is considered a great fit for large deployment, as it provides exporters
    with components that ship various amounts of metrics to a Prometheus server. Additionally,
    the tool includes an alerting mechanism to manage alerts and notifications based
    on configured thresholds. Unlike other tools, such as Zabbix or Nagios, Prometheus
    offers flexible ways to feed metrics and data collected in its format. This is
    performed by third-party exporters that provide Prometheus-compatible metrics
    with less effort to develop separate converters for each system of the software
    framework. Some of the official exporters are maintained in Prometheus’s GitHub
    repository and can be found at [https://github.com/prometheus](https://github.com/prometheus)
    . In the OpenStack world, a vast number of metrics can be exported to Prometheus
    via the OpenStack Prometheus exporter, with a full list at [https://github.com/openstack-exporter/openstack-exporter](https://github.com/openstack-exporter/openstack-exporter)
    . As shown in the following diagram, Prometheus also has a discovery service that
    can be used by the OpenStack APIs to explore and generate data metrics for the
    Prometheus server. This is mainly useful for reporting metrics for virtual machines
    – for example, when utilizing the service discovery on the Nova API service. As
    soon as metrics are collected and stored by Prometheus, a single pane of glass
    should exist to consolidate and organize all data in comprehensible dashboards.
    Grafana is a great open-source tool for metrics visualization that can be configured
    to use Prometheus as its data source.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**Prometheus**允许你收集并存储作为时间序列数据的度量值。Prometheus被认为非常适合大规模部署，因为它提供了导出器和组件，将各种度量数据发送到Prometheus服务器。此外，该工具包括一个警报机制，用于根据配置的阈值管理警报和通知。与Zabbix或Nagios等其他工具不同，Prometheus提供了灵活的方式来传输其格式收集的度量值和数据。这是通过第三方导出器来完成的，第三方导出器提供了与Prometheus兼容的度量值，减少了为每个软件框架的系统开发单独转换器的工作量。一些官方导出器托管在Prometheus的GitHub仓库中，可以在[https://github.com/prometheus](https://github.com/prometheus)找到。在OpenStack世界中，可以通过OpenStack
    Prometheus导出器将大量度量数据导出到Prometheus，完整列表请见[https://github.com/openstack-exporter/openstack-exporter](https://github.com/openstack-exporter/openstack-exporter)。如下面的图示所示，Prometheus还具有一个发现服务，OpenStack
    API可以使用该服务来探索和生成Prometheus服务器的数据度量。这主要用于报告虚拟机的度量数据——例如，在Nova API服务中利用服务发现时。当Prometheus收集并存储度量数据后，应该有一个统一的视图来整合和组织所有数据，并展示在易于理解的仪表板上。Grafana是一个出色的开源工具，用于度量数据的可视化，可以配置为使用Prometheus作为数据源。'
- en: '![Figure 8.1 – A Prometheus integration and monitoring stack in OpenStack](img/B21716_08_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – OpenStack中的Prometheus集成和监控堆栈](img/B21716_08_01.jpg)'
- en: Figure 8.1 – A Prometheus integration and monitoring stack in OpenStack
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – OpenStack中的Prometheus集成和监控堆栈
- en: In the following section, Prometheus will be deployed and integrated as the
    main monitoring tool in the OpenStack environment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，Prometheus将作为主要监控工具在OpenStack环境中进行部署和集成。
- en: Deploying infrastructure monitoring
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署基础设施监控
- en: 'Adopting the Prometheus and Grafana pair as a central monitoring and observability
    solution for the OpenStack infrastructure is a quick win for cloud operators,
    due to the well-tested and integrated monitoring solution in the latest OpenStack
    releases. Kolla Ansible promotes and supports Prometheus and Grafana integration
    in the latest code releases. An additional monitoring host can be dedicated to
    run the Prometheus server and, optionally, Grafana. This can be achieved through
    the following configuration in the infrastructure as code settings:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将Prometheus和Grafana组合作为OpenStack基础设施的中央监控和可观察性解决方案，对于云操作员来说是一个快速的胜利，因为在最新的OpenStack版本中，这种监控解决方案经过了良好的测试和集成。Kolla
    Ansible在最新的代码发布中推动并支持Prometheus和Grafana的集成。可以专门设置一个额外的监控主机来运行Prometheus服务器，并可选地运行Grafana。这可以通过以下配置在基础设施代码设置中实现：
- en: 'Update the **multi_packtpub_prod** inventory file by adding the new monitoring
    host, as follows:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加新的监控主机来更新**multi_packtpub_prod**清单文件，如下所示：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add the Prometheus service to the monitoring host group:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Prometheus服务添加到监控主机组：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Add the built-in Prometheus alert manager service to the monitoring host group:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将内置的Prometheus警报管理服务添加到监控主机组：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Include the OpenStack Prometheus exporter in the monitoring host group. This
    will create a dedicated service container to automatically pull formatted Prometheus
    data from the OpenStack services’ metrics:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将OpenStack Prometheus导出器包括在监控主机组中。这将创建一个专用的服务容器，自动从OpenStack服务的指标中拉取格式化的Prometheus数据：
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add the Prometheus Node Exporter role to run in each host. The Node Exporter
    provides a wide range of Linux operating system and hardware metrics:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Prometheus Node Exporter角色添加到每个主机中运行。Node Exporter提供了广泛的Linux操作系统和硬件指标：
- en: '[PRE4]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Prometheus provides a dedicated exporter for the **MariaDB database** . The
    MySQL server metrics exposed through the exporter provide an extended list of
    data metrics available for scraping. The Prometheus MySQL exporter can be added
    to the **mariadb** role running in the cloud controller nodes:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prometheus提供了一个专门的导出器用于**MariaDB数据库**。通过导出器暴露的MySQL服务器指标提供了一个可供抓取的扩展数据指标列表。Prometheus
    MySQL导出器可以添加到运行在云控制节点中的**mariadb**角色：
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Similarly, Prometheus can be configured to scrape the load balancing metrics
    and stats running HAProxy via its dedicated exporter running in the load balancer
    nodes:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，Prometheus可以配置为通过其在负载均衡器节点中运行的专用导出器抓取HAProxy的负载均衡指标和统计信息：
- en: '[PRE6]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Another available exporter that exposes both host and instance metrics is the
    **Prometheus Libvirt exporter** . This exporter collects data from the **Libvirt
    API** and exposes useful metrics for compute nodes and running instances. The
    **libvirt** exporter can be installed in the compute nodes as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个可用的导出器是**Prometheus Libvirt导出器**，它暴露了主机和实例的指标。该导出器从**Libvirt API**收集数据，并暴露计算节点和运行中的实例的有用指标。**libvirt**导出器可以在计算节点中安装，如下所示：
- en: '[PRE7]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For a broader monitoring setup across the OpenStack infrastructure, consider
    keeping an eye on the status of the containers running in each node. The Prometheus
    **cAdvisor** (short for **Container Advisor** ) exporter is designed to collect,
    analyze, aggregate, and export container performance and metrics data. The **cAdvisor**
    exporter will run across all OpenStack nodes as follows:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于OpenStack基础设施的更广泛监控设置，建议关注每个节点上运行的容器状态。Prometheus **cAdvisor**（即**Container
    Advisor**）导出器旨在收集、分析、汇总和导出容器性能和指标数据。**cAdvisor**导出器将跨所有OpenStack节点运行，如下所示：
- en: '[PRE8]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The next step is to enable and install the Prometheus service using **kolla-ansible**
    . At the time of writing this edition, **kolla-ansible** will install the latest
    version of the Prometheus server after 2.0 by adding the following variable to
    the **globals.yml** file:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用**kolla-ansible**启用并安装Prometheus服务。在撰写本版本时，**kolla-ansible**将通过将以下变量添加到**globals.yml**文件中，安装Prometheus服务器的2.0之后的最新版本：
- en: '[PRE9]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For data exposure and visualization, enable Grafana in the **globals.yml**
    file:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启用数据暴露和可视化，请在**globals.yml**文件中启用Grafana：
- en: '[PRE10]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**kolla-ansible** allows custom settings for the monitoring layer for both
    the Prometheus and Grafana services. Once both services are installed, you will
    need to provide a username and password for access. By default, **kolla-ansible**
    installs Prometheus using basic authentication via HTTP, with an **admin** user
    and a Grafana user, **grafana** . Both default variables are assigned to the **/kolla-ansible/ansible/group_vars/all.yml**
    file as follows:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**kolla-ansible**允许为Prometheus和Grafana服务的监控层设置自定义配置。一旦安装了这两个服务，您将需要提供一个用户名和密码以进行访问。默认情况下，**kolla-ansible**通过HTTP使用基本认证安装Prometheus，并使用**admin**用户和Grafana用户**grafana**。这两个默认变量分配到**/kolla-ansible/ansible/group_vars/all.yml**文件中，如下所示：'
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For custom user access settings, you can override the default usernames by
    adjusting the variables directly from the **all.yml** file or in the **prometheus_basic_auth_users**
    configuration section in the **/kolla-ansible/ansible/roles/prometheus/defaults/main.yml**
    file, as follows:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于自定义用户访问设置，您可以通过直接调整**all.yml**文件中的变量或在**/kolla-ansible/ansible/roles/prometheus/defaults/main.yml**文件中的**prometheus_basic_auth_users**配置部分覆盖默认的用户名，如下所示：
- en: '[PRE12]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'User passwords for both Prometheus and Grafana can be reconfigured in the **/etc/kolla/passwords.yml**
    file, where all service secrets are defined. Both passwords are defined with the
    following variables:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Prometheus和Grafana的用户密码可以在**/etc/kolla/passwords.yml**文件中重新配置，该文件定义了所有服务的密钥。两个密码都使用以下变量定义：
- en: '[PRE13]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The other option is to create new users using either custom passwords or the
    default ones, without overriding the existing ones, by extending the **prometheus_basic_auth_users_extra**
    variable in the **/kolla-ansible/ansible/roles/prometheus/defaults/main.yml**
    file, as follows:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个选项是通过扩展**prometheus_basic_auth_users_extra**变量，在**/kolla-ansible/ansible/roles/prometheus/defaults/main.yml**文件中，使用自定义密码或默认密码创建新用户，而不覆盖现有用户，如下所示：
- en: '[PRE14]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Another great metrics-scraping way is to configure a custom Prometheus run
    by listing a set of startup parameters in the **prometheus_cmdline_extras** variable,
    in the **globals.yml** file. For example, Prometheus can be configured to retain
    metrics data up to a specific size, via the **--storage.tsdb.retention.size**
    parameter. In the following listing, a customized configuration is defined to
    limit the maximum number of simultaneous connections on the Prometheus dashboard
    to **30** connections, adjust the log severity level to include **error** (the
    default severity level is **info** ), increase the frequency of resending an alert
    to the Prometheus alert manager every **30** seconds, and retain metrics data
    for **30** days in storage:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一种很好的度量抓取方式是通过在**globals.yml**文件中的**prometheus_cmdline_extras**变量中列出一组启动参数来配置自定义Prometheus运行。例如，可以配置Prometheus通过**--storage.tsdb.retention.size**参数将度量数据保持到特定的大小。在以下列出的配置中，定义了一个自定义配置，用于将Prometheus仪表板上最大同时连接数限制为**30**个连接，调整日志严重性级别以包含**error**（默认严重性级别为**info**），每**30**秒将警报重新发送到Prometheus警报管理器，并将度量数据在存储中保留**30**天：
- en: '[PRE15]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Important note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A full list of the Prometheus startup parameters can be found at [https://prometheus.io/docs/prometheus/latest/command-line/prometheus/](https://prometheus.io/docs/prometheus/latest/command-line/prometheus/)
    .
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[https://prometheus.io/docs/prometheus/latest/command-line/prometheus/](https://prometheus.io/docs/prometheus/latest/command-line/prometheus/)
    找到Prometheus启动参数的完整列表。
- en: 'Run a pipeline to deploy the Prometheus server, Grafana, Prometheus cAdvisor,
    and relevant exporter components. Once the deployment pipeline is completed, verify
    the newly deployed containers by running the following command line in one of
    the controller nodes:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行管道来部署Prometheus服务器、Grafana、Prometheus cAdvisor和相关的导出器组件。一旦部署管道完成，通过在其中一个控制节点运行以下命令行来验证新部署的容器：
- en: '[PRE16]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We get the following output:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们获得了如下输出：
- en: '![Figure 8.2 – Listing the Prometheus and Grafana Kolla containers](img/B21716_08_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 列出Prometheus和Grafana的Kolla容器](img/B21716_08_02.jpg)'
- en: Figure 8.2 – Listing the Prometheus and Grafana Kolla containers
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 列出Prometheus和Grafana的Kolla容器
- en: In the following section, we will check out the Prometheus-collected metrics
    and build insightful dashboards using Grafana.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将查看Prometheus收集的度量标准，并使用Grafana构建有洞察力的仪表板。
- en: Exploring the monitoring data
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索监控数据
- en: 'The latest **kolla-ansible** deployment enables the Prometheus server to start
    collecting different metrics available for scraping that can be checked on the
    Prometheus dashboard. The main Prometheus dashboard can be accessed by pointing
    to the IP address of the monitoring host: **http://host_monitoring_ip/prometheus**
    , where **host_monitoring_ip** is the IP address of the monitoring host running
    the Prometheus server.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的**kolla-ansible**部署使得Prometheus服务器开始收集可以在Prometheus仪表板上检查的不同度量标准。主Prometheus仪表板可以通过指向监控主机的IP地址访问：**http://host_monitoring_ip/prometheus**，其中**host_monitoring_ip**是运行Prometheus服务器的监控主机的IP地址。
- en: '![Figure 8.3 – The Prometheus dashboard interface](img/B21716_08_03.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – Prometheus仪表板界面](img/B21716_08_03.jpg)'
- en: Figure 8.3 – The Prometheus dashboard interface
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – Prometheus仪表板界面
- en: Prometheus collects an exhaustive list of metrics from each installed exporter
    across all defined hosts in the inventory. It uses **labels** to define each metric’s
    target endpoint. For example, once the OpenStack exporter is installed, Prometheus
    starts collecting OpenStack-related metrics that can be checked by pointing to
    **http://host_monitoring_ip:9090/targets** , where **host_monitoring_ip** is the
    IP address of the monitoring host running the Prometheus server.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus从清单中定义的所有主机的每个已安装导出器收集详尽的度量标准。它使用**labels**来定义每个度量的目标端点。例如，一旦安装了OpenStack导出器，Prometheus就开始收集与OpenStack相关的度量标准，可以通过指向**http://host_monitoring_ip:9090/targets**来检查，**host_monitoring_ip**是运行Prometheus服务器的监控主机的IP地址。
- en: 'Collected metrics will be visible in the Prometheus dashboard as soon as the
    exporter service in each node sends metrics data for scraping. It might take a
    while to visualize all the metrics in the Prometheus dashboard. For example, the
    following listing shows that the initial OpenStack service exported metrics through
    **http://host_monitoring_ip:9100/metrics** , where **host_monitoring_ip** is the
    IP address of the monitoring host running the Prometheus server:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个节点的导出服务发送指标数据进行抓取，收集的指标将在 Prometheus 仪表板中显示。可能需要一些时间才能在 Prometheus 仪表板中可视化所有指标。例如，以下列表显示了初始
    OpenStack 服务通过 **http://host_monitoring_ip:9100/metrics** 导出了指标，其中 **host_monitoring_ip**
    是运行 Prometheus 服务器的监控主机的 IP 地址：
- en: '![Figure 8.4 – A listing of the collected OpenStack service metrics in Prometheus](img/B21716_08_04.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – Prometheus 中收集的 OpenStack 服务指标列表](img/B21716_08_04.jpg)'
- en: Figure 8.4 – A listing of the collected OpenStack service metrics in Prometheus
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – Prometheus 中收集的 OpenStack 服务指标列表
- en: 'The Node Exporter service installed on each OpenStack physical machine to gather
    host metrics, such as memory usage and stats, can be checked through the metrics
    listing at the same Prometheus dashboard URL, as shown in the following example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 安装在每台 OpenStack 物理机器上的 Node Exporter 服务，用于收集主机指标，如内存使用情况和统计信息，可以通过在同一 Prometheus
    仪表板 URL 上查看指标列表来检查，如以下示例所示：
- en: '![Figure 8.5 – A listing of the collected memory metrics and stats in Prometheus](img/B21716_08_05.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – Prometheus 中收集的内存指标和统计信息列表](img/B21716_08_05.jpg)'
- en: Figure 8.5 – A listing of the collected memory metrics and stats in Prometheus
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – Prometheus 中收集的内存指标和统计信息列表
- en: With more nodes joining the OpenStack infrastructure, the number of collected
    metrics could be doubled, which would affect the monitoring host performance and
    disk capacity. For this reason, it is highly encouraged to use more than one Prometheus
    server instance to handle more metrics export and scraping jobs. Prometheus is
    designed to scale well in large deployments, with gazillions of resource types
    and metrics.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 随着更多节点加入 OpenStack 基础设施，收集的指标数量可能会增加一倍，这将影响监控主机的性能和磁盘容量。因此，强烈建议使用多个 Prometheus
    服务器实例来处理更多的指标导出和抓取任务。Prometheus 被设计为在大规模部署中具有良好的可扩展性，能够支持成千上万的资源类型和指标。
- en: 'Browsing hundreds of metrics on the Prometheus dashboard to follow up on hundreds
    of service statuses and related usage information can be cumbersome. Prometheus
    can be very helpful with most essential operational tasks, including the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Prometheus 仪表板上浏览数百个指标，以跟踪数百个服务状态和相关的使用信息可能会很麻烦。Prometheus 在大多数关键操作任务中都能发挥很大作用，包括以下任务：
- en: Metrics data collection configuration
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标数据收集配置
- en: Management and configuration of alerts
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告警的管理与配置
- en: Troubleshooting metrics export issues
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除指标导出问题
- en: Prometheus is a highly configurable tool and provides a native command-line
    interface to customize metrics collection and scraping settings, which are beyond
    the scope of this book. We will use the exported metrics data by Prometheus and
    visualize it in a simple Grafana dashboard.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 是一个高度可配置的工具，提供本地命令行接口，以自定义指标收集和抓取设置，这超出了本书的范围。我们将使用 Prometheus 导出的指标数据，并在简单的
    Grafana 仪表板中可视化它。
- en: The latest **kolla-ansible** run should enable the Grafana service with the
    default dashboard and configure Prometheus as a data source. To validate the Prometheus
    data source, point to **http://host_monitoring_ip:3000** , where **host_monitoring_ip**
    is the IP address of the monitoring host running the Grafana instance. Provide
    the username and password to access the Grafana dashboard configured in the previous
    steps. If you did not customize the Grafana access credentials, use the username
    and password pair as **admin** / **admin** , respectively.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 **kolla-ansible** 运行应启用 Grafana 服务，并使用默认仪表板配置 Prometheus 作为数据源。要验证 Prometheus
    数据源，请访问 **http://host_monitoring_ip:3000**，其中 **host_monitoring_ip** 是运行 Grafana
    实例的监控主机的 IP 地址。提供用户名和密码以访问前面步骤中配置的 Grafana 仪表板。如果你没有自定义 Grafana 访问凭据，请使用默认的用户名和密码组合
    **admin** / **admin**。
- en: 'Once logged in, navigate in the dashboard to the settings icon | **Configuration**
    | **Data sources** , and make sure that the Prometheus data source is added. Verify
    that the data source is functional, and no errors are reported. Click on the **Save
    & test** button, which should display that the data source configuration is functional,
    as shown in the following screenshot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，在仪表盘中导航至设置图标 | **配置** | **数据源**，并确保已添加 Prometheus 数据源。验证数据源是否正常工作，并且没有报告错误。点击
    **保存并测试** 按钮，应该显示数据源配置正常，如下图所示：
- en: '![Figure 8.6 – Adding a Prometheus data source in the Grafana dashboard](img/B21716_08_06.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 在 Grafana 仪表盘中添加 Prometheus 数据源](img/B21716_08_06.jpg)'
- en: Figure 8.6 – Adding a Prometheus data source in the Grafana dashboard
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 在 Grafana 仪表盘中添加 Prometheus 数据源
- en: 'Once the data source is checked, building dashboards in Grafana is straightforward
    and can be done either by creating freestyle dashboards or by importing dashboards
    from the Grafana website catalog. [https://grafana.com/](https://grafana.com/)
    exposes different dashboards for each Prometheus exporter, identified by a dashboard
    ID that can be simply added to the local Grafana server. You can also import the
    dashboard in the JSON format and copy and paste it into the Grafana import section.
    In the next wizard, we will import Grafana dashboards for the OpenStack and Node
    Exporters using the **dashboard import IDs** option. To do so, simply navigate
    to the Grafana home page, then click on the **+** icon and select **Import** :'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据源检查完毕，在 Grafana 中构建仪表盘就变得非常简单，可以通过创建自由格式的仪表盘或从 Grafana 网站目录导入仪表盘来完成。[https://grafana.com/](https://grafana.com/)
    提供了每个 Prometheus 导出器的不同仪表盘，按仪表盘 ID 标识，可以轻松地将其添加到本地 Grafana 服务器中。你也可以导入 JSON 格式的仪表盘，并将其复制并粘贴到
    Grafana 导入部分。在下一个向导中，我们将使用 **仪表盘导入 ID** 选项导入 OpenStack 和 Node Exporters 的 Grafana
    仪表盘。为此，只需导航到 Grafana 首页，然后点击 **+** 图标并选择 **导入**：
- en: '![Figure 8.7 – Adding a dashboard in Grafana](img/B21716_08_07.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 在 Grafana 中添加仪表盘](img/B21716_08_07.jpg)'
- en: Figure 8.7 – Adding a dashboard in Grafana
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 在 Grafana 中添加仪表盘
- en: 'Starting with the OpenStack metrics exporter, navigate to the OpenStack Grafana
    dashboard at [https://grafana.com/grafana/dashboards/9701-openstack-dashboard/](https://grafana.com/grafana/dashboards/9701-openstack-dashboard/)
    and copy the ID of the dashboard, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从 OpenStack 指标导出器开始，导航到 OpenStack Grafana 仪表盘 [https://grafana.com/grafana/dashboards/9701-openstack-dashboard/](https://grafana.com/grafana/dashboards/9701-openstack-dashboard/)，并复制仪表盘的
    ID，如下所示：
- en: '![Figure 8.8 – Obtaining the dashboard ID](img/B21716_08_08.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – 获取仪表盘 ID](img/B21716_08_08.jpg)'
- en: Figure 8.8 – Obtaining the dashboard ID
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – 获取仪表盘 ID
- en: 'Paste the copied dashboard ID into the Grafana dashboard **Import** page. The
    dashboard ID should be **9701** :'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将复制的仪表盘 ID 粘贴到 Grafana 仪表盘 **导入** 页面。仪表盘 ID 应为 **9701**：
- en: '![Figure 8.9 – Importing the dashboard ID into Grafana](img/B21716_08_09.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 将仪表盘 ID 导入到 Grafana 中](img/B21716_08_09.jpg)'
- en: Figure 8.9 – Importing the dashboard ID into Grafana
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 将仪表盘 ID 导入到 Grafana 中
- en: 'The next page of the import wizard will guide you through the settings of the
    imported dashboard. Enter the name of the dashboard of your choice, and make sure
    that the **Prometheus** data source is selected, as shown in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 导入向导的下一页将引导你完成导入仪表盘的设置。输入你选择的仪表盘名称，并确保选择了**Prometheus**数据源，如下图所示：
- en: '![Figure 8.10 – Configuring the dashboard with Prometheus as the data source](img/B21716_08_10.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – 配置仪表盘，使用 Prometheus 作为数据源](img/B21716_08_10.jpg)'
- en: Figure 8.10 – Configuring the dashboard with Prometheus as the data source
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 配置仪表盘，使用 Prometheus 作为数据源
- en: 'Once the import wizard has completed, the imported Grafana dashboard for the
    OpenStack exporter metrics will display a variety of graphs and metrics, such
    as a generic overview of a resource’s usage per OpenStack service. Some highlights
    of the services metrics are as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦导入向导完成，导入的 OpenStack 导出指标的 Grafana 仪表盘将显示各种图表和指标，例如每个 OpenStack 服务资源使用情况的概览。服务指标的一些亮点如下：
- en: The current existing Keystone users, groups, and projects
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前存在的 Keystone 用户、用户组和项目
- en: Neutron resources such as the floating IPs, virtual networks, and security groups
    deployed
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neutron 资源，如部署的浮动 IP、虚拟网络和安全组
- en: The number of running instances by the Nova service
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nova 服务运行的实例数量
- en: The current count of Glance images
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前 Glance 镜像的数量
- en: The volumes and snapshots managed by the Cinder service
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cinder 服务管理的卷和快照
- en: The overall usage of the CPU, RAM, and disk resources
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU、RAM 和磁盘资源的总体使用情况
- en: 'Several dashboards can be displayed, as shown in the following screenshot:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可以显示多个仪表板，如下所示的截图所示：
- en: '![Figure 8.11 – The imported generic dashboard with Prometheus as the data
    source](img/B21716_08_11.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 以 Prometheus 为数据源的通用导入仪表板](img/B21716_08_11.jpg)'
- en: Figure 8.11 – The imported generic dashboard with Prometheus as the data source
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 以 Prometheus 为数据源的通用导入仪表板
- en: The imported dashboard has more insights into each service and its relative
    subcomponents, such as API services and agents.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 导入的仪表板提供了对每个服务及其相关子组件（如 API 服务和代理）的更多洞察。
- en: '![Figure 8.12 – The imported dashboards for each OpenStack API service status](img/B21716_08_12.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – 每个 OpenStack API 服务状态的导入仪表板](img/B21716_08_12.jpg)'
- en: Figure 8.12 – The imported dashboards for each OpenStack API service status
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 每个 OpenStack API 服务状态的导入仪表板
- en: Another useful dashboard should exist to monitor the hosts themselves running
    the cloud environment, via the Prometheus Node Exporter. The Grafana template
    dashboard can be found at [https://grafana.com/grafana/dashboards/1860-node-exporter-full/](https://grafana.com/grafana/dashboards/1860-node-exporter-full/)
    , with the dashboard ID **1860** . Follow the same import instructions performed
    for the OpenStack exporter dashboard to make sure the data source selected is
    Prometheus.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的仪表板应存在，用于监控运行云环境的主机，通过 Prometheus Node Exporter。Grafana 模板仪表板可以在 [https://grafana.com/grafana/dashboards/1860-node-exporter-full/](https://grafana.com/grafana/dashboards/1860-node-exporter-full/)
    上找到，仪表板 ID 为 **1860**。请按照与 OpenStack 导出仪表板相同的导入说明操作，确保选定的数据源为 Prometheus。
- en: 'Once imported, a new Grafana namespace will be created, with Prometheus as
    the data source. The collected data reflects granular and insightful information
    about the operating system and hardware metrics collected from each OpenStack
    host, using the installed Node Exporter service:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 导入后，将创建一个新的 Grafana 命名空间，数据源为 Prometheus。收集的数据反映了来自每个 OpenStack 主机的操作系统和硬件指标的详细和深刻信息，使用已安装的
    Node Exporter 服务：
- en: '![Figure 8.13 – A consolidated dashboard for OpenStack](img/B21716_08_13.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.13 – OpenStack 的整合仪表板](img/B21716_08_13.jpg)'
- en: Figure 8.13 – A consolidated dashboard for OpenStack
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – OpenStack 的整合仪表板
- en: Similarly, you can import ready-to-go Grafana dashboards for each installed
    Prometheus exporter. The database metrics dashboard can be found at [https://grafana.com/grafana/dashboards/14057-mysql/](https://grafana.com/grafana/dashboards/14057-mysql/)
    , and HAProxy is available at [https://grafana.com/grafana/dashboards/2428-haproxy/](https://grafana.com/grafana/dashboards/2428-haproxy/)
    .
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你可以为每个安装的 Prometheus 导出器导入现成的 Grafana 仪表板。数据库指标仪表板可以在 [https://grafana.com/grafana/dashboards/14057-mysql/](https://grafana.com/grafana/dashboards/14057-mysql/)
    上找到，HAProxy 仪表板则可以在 [https://grafana.com/grafana/dashboards/2428-haproxy/](https://grafana.com/grafana/dashboards/2428-haproxy/)
    上找到。
- en: As far as data collected from the hosts running OpenStack services is concerned,
    the Prometheus Node Exporter service supports proactively identifying possible
    system anomalies. The imported dashboard consolidates all hosts’ metrics in one
    single pane of glass that can be filtered by one or a group of hosts, offering
    a more custom visualization experience.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关于从运行 OpenStack 服务的主机收集的数据，Prometheus Node Exporter 服务支持主动识别可能的系统异常。导入的仪表板将所有主机的指标整合在一个界面中，可以通过单个或一组主机进行过滤，提供更自定义的可视化体验。
- en: Next, we will move on to keep track of the workloads running on top of the OpenStack
    private cloud.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将跟踪运行在 OpenStack 私有云上的工作负载。
- en: OpenStack workload monitoring
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 工作负载监控
- en: 'The next remaining part of the operational task of cloud operators is to provide
    monitoring support to the running workloads of the OpenStack tenants. Operators
    should also monitor some generic metrics of the provisioned resources by tenants
    to anticipate quota limitations and gain insightful analysis of the infrastructure
    usage. This can be achieved by using the same toolsets covered in the previous
    section. Workload monitoring focuses more on the resources deployed by each tenant
    – more precisely, the Nova instances. Prometheus is capable of querying the Nova
    API, and along with its labeling configuration settings, you should be able to
    filter and organize a list of thousands of instances into a set of scraped targets.
    In the previous section, the Prometheus Libvirt exporter was deployed in each
    compute node (see *Step 8* of the *Deploying infrastructure monitoring* section).
    Thus, there is no need to reconfigure any part of the infrastructure hosts. Instances
    metrics should be available in the Prometheus server, but identifying any reported
    instances with additional information is optional. Prometheus can be configured
    to discover instances in OpenStack by applying exporter configuration and the
    **relabel_configs** settings, as shown in the following example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 云运营商的下一项操作任务是为 OpenStack 租户的运行工作负载提供监控支持。运营商还应监控租户所提供资源的一些通用指标，以预测配额限制并深入分析基础设施的使用情况。可以使用上一节中提到的相同工具集来实现这一目标。工作负载监控更多关注每个租户所部署的资源——更准确地说，是
    Nova 实例。Prometheus 能够查询 Nova API，并通过其标签配置设置，你应该能够将数千个实例筛选并组织成一组抓取目标。在上一节中，Prometheus
    Libvirt 导出器已经在每个计算节点上部署（见 *部署基础设施监控* 部分的 *步骤 8*）。因此，无需重新配置任何基础设施主机部分。实例指标应可在 Prometheus
    服务器中使用，但是否为任何报告的实例提供附加信息是可选的。Prometheus 可以通过应用导出器配置和 **relabel_configs** 设置来配置发现
    OpenStack 实例，以下是一个示例：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The tag-relabeling configuration helps to pull the metadata of discovered instances
    that matches a regular expression in Prometheus. In the previous example, the
    **__meta_openstack_(.+)** regular expression is applied and added for each metric
    received for all different source labels. This is useful to visualize tenant resources
    with more specific information, such as the instance name, IP address, and instance
    status, as shown in the following Prometheus server endpoint dashboard example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 标签重新标记配置有助于提取与 Prometheus 中的正则表达式匹配的已发现实例的元数据。在前面的示例中，**__meta_openstack_(.+)**
    正则表达式被应用并添加到所有不同源标签接收的每个指标中。这有助于通过更具体的信息（例如实例名称、IP 地址和实例状态）来可视化租户资源，如下所示的 Prometheus
    服务器端点仪表板示例：
- en: '![Figure 8.14 – A listing of instance metrics in the Prometheus dashboard](img/B21716_08_14.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 – Prometheus 仪表板中的实例指标列表](img/B21716_08_14.jpg)'
- en: Figure 8.14 – A listing of instance metrics in the Prometheus dashboard
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – Prometheus 仪表板中的实例指标列表
- en: '**kolla-ansible** ’s default Prometheus configuration does not include a full
    set of labels for each exporter type. The main configuration of Prometheus for
    all exporters can be found in the **kolla-ansible/ansible/roles/prometheus/templates/prometheus.yml.j2**
    file. One way to include a custom relabeling configuration is to edit the latter
    file and amend each exporter section settings. The recommended way is to create
    a new custom configuration file – for example, one named **custom.yml** under
    the new config directory, such as **/etc/kolla/config/prometheus/prometheus.yml.d/**
    , and apply the new additional settings by running the pipeline. **kolla-ansible**
    will merge the additional configurations in the running Prometheus server, but
    bear in mind that that will require a short restart of the server to load the
    new configuration.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible** 的默认 Prometheus 配置不包括每个导出器类型的完整标签集。所有导出器的 Prometheus 主要配置可以在
    **kolla-ansible/ansible/roles/prometheus/templates/prometheus.yml.j2** 文件中找到。包含自定义重新标签配置的一种方法是编辑该文件并修改每个导出器部分的设置。推荐的方式是创建一个新的自定义配置文件，例如在新配置目录下创建一个名为
    **custom.yml** 的文件，如 **/etc/kolla/config/prometheus/prometheus.yml.d/**，并通过运行管道应用新的附加设置。**kolla-ansible**
    会将附加配置合并到正在运行的 Prometheus 服务器中，但请注意，这将需要短暂重启服务器以加载新的配置。'
- en: Visualizing the instance metrics in Grafana would require only building corresponding
    dashboards. Similar to how observability is performed for the host metrics, the
    Node Exporter and Libvirt template dashboards combine instance-specific metrics
    to build custom tabs for instances’ resource usage per compute node. This includes
    an overview of the total resource consumption, such as the CPU, RAM, and allocated
    volumes. At the time of writing this edition, importing Grafana dashboards for
    virtual machine metrics using the Prometheus discovery service is not available.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Grafana 中可视化实例指标只需构建相应的仪表盘。类似于主机指标的可观察性操作，Node Exporter 和 Libvirt 模板仪表盘结合实例特定的指标，为每个计算节点的实例资源使用构建自定义选项卡。这包括总资源消耗概览，如
    CPU、RAM 和分配的磁盘。撰写本版本时，通过 Prometheus 探测服务导入虚拟机指标的 Grafana 仪表盘尚不可用。
- en: A Grafana dashboard for instance metrics from [grafana.com](http://grafana.com)
    can be found at [https://grafana.com/grafana/dashboards/15330-openstack-instance-metrics/](https://grafana.com/grafana/dashboards/15330-openstack-instance-metrics/)
    with the dashboard ID **15330** . The dashboard cannot be used without a fully
    running **telemetry OpenStack** service (composed of **Ceilometer** and **Gnocchi**
    ), which will be covered in the next section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 [https://grafana.com/grafana/dashboards/15330-openstack-instance-metrics/](https://grafana.com/grafana/dashboards/15330-openstack-instance-metrics/)
    找到来自 [grafana.com](http://grafana.com) 的实例指标 Grafana 仪表盘，仪表盘 ID 为 **15330**。在没有完全运行的
    **telemetry OpenStack** 服务（包括 **Ceilometer** 和 **Gnocchi**）的情况下，无法使用此仪表盘，后续章节将详细介绍该服务。
- en: Telemetry and charging back
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遥测与计费
- en: 'OpenStack provides telemetry for tenant environments with the Ceilometer service.
    The Ceilometer project aims to support cloud operators to collect metrics, further
    assisting and understanding resource utilization. This enables us to address infrastructure
    scalability issues and update resource planning for expansion and growth before
    hitting the resource limits. Since the Queens OpenStack release, the telemetry
    module has been extended with additional components that work in tandem with Ceilometer,
    including the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 为租户环境提供遥测服务，Ceilometer 服务为其中的一部分。Ceilometer 项目的目标是帮助云操作员收集指标，进一步协助理解资源的利用情况。这使我们能够在资源达到限制之前，解决基础设施的可扩展性问题，并更新资源规划以支持扩展和增长。从
    Queens OpenStack 版本开始，遥测模块已扩展为包含与 Ceilometer 协同工作的其他组件，包括以下内容：
- en: '**Aodh** : An alarming service that acts upon collected metrics data'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Aodh** ：一个基于收集的指标数据进行处理的告警服务'
- en: '**Gnocchi** : A data store and collection of time series data'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gnocchi** ：时间序列数据的存储和收集'
- en: '**Panko** : A data store for events data generated by Ceilometer'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Panko** ：Ceilometer 生成的事件数据存储'
- en: Important note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At the time of writing this edition, the **Panko** module has been deprecated
    and is no longer maintained by the OpenStack community.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本版本时，**Panko** 模块已被弃用，且不再由 OpenStack 社区维护。
- en: The next section will highlight the telemetry stack in more detail.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将更详细地介绍遥测堆栈。
- en: The Ceilometer anatomy
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ceilometer 架构
- en: The Ceilometer project originally aimed (until the Folsom release) to collect
    resources’ usage metrics from cloud tenants and transform them into billable items
    for invoicing purposes. The continuous development of the Ceilometer project has
    enabled additional monitoring and alarming capabilities. They have been split
    into their own modules since the Liberty release.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Ceilometer 项目最初的目标（直到 Folsom 版本）是从云租户处收集资源使用指标，并将其转化为可开具账单的项目。Ceilometer 项目的持续发展使其具备了更多的监控和告警功能。从
    Liberty 版本开始，这些功能被分离到独立的模块中。
- en: As shown in the following diagram, Ceilometer relies on agents to collect, process,
    store, and retrieve data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，Ceilometer 依赖代理来收集、处理、存储和检索数据。
- en: '![Figure 8.15 – A general overview of the Ceilometer architecture](img/B21716_08_15.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.15 – Ceilometer 架构的总体概述](img/B21716_08_15.jpg)'
- en: Figure 8.15 – A general overview of the Ceilometer architecture
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 – Ceilometer 架构的总体概述
- en: 'Each data workflow is actioned by a dedicated agent that can be listed as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据工作流由一个专用代理执行，代理清单如下：
- en: '**Polling agents** : These regularly poll each OpenStack infrastructure service
    to formulate measurements via API calls. A compute polling agent gathers statistics
    from instances running in a compute node and polls them to the message queue.
    The central agent runs in a central management server in OpenStack, such as a
    cloud controller. It polls statistics of resources other than instances.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Polling agents** : 这些代理定期轮询每个 OpenStack 基础设施服务，通过 API 调用来生成度量。计算轮询代理从计算节点中运行的实例收集统计数据，并将其轮询到消息队列。中央代理运行在
    OpenStack 中的中央管理服务器上，如云控制器。它轮询除实例之外的资源统计信息。'
- en: '**Notification agents** : These listen periodically on the message queue bus,
    collect new notification messages set by different OpenStack services, and translate
    them into metrics before pushing them back to the appropriate message queue bus.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Notification agents** : 这些代理定期监听消息队列总线，收集由不同 OpenStack 服务设置的新通知消息，并将其转换为度量，然后将其推送回相应的消息队列总线。'
- en: '**Collector agents** : These monitor the message queue, gather samples, and
    collect the metering messages generated by either polling or notification agents.
    Therefore, the new metering messages will be recorded in a backend storage.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Collector agents** : 这些代理监控消息队列，收集样本，并收集由轮询或通知代理生成的度量消息。因此，新的度量消息将被记录到后端存储中。'
- en: '**API service** : This exposes a standard API that provides access to the internal
    Ceilometer database, if enabled, to query metering data against it.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API service** : 如果启用，它将公开一个标准 API，提供对内部 Ceilometer 数据库的访问，以查询相应的计量数据。'
- en: 'Gathering data is mainly performed by agents using a pipeline mechanism. Internally,
    they periodically send requests for sample objects that reflect a certain meter.
    Every sample request will be forwarded to the pipeline. Once passed to the pipeline,
    meters can be manipulated by several transformer types:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集主要通过代理使用管道机制执行。它们定期发送请求以获取反映某个度量的样本对象。每个样本请求都将转发到管道。一旦传递到管道，度量可以由多个转换器类型进行处理：
- en: '**Accumulator** : This accumulates multi-values and sends them in a batch.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Accumulator** : 这是累积多个值并将其批量发送的功能。'
- en: '**Aggregator** : This aggregates multi-values into one arithmetic. This includes
    arithmetic functions to compute the percentage.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Aggregator** : 这将多个值汇聚为一个算术结果。包括用于计算百分比的算术函数。'
- en: '**Rate of change** : This identifies trends by deriving another meter from
    the previous data.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rate of change** : 通过从先前的数据推导出另一个度量来识别趋势。'
- en: '**Unit conversion** : This gives the type of unit conversion to be used.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unit conversion** : 这是用于单位转换的类型。'
- en: 'Once manipulated and transformed, a meter might follow its path via one of
    the multiple publisher types:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦经过处理和转换，度量可能通过多种发布器类型之一继续其路径：
- en: '**Notifier** : This is the meter data pushed over a reliable messaging queue.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Notifier** : 这是通过可靠的消息队列推送的度量数据。'
- en: '**rpc** : This is the synchronous RPC meter data publisher.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rpc** : 这是同步的 RPC 度量数据发布器。'
- en: '**udp** : This is the meter data sent over the UDP.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**udp** : 这是通过 UDP 发送的度量数据。'
- en: '**file** : This is the meter data sent into a file.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**file** : 这是将度量数据发送到文件中的功能。'
- en: 'The aforementioned components are illustrated as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 上述组件的说明如下：
- en: '![Figure 8.16 – Metrics data transformation and publishing through metric pipelines](img/B21716_08_16.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.16 – 通过度量管道进行度量数据转换和发布](img/B21716_08_16.jpg)'
- en: Figure 8.16 – Metrics data transformation and publishing through metric pipelines
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 – 通过度量管道进行度量数据转换和发布
- en: Data collected can be stored in different types of databases supported by Ceilometer.
    Originally, MongoDB was a widely adopted data store for the telemetry metric storage.
    Due to MongoDB’s scalability and performance issues, Gnocchi has been evaluated
    as a well-suited time series database for Ceilometer, which will be covered in
    the next section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 收集到的数据可以存储在 Ceilometer 支持的不同类型的数据库中。最初，MongoDB 是一个广泛采用的数据存储用于遥测度量存储。由于 MongoDB
    在可扩展性和性能方面的问题，Gnocchi 被评估为一个适合 Ceilometer 的时间序列数据库，相关内容将在下一节中介绍。
- en: The Gnocchi data store
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gnocchi 数据存储
- en: Since the Train OpenStack release, **Gnocchi** has been considered the default
    and official supported storage service for Ceilometer. Given the performance and
    scalability issues of former metrics data store options, the introduction of Gnocchi
    as a time series database has solved main bottlenecks issues when processing and
    storing metrics data in large-scale environments. Data samples are no longer written
    directly into a database. Rather, they are converted into Gnocchi elements and
    posted afterward on its native API. Aggregated data is recorded in a time series.
    Each converted sample presents a data point that has a timestamp and measurement.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 Train 版本的 OpenStack 发布以来，**Gnocchi**被认为是 Ceilometer 的默认和官方支持的存储服务。考虑到之前的指标数据存储选项在性能和可扩展性方面的问题，引入
    Gnocchi 作为时间序列数据库，解决了在大规模环境中处理和存储指标数据时的主要瓶颈问题。数据样本不再直接写入数据库，而是转换为 Gnocchi 元素，并随后通过其原生
    API 提交。聚合数据以时间序列的形式记录。每个转换后的样本表示一个数据点，具有时间戳和度量值。
- en: 'The hallmark of data optimization using Gnocchi is indexing resources and their
    associated attributes. As a result, data searches are quicker. As mentioned previously,
    Gnocchi offers a scalable metric storage design by utilizing metric data storage
    in a scalable storage system, such as **Swift** and **Ceph** . As shown in the
    next figure, Gnocchi can be configured to aggregate and store measures in Swift
    or Ceph as a storage backend, using its built-in storage drivers:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Gnocchi 进行数据优化的特点是为资源及其相关属性建立索引。因此，数据搜索速度更快。如前所述，Gnocchi 通过利用可扩展存储系统（如**Swift**和**Ceph**）中的指标数据存储，提供了可扩展的指标存储设计。如下一图所示，Gnocchi
    可以配置为在 Swift 或 Ceph 中聚合和存储度量数据，作为存储后端，使用其内置的存储驱动程序：
- en: '![Figure 8.17 – The Gnocchi data store multi-backend support](img/B21716_08_17.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.17 – Gnocchi 数据存储多后端支持](img/B21716_08_17.jpg)'
- en: Figure 8.17 – The Gnocchi data store multi-backend support
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 – Gnocchi 数据存储多后端支持
- en: Data is processed by **metricd** for aggregation and storage. **metricd** handles
    flushing metrics marked for deletion. Gnocchi provides a compatible **statsd**
    protocol ( **gnocchi-statsd** ) that listens to the incoming metrics. **statsd**
    will be used later for Gnocchi deployment.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通过**metricd**进行聚合和存储处理。**metricd**负责刷新标记为删除的指标。Gnocchi 提供了兼容的**statsd**协议（**gnocchi-statsd**），用于监听传入的指标。**statsd**将在后续的
    Gnocchi 部署中使用。
- en: Alerting with Aodh
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Aodh 进行警报设置
- en: Since the Liberty release, the Ceilometer alarm module has been forked and renamed
    to **Aodh** , which takes the lead in triggering alarms based on custom rules.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Liberty 版本以来，Ceilometer 警报模块被分叉并重命名为**Aodh**，它负责根据自定义规则触发警报。
- en: 'As shown in the following diagram, a significant difference can be noticed
    when comparing the current alarm service to Ceilometer’s preceding one – the ability
    to scale horizontally when hitting more load. Using the same messaging queue,
    Aodh exposes an event listener that catches new notifications and provides an
    instant response time, with zero latency. Aodh listeners rely on predefined alarms
    that will trigger instantly based on events and configured measures. In this case,
    reacting against auto-scaling conditions when using an orchestration service (e.g.,
    **Heat** ) becomes very useful:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，将当前的警报服务与 Ceilometer 之前的服务进行比较时，可以明显看到一个区别——当负载增加时，能够进行水平扩展。使用相同的消息队列，Aodh
    提供了一个事件监听器，捕捉新的通知并提供即时响应，零延迟。Aodh 监听器依赖于预定义的警报，根据事件和配置的度量值立即触发。在使用编排服务（例如**Heat**）时，反应自动扩展条件变得非常有用：
- en: '![Figure 8.18 – An Aodh alarm overview in OpenStack](img/B21716_08_18.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.18 – OpenStack 中的 Aodh 警报概述](img/B21716_08_18.jpg)'
- en: Figure 8.18 – An Aodh alarm overview in OpenStack
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18 – OpenStack 中的 Aodh 警报概述
- en: Aodh is constructed mainly by an API server ( **aodh-api** ) that provides access
    to the data store. The Aodh alarm evaluator ( **aodh-evaluator** ) triggers alarms,
    based on the collected statistical trends crossing a threshold within a certain
    period. Alarms are triggered by an Aodh notification listener ( **aodh-listener**
    ), based on rules against events reported by the notification agents. The Aodh
    alarm notifier ( **aodh-notifier** ) enables alarm settings based on the threshold
    for a collection of samples.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Aodh 主要由一个 API 服务器（**aodh-api**）构成，提供对数据存储的访问。Aodh 警报评估器（**aodh-evaluator**）基于收集到的统计趋势，在一定时间内超过阈值时触发警报。警报由
    Aodh 通知监听器（**aodh-listener**）触发，基于通知代理报告的事件与规则匹配。Aodh 警报通知器（**aodh-notifier**）根据一组样本的阈值启用警报设置。
- en: Aodh notification supports both event- and threshold-based alarms. In the latest
    OpenStack releases, Aodh queries measurements by default from the Gnocchi data
    store.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Aodh通知支持基于事件和阈值的告警。在最新的OpenStack版本中，Aodh默认从Gnocchi数据存储查询度量数据。
- en: Deploying the telemetry service
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署遥测服务
- en: 'The current **kolla-ansible** repository supports a full deployment of the
    telemetry service in OpenStack. This includes the Ceilometer, Aodh, and Gnocchi
    services. As we have already deployed the Ceilometer service in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane – Shared Services* , we will walk through a detailed
    configuration of the Ansible roles for the telemetry stack:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的**kolla-ansible**仓库支持在OpenStack中完全部署遥测服务。这包括Ceilometer、Aodh和Gnocchi服务。由于我们已经在[*第3章*](B21716_03.xhtml#_idTextAnchor108)《OpenStack控制平面
    - 共享服务》中部署了Ceilometer服务，接下来我们将详细介绍遥测栈的Ansible角色配置：
- en: 'Make sure the Ceilometer service is added to run it as part of the cloud controller
    nodes if it is not enabled by adding it to the **multi_packtpub_prod** inventory
    file, as follows:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果Ceilometer服务尚未启用，则确保将其添加到云控制节点组中，可以通过将其添加到**multi_packtpub_prod**库存文件来实现，如下所示：
- en: '[PRE18]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Add the Ceilometer central polling and notification agents to the cloud controller
    group:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Ceilometer中央轮询和通知代理添加到云控制节点组：
- en: '[PRE19]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Add the ceilometer polling agent for the compute nodes group:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Ceilometer轮询代理添加到计算节点组：
- en: '[PRE20]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the Gnocchi service as part of the cloud controller nodes group:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为云控制节点组的一部分运行Gnocchi服务：
- en: '[PRE21]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Assign the Gnocchi components, including the API, **statsd** (the **gnocchi-statsd**
    daemon), and **metricd** (the **gnocchi-metricd** daemon) services, to run as
    part of the Gnocchi instance in the cloud controller nodes group:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Gnocchi组件，包括API、**statsd**（**gnocchi-statsd**守护进程）和**metricd**（**gnocchi-metricd**守护进程）服务，分配给作为云控制节点组中Gnocchi实例的一部分运行：
- en: '[PRE22]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Run the Aodh alarm service as part of the cloud controller nodes group:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为云控制节点组的一部分运行Aodh告警服务：
- en: '[PRE23]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Assign the Aodh components, including the API, evaluator, listener, and **notifier**
    services, to run as part of the Aodh instance in the cloud controller nodes group:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Aodh组件，包括API、评估器、监听器和**通知器**服务，分配给作为云控制节点组中Aodh实例的一部分运行：
- en: '[PRE24]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, make sure the Ceilometer service is enabled in the **globals.yml** file:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，确保在**globals.yml**文件中启用Ceilometer服务：
- en: '[PRE25]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Enable the Gnocchi service in the **globals.yml** file and, optionally, the
    **statsd** protocol support that will install and run the **gnocchi-statd** daemon:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**globals.yml**文件中启用Gnocchi服务，并可选择启用**statsd**协议支持，该支持将安装并运行**gnocchi-statsd**守护进程：
- en: '[PRE26]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The last configuration in the **globals.yml** file is to enable the Aodh alerting
    service by setting its corresponding variable to **yes** :'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**globals.yml**文件中的最后一个配置是通过将相应的变量设置为**yes**来启用Aodh告警服务：'
- en: '[PRE27]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Run the deployment pipeline and make sure that no errors have been reported
    when installing the telemetry service. Verify that the telemetry service containers
    are up and running in one of the cloud controller nodes using the following command
    line:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行部署管道，并确保在安装遥测服务时没有报告错误。使用以下命令行验证遥测服务容器是否在某个云控制节点中正常运行：
- en: '[PRE28]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is the output we get:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们得到的输出：
- en: '![Figure 8.19 – A listing of the deployed telemetry Kolla containers](img/B21716_08_19.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图8.19 – 部署的遥测Kolla容器列表](img/B21716_08_19.jpg)'
- en: Figure 8.19 – A listing of the deployed telemetry Kolla containers
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 – 部署的遥测Kolla容器列表
- en: Once the trio of Ceilometer, Gnocchi, and Aodh are joined to the OpenStack deployment,
    you should have a fully functional telemetry service. Navigating through Ceilometer
    metrics is straightforward and can be used to configure alarms via Aodh. Metrics
    and alarms are essential elements to tackle daily monitoring operations. However,
    even these would not be sufficient without the addition of service logs, which
    we will cover in the next section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Ceilometer、Gnocchi和Aodh三者加入到OpenStack部署中，你应该拥有一个完全功能的遥测服务。浏览Ceilometer指标非常简单，并且可以通过Aodh配置告警。指标和告警是应对日常监控操作的关键元素。然而，即使有了这些，也不足以应对问题，必须加入服务日志，我们将在下一节进行介绍。
- en: Grasping centralized logging
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握集中式日志记录
- en: The second essential part of operating an OpenStack cloud environment is to
    embrace the inspection of events in the infrastructure through logging information.
    With dozens of deployed services involved in the OpenStack setup, cloud operators
    should enable logging in each service, running at least within the control plane
    for troubleshooting, analysis, and even creating custom metrics for alerting.
    Depending on the OpenStack deployment tool and configuration settings used, the
    standard services in Linux/Unix systems write their logs in the **/var/log/**
    directory and subdirectories.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 操作 OpenStack 云环境的第二个关键部分是通过日志信息来检查基础设施中的事件。由于 OpenStack 配置中涉及数十个部署的服务，云操作员应该在每个服务中启用日志记录，至少在控制平面内启用日志，以便进行故障排除、分析，甚至创建自定义度量标准以供警报使用。根据所使用的
    OpenStack 部署工具和配置设置，Linux/Unix 系统中的标准服务将其日志写入**/var/log/** 目录及其子目录中。
- en: 'With the tons of log data generated, parsing and processing it all presents
    a hindrance to extracting meaningful information or troubleshooting unexpected
    issues, which take a longer time to solve. To overcome such a challenge, your
    log environment must evolve to become centralized. A good option is to start flowing
    logs in a dedicated **rsyslog** server. You might put in so much data that your
    log server starts starving for larger storage capacity. Furthermore, archiving
    the aforementioned data will not be useful when you need to extract information
    for a particular context. Additionally, correlating log data that has different
    formats (taking into consideration the RabbitMQ and MySQL logs) with generated
    events might even be impossible. So, what we need at this point is a set of quality
    requirements for a good OpenStack logging experience, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大量日志数据的生成，解析和处理所有日志数据成了提取有意义信息或排查意外问题的障碍，这些问题可能需要更长时间才能解决。为了克服这一挑战，你的日志环境必须发展为集中式。一个好的选择是开始将日志流入专用的**rsyslog**
    服务器。你可能会输入如此多的数据，以至于日志服务器开始缺乏更大的存储容量。此外，当你需要为特定上下文提取信息时，归档上述数据将没有用处。此外，将具有不同格式的日志数据（考虑到
    RabbitMQ 和 MySQL 日志）与生成的事件进行关联甚至可能变得不可能。所以，在这个阶段，我们需要一套质量要求来提供良好的 OpenStack 日志体验，如下所示：
- en: A parsing capability for log metadata
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志元数据的解析能力
- en: Fast indexing of log data
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速的日志数据索引
- en: A comprehensible presentation of data
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于理解的数据展示
- en: Metrics constructed from custom log data aggregations
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于自定义日志数据聚合构建的度量标准
- en: Generated logs in OpenStack are structured, making them suitable for processing,
    and can be parsed by several tools, including the open-source **ELK** stack, which
    is composed of **Elasticsearch** , **Logstash** , and **Kibana** , or even by
    commercial third-party tools, such as Datadog and Splunk. ELK is the most adopted
    tool as a logging pipeline in large OpenStack deployments. Most OpenStack deployment
    tools, other than **kolla-ansible** , are supported by an ELK stack installation
    out of the box. Although the ELK stack is still a valid option to manage logs
    in OpenStack, the latest OpenStack releases have shifted gears to adopt a forked
    open-source Elasticsearch solution, and **OpenSearch** was announced as the new
    way to handle logging in the next OpenStack operations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 中生成的日志是结构化的，适合处理，并且可以通过多种工具解析，包括开源的**ELK** 堆栈，ELK 由**Elasticsearch**、**Logstash**
    和 **Kibana** 组成，甚至可以通过商业第三方工具，如 Datadog 和 Splunk 来解析。ELK 是在大型 OpenStack 部署中最常用的日志处理工具。除
    **kolla-ansible** 外，大多数 OpenStack 部署工具开箱即用地支持 ELK 堆栈。尽管 ELK 堆栈仍然是管理 OpenStack
    日志的有效选择，但最新的 OpenStack 版本已转向采用分叉的开源 Elasticsearch 解决方案，并宣布**OpenSearch**将成为未来
    OpenStack 操作中处理日志的新方式。
- en: OpenSearch under the hood
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenSearch 引擎
- en: 'OpenSearch is an open-source software offered under the Apache version 2.0
    license. It has become a very popular search engine and log analytics suite, due
    to its large community and active commits, and originated from the Elasticsearch
    project. The OpenSearch suite includes a search engine, data store, and dashboards
    for visualizations, which originated from Kibana. The OpenSearch architecture
    is based on a distributed design, where different components can run in different
    cluster nodes. The following diagram shows an overview of a logging solution based
    on OpenSearch for OpenStack:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: OpenSearch 是一个开源软件，采用 Apache 2.0 版本许可。由于其庞大的社区和活跃的提交，它已经成为一个非常流行的搜索引擎和日志分析套件，并且源自
    Elasticsearch 项目。OpenSearch 套件包括一个搜索引擎、数据存储和用于可视化的仪表板，这些都源自 Kibana。OpenSearch
    架构基于分布式设计，不同的组件可以在不同的集群节点上运行。下图展示了基于 OpenSearch 的 OpenStack 日志解决方案概览：
- en: '![Figure 8.20 – An overview of the OpenSearch logging cluster in an OpenStack
    environment](img/B21716_08_20.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.20 – OpenStack 环境中 OpenSearch 日志集群概览](img/B21716_08_20.jpg)'
- en: Figure 8.20 – An overview of the OpenSearch logging cluster in an OpenStack
    environment
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.20 – OpenStack 环境中 OpenSearch 日志集群概览
- en: 'As shown in the previous diagram, a typical OpenSearch environment for mid
    to large deployments consists of a variety of cluster node roles, such as the
    following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，适用于中到大规模部署的典型 OpenSearch 环境包括多种集群节点角色，例如以下几种：
- en: '**The cluster manager node** : This keeps track of the OpenSearch formation
    cluster state, including node state changes (i.e., joiners and leavers) and health,
    indexing management, and shard allocation. For high availability reasoning, it
    is recommended to run at least two manager nodes for production use cases.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群管理节点**：此节点跟踪 OpenSearch 集群状态，包括节点状态变化（如加入和离开）及健康状态、索引管理和分片分配。为了高可用性，建议在生产环境中至少运行两个管理节点。'
- en: '**The coordinator node** : This receives requests initiated from a dashboard
    or client libraries and delegates them to the right shards on the data nodes,
    and then fetches, aggregates, and returns the data results to the client.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协调节点**：此节点接收来自仪表板或客户端库发起的请求，并将它们委派给数据节点上的正确分片，然后获取、聚合并将数据结果返回给客户端。'
- en: '**The data source node** : This represents the worker horses of the OpenSearch
    cluster that store the data and perform different data-related operations, including
    indexing, aggregation, and searching.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据源节点**：此节点代表 OpenSearch 集群的工作马，存储数据并执行不同的数据相关操作，包括索引、聚合和搜索。'
- en: Important note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Other types, including the **master-eligible** and **ingest** nodes, can form
    part of an OpenSearch formation cluster. Any node that is not marked as a master
    is designated with a master-eligible role. Ingest nodes are recommended for heavy
    data ingestion pipelines and to offload indexing workloads from data nodes.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的节点，包括 **主节点候选** 和 **摄取节点**，也可以成为 OpenSearch 集群的一部分。任何未被标记为主节点的节点都被指定为主节点候选角色。摄取节点适用于大规模数据摄取管道，并将索引工作负载从数据节点中卸载。
- en: OpenSearch is highly extensible, and several types of plugins can be installed
    for an enhanced search experience, security features, machine learning, and performance
    analysis. The available plugins are listed at [https://opensearch.org/docs/latest/install-and-configure/plugins/#available-plugins](https://opensearch.org/docs/latest/install-and-configure/plugins/#available-plugins)
    .
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: OpenSearch 是高度可扩展的，可以安装多种类型的插件，以增强搜索体验、安全功能、机器学习和性能分析。可用的插件列表请参见 [https://opensearch.org/docs/latest/install-and-configure/plugins/#available-plugins](https://opensearch.org/docs/latest/install-and-configure/plugins/#available-plugins)。
- en: The OpenSearch solution comes with a distributed architecture, enabling you
    to scale for large search and analytics operations. Niche features are supported
    by the OpenSearch core search service, making it easy to run log analytics, data
    ingestion, and observability capabilities. Additionally, performance has become
    a valid reason to adopt OpenSearch, due to additional ways to handle shards across
    multiple nodes that speed up the search operation. The amount of data generated
    by different OpenStack and common infrastructure services can be immense and can
    be fed into an OpenSearch cluster formation. By providing a minimum size of an
    OpenSearch cluster, cloud operators can start exploring a rich search and visualization
    experience. This enables you to create visualizations to spot the malfunctioning
    parts of complex and large systems that are hard to detect with basic metrics.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: OpenSearch 解决方案采用分布式架构，使您能够为大规模搜索和分析操作进行扩展。OpenSearch 核心搜索服务支持多种独特功能，使得日志分析、数据摄取和可观察性功能的运行变得更加容易。此外，由于提供了在多个节点上处理分片的额外方式，从而加速了搜索操作，性能也成为了采用
    OpenSearch 的有效理由。不同的 OpenStack 和常见基础设施服务生成的数据量可能巨大，可以输入到 OpenSearch 集群中。通过提供 OpenSearch
    集群的最小规模，云操作员可以开始探索丰富的搜索和可视化体验。这使得您能够创建可视化图表，发现复杂且大型系统中难以通过基本指标检测到的故障部件。
- en: Deploying OpenSearch
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 OpenSearch
- en: 'At the time of writing this edition, **kolla-ansible** supports OpenSearch
    installation. The community recommends the adoption of the OpenSearch option,
    which can be deployed in a few steps. It is highly recommended to dedicate a logging
    cluster that runs different OpenSearch node types, as covered in the previous
    section. Other considerations should be taken for each OpenSearch node requirement
    so that you can tailor your logging cluster, based on the required performance.
    For example, data nodes would require more disk space and could be upgraded with
    fast disks with higher IOPS, such as **solid-state drives** ( **SSDs** ). Master
    and coordinating nodes are more demanding computers that can be deployed with
    higher CPU power. For the next setup, we will run a simple instance of the OpenSearch
    service as part of the control node:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本版时，**kolla-ansible** 支持 OpenSearch 安装。社区推荐采用 OpenSearch 选项，它可以通过几个步骤进行部署。强烈建议专门配置一个日志集群，该集群运行不同类型的
    OpenSearch 节点，正如前面一节所述。对于每个 OpenSearch 节点的需求，还应做其他考虑，以便您可以根据所需的性能量身定制日志集群。例如，数据节点需要更多的磁盘空间，并可以通过配备更高
    IOPS 的快速磁盘（如 **固态硬盘**（**SSDs**））进行升级。主节点和协调节点是更高要求的计算机，可以配备更强大的 CPU。对于下一个设置，我们将在控制节点中运行一个简单的
    OpenSearch 服务实例：
- en: 'Assign the OpenSearch engine and **dashboards** roles to the control node:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 OpenSearch 引擎和 **dashboards** 角色分配给控制节点：
- en: '[PRE29]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Change the following variable in the **globals.yml** file to install OpenSearch
    as a central logging service:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **globals.yml** 文件中更改以下变量，以将 OpenSearch 安装为中央日志服务：
- en: '[PRE30]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'By default, Kolla deploys the OpenSearch service to listen on port **9200**
    . When deployed in the controller nodes, OpenSearch Dashboards should be accessible
    via the reserved VIP assigned to Keepalived to run the cloud controller clusters.
    Optionally, you can assign an FQDN and populate the hostname internally across
    all OpenStack nodes. The dashboards can be accessed with a browser internally
    on port **5601** and the configured **kolla_internal_fqdn** variable set in the
    **globals.yml** file that points to the **kolla_internal_vip_address** variable.
    Make sure that at least the **kolla_internal_vip_address** variable is set to
    the VIP address or mapped DNS endpoint name:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，Kolla 会部署 OpenSearch 服务，并使其监听 **9200** 端口。当部署在控制节点上时，OpenSearch Dashboards
    应该能够通过分配给 Keepalived 的预留 VIP 访问，以运行云控制器集群。您还可以选择为其分配一个 FQDN，并在所有 OpenStack 节点内部填充主机名。仪表盘可以通过浏览器在内部的
    **5601** 端口访问，且在 **globals.yml** 文件中设置的 **kolla_internal_fqdn** 变量指向 **kolla_internal_vip_address**
    变量。请确保至少设置 **kolla_internal_vip_address** 变量为 VIP 地址或映射的 DNS 端点名称：
- en: '[PRE31]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'OpenStack dashboards are created with the default **''opensearch''** username,
    and the password can be edited in the **kolla-ansible/etc/kolla/passwords.yml**
    file by setting the following variable:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenStack 仪表盘使用默认的 **'opensearch'** 用户名创建，密码可以通过设置以下变量，在 **kolla-ansible/etc/kolla/passwords.yml**
    文件中进行编辑：
- en: '[PRE32]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Other advanced and useful configurations that can be performed in advance to
    address common OpenSearch operation settings are the log retention policies, which
    are critical when dealing with disk-size management, data rotation, and the life
    cycle. OpenSearch comes with an **Index State Management** plugin to manage data
    rotation through the definition of log retention policies. Make sure that is enabled
    in the Ansible OpenSearch role defined in the **ansible/roles/opensearch/defaults/main.yml**
    file by checking the following variable:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他可以提前执行的高级且有用的配置，以解决常见的 OpenSearch 操作设置包括日志保留策略，这对于处理磁盘空间管理、数据轮换和生命周期至关重要。OpenSearch
    配备了**索引状态管理**插件，通过定义日志保留策略来管理数据轮换。确保在 **ansible/roles/opensearch/defaults/main.yml**
    文件中定义的 Ansible OpenSearch 角色中启用此功能，方法是检查以下变量：
- en: '[PRE33]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Once enabled, OpenSearch provides an indices data life cycle by defining the
    soft and hard retention periods. The soft retention period is defined by the **opensearch_soft_retention_period_days**
    variable in the **ansible/roles/opensearch/defaults/main.yml** file where indices
    are closed, no longer active, and staged for deletion, but they still occupy space
    on the disk and can be reopened. The hard retention period is the duration after
    which indices are deleted permanently and no longer occupy a space on the disk.
    They are defined by the **opensearch_hard_retention_period_days** variable in
    the **ansible/roles/opensearch/defaults/main.yml** file. The default soft and
    hard duration periods are set to **30** and **60** , respectively. That can be
    edited either by creating new variables in the **globals.yml** file and referencing
    them in the OpenSearch playbook, or directly in the **ansible/roles/opensearch/defaults/main.yml**
    file by changing the soft duration to **45** and the hard one to **90** days:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用后，OpenSearch 通过定义软保留期和硬保留期提供索引数据生命周期。软保留期由**opensearch_soft_retention_period_days**变量在
    **ansible/roles/opensearch/defaults/main.yml** 文件中定义，其中索引被关闭，不再活跃，并处于删除阶段，但它们仍然占用磁盘空间并且可以重新打开。硬保留期是索引被永久删除并不再占用磁盘空间的持续时间。它们由**opensearch_hard_retention_period_days**变量在
    **ansible/roles/opensearch/defaults/main.yml** 文件中定义。默认的软保留期和硬保留期分别设置为**30**和**60**天。可以通过在
    **globals.yml** 文件中创建新变量并在 OpenSearch playbook 中引用它们，或者直接在 **ansible/roles/opensearch/defaults/main.yml**
    文件中修改软保留期为**45**天，硬保留期为**90**天来编辑这些值：
- en: '[PRE34]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Run the pipeline and ensure that the output does not contain errors in the
    job configuration. After completion, run the following command line to validate
    the additional Kolla containers:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行管道并确保输出不包含作业配置中的错误。完成后，运行以下命令行以验证额外的 Kolla 容器：
- en: '[PRE35]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here is the output:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 8.21 – Listing of OpenSearch Kolla containers](img/B21716_08_21.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.21 – 列出 OpenSearch Kolla 容器](img/B21716_08_21.jpg)'
- en: Figure 8.21 – Listing of OpenSearch Kolla containers
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21 – 列出 OpenSearch Kolla 容器
- en: 'Point to the OpenSearch Dashboards URL from a browser at **http://10.0.0.47:5601**
    . Enter the **''opensearch''** username and configured password to start a log
    search:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中访问 OpenSearch Dashboards 的 URL **http://10.0.0.47:5601**。输入**'opensearch'**用户名和配置的密码以开始日志搜索：
- en: '![Figure 8.22 – The OpenSearch login page](img/B21716_08_22.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.22 – OpenSearch 登录页面](img/B21716_08_22.jpg)'
- en: Figure 8.22 – The OpenSearch login page
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.22 – OpenSearch 登录页面
- en: 'To start searching logs after the first login, an index pattern must be created
    to pull data from the logs’ sources. The index pattern points to a specific index
    that can pull log data from a specific date, for example. From the **OpenSearch
    Dashboards** panel, click on the **Index Patterns** button, as follows:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在首次登录后开始搜索日志，必须创建索引模式以从日志源提取数据。索引模式指向一个特定的索引，可以从指定日期提取日志数据，例如。从**OpenSearch
    Dashboards**面板中，点击**索引模式**按钮，如下所示：
- en: '![Figure 8.23 – Creating index patterns in the OpenSearch dashboard](img/B21716_08_23.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.23 – 在 OpenSearch 仪表板中创建索引模式](img/B21716_08_23.jpg)'
- en: Figure 8.23 – Creating index patterns in the OpenSearch dashboard
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.23 – 在 OpenSearch 仪表板中创建索引模式
- en: 'Next, specify the index pattern settings by applying a filter to pull and aggregate
    data. In the current example, the **timestamp** filter will be applied. By default,
    OpenSearch associates a unique identifier to each index pattern, as follows:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过应用过滤器来指定索引模式设置，以提取并聚合数据。在当前示例中，将应用**时间戳**过滤器。默认情况下，OpenSearch 会为每个索引模式关联一个唯一标识符，如下所示：
- en: '![Figure 8.24 – Applying a filter for the index pattern](img/B21716_08_24.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.24 – 为索引模式应用过滤器](img/B21716_08_24.jpg)'
- en: Figure 8.24 – Applying a filter for the index pattern
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.24 – 为索引模式应用过滤器
- en: 'Once created, the index pattern will apply the filter to the source data and
    visualize the field’s mapped core type, as saved by OpenSearch. For example, the
    created index pattern references a set of fields that are visible in the **Discover**
    tab on the upper side of the dashboard:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建后，索引模式将把过滤器应用于源数据，并可视化 OpenSearch 保存的字段映射核心类型。例如，创建的索引模式引用了一组字段，这些字段在仪表板上方的
    **发现** 标签中可见：
- en: '![Figure 8.25 – Listing the field selection for the index pattern](img/B21716_08_25.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.25 – 列出索引模式的字段选择](img/B21716_08_25.jpg)'
- en: Figure 8.25 – Listing the field selection for the index pattern
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.25 – 列出索引模式的字段选择
- en: 'Logs can be filtered using the search bar in the **Discover** panel, which
    lists all the associated events ending with a time frame, as shown in the following
    figure:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用 **发现** 面板中的搜索栏对日志进行过滤，搜索栏列出所有带有时间框架的相关事件，如下图所示：
- en: '![Figure 8.26 – Applying time frames for logs filtering](img/B21716_08_26.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.26 – 为日志过滤应用时间框架](img/B21716_08_26.jpg)'
- en: Figure 8.26 – Applying time frames for logs filtering
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.26 – 为日志过滤应用时间框架
- en: 'Further filtering can be applied using the search filters bar. In the following
    example, an added filter is applied to pull all data with an **ERROR** value in
    the **log_level** field:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用搜索过滤器栏进一步应用过滤。在下例中，添加的过滤器用于拉取 **log_level** 字段中值为 **ERROR** 的所有数据：
- en: '![Figure 8.27 – Applying a log_level filter for log filtering](img/B21716_08_27.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.27 – 为日志过滤应用 log_level 过滤器](img/B21716_08_27.jpg)'
- en: Figure 8.27 – Applying a log_level filter for log filtering
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.27 – 为日志过滤应用 log_level 过滤器
- en: 'To quickly monitor the trend of the pulled data with a specific filter, it
    is important to use the power of OpenSearch’s visualization capability. With a
    rich variety of supported dashboard objects, operators can build different types
    of visualizations for each search-filtered query. To create a visualization, point
    to the **Visualize** tab and click on the **Create Visualization** button. From
    the returned list, the **Vertical Bar Chart** visualization type is selected to
    expose the collected error occurrences in the last hour from the last saved query
    search. Once created, a new graph is generated that can be broken up into more
    specific fields, referred to as **Buckets** . Data graph aggregation can be performed
    by adding, for example, a bucket in the created graph settings and selecting the
    **X-axis** option. As shown in the next example, the data aggregation is fixed
    with the **Date Histogram** type, with the default **@** **timestamp** field:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速监控使用特定过滤器拉取的数据趋势，利用 OpenSearch 的可视化能力至关重要。通过丰富多样的仪表板对象，操作员可以为每个搜索过滤后的查询构建不同类型的可视化图表。要创建可视化图表，指向
    **可视化** 标签并点击 **创建可视化图表** 按钮。从返回的列表中选择 **垂直条形图** 可视化类型，以展示从最后保存的查询中收集的过去一小时内的错误发生次数。创建后，会生成一个新的图表，可以将其细分为更具体的字段，称为
    **桶**。通过在创建的图表设置中添加一个桶并选择 **X 轴** 选项，可以执行数据图表聚合。如下例所示，数据聚合已固定为 **日期直方图** 类型，默认使用
    **@** **时间戳** 字段：
- en: '![Figure 8.28 – Data aggregation using OpenSearch buckets](img/B21716_08_28.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.28 – 使用 OpenSearch 桶进行数据聚合](img/B21716_08_28.jpg)'
- en: Figure 8.28 – Data aggregation using OpenSearch buckets
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28 – 使用 OpenSearch 桶进行数据聚合
- en: 'Once updated, a generic overview of the error rate across all pulled log data
    can be visualized in a single pane of glass:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新后，可以在单一视图中可视化所有拉取日志数据的错误率概览：
- en: '![Figure 8.29 – The log visualization dashboard based on the applied filters](img/B21716_08_29.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.29 – 基于已应用过滤器的日志可视化仪表板](img/B21716_08_29.jpg)'
- en: Figure 8.29 – The log visualization dashboard based on the applied filters
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.29 – 基于已应用过滤器的日志可视化仪表板
- en: OpenSearch dashboards are highly customizable so that more data aggregation
    can be performed for more granular data presentation. For example, the previous
    graph representation can be saved in a dedicated service state dashboard, and
    then you can add a new visualization from the same graph by exposing the errors
    for each service.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: OpenSearch 仪表板具有高度可定制性，可以执行更多数据聚合，以便呈现更精细的数据。例如，之前的图形表示可以保存在专用的服务状态仪表板中，然后你可以通过暴露每个服务的错误信息，从相同的图表添加新的可视化。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered two essential practices for operating an OpenStack environment,
    exploring the possible ways to monitor and log. Both deployed monitoring and logging
    stacks can vary from one setup to another, depending on the operator’s experience
    and their familiarity with a specific tool, or some other third-party solutions.
    One of the best practices is to always monitor what is happening under and on
    top of the cloud platform, providing the necessary ways to capture metrics and
    collect data before it reaches a higher severity level and you have to start dealing
    with incidents. Prometheus, as demonstrated throughout this chapter, provides
    a multitude of configurations, and with the simple integration of exporters, cloud
    operators can easily ship metrics in an automated fashion. A single pane, Grafana,
    centralizes all the metrics in comprehensive dashboards, and operators can closely
    observe different system elements’ trends. The OpenStack telemetry service, commonly
    referred to as the Ceilometer, Aodh, and Gnocchi trio, was covered and deployed
    to start charging and tracking resource usage for tenants. At the end of the chapter,
    the logging pipeline challenge was explored, as well as a widely adopted solution,
    OpenSearch, that enables the consolidation of the immense amount of generated
    logs in each corner of OpenStack, where you can explore suspicious events to remediate
    them proactively. Logs provide valuable information if combined in a robust pipeline
    for analysis, and cloud operators can identify anomalies that are hard to detect
    with simple metrics. Early detection of signs of failure can help to evaluate
    cloud infrastructure performance and provide feedback, enhancing an environment’s
    services to keep up with the promised SLA.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了操作 OpenStack 环境的两个基本实践，探讨了可能的监控和日志记录方式。部署的监控和日志堆栈可能因操作环境而异，这取决于操作员的经验、对特定工具的熟悉程度或其他第三方解决方案。最佳实践之一是始终监控云平台下方和上方发生的事情，提供捕获指标和收集数据的必要方式，以便在问题达到更高严重性等级之前进行处理，而不是等到必须开始应对事件时再采取行动。正如本章所展示的，Prometheus
    提供了多种配置方式，并通过简单集成导出器，云操作员可以轻松实现自动化方式收集指标。单一面板工具 Grafana 将所有指标集中在综合仪表板中，操作员可以密切观察不同系统元素的趋势。OpenStack
    的遥测服务，通常称为 Ceilometer、Aodh 和 Gnocchi 三重奏，已被涵盖并部署，以开始为租户计费和跟踪资源使用情况。在本章结尾，探讨了日志记录管道的挑战，以及一个广泛采用的解决方案
    OpenSearch，它能够整合 OpenStack 各个角落生成的大量日志，操作员可以在其中探索可疑事件，并积极进行修复。日志提供了有价值的信息，如果结合在一个强大的管道中进行分析，云操作员可以发现那些通过简单指标难以检测的异常情况。故障迹象的早期检测有助于评估云基础设施性能并提供反馈，从而改善环境服务，确保服务水平协议（SLA）的兑现。
- en: Besides setting a logging pipeline, the next chapter will cover how to conduct
    benchmarking to gain more insights into the OpenStack infrastructure and evaluating
    the private cloud capacity.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设置日志记录管道，下一章将介绍如何进行基准测试，以深入了解 OpenStack 基础设施，并评估私有云的容量。
