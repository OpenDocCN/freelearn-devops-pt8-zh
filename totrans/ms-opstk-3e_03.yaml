- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Control Plane – Shared Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “The value of an idea lies in the using of it.”
  prefs: []
  type: TYPE_NORMAL
- en: – Thomas A. Edison
  prefs: []
  type: TYPE_NORMAL
- en: 'Addressing the challenge of business continuity within the OpenStack private
    cloud comes down to the traits and capabilities of its control plane. As highlighted
    in the first chapter, a variety of core services, including compute, imaging,
    storage, networking, and identity, should be designed for deployment with scalability
    and security in mind at an early stage. The other facet of such a requirement
    is to treat other non-native OpenStack services the same way as core ones by designing
    for failure and scalability. Such services include messaging queues and databases.
    As we consider best practices for deploying to production, it is essential to
    iterate through each service that would be part of the OpenStack control plane.
    Another challenge that arises during the deployment is the way OpenStack services
    are built and deployed. As we saw in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right Way (DevSecOps)* , containers to
    run our OpenStack services, breaking the control plane services into smaller chunks
    of units, will increase the reliability and scalability of each running service.
    In this chapter, we will extend our designed layout to a production-ready environment
    by going through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the perimeter of the OpenStack control plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying different OpenStack services taking up part of the control plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a first production deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the next OpenStack environment using kolla-ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncovering additional custom options for the OpenStack control plane deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenStack control plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we started designing our first iteration layout, we encountered a variety
    of OpenStack services, each composed of several components. One or several components
    can be designed to run in separate physical machines assigned with roles for logical
    operations and management reasons. As highlighted in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Considerations* , separating the roles logically
    will support the orchestration of the OpenStack deployment and operation. The
    term *control plane* is widely used in the network glossary. It dictates how to
    process and run requests. As an abstract concept, the control plane can be formed
    of one or many system components that centrally manage and describe the way a
    request can be fulfilled. When referring to the OpenStack logical layout, a cloud
    controller is designated to run most of the pieces of the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Running the control plane – cloud controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OpenStack APIs, scheduler services (including compute, storage, and file
    share schedulers), API endpoints, databases, and messaging queues are counted
    as part of the OpenStack control plane. This section iterates through each service
    that is part of the cloud controller setup and summarizes them into two main categories,
    OpenStack and infrastructure services, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – An overview of the OpenStack control plane](img/B21716_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – An overview of the OpenStack control plane
  prefs: []
  type: TYPE_NORMAL
- en: We will walk through the control plane in the following subsections and examine
    each service, its subcomponents, and the latest updates.
  prefs: []
  type: TYPE_NORMAL
- en: Identity service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The identity service, as explored in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Considerations* , presents a vital component
    to authorize and authenticate service communication to accomplish a specific request,
    such as storage creation or instance provisioning. As part of the control plane,
    Keystone consolidates a catalog of all existing OpenStack services to be reached
    through its API.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to provide a glossary of key terms of Keystone when running
    the control plane services. This is to ensure compliance and governance policies
    when dealing with different sets of domains, projects, and users as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Project** : The concept of a *project* has replaced that of a *tenant* within
    version 3 of the Identity API. The concept did not change, as a project is a logical
    construction of a set of resources isolating users and groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain** : Going up a layer, a domain contains a set of users, groups, and
    projects. This abstract isolation could be very helpful for large enterprises
    with different sets of departments, for example. Each department can be allocated
    a domain made up of different projects and users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Role** : One of the key mechanisms of the Keystone service since its early
    days is the concept of the *role* . A user or group of users can be assigned to
    different projects with different authorization layouts without the need to create
    multiple users with different assignments and switch manually to a different environment
    to access specific resources through different projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Catalog** : Keystone grants access to different services through its discovery
    service. Those services, from a Keystone perspective, are exposed as OpenStack
    service endpoints and registered in the service catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token** : Accessing a specific service must be done through obtaining a token.
    It can be considered as an information card that validates the user, as well as
    the expiration date and time. It also checks which project(s) the token has been
    associated with, and to which endpoints. The token concept itself has passed through
    different versions and ways of working. When expanding a production OpenStack
    environment, it is necessary to have a basic understanding of the scoping mechanism
    for the Keystone token. As a large number of users will be competing to access
    services and resources, scoping will help to better organize the access boundaries
    in a Keystone request, which can be categorized into two groups:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scoped token** : Once a token request is created, the Identity API will specify
    which OpenStack services (endpoints) will be used by the requester and its associated
    roles. It mainly contains information such as the reference to the project and
    domain ID.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unscoped token** : Unlike the scoped token, the created token request will
    not contain any specific authorization scope to access services. When users present
    credentials for the first time, the usage of unscoped tokens avoids an excessive
    loop of authentication flow. At a later time, the unscoped token can be narrowed
    down to a scoped one to limit authorization to a specific service component(s).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User** : This is an API requester to consume OpenStack services and resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group** : This is a collection of users or API requesters belonging to the
    same domain, as explained previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keystone API has been improved by providing more options for authentication
    mechanisms aside from the traditional password way, such as SAML2 and OpenID Connect
    based on OAuth. Identity federation support has been a big milestone for the Keystone
    implementation, enabling pluggable **Identity Providers** ( **IdP** ) through
    different backends, including SQL, LDAP, and even a federated IdP. Within the
    latest OpenStack releases, including Antelope and Bobcat, several backends have
    been supported, allowing enterprises to leverage their existing authentication/authorization
    systems for more consolidated management.
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief summary of the current supported authentication backends is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LDAP** : Many organizations might have existing LDAP instances and require
    a set of security policies to be followed for different compliance reasons. In
    this setup, Keystone will need to grant access to the LDAP servers to read and
    write and will not handle an IdP role internally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQL** : Most of the default OpenStack deployments come with SQL configuration
    to store user information in a SQL database and provide direct interaction with
    the Keystone service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Federated IdP** : This is one of the most common setups, allowing organizations
    to leverage a complete consolidated identity point where Keystone will establish
    a trust relationship and act as a service provider. The OpenStack community has
    developed more stable releases around this layout, offering more ways of authentication
    against multiple backends, such as SQL, LDAP, and Active Directory, through SAML
    assertions. Lately, there has been more work on OpenID Connect and OAuth in its
    second version. This even allows authentication using public social login platforms
    such as Google and Facebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Keystone Identity API supports the full **Create, Read, Update, and Delete**
    ( **CRUD** ) RESTful operations API. Commands are fired using the OpenStack CLI
    and not the Keystone CLI as the latter is no longer supported within Antelope
    and later releases, and the latest OpenStack releases that have adopted Identity
    API version 3.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on which authentication and authorization strategy you adopt, the
    mechanism of Keystone is the same when dealing with pure OpenStack ecosystem workflows.
    The Identity API’s interaction with the rest of the services might be overwhelming
    to tackle when dealing with a large infrastructure layout. At this point, once
    we have iterated through the latest state of the art in Keystone, a brief overview
    of how interactions work will be helpful before listing the next services. We
    should remember that Keystone will turn out to be the security hub of AA in the
    OpenStack ecosystem where default configurations would not be ideal to avoid possible
    security gaps and further boost your compliance and governance.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Keystone configuration can be customized through the Keystone **kolla-ansible**
    playbook located at **/kolla-ansible/ansible/roles/keystone** . At the time of
    writing this book, the default **kolla-ansible** configuration within the Antelope
    and later releases uses Keystone fernet tokens to authenticate uses instead of
    traditional **Public Key Infrastructure** ( **PKI** )-based tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following workflow briefly demonstrates typical Keystone request and response
    interactions between an API requester (user or system) and identity service to
    access a specific OpenStack service:'
  prefs: []
  type: TYPE_NORMAL
- en: A user or API requester authenticates against Keystone service using credentials.
    Identity API receives the request and grants a token to the requester once authenticated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user or API requester presents the request with the received token to access
    the demanded service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The demanded service validates the eligibility of the presented token with the
    identity service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once validated, the requested service replies with a response to the API requester.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This workflow is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Keystone API request processing](img/B21716_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Keystone API request processing
  prefs: []
  type: TYPE_NORMAL
- en: As noted previously, any request to access a specific service or resource will
    engage the identity service and later interactions will be handled by API requests.
    Taking a more complex scenario, such as virtual machine creation, more interactions
    will be involved. This would engage more services to be contacted, such as image,
    network, and storage, forming the essential resources to run an instance. Hence,
    Keystone will validate each request by using the original authentication token.
    A new desired service request will require a newly generated authenticated request
    using the first authentication token.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Keystone now has OAuth 2.0 Mutual TLS support. Keystone Auth has a new plugin
    for the OAuth 2.0 Device Authorization Grant.
  prefs: []
  type: TYPE_NORMAL
- en: Compute service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our control plane includes all compute service components except **nova-compute**
    , which will be running in dedicated hosts. As covered in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Considerations* , the composition of different
    compute daemons constructs the main capability of the OpenStack ecosystem. The
    following Nova services will run as part of the cloud controller stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '**nova-api** : This is the primary interface to handle compute requests, including
    the creation, listing, deletion, and management of compute resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nova-scheduler** : This is a set of filters and weights to determine the
    most suitable compute hosts to run an instance based on custom or default characteristics
    such as host location, memory, or CPU architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nova-conductor** : This is considered a novel addition to the Nova architecture
    prior to the latest OpenStack release, **nova-conductor** insulates the compute
    process from contacting the database directly (counted as not trusted from a security
    perspective) and connects the database directly instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nova-novncproxy** : This provides VNC console access to the instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Nova playbook is located at **/kolla-ansible/ansible/roles/nova** in the
    **kolla-ansible** repository. Note that the playbook does not separate each compute
    component on its own.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Placement service, as highlighted in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Consideration* , is considered a great addition
    to the compute service to conduct fine-grained prefiltering and achieve advanced
    instance scheduling not only by checking compute resources but also for networking
    and storage pools. The Placement service was introduced as part of the Nova architecture,
    and due to its evolution, it has been turned into a separate project. Some important
    constructs that use this service are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource providers** : The Placement service tracks the underlying resources
    (types and occurrences), such as compute, storage pools, and networking, in the
    form of abstracted data model objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource class** : Tracked resources are classified by types that are categorized
    into two main sets of resource classes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard** : **MEMORY_MB** , **VCPU** , **DISK_GB** , **IPV4_ADDRESS** ,
    **PCI_DEVICE** , **SRIOV_NET_VF** , **NUMA_CORE** , **NUMA_SOCKET** , **VGPU**
    , and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom** : Starting with the **CUSTOM_** prefix, custom resources can be
    created and added to the resource classes of the Placement service.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inventories** : A collection of a set of resource classes provided by a given
    resource provider. For example, a resource provider, **RP01** , is associated
    with four resource classes: **16 VCPU** , **4096 MEMORY_MB** , **200 DISK_GB**
    , and **50 IPV4_ADDRESS** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traits** : These represent the characteristics of a resource provider. For
    example, vCPU provided by a resource provider, **RP01** , can be **HW_CPU_X86_SVM**
    (SVM x86 hardware architecture) and **DISK_GB** can be **Solid-State Drive** (
    **SSD** ). Both traits assigned to **RP01** will be noted as **is_X86_SVM** and
    **is_SSD** for both **VPCU** and **DISK_GB** resources, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Placement service provides standard traits; however, operators can still
    create custom traits by using the placement API CLI. Traits can be specified in
    the Glance image characteristics, or the extra specs of the instance flavor used
    for the instance launch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Allocation** : An instance that occupies resources in a compute node is considered
    a consumer for a resource provider. This occupation of resources is represented
    as *allocation* stored in a data model record.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Allocation candidates** : After processing each request, the Placement service
    reshuffles the existing resource providers and returns the latest available group
    to be available for the next suitable requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Through the Placement service, more advanced filtering options can be achieved.
    The following workflow demonstrates how the networking service interacts with
    the compute service through placement. The desired scenario is to schedule instance
    provisioning in a compute node based on the network bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: An inventory is created through the compute service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A resource provider is created by the compute node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the compute resource provider, a networking resource provider is created
    by the networking service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The networking service reports the bandwidth inventories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The networking service provides a resource request for a port.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The compute service receives the resource request from the port and adds it
    to the API request in a **GET /** **allocation_candidates** request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The exposed candidates will be filtered by the Placement service and the compute
    service will pick up the most suitable one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The compute service claims the resources in the Placement service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The compute service exchanges the allocation update with the networking service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Placement service updates its allocation candidates for the next request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The placement API provides additional capabilities by assisting the Nova scheduling
    process. Prior to the Newton release, only computing resource usage (such as CPU
    and RAM) was tracked and managed. With the addition of multiple resource providers
    (storage and network services), the placement API simplifies resource recording
    and scheduling through a unified management method.
  prefs: []
  type: TYPE_NORMAL
- en: Image service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Glance API is part of the control plane. However, as was highlighted in
    [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack – Design
    Consideration* , storing the images can be handled by different data store types,
    such as Swift object storage, AWS S3, the Ceph storage system, VMware, filesystems,
    and, recently, the Cinder block storage service. The evolution of the Glance service
    has been marked mostly by the flexibility of the storage image backend support
    within the latest OpenStack releases. Within Antelope and later releases, the
    Glance API service can be configured to support a list of backends at the same
    time, where the Glance API will process the disk image operation using the default
    data store; otherwise, it will go through the defined backend list.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: By default, the imaging service playbook supports file, HTTP, VMware, Cinder,
    Swift, and Ceph backends located in the **/kolla-ansible/ansible/roles/glance/defaults/main.yml**
    file. Enabling or disabling additional image storage backends can be done from
    the **/kolla-ansible/etc/kolla/globals.yml** file by setting **yes** or **no**
    to **glance_backend_ceph** , **glance_backend_file** , **glance_backend_swift**
    , and **glance_backend_vmware** for Ceph, file, Swift and VMware storage backends,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **glance-registry** process running in the control plane will handle the
    metadata updates for each associated image in the database, including the image
    location defined in the storage backend path, as illustrated in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The Glance control plane components](img/B21716_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The Glance control plane components
  prefs: []
  type: TYPE_NORMAL
- en: The other great addition to this Glance storage support capability is that the
    control plane can be customized to support multiple block storage backends for
    Glance. If two Cinder storage volumes are created, a property can be assigned
    to a volume, for example, SSD or HDD, where the Glance API will be instructed
    to use the desired Cinder volume based on the cloud operator’s desired configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Networking service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike many other OpenStack services, the Neutron service is considered more
    complex due to the nature of its evolution through the OpenStack releases as well
    as its advanced virtual networking features and extensions. As demonstrated in
    [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack – Design
    Consideration* , the networking capaci ty and segmentation should be planned in
    advance not only to accommodate more circulating traffic but also to take advantage
    of virtual networking scalability. For this, a network node is dedicated to handling
    different types of tenants and providers by design. Meanwhile, the main Neutron
    API will run on the controller node as part of the control plane. The number of
    Neutron drivers and plugins is increasing with each OpenStack release; within
    the Antelope release, most of the well-known SDN driver mechanisms are supported,
    including **openvswitch** , **OVN** , **linux bridge** , **l2population** , and
    **baremetal** . Advanced service plugins providing virtual network features such
    as VPNs, firewalling, and routing run on a separate network node. It is strongly
    recommended to consider hardware network bonding for both cloud controllers and
    network nodes for performance and resiliency reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Networking configuration through the **kolla-ansible** infrastructure code repository
    can take different forms depending on the networking features to be added. All
    mechanisms, drivers, and service plugins are listed in the Neutron file ( **/kolla-ansible/ansible/roles/neutron/defaults/main.yml**
    ). Neutron drivers such as Open vSwitch and OVN run on their own playbooks located
    in the same folder path, **/kolla-ansible/ansible/roles/openvswitch** and **/**
    **kolla-ansible/ansible/roles/ovn-controller** , respectively
  prefs: []
  type: TYPE_NORMAL
- en: Block storage service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cinder’s main mission since its incubation within the OpenStack ecosystem has
    been to provide persistent storage to instances. Unlike ephemeral storage, the
    block storage concept has been included in OpenStack since Bexar’s early release
    and kept as part of the Nova project. However, since the Folsom release, it has
    been changed to its own service. There are many reasons for this separation; the
    main reason is the development of the Cinder architecture. Some of the block storage
    subcomponents are considered part of a control plane, including **cinder-api**
    and **cinder-scheduler** , as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Cinder control plane components](img/B21716_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Cinder control plane components
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the block storage extensions in more depth in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Storage – Block, Object, and* *File Shares* .
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Block storage is not enabled by default in the current **kolla-ansible** infrastructure
    code repository. Setting **enable_cinder** to **'yes'** in the **/kolla-ansible/etc/kolla/globals.yml**
    file will enable created instances to store their disks in the associated volume.
    Supported Cinder storage backends are listed in the Cinder playbook defined in
    the **/kolla-ansible/ansible/roles/cinder/default/main.yml** file and referenced
    with the **cinder_backend** configuration stanza code. Make sure to prepare an
    initial LVM volume named **cinder-volumes** as the **kolla-ansible** wrapper will
    expect an existing volume group with the mentioned name as stated in the **globals.yml**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Object storage service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Swift is made up of several subcomponents to manage objects, including accounts,
    containers, and objects. At the highest level, the main interface to interact
    with the rest of the object storage ring is the **swift-proxy-server** daemon,
    which will be running on the cloud controller instance(s).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Swift is disabled by default in the current **kolla-ansible** infrastructure
    code repository. It can be enabled by setting **enable_swift** to **yes** in the
    **/** **kolla-ansible/etc/kolla/globals.yml** file.
  prefs: []
  type: TYPE_NORMAL
- en: File-sharing service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have aimed to provide a managed shared file service in our initial production
    draft, a few components of the Manila service will be part of the control plane,
    including **manila-api** and **manila-scheduler** . Like Cinder, the backend storage
    flavors of the file-sharing service can vary. We will cover more configuration
    details of the file-sharing service in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Storage – Block, Object, and* *File Shares* .
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The file-sharing service is disabled by default in the current **kolla-ansible**
    infrastructure code repository. Enabling the Manila service can be done by adjusting
    the **enable_manila** setting to **yes** in the **/** **kolla-ansible/etc/kolla/globals.yml**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As stated in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack
    – Design Considerations* , Ceilometer has been changed to act solely as a metric
    collector of resources running under the OpenStack command. The **ceilometer-api**
    component is part of the control plane. Considering the way Ceilometer functions,
    its agent-based architecture involves the existence of data collection agents
    that collect metric data via REST APIs and poll resources. In addition to **ceilometer-api**
    , a ceilometer polling agent also runs in the controller node with a central agent
    to poll all resources created by various tenants under the OpenStack infrastructure.
    A notification agent listening to the message bus to consume notification data
    emitted by agents can optionally run on the cloud controller node.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Ceilometer service is disabled by default in the current **kolla-ansible**
    infrastructure code repository. Set **enable_ceilometer** to **yes** in the **/kolla-ansible/etc/kolla/globals.yml**
    file. The default playbook configuration of Ceilometer is located under the **/**
    **kolla-ansible/ansible/roles/ceilometer** directory.
  prefs: []
  type: TYPE_NORMAL
- en: Alarming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alarming was originally designed as part of the Ceilometer features and was
    decoupled to run on its own project with the code name **Aodh** . The alarming
    components can live within the cloud controller node, which includes additional
    components for alarm evaluation, a notifier, and a state database.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Aodh service is disabled by default in the current **kolla-ansible** infrastructure
    code repository. Set **enable_aodh** to **yes** in the **/kolla-ansible/etc/kolla/globals.yml**
    file. The default playbook configuration of Aodh is located under the **/** **kolla-ansible/ansible/roles/aodh**
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Dashboard service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Horizon runs behind an Apache web server instance that serves as a frontend
    to expose OpenStack services in a dashboard, but not all non-core services are
    reflected in the dashboard by default. Horizon needs to be configured by enabling
    directives for certain services and features, such as VPN as a service. Horizon
    is based on the Python Django framework, which could be overutilized if the number
    of end users keeps increasing and requests are performed excessively. Scaling
    the dashboard is something one should expect, especially if most of the supported
    directives for additional services are enabled and exposed to a wider audience.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Horizon is counted as a core service in the **kolla-ansible** infrastructure
    code repository by default. The **enable_openstack_core** stanza configuration
    is set to **yes** by default, which includes the Horizon service in the **/kolla-ansible/etc/kolla/globals.yml**
    file. In the same file, you can enable additional Horizon directives to be managed
    through the dashboard by commenting out the **enable_horizon_service** configuration
    line where the service refers to the additional OpenStack service. By default,
    all additional services supported by the dashboard are enabled in the Horizon
    playbook, as reflected in the **/** **kolla-ansible/ansible/roles/horizon/default/main.yml**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: As we have covered most of the OpenStack services that will be part of the control
    plane, the next section will iterate through the non-OpenStack services counted
    as shared services.
  prefs: []
  type: TYPE_NORMAL
- en: Shared services – infrastructure services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack –
    Cloud Design Considerations* , we identified shared services as being non-OpenStack
    services. Shared services, such as queuing messages, databases, and, optionally,
    the caching layer, are the most critical ones besides other core services.
  prefs: []
  type: TYPE_NORMAL
- en: Message queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As part of the control plane, RabbitMQ is the most commonly used messaging service
    in OpenStack. Dedicating separate nodes for the queue is highly recommended but
    is not a *must-have* . Vast deployment based on multi-regional OpenStack environments
    is a suitable case to run in a dedicated RabbitMQ cluster due to the meshed requests
    of different services, including compute, storage, and networks.
  prefs: []
  type: TYPE_NORMAL
- en: The other overlooked aspect of the queuing message is the security requirement.
    RabbitMQ sits between most OpenStack services as a communication hub. It is important
    to keep message exchanges secured. RabbitMQ supports TLS, which protects subscribers
    and clients from tampered-with messages and forces encryption in transit.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The RabbitMQ TLS option is disabled by default in the **kolla-ansible** infrastructure
    code repository. It can be enabled by setting **rabbitmq_enable_tls** to **yes**
    in the **/kolla-ansible/etc/kolla/globals.yml** file. Additional RabbitMQ configurations
    can be adjusted in its associated playbook, such as message and queue expiration,
    which are located in the **/** **kolla-ansible/ansible/roles/rabbitmq/default/main.yml**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other vital piece of the OpenStack puzzle ecosystem is the database holding
    updates of all service states and user-tracking information. MariaDB is considered
    the database engine as part of the control plane, made better with the Galera
    wrapper for clustering in multi-master cluster mode.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: MariaDB clustering relies on HAProxy for database instance switching. More details
    will be discussed in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running
    a Highly Available Cloud – Meeting the SLA* . The default database engine is configured
    with MariaDB in the **kolla-ansible** infrastructure code repository.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Caching comes as an optional piece of our control plane. However, it is strongly
    recommended to begin with caching due to its light footprint in cloud controller
    utilization. Memcached is becoming a standard to provide ephemeral data between
    different OpenStack services, such as API interaction information between the
    identity service and the rest of the services demanding token validation.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Memcached is enabled in the **kolla-ansible** infrastructure code repository
    by default. The maximum cached memory size and number of connections can be set
    in the Memcached playbook located in the **/** **kolla-ansible/ansible/roles/memcached/default/main.yml**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the control plane of OpenStack is an essential step before starting
    the deployment of the private cloud. That will help to identify which design layout
    will fit in your environment to ensure high availability as the highest priority.
    The next section will iterate through the next design draft scoped around the
    OpenStack control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Arming the control plane – prepare for failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The composition of different components around the OpenStack control plane requires
    more effort to keep them up and running with maximum resiliency. There are a variety
    of ways to achieve maximum availability for the control plane, but they should
    be prepared and tested in advance. Within the latest releases of OpenStack, several
    tools have been adopted to ensure a scalable and highly available control plane.
    Another approach is simply reusing some patterns that already exist, such as highly
    available database layouts or messaging bus clustering. We will discuss each of
    the OpenStack layers’ high availability in more depth in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* . It is essential, at this
    level, to define a strategy to empower the control plane from both scalability
    and availability perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both terms should be considered from the first iteration of the design plan
    by addressing the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How can a control plane respond to a large number of API requests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can hardware failure be faced and resolved in the least amount of time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which design patterns can be used to ensure a stable and resilient control plane?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we handle the growth of infrastructure services such as databases and
    messaging buses?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The OpenStack ecosystem has been designed to scale massively. Armed with its
    modular architecture, designing the control plane for resiliency is an open door
    for different ways of implementation. A summary of possible known design options
    to prepare for a successful control plane deployment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Active/active** : All cluster nodes participate in processing requests. This
    pattern is mainly associated with an *asymmetric clustering* layout whereby an
    entry point such as a load balancer distributes the requests across active control
    plane services. An active/active setup increases the performance and scalability
    of the cloud infrastructure by distributing the load across an army of nodes.
    Automating the subscriptions or discarding a node can be orchestrated with a load
    balancer and does not necessarily require a resource manager depending on the
    load balancer’s capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active/passive** : Unlike the previous mode, active nodes serve requests
    while standby nodes take over only when one or several active nodes fail. Passive
    control plane nodes are referred to as *sleepy watchers* and do not require an
    additional frontend point such as a load balancer, unlike in active/active mode.
    Such designs are associated mainly with *symmetric clustering* whereby resource
    manager software evaluates and ensures that the active control plane component
    is the only one serving the requests at any point in time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The **kolla-ansible** infrastructure code repository comes with predefined roles
    for a highly available control plane defined in the **globals.yml** file with
    the **'hacluster'** stanza configuration. Most of the adopted clustering software
    tools within the OpenStack control plane services, including the Antelope release,
    use a few **HAProxy** instances for load balancing and Keepalived for dynamic
    cluster node health checks based on the concept of **Virtual IP** ( **VIP** )
    addresses. The HAProxy playbook comes with the latest customizable configuration
    settings located under the **/kolla-ansible/ansible/roles/loadbalancer/** directory.
    [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly Available
    Cloud – Meeting the SLA* , will cover the HAProxy and Keepalived setup in more
    detail to deploy a highly available OpenStack control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Designing for control plane failure would depend on the target **Service-Level
    Agreement** ( **SLA** ) and available options within your infrastructure. Active/active
    mode could be a *best-effort* scalable and highly available implementation but
    would cost extra by dedicating either a hardware or software appliance to assign
    the load-balancing role of the cluster. The choice of database is another dilemma
    that most cloud architects and operators pay special attention to due to complexity
    and performance concerns. For example, an organization’s policy dictates the usage
    of the Oracle database engine. Setting an active-active database setup might not
    be straightforward to address control plane high-availability requirements because
    that might require more specific tools and additional integration, unlike using
    other engines such as MySQL Galera.
  prefs: []
  type: TYPE_NORMAL
- en: Planning for more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Relying on the CI/CD tool as a source of truth is a best practice to keep a
    consistent control plane services cluster. As we will cover in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , adding a new cloud controller
    node should be done transparently without disturbing the running API services.
    That is another valid reason to develop more integration tests of service consistency
    during a new control plane node deployment in a separate environment. A monitoring
    system should feed back the capacity, service availability, and resource usage
    to ensure a successful production day if all is as expected.
  prefs: []
  type: TYPE_NORMAL
- en: The process of deploying an additional cloud controller node to empower the
    control plane can be applied the same way when joining additional control plane
    services. Deploying more OpenStack services will expose more APIs to interact
    with the other nodes and handle more user or service requests. This can increase
    the load by hundreds of API requests per minute, and undoubtedly more CPU, memory,
    networking, and disk will be consumed. Raising performance and scalability concerns
    for such matters should not limit the growth of your OpenStack cloud services
    portfolio as there are several ways to address such a challenge. As long as automation
    is preserved throughout all operations tasks, the deployment of a new OpenStack
    component that counts as a new control plane service should pass through the CI/CD
    stages. The new component should pass through the testing and development environments
    and send feedback with a few metrics such as capacity and usage limits before
    promoting it to the production environment. A design decision can be made afterward,
    for example, by running the new component in the same cloud controller cluster
    or dedicating a separate physical or virtual node to run.
  prefs: []
  type: TYPE_NORMAL
- en: The addition of a new API service as part of the control plane should be reflected
    across the nodes running the specific service role. That is where the CI/CD system
    will shine and ensure consistency when dealing with the exponential growth of
    the infrastructure in terms of nodes and services.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A few other OpenStack services will be covered and integrated in the next chapters
    as part of the control plane, such as **Magnum** and **Zun** in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity and Flavors* ; LBaaSv2 in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* ; and Watcher
    in [*Chapter 9*](B21716_09.xhtml#_idTextAnchor204) , *Benchmarking the Infrastructure
    – Evaluating Resource Capacity* *and Optimization* .
  prefs: []
  type: TYPE_NORMAL
- en: Extending the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once different services that construct the control plane have been identified,
    it is a good time to update the initial design draft. An ideal way to start deployment
    with the upcoming production stage in mind is to assign each physical node a set
    of roles to run different OpenStack services. That brings us to the following
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud controller node** : This runs the control plane services including
    OpenStack APIs, identity, imaging, file sharing, telemetry, and dashboard services.
    Shared services, including a database with a MySQL Galera cluster, the RabbitMQ
    messaging service, and Memcached, will be part of the assigned cloud controller
    node role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute node** : This runs KVM as the main hypervisor with Nova compute and
    networking agent services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network node** : This runs Neutron agent services, including **layer3** ,
    **lbaas** , **dhcp** , and plugin agents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage node** : This runs Cinder volume services interfacing with the Cinder
    scheduler to provide block storage resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployer node** : This runs the orchestration deployment toolchain, including
    Ansible, the Kolla builder, and the CI server. The deployer node will optionally
    host a local private Docker registry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The updated design draft exposes a multi-node OpenStack environment, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – A multi-node OpenStack deployment layout](img/B21716_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – A multi-node OpenStack deployment layout
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the availability of your hardware, the network and storage roles
    can be assigned to the cloud controller node. Use the monitoring metrics data
    to decide in advance the right moment when those services should be installed
    in separate physical nodes. Splitting storage and network services is recommended
    when dealing with the exponential growth of the OpenStack infrastructure usage.
    If you are planning to run the nodes in a virtual environment using tools such
    as VMware ESXi or VirtualBox, make sure to have the hardware virtualization extensions
    installed in addition to the virtual networking configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As depicted in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting
    OpenStack – Design Considerations* , a few networks should exist and be prepared
    in advance. Preparing the physical networking beforehand is essential as part
    of the extension of the production layout in the next iterations. The proposed
    standard networks in our initial draft include management, tenant, storage, and
    external networks connecting different nodes in our initial cluster layout.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following interfaces will be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**eth0** : The management interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eth1** : The overlay and tenant interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eth2** : The external interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eth3** : The storage interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, four distinct types of traffic are considered
    for the OpenStack networking setup, including management, tenant and overlay,
    storage, and external. Note that a network node is dedicated to managing various
    OpenStack networking services that would increase the performance with fewer chances
    of network saturation at the cloud controller node level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Physical network segmentation layout](img/B21716_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Physical network segmentation layout
  prefs: []
  type: TYPE_NORMAL
- en: 'At the switching layer, networks are organized logically through the VLANs,
    where each network is assigned a VLAN ID coupled with the assigned network IP
    subnetwork address pool. The following table provides an example of initial network
    mapping given a reserved IP pool of **10.0.0.0/8** :'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Network name** | **IP Pool** | **VLAN ID** |'
  prefs: []
  type: TYPE_TB
- en: '| Management | **10.0.0.0/24** | **100** |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay and tenant | **10.10.0.0/24** | **200** |'
  prefs: []
  type: TYPE_TB
- en: '| External | **10.20.0.0/24** | **300** |'
  prefs: []
  type: TYPE_TB
- en: '| Storage | **10.30.0.0/24** | **400** |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – The OpenStack network IP addresses
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When managing OpenStack packages internally, the management interface does not
    require outbound access to the external network to reach the internet to download
    packages. The deployer instance can be connected to the management network within
    the same VLAN to install the required packages directly. External network access
    can be limited to the deployer instance and tenant network for future instances
    of external access.
  prefs: []
  type: TYPE_NORMAL
- en: As we are dealing with OpenStack services running in Docker containers, it is
    essential to take the created virtual network components under the hood into consideration.
    Once the OpenStack environment is deployed, Kolla will use the Neutron networking
    model to create a few bridges, ports, and namespaces across each node in the cluster.
    As we will see in the next configuration section, Kolla maps each network interface
    to a Neutron physical network denoted as **physnet** . Each container bridge created
    by Neutron is defined in the **neutron_bridge_name** stanza code configuration
    to set the default Kolla naming convention stated as **br_ex** . As mentioned
    earlier, the **kolla-ansible** deployment mode is highly customizable, so we can
    assign meaningful names for each bridge mapped to each network interface, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The assigned network interface will be used by Neutron to access the public
    network to allow tenants to create routers and assign floating IPs to workloads
    requiring internet access defined in the Kolla **/etc/kolla/gloabls.yml** configuration
    file by assigning the **neutron_external_interface** setting to the dedicated
    interface, **eth2** , on the compute node.
  prefs: []
  type: TYPE_NORMAL
- en: The network bonding layout is highly recommended to be configured in advance
    for each host. As discussed earlier, the bonding configuration depends on the
    tools and running software when using Linux as an operating system that varies
    from one distribution to another. Commonly used bonding configurations are the
    native Linux bonds and Open vSwitch.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonding configuration is out of the scope of this book. A good source of bonding
    configuration for Ubuntu using the kernel module can be found here: [https://help.ubuntu.com/community/UbuntuBonding](https://help.ubuntu.com/community/UbuntuBonding
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Listing the cluster specs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The initial draft of the physical layout in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Consideration* , can be considered as a starting
    point to pick up our hardware specs for each host. With slight differences between
    host roles, the following table summarizes a typical production hardware and software
    setup for each host:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hostname** | **CPU** **cores** | **RAM** | **Disk** | **Network** | **Software**
    | **Role** |'
  prefs: []
  type: TYPE_TB
- en: '| **cc01.os** | 8 | 128 GB | 250 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH, Python
    2.7 or greater, NTP client | Cloud controller |'
  prefs: []
  type: TYPE_TB
- en: '| **cn01.os** | 12 | 240 GB | 500 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH,
    Python 2.7 or greater, NTP client | Compute node |'
  prefs: []
  type: TYPE_TB
- en: '| **net01.os** | 4 | 32 GB | 250 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH, Python2.7
    or greater, NTP client | Network node |'
  prefs: []
  type: TYPE_TB
- en: '| **storage01.os** | 4 | 32 GB | 1 TB | 4x10 GB | Ubuntu 22 LTS, OpenSSH, python2.7
    or greater, NTP client | Storage Node |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – OpenStack hosts configurations
  prefs: []
  type: TYPE_NORMAL
- en: Note that OpenStack is designed to run on commodity hardware. The table did
    not mention any precise hardware model that would depend on the set budget and
    resources to kick off a first setup. As we planned, in the first chapter, to accommodate
    200 instances with the *small* flavor during our first deployment, the production
    setup should be capable of being expanded with more hardware and scale horizontally
    for each layer of the OpenStack ecosystem. The **undercloud** monitoring tools
    should regularly gather data on the performance and usage of each node to help
    in deciding the right sizing and additional resources needed to be plugged in.
    The first hosts and hardware listings should not welcome any production workload
    before arming the cluster with a layer of redundancy that will be discussed in
    detail in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly
    Available Cloud – Meeting* *the SLA* .
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the infrastructure code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the same **kolla-ansible** repository, we can customize the setup of different
    OpenStack services, including the control plane services, in the same way as highlighted
    in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking Off the OpenStack
    Setup – The Right* *Way (DevSecOps)* .
  prefs: []
  type: TYPE_NORMAL
- en: As a best practice, use a separate branch before merging to master and then
    start the deployment into the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the host groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **kolla-ansible** infrastructure code defines the target hosts and associated
    parameters such as hostnames and network IP addresses in the **/ansible/inventory**
    directory. The listing of host groups can be customized based on roles associated
    with the initial draft. The format of an inventory file is straightforward as
    highlighted in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking Off
    the OpenStack Setup – The Right Way (DevSecOps)* . The following table defines
    the different host groups in our next deployment iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **OpenStack Kolla – Ansible** **host groups** | **OpenStack** **node role**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **control** | Runs most of the OpenStack control plane, including core service
    APIs, database, and messaging queue services |'
  prefs: []
  type: TYPE_TB
- en: '| **compute** | Hosts running the **nova-compute** service |'
  prefs: []
  type: TYPE_TB
- en: '| **network** | Dedicated to network nodes running Neutron plugins and agents
    |'
  prefs: []
  type: TYPE_TB
- en: '| **haproxy** | Target hosts running HAProxy |'
  prefs: []
  type: TYPE_TB
- en: '| **storage** | Runs the Cinder volume service |'
  prefs: []
  type: TYPE_TB
- en: '| **monitoring** | Dedicated host(s) for monitoring OpenStack servers running
    Prometheus and Grafana |'
  prefs: []
  type: TYPE_TB
- en: '| **logging** | Runs and processes logs for different OpenStack services |'
  prefs: []
  type: TYPE_TB
- en: '| **deployment** | Runs the deployment of the infrastructure code using Ansible
    and Kolla |'
  prefs: []
  type: TYPE_TB
- en: '| **object** | Runs the Swift object storage server’s instances |'
  prefs: []
  type: TYPE_TB
- en: Table 3.3 – OpenStack Ansible host groups
  prefs: []
  type: TYPE_NORMAL
- en: The inventory file is highly customizable and can be adapted to your needs depending
    on whether you need to accommodate different services in a new role or separate
    more core services in dedicated hosts. Our next step will consider the deployment
    of a multi-node setup, including one dedicated host for each cloud controller,
    compute, storage, and network role, respectively. This deployment should not be
    counted as production-ready as each role will be deployed in a single node that
    does not yet meet the high-availability aspect. We will cover high availability
    and fault tolerance in OpenStack deployment in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting* *the SLA* .
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the source of truth, the deployer node where the CI/CD chain
    lives, create a new branch dedicated to a staging environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Create a new file named **multi_packtpub_prod** in the cloned repository, **/ansible/inventory**
    . A simple way to assign different host groups from a default multi-node configuration
    file can be found in the same directory, **/ansible/inventory** , named **multinode**
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration for each host group is formatted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The inventory file can be customized in different ways by assigning explicitly
    a single or several OpenStack services to run on specific hosts. That can be accomplished
    by adding the following configuration stanza:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the following configuration instructs **kolla-ansible** to install
    the Swift service on the cloud control node group assigned as control:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous configuration stanza will install and run all Swift components
    in the control host group. With a more granular configuration, it is possible
    to instruct which component will be installed in which host. If none is mentioned,
    the service will be installed in the control host group. The next example instructs
    **kolla-ansible** to install other components rather than **swift-proxy-server**
    in the host group referenced as **object_storage** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For our next iteration, the inventory section pointing to the shared services
    such as the database caching layer and messaging queue is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Core control plane services will be running on the cloud controller host group,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'More granular shared and control plane service configurations can be customized
    per service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the image service, Glance can be configured as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All compute components except **nova-compute** can be configured as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Placement service API can be configured as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The block storage API and volume scheduler service can be configured as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The object storage proxy component can be configured as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The telemetry core and notification components can be configured as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The shared services and control plane sections can be defined in different ways.
    More services and components would inherit the same configuration once the design
    of a new project has been drafted to run in the cloud controller nodes. As we
    are not covering all services in this chapter, we will stick to our second iteration
    defined earlier and more control plane services will be added throughout the next
    chapters of the book. The inventory file will be the source of truth that identifies
    the latest version of the services running in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next class of the inventory section defines the compute target nodes and
    the respective components they will be running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Optionally, as each compute node would require the collection of metrics for
    further monitoring, **collectd** will run on the compute host group in addition
    to the **ceilometer-compute** and **ipmi** components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As each compute node would interconnect with other nodes to serve tenant networks,
    a few Neutron agents, as designed in the previous section, will be running on
    the compute host group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: OVN will be discussed in detail in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Networking – Connectivity and Managed Service Options* , as being
    the most commonly used networking service in OpenStack within the latest releases,
    including Antelope.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenStack networking service is designed to run on its own host, except
    for its API, as part of the cloud control plane. The following code defines the
    Network node and associated components in the inventory file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'All Neutron services and agents except for its API will run on the network
    node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, the **manila-share** component will run on the network host group
    that will use the Neutron plugin to provide network access to the file share resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The OVN controller was configured to run on the compute node as well as in
    the network host group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**ovn-controller-network** inherits the association with network host groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the OVN mechanism uses its own database that can run on the cloud
    controller host group. The OVN database itself supports three different schemas
    that can be running on the same cloud controller host group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The next stanza of the inventory is the storage layer in which initially, a
    single storage node will be defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The storage node will run the Cinder volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the inventory file includes the deployer host itself, defined
    as localhost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Other services can be counted as additional services to install if you count
    running a custom **Zabbix agent** , for example, in all nodes or specific scanning
    tools reporting to a central hub. A simple way to do this is by creating a new
    stanza referring to all host groups defined in the inventory. For example, the
    following code section instructs Ansible to install a few tools, such as **cron**
    , and services to manage and collect logs using **fluentd** for Kolla containers
    running our OpenStack services. Optionally, **kolla-ansible** comes with **kolla-toolbox**
    to quickly run specific command lines in a Docker container once installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In a multi-node setup, make sure that the deployer machine can reach all other
    hosts to run the Ansible playbooks. It is possible to specify in the inventory
    file how Ansible should interact with each target host. In the current setup,
    SSH keys have been generated and pushed to each target host to run Ansible against
    during the deployment. Bear in mind that hostnames should be propagated in **/etc/hosts**
    in each node if IP addresses are not used in the inventory file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the file, commit it, and push it to the created branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Once the host mapping and branching configuration are properly done, it is time
    to dive deep into the **kolla-ansible** code and customize the future OpenStack
    parameters for different services.
  prefs: []
  type: TYPE_NORMAL
- en: Parameterizing the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next key file is the **globals.yml** configuration file defined in the **/etc/kolla/**
    directory. This file acts as the central configuration hub for all OpenStack environments.
    The default file that comes with the community **kolla-ansible** repository defines
    most of the OpenStack services configuration for each service option. You can
    copy and comment out the desired configuration line or generate a new one holding
    the OpenStack configuration settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **globals.yml** file in the current iteration includes the following configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Ubuntu as the Kolla base container distribution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the latest stable OpenStack release:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign an unused default VIP to one controller interface:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use KVM as the default hypervisor driver:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the Neutron Open vSwitch plugin:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enable Neutron provider networks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign a network interface for API services:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign a network interface for the Neutron external port:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the local Docker registry as created in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right* *Way (DevSecOps)* :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the file, commit, and push it to the created branch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Some configuration parameters in the **kolla-ansible** community repository
    might be updated from one OpenStack release to another, such as new configuration
    parameters in the **globals.yml** file or the repository structure itself. It
    is highly encouraged to cast an eye on the repository documentation before deploying
    a specific release. The book uses the latest stable release. Make sure to check
    the branch name and its relevant tags for the first pull.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **globals.yml** file does not provide all granular configuration settings
    by default. For example, the maximum connections of HAProxy and the maximum amount
    of memory of a Memcached instance are more specific settings that cannot be directly
    adjusted in the **globals.yml** file.
  prefs: []
  type: TYPE_NORMAL
- en: One way of conducting more advanced configuration in an OpenStack production
    environment is by going through the Ansible playbooks. Keep in mind that the **globals.yml**
    file is a high-level representation on how to deploy an OpenStack environment
    from one single hub file.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three different levels of configuring an OpenStack deployment using
    **kolla-ansible** , in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '**/etc/kolla/globals.yml** : This is the first hub of generic and custom OpenStack,
    including shared services settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**/kolla-ansible/ansible/group_vars/all.yml** : This reflects the global and
    specific settings of the OpenStack services. Once they are edited and running,
    the settings in the **all.yml** file will be merged with the **globals.yml** file.
    Settings that are not declared explicitly in the **globals.yml** file will use
    the default ones in **all.yml** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ansible playbooks** : Few cases would require the introduction of an advanced
    configuration in a specific service. Each Ansible playbook should include all
    available options for a given service or component. Ansible playbooks are located
    under **/kolla-ansible/ansible/roles** , where each OpenStack component and shared
    service available within Antelope and later releases are listed. As highlighted
    in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking Off the OpenStack
    Setup – The Right Way (DevSecOps)* ,in the discussion of how an Ansible playbook
    is constructed, adjusting a setting for a specific service is straightforward.
    For example, changing the maximum amount of memory in Memcached can be performed
    by simply updating the **memcached_max_memory** configuration stanza in the **main.yml**
    file located under the **/** **roles/memcached/defaults/** folder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing the control plane configuration is pretty straightforward by simply
    looking at the Ansible role and playbook for a given OpenStack core service. **/kolla-ansible/ansible/roles/**
    includes all sets of OpenStack core services and other projects officially incubated
    within the latest stable release. Note that there is a direct way of reflecting
    a playbook setting in the **globals.yml** or **all.yml** file.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dedicating a staging environment to run deployments before running in production
    is one of the development pillars that is most touted in discussions about code
    and artifact release stability. There is always a temptation to skip a second
    environment such as staging when deploying code, especially with resource-hungry
    environments such as OpenStack. Staging can be expensive, but thinking of the
    outcome provides a boost of confidence to mimic a production environment before
    merging that code to the master branch. A common solution to the cost challenge
    if you are not able to afford an exact copy of the production setup is to use
    a smaller version of the production environment but keep the same design layout.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following snippet, all required steps will be automated through a Jenkins
    pipeline targeting the **multi_packtpub_stg** branch. To do so, create a new Jenkins
    file targeting the staging branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Commit and push the created file to the source code repository with the new
    branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As demonstrated in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking
    Off the OpenStack Setup – The Right Way (DevSecOps)* , create a new job by selecting
    **New Item** from the top-left menu in the Jenkins user interface. Select **Pipeline**
    as the type of our job with a desired name for running the staging pipeline. Follow
    the same steps for the staging pipeline by specifying the path of the Jenkins
    file from the repository. Different build stages are visible as coded in the pipeline,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – An OpenStack staging environment deployment](img/B21716_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – An OpenStack staging environment deployment
  prefs: []
  type: TYPE_NORMAL
- en: The first build run can take longer to install the required packages and dependencies
    on the Jenkins server. Automating the deployment pipeline is very handy as orchestrating
    all steps from one single pipeline file reduces the time to run specific steps
    manually in case an error occurs in one of the pipeline stages. A Jenkins build
    can be configured to pull source code on a regular basis ( **cron** , for example)
    or trigger a build when new code is pushed and merged to the development branch.
    Note that there are multiple ways to achieve more robust deployment pipelines
    by integrating more unit tests. We have, so far, built an initial deployment pipeline.
    From it, installing additional services or making changes can be performed from
    one single place and with more control. Throughout the next chapters, we will
    stick to the same deployment approach armed with best practices by keeping deployments
    in small batches, continuous, and test-driven.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we went through a cornerstone of OpenStack deployment: the
    control plane. Identifying each component role within the control plane layer
    supports the preparation of a highly available and scalable orchestration layer
    that will we cover in more detail in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud– Meeting the SLA* . Some of the core services,
    such as Keystone, Glance, and Placement, were covered in more detail to highlight
    different options that your next production iteration can support through identity
    consolidation or an imaging backend, for example. Note that the control plane
    is not limited only to the deployed services we have explored so far in this chapter.
    The OpenStack ecosystem is still growing with additional services; hence, more
    APIs can be part of the control plane. From an architectural perspective, the
    Designate API can be part of the control plane and run in the same cloud controller,
    whereas other management components can run on a dedicated host to enhance DNS
    request performance in a large-scale environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, regardless of which option you choose, the control plane must
    be prepared for failure and proven to be scalable, as we learned in this chapter.
    Adding or removing a service unit within the control plane cluster should not
    be a disruptive operation. That was a valid reason to adopt a container-based
    deployment approach for our OpenStack production environment. The chapter extended
    the first deployment using **kolla-ansible** by going into more depth on a variety
    of control plane playbooks with additional possible advanced settings. Although
    the chapter did not cover the high-availability aspect of the control plane in
    detail, the design draft should be capable of enforcing the control plane with
    fault tolerance before exposing the cloud environment for consumption.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will extend our deployment by going through the compute layer.
    As we have separated our control plane and designed only the compute API and scheduler
    to be part of the cloud controller, the next step is to dive into the OpenStack
    compute service and uncover various options for deployment as well as features,
    including containerization options, within the latest OpenStack releases.
  prefs: []
  type: TYPE_NORMAL
