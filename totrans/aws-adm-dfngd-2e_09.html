<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Powering Analytics Using Amazon EMR and Amazon Redshift</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned about two really useful services that developers can leverage to build highly scalable and decoupled applications in the cloud: Amazon SNS and Amazon SQS.</p>
<p>In this chapter, we will be turning things up a notch and exploring two amazingly powerful AWS services that are ideal for processing and running large-scale analytics and data warehousing in the cloud: Amazon EMR and Amazon Redshift.</p>
<p>Keeping this in mind, let's have a quick look at the various topics that we will be covering in this chapter:</p>
<ul>
<li>Understanding the AWS analytics suite of services with an in-depth look at Amazon EMR, along with its use cases and benefits</li>
<li>Introducing a few key EMR concepts and terminologies, along with a quick getting started tour</li>
<li>Running a sample workload on EMR, using steps</li>
<li>Introducing Amazon Redshift</li>
<li>Getting started with an Amazon Redshift cluster</li>
<li>Working with Redshift databases and tables</li>
<li>Loading data from Amazon EMR into Amazon Redshift</li>
</ul>
<p>So without any further ado, let's get started right away!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding the AWS analytics suite of services</h1>
                </header>
            
            <article>
                
<p>With the growth of big data and its adoption across organizations on the rise, many cloud providers today provide a plethora of services that are specifically designed to run massive computations and analytics on large volumes of data. AWS is one such cloud provider that also has invested a lot into the big data and analytics paradigm with a host of services offering ready-to-use frameworks, business insights and data warehousing solutions, as well. Here is a brief explanation of the AWS analytics suite of services:</p>
<ul>
<li><strong>Amazon EMR</strong>: Amazon <strong>Elastic MapReduce</strong> or <strong>EMR</strong> is a quick and easy to use service that provides users with a scalable, managed Hadoop ecosystem and framework. You can leverage EMR to process vast amounts of data without having to worry about configuring the underlying Hadoop platform. We will be learning and exploring more on EMR in the subsequent sections of this chapter.</li>
<li><strong>Amazon Athena</strong>: Amazon Athena takes big data processing up a notch by providing a standard SQL interface for querying data that is stored directly on Amazon S3. With Athena, you do not have any underlying hardware to manage or maintain; it is all managed by AWS itself. This <em>serverless</em> approach makes Athena ideal for processing data that does not require any complex ETL processing. All you need to do is create a schema, point Athena to your data on Amazon S3, and start querying it using simple SQL syntax.</li>
<li><strong>Amazon Elasticsearch Service</strong>: Amazon Elasticsearch Service provides a managed deployment of the popular open source search and analytics engine: Elasticsearch. This service comes in really handy when you wish to process streams of data originating from various sources such as logs generated from instances, and so on.</li>
<li><strong>Amazon Kinesis</strong>: Unlike the other services discussed so far, Amazon Kinesis is more of a streaming service provided by AWS. You can use Amazon Kinesis to push vast amounts of data originating from multiple sources, into one or more streams that can be consumed by other AWS services for performing analytics and other data processing processes.</li>
<li><strong>Amazon QuickSight</strong>: Amazon QuickSight is an extremely cost-effective business insights solution that can be used to perform fast ad hoc analysis on data.</li>
<li><strong>Amazon Redshift</strong>: Amazon Redshift is a petabyte-scale data warehousing solution provided by AWS that you can leverage for analyzing your data, using an existing set of tools. We will be learning more about Redshift a bit later during this chapter. The services are depicted here:</li>
</ul>
<div class="mce-root CDPAlignCenter CDPAlign"><img style="font-size: 1em" height="781" width="1349" src="Images/18d0ba95-6fe1-473d-84e2-f3aab87c99c3.png"/></div>
<ul>
<li><strong>AWS Data Pipeline:</strong> Moving large amounts of data between AWS services can be difficult to perform, especially when the data sources vary. AWS Data Pipeline makes it easier to transfer data between different AWS storage and compute services, as well as helping in the initial transformation and processing of data. You can even use Data Pipeline to transfer data reliably from an on-premise location into AWS storage services, as well.</li>
<li><strong>AWS Glue</strong>: AWS Glue is a managed <strong>ETL</strong> (<strong>Extract</strong>, <strong>Transform</strong> and <strong>Load</strong>) service recently launched by AWS. Using AWS Glue greatly simplifies the process of preparing, extracting, and loading data from large datasets into an AWS storage service.</li>
</ul>
<p>With this brief overview of the AWS analytics suite of services, let's now move forward and get started with understanding a bit more about Amazon EMR!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introducing Amazon EMR</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, Amazon EMR is a managed service that provides big data analytics frameworks, such as Apache Hadoop and Apache Spark straight out of the box and ready for use. Using Amazon EMR, you can easily perform a variety of use cases such as batch processing, big data analytics, low-latency querying, data streaming, or even use EMR as a large datastore itself!</p>
<p>With Amazon EMR, there is very little underlying infrastructure to manage on your part. You simply have to decide the number of instances you initially want to run your EMR cluster on and start consuming the framework for analytics and processing. Amazon EMR provides you with features that enable you to scale your infrastructure based on your requirements, without affecting the existing setups. Here is a brief look at some of the benefits that you can obtain by leveraging Amazon EMR for your own workloads:</p>
<ul>
<li><strong>Pricing</strong>: Amazon EMR relies on EC2 instances to spin up your Apache Hadoop or Apache Spark clusters. Although you can vary costs by selecting the instance types for your cluster from large to extra large and so on, the best part of EMR is that you can also opt between using a combination of on-demand EC2 instances, reserved and spot instances based on your setup, thus providing you with flexibility at significantly lower costs.</li>
<li><strong>Scalability</strong>: Amazon EMR provides you with a simple way of scaling running workloads, depending on their processing requirements. You can resize your cluster or its individual components as you see fit and additionally, configure one or more instance groups for a guaranteed instance availability and processing.</li>
<li><strong>Reliability</strong>: Although you, as an end user, have to specify the initial instances and their sizes, AWS ultimately ensures the reliability of the cluster by swapping out instances that either have failed or are going to in the due course of time.</li>
<li><strong>Integration</strong>: Amazon EMR integrates with the likes of other AWS services to provide your cluster with additional storage, network, and security requirements. You can use services such as Amazon S3 to store both the input as well as the output data, AWS CloudTrail for auditing the requests made to EMR, VPC to ensure the security of your launched EMR instances and much more!</li>
</ul>
<p>With these details in mind, let's move an inch closer to launching our very own EMR cluster by first visiting some of its key concepts and terminologies.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Concepts and terminologies</h1>
                </header>
            
            <article>
                
<p>Before we get started with Amazon EMR, it is important to understand some of its key concepts and terminologies, starting out with clusters and nodes:</p>
<ul>
<li><strong>Clusters</strong>: Clusters are the core functioning component in Amazon EMR. A cluster is a group of EC2 instances that together can be used to process your workloads. Each instance within a cluster is termed as a node and each node has a different role to perform within the cluster.</li>
</ul>
<ul>
<li><strong>Nodes</strong>: Amazon EMR distinguishes between clusters instances by providing them with one of these three roles:
<ul>
<li><strong>Master node</strong>: An instance that is responsible for the overall manageability, working and monitoring of your cluster. The <em>master node</em> takes care of all the data and task distributions that occur within the cluster.</li>
<li><strong>Core node</strong>: The core nodes are very similar to the master node; however, they are primarily used to run tasks and store data on your <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>). The core node can also contain some additional software components of Hadoop applications within itself.</li>
<li><strong>Task node</strong>: Task nodes are only designed to run tasks. They do not contain any additional software components of Hadoop applications within themselves and are optional when it comes to the cluster's deployment.</li>
</ul>
</li>
<li><strong>Steps</strong>: Steps are simple tasks or jobs that are submitted to a cluster for processing. Each step contains some instructions on how the particular job is to be performed. Steps can be ordered such that a particular step can be used to fetch the input data from Amazon S3, while a second step can be used to run a Pig or Hive query against it, and finally a third step to store output data to say Amazon DynamoDB. If one step fails, the subsequent steps are automatically cancelled from execution, however, you can choose to overwrite this behavior by selecting your steps to ignore failures and process further.</li>
</ul>
<p>Apart from these concepts, you will additionally be required to brush up on your Apache Hadoop framework and terminologies, as well. Here's a quick look at some of the Apache frameworks and applications that you will come across while working with Amazon EMR:</p>
<ul>
<li><strong>Storage</strong>: A big part of EMR is how the data is actually stored and retrieved. The following are some of the storage options that are provided to you while using Amazon EMR:
<ul>
<li><strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>): As the name suggests, HDFS is a distributed and scalable filesystem that allows data to be stored across the underlying node instances. By default, the data is duplicated and stored across the instances present in the cluster. This provides high availability and data resiliency in case of an instance failure. You can read more about HDFS at: <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html</a>.</li>
<li><strong>EMR File System</strong> (<strong>EMRFS</strong>): EMRFS is an extension of the HDFS filesystem, using which you can access and store data directly on Amazon S3, just as a normal filesystem.</li>
<li><strong>Local filesystem</strong>: Apart from HDFS, each instance within the cluster is also provided with a small block of pre-attached ephemeral disks which is also called the local filesystem. You can use this local filesystem to store additional software or applications required by your Hadoop frameworks.</li>
</ul>
</li>
<li><strong>Frameworks</strong>: As mentioned before, Amazon EMR provides two data processing frameworks that you can leverage based on your processing needs: Apache Hadoop MapReduce and Apache Spark:
<ul>
<li><strong>Apache Hadoop MapReduce</strong>: MapReduce is by far the most commonly used and widely known programming model when it comes to building distributed applications. The open source model relies on a <kbd>Mapper</kbd> function that maps the data to sets of key-value pairs and a <kbd>Reducer</kbd> function that combines these key-value pairs, applies some additional processing, and finally generates the desired output. To know more about MapReduce and how you can leverage it, check out this URL: <a href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html">https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html</a>.</li>
<li><strong>Apache Spark</strong>: Apache Spark is a fast, in-memory data processing model using which a developer can process streaming, machine learning or SQL workloads that require fast iterative access to datasets. It is a cluster framework similar to Apache Hadoop; however, Spark leverages graphs and in-memory databases for accessing your data. You can read more about Spark at <a href="https://spark.apache.org/">https://spark.apache.org/</a>.</li>
</ul>
</li>
<li><strong>Applications and programs</strong>: With the standard data processing framework, Amazon EMR also provides you with additional applications and programs that you can leverage to build native distributed applications. Here's a quick look into a couple of them:
<ul>
<li><strong>YARN</strong>: <strong>Yet Another Resource Negotiator</strong>, is a part of the Hadoop framework and provides management for your cluster's data resources</li>
<li><strong>Hive</strong>: Hive is a distributed data warehousing application that leverages standard SQL to query extremely large datasets stored on the HDFS filesystem.</li>
</ul>
</li>
</ul>
<p>There are yet many other applications and programs made available for use by Amazon EMR, such as Apache Pig, Apache HBase, Apache Zookeeper, and so on. In the next section, we will be looking at how to leverage these concepts and terminologies to create our very own Amazon EMR Cluster, so let's get busy!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting started with Amazon EMR</h1>
                </header>
            
            <article>
                
<p>With the basics covered, in this section we will be working with the Amazon EMR dashboard to create our very first cluster. However, before we get going, here's a small list of prerequisite steps that we need to complete first.</p>
<p>To begin with, we will need to create an Amazon S3 bucket that will be used to store the output, logs generated by EMR, as well as some additional script and software files:</p>
<ol>
<li>From the AWS Management Console, filter and select the <span class="packt_screen">Amazon S3</span> service by using the <span class="packt_screen">Filter</span> option. Alternatively, launch the Amazon S3 dashboard by navigating to this URL: <a href="https://s3.console.aws.amazon.com/s3/">https://s3.console.aws.amazon.com/s3/</a>.</li>
<li>Next, select the <span class="packt_screen">Create bucket</span> option. In the <span class="packt_screen">Create bucket</span> wizard, provide a suitable <span class="packt_screen">Bucket name</span> followed by the selection of an appropriate <span class="packt_screen">Region</span> to create the bucket in. For this use case, the EMR cluster, as well as the S3 buckets, are created in the <strong>US East (Ohio)</strong> region, however you can select an alternative based on your requirements. Click on <span class="packt_screen">Next</span> to continue with the process.</li>
<li>On the <span class="packt_screen">Set properties</span> page, you can optionally choose to provide some <em>tags</em> for your bucket for cost allocations and tracking purposes. Click <span class="packt_screen">Next</span> to continue.</li>
<li>In the <span class="packt_screen">Set permissions</span> page, ensure that the no public read access is granted to the bucket. Click on <span class="packt_screen">Next</span> to review the settings and finally, select <span class="packt_screen">Create bucket</span> to complete the process.</li>
<li>Once the bucket is created, use the <span class="packt_screen">Create folder</span> option to create dedicated folders for storing the logs, output, as well as some additional scripts that we might use in the near future. Here is a representational screenshot of the bucket after you have completed all of the previous steps:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="231" width="560" src="Images/e2af9cde-bd0b-4a0b-9f3d-bdd10f47210c.png"/></div>
<ol start="6">
<li>With the bucket created and ready for use, the next prerequisite item left to create is a key pair using which you can SSH into your EC2 instances. Ensure that the key pair is created in the same region (<strong>US East (Ohio)</strong> in this case) as your EMR cluster.</li>
</ol>
<p>Now that the prerequisites are out of the way, we can finally get started with our EMR cluster setup!</p>
<ol>
<li>From the AWS Management Console, filter and select the <span class="packt_screen">Amazon EMR</span> service by using the <span class="packt_screen">Filter</span> option. Alternatively, launch the Amazon EMR dashboard by selecting this URL: <a href="https://us-east-2.console.aws.amazon.com/elasticmapreduce/home">https://us-east-2.console.aws.amazon.com/elasticmapreduce/home</a>.</li>
</ol>
<ol start="2">
<li>Since this is the first time we've created an EMR cluster, select the <span class="packt_screen">Create cluster</span> option to get started.</li>
<li>You can configure your EMR cluster using two ways: a fast and easy <span class="packt_screen">Quick Options</span> which is shown to you by default, and an <span class="packt_screen">Advanced options</span> page where you can select and configure the individual items for your cluster. In this case, we will go ahead and select <span class="packt_screen">Go to advanced options</span>.</li>
<li>The Advanced options page provides us with a four-step wizard that essentially guides us to configuring a fully functional EMR cluster. To begin with, the first step is where you can select and customize the <em>software</em> that you wish to install on your EMR cluster.</li>
<li>From the <span class="packt_screen">Release</span> drop-down list, select the appropriate EMR release that you would like to work with. The latest version released as of writing this book is <kbd>emr-5.11.1</kbd>. Each release contains several distributed applications available for installation on your cluster. For example, selecting emr-5.11.1 which is a 2018 release, contains Hadoop v2.7.3, Flink v1.3.2, Ganglia v3.7.2, HBase v1.3.1, and many other such applications and software.</li>
</ol>
<div style="padding-left: 60px" class="packt_infobox">For a complete list of available EMR releases and their associated software versions, go to <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html</a>.</div>
<ol start="6">
<li>In this case, I have gone ahead and selected the basic applications that we will be requiring for this scenario, including Hadoop, Hive and Hue. Feel free to select other applications as per your requirements.</li>
</ol>
<ol start="7">
<li>The next couple of sections are optional, however, it is important to know their purpose:
<ul>
<li><strong>AWS Glue Data Catalog settings</strong>: With EMR version 5.8.0 and above, you optionally have the choice to configure Spark SQL to use the AWS Glue Data Catalog (an external Hive table) as its metastore.</li>
<li><strong>Edit software settings</strong>: You can use this option to override the default configuration settings for certain applications. This is achieved by providing a configuration object in the form of a JSON file. You can either <span class="packt_screen">Enter configuration</span> or <span class="packt_screen">Load JSON from S3</span> as well:</li>
</ul>
</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="421" width="758" src="Images/bc0d8804-5937-4c76-8614-3a2621dea03f.png"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Add steps</strong>: The final optional parameter left on the <span class="packt_screen">Software Configuration</span> page is the <em>add steps</em>. As discussed briefly earlier in this chapter, steps are essentially a unit of work that we submit to the cluster. This can be something as trivial as loading input data from S3, or processing and running a MapReduce job on the data. We will be exploring steps a little more in detail a bit later in this chapter, so leave this field to its default value and select <span class="packt_screen">Next</span> to continue with the process.</li>
</ul>
</li>
</ul>
<ol start="8">
<li>The second step in the <span class="packt_screen">Advanced options</span> wizard is configuring the cluster's hardware, or the instance configurations, as well as the cluster's networking.</li>
</ol>
<p style="padding-left: 90px">EMR provides two options: instance fleets and instance groups; both explained briefly here:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Instance fleets</strong>: Instance fleets allows you to specify a target capacity for the instances present in a cluster. With this option, you get the widest variety of instance provisioning options where you can leverage mixed instance types for your nodes, and even go for different purchasing options for the same. With each instance fleet that created, you get to establish a target capacity for on-demand, as well as for spot instances.</li>
</ul>
</li>
</ul>
<div style="padding-left: 60px" class="packt_infobox">You can have only one instance fleet per node type (master, core, task).</div>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Instance groups</strong>: Instance groups on the other hand do not offer many custom configurable options per node type. In instance groups, each node consists of the same instance type and the same purchasing option, as well. Once these settings are configured during the cluster's creation, they cannot be altered; however, you can always add more instances as you see fit.</li>
</ul>
</li>
</ul>
<ol start="9">
<li>For this particular use case, we are going to go ahead and select <span class="packt_screen">Uniform instance groups,</span> as depicted in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="293" width="740" src="Images/6a18bcf6-788e-4f31-84a4-b95cef2065d8.png"/></div>
<ol start="10">
<li>Next, from the <span class="packt_screen">Network</span> drop-down list, select the appropriate <em>VPC</em> in which you wish to launch your EMR cluster. You can alternatively choose to create a new VPC specifically for EMR, using the adjoining <span class="packt_screen">Create a VPC</span> option.</li>
<li>Similarly, select the appropriate subnet from the <span class="packt_screen">EC2 Subnet</span> drop-down list.</li>
<li>Finally, assign a value for the <span class="packt_screen">Root device EBS volume size</span> that will be provisioned for each instance in the cluster. You can provide values between 10 GB and 100 GB.</li>
<li>Using the edit options provided, you can additionally configure the <span class="packt_screen">Instance type</span>, the <span class="packt_screen">Instance count</span> as well as the <span class="packt_screen">Purchasing option</span> for each node type, as depicted in the following screenshot. Note that these options are provided because we selected instance groups as our preferred mode of instance configurations. The options will vary if the <span class="packt_screen">Instance Fleet</span> option is selected:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="383" width="993" src="Images/ee8245bb-2d24-4907-be69-600417084e67.png"/></div>
<ol start="14">
<li>You can additionally choose to enable autoscaling for the <span class="packt_screen">Core</span> and <span class="packt_screen">Task</span> nodes by selecting the <span class="packt_screen">Not enabled</span> option under the <span class="packt_screen">Auto scaling</span> column. Subsequently, you can add additional task instance groups by selecting the <span class="packt_screen">Add task instance group</span> option, as well. Once done, select the <span class="packt_screen">Next</span> option to proceed with the set up.</li>
<li>The third step in the <span class="packt_screen">Advanced options</span> provides general configurations that you can set, based on your requirements. To start off, provide a suitable <span class="packt_screen">Cluster name</span> followed by selecting the <span class="packt_screen">Logging</span> option for your EMR cluster. Use the folder option to browse to our newly created S3 bucket, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/aa154fd4-d9ef-41bd-9df1-130767cb5bb3.png" width="758" height="229"/></div>
<ol start="16">
<li>You can additionally enable the <span class="packt_screen">Termination protection</span> option to prevent against accidental deletions of your cluster.</li>
<li>Moving on, the final configuration item left on the cluster's <span class="packt_screen">General Options</span> page is the <span class="packt_screen">Bootstrap Actions</span>. Bootstrap actions as the name implies are certain scripts or code that you wish to execute on your cluster's instances at the time of booting up. This feature thus comes in very handy when you have to add new instances to an existing running cluster.</li>
</ol>
<div style="padding-left: 60px" class="packt_infobox">Bootstrap actions are executed using the Hadoop user by default. You can switch to root privileges by using the <kbd>sudo</kbd> command.</div>
<p style="padding-left: 90px">There are two types of Bootstrap actions that you can execute on your instances:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><span class="packt_screen">Run if</span>: The <span class="packt_screen">Run if</span> action executes an action when an <em>instance-specific</em> value is found in either the <kbd>instance.json</kbd> or the <kbd>job-flow.json</kbd> file. This is a predefined bootstrap action and comes in very handy when you only want to execute the action on a particular type of instance, for example, execute the bootstrap action only if the instance type is <kbd>master</kbd>.</li>
<li><span class="packt_screen">Custom action</span>: Custom actions leverage your own scripts to perform a customized bootstrap action.</li>
</ul>
</li>
</ul>
<ol start="18">
<li>To create a bootstrap action, select the <span class="packt_screen">Configure and add</span> option from the <span class="packt_screen">Add Bootstrap Action</span>. Make sure the <span class="packt_screen">Run if</span> action is selected before proceeding.</li>
</ol>
<ol start="19">
<li>This will bring up the <span class="packt_screen">Add Bootstrap Action</span> dialog as depicted in the following screenshot. Type in a suitable <span class="packt_screen">Name</span> for your <span class="packt_screen">Run if</span> action. Since the <span class="packt_screen">Run if</span> action is a predefined bootstrap action, the script's location is not an editable field. You can, however, add <span class="packt_screen">Optional arguments</span> for the script, as shown here. In this case, the <span class="packt_screen">Run if</span> action will only echo the message if the instance is a <strong>master</strong>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/9d1acc81-06a7-49d4-acbc-861e2219d5cd.png" width="598" height="266"/></div>
<ol start="20">
<li>Click on <span class="packt_screen">Add</span> once done. Similarly, you can add your custom bootstrap actions as well, by placing the executable scripts in the Amazon S3 bucket that we created during the prerequisite phase of this chapter and providing that path here.</li>
<li>Moving on to the final step in this cluster creation process, on the <span class="packt_screen">Security Options</span> page, you can review the various permissions, roles, authentication, and encryption settings that the cluster will use once it's deployed. Start off by selecting the <span class="packt_screen">EC2 key pair</span> that we created at the start of this chapter. You can additionally opt to change the <span class="packt_screen">Permissions</span> or use the default ones provided.</li>
<li>Once done, click on <span class="packt_screen">Create cluster</span> to complete the process.</li>
</ol>
<p>The cluster's creation takes a couple of minutes, depending on the number of instances selected for the cluster, as well as the software identified to be installed. Once done, you can use the EMR dashboard to view the cluster's health status and other vital information.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Connecting to your EMR cluster</h1>
                </header>
            
            <article>
                
<p>Once you have provisioned the EMR cluster, you should see its state change from <span class="packt_screen">Starting</span> to <span class="packt_screen">Bootstrapping</span> to finally into a <span class="packt_screen">Running</span> state. If you do not have any jobs currently executing, then your cluster may go into a <span class="packt_screen">Waiting</span> state as well. Here, you can now start using the EMR cluster for running your various jobs and analysis. But before that, here's a quick introduction of a few ways in which you can connect to your running EMR cluster.</p>
<p>First up, connecting to the master node using a simple SSH. Connecting to the master node via SSH can be used for monitoring the cluster, viewing Hadoop's log flies or for even running an interactive shell for Hive or Pig programming:</p>
<ol>
<li>To do so, log in to your Amazon EMR dashboard and select your newly created cluster's name from the <span class="packt_screen">Cluster list</span> page. This will display the clusters <span class="packt_screen">Details</span> page where you can manage, as well as monitor your cluster.</li>
<li>Next, copy the <span class="packt_screen">Master public DNS</span> address. Once copied, open up a PuTTY Terminal and paste the copied public DNS in the <span class="packt_screen">Host Name (or IP Address)</span> field.</li>
<li>Convert the key pair that you associated with this EMR cluster into a private key and attach that private key in PuTTY by selecting the <span class="packt_screen">Auth</span> option present under the <span class="packt_screen">SSH</span> section.</li>
<li>Once done, click on <span class="packt_screen">Open</span> to establish the connection. At the certificate dialog, accept the certificate and type in <kbd>Hadoop</kbd> as the username when prompted. You should get SSH access into your cluster's master node now!</li>
</ol>
<p>The same task can be performed using the AWS CLI as well:</p>
<ol>
<li>From the Terminal, first type in the following command to retrieve the running cluster's ID. The cluster's ID will be in this format <kbd>j-XXXXXXXX</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong># aws emr list-clusters</strong> </pre>
<ol start="2">
<li>To list the instances running in your cluster, use the cluster ID obtained from the previous command's output in the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong># aws emr list-instances --cluster-id &lt;CLUSTER_ID&gt;</strong></pre>
<div style="padding-left: 60px" class="packt_infobox">Copy the <kbd>PublicDnsName</kbd> value from the output of this command. You can then use the following set of commands to get access to your master node.</div>
<ol start="3">
<li>Ensure that the cluster's private key has the necessary permissions:</li>
</ol>
<pre style="padding-left: 60px"><strong># chmod 400 &lt;PRIVATEKEY.pem&gt;</strong></pre>
<ol start="4">
<li>Once done, SSH to the master node using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong># ssh hadoop@&lt;PUBLIC_DNS_NAME&gt; -i &lt;PRIVATEKEY.pem&gt;</strong></pre>
<p>You can additionally connect to the various application web interfaces, such as <em>Hue</em> or the <em>Hadoop HDFS NameNode,</em> using a few simple steps:</p>
<ol>
<li>To get started, you will once again require the public DNS name of your master node. You can obtain that from the EMR dashboard or by using the CLI steps we just walked through.</li>
<li>Next, using PuTTY , paste the public DNS name in the <span class="packt_screen">Host Name (or IP Address)</span> field as done earlier. Browse and load the private key using the <span class="packt_screen">Auth</span> option as well.</li>
<li>Under the <span class="packt_screen">SSH</span> option from PuTTY's navigation pane, select <span class="packt_screen">Tunnels</span>.</li>
<li>Fill in the required details as mentioned in the following list:
<ul>
<li>Set source port field to <kbd>8157</kbd></li>
<li>Enable the <span class="packt_screen">Dynamic</span> and <span class="packt_screen">Auto</span> options</li>
</ul>
</li>
<li>Once completed, select <span class="packt_screen">Add</span> and finally <span class="packt_screen">Open</span> the connection.</li>
</ol>
<p>This form of tunnelling or port forwarding is essential as the web interfaces can only be viewed from the master node's local web server. Once completed, launch your favorite browser and view the respective web interfaces, as given here:</p>
<ul>
<li>For accessing Hue, type in the following in your web browser:</li>
</ul>
<pre style="padding-left: 60px">http://&lt;PUBLIC_DNS_NAME&gt;:8888/</pre>
<ul>
<li>For accessing the Hadoop HDFS NameNode, type in the following:</li>
</ul>
<pre style="padding-left: 60px">http:// &lt;PUBLIC_DNS_NAME&gt;::50070/</pre>
<div class="CDPAlignCenter CDPAlign"><img src="Images/cad43e66-7ae7-415f-a84f-50ece801869f.png" width="792" height="325"/></div>
<p>You can even use the CLI to create a tunnel. To do so, substitute the public DNS name and the private key values in the following command:</p>
<pre><strong># ssh -i &lt;PRIVATEKEY.pem&gt; -N -D 8157 hadoop@&lt;PUBLIC_DNS_NAME&gt;</strong> </pre>
<div class="packt_infobox">The <kbd>-D</kbd> flag indicates that the port forwarding is dynamic.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running a job on the cluster</h1>
                </header>
            
            <article>
                
<p>With the connectivity established, you can now execute jobs as one or more steps on your cluster. In this section, we will be demonstrating the working of a step using a simple example which involves the processing of a few Amazon CloudFront logs. The details of the sample data and script can be found at: <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-prepare-data-and-script.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-prepare-data-and-script.html</a>. You can use similar techniques and bases to create and execute your own jobs as well:</p>
<ol>
<li>To get started with a job, from the EMR dashboard select your cluster's name from the <span class="packt_screen">Cluster list</span> page. This will bring up the newly created clusters details page. Here, select the <span class="packt_screen">Steps</span> tab.</li>
</ol>
<ol start="2">
<li>Since this is going to be our first step, go ahead and click on the <span class="packt_screen">Add step</span> option. This brings up the <span class="packt_screen">Add step</span> dialog as shown in the following screenshot. Fill in the required information as described and, once all the fields are filled in, click on <span class="packt_screen">Add</span> to complete the step's creation:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/389a4fb3-1fe5-4c5b-a0ab-0d1baaaa8466.png" width="774" height="413"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li><span class="packt_screen">Step type</span>: You can choose between various options such as <span class="packt_screen">Streaming program</span> which essentially will prompt you to provide <kbd>Mapper</kbd> and <kbd>Reducer</kbd> function details, or alternatively, you can also select <span class="packt_screen">Hive program</span>, <span class="packt_screen">Pig program</span>, <span class="packt_screen">Spark program</span> or a <span class="packt_screen">Custom application</span>. In this case, we select the <span class="packt_screen">Hive program</span> option.</li>
<li><span class="packt_screen">Name</span>: A suitable name for your step.</li>
<li><span class="packt_screen">Script S3 location</span>: Provide the Hive script's location here. Since we are using a predefined script, simply replace the <kbd>&lt;REGION&gt;</kbd> field with your EMR's operating region: <kbd>s3://&lt;REGION&gt;.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q</kbd>.</li>
<li><span class="packt_screen">Input S3 location</span>: Provide the input data file's location here. Replace the <kbd>&lt;REGION&gt;</kbd> placeholder with your EMR's operating region as done before: <kbd>s3://&lt;REGION&gt;.elasticmapreduce.samples</kbd>.</li>
<li><span class="packt_screen">Output S3 location</span>: Specify where the processed output files have to be stored. In this case, I'm using the custom S3 bucket that we created as a prerequisite step during the EMR cluster creation. You can provide any other alternative bucket as well.</li>
<li><span class="packt_screen">Arguments</span>: You can use this field to provide any optional arguments required by the script to run. In this case, copy, and paste the following <kbd>-hiveconf hive.support.sql11.reserved.keywords=false</kbd>.</li>
<li><span class="packt_screen">Action on failure</span>: You can optionally choose what EMR should do in case the step's execution undergoes a failure. In this case, we have selected the default <span class="packt_screen">Continue</span> value.</li>
</ul>
</li>
</ul>
<ol start="3">
<li>Once the required fields are filled out, click on <span class="packt_screen">Add</span> to complete the process.</li>
</ol>
<p>The step now starts executing the supplied script on the EMR cluster. You can view the progress by viewing the changes in the step's status from <span class="packt_screen">Pending</span> to <span class="packt_screen">Running</span> to <span class="packt_screen">Completed,</span> as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/ff0fe102-f8c0-4afe-b976-dbbe1629343a.png" width="804" height="201"/></div>
<p>Once the job completes its execution, head back to your Amazon S3's output bucket and view the output of the processing. In this case, the output contains the number of access requests made to CloudFront, sorted by the operating system.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Monitoring EMR clusters</h1>
                </header>
            
            <article>
                
<p>The EMR dashboard provides a rich feature set using which you can manage and monitor your EMR clusters all from one place. You can additionally view logs and leverage Amazon CloudWatch as well to track the performance of your cluster.</p>
<p>In this section, we will be looking at a few simple ways using which you can monitor your EMR clusters. To start off, let's look at how to monitor the status of your cluster using the EMR dashboard:</p>
<ol>
<li>From the EMR dashboard, select your cluster name from the cluster list page. This will bring up the newly created cluster's details page. Here, select the <span class="packt_screen">Events</span> tab, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="200" width="1020" src="Images/1dc7acf7-44de-4553-ab9f-c7a2326fff89.png"/></div>
<p style="padding-left: 90px">The <span class="packt_screen">Events</span> tab allows you to view the event logged by your cluster. You can use this to view events generated by the cluster, by running applications, by step execution and much more.</p>
<ol start="2">
<li>The dashboard also provides an in-depth look into the performance of the cluster over a period. To view the performance indicators, select the <span class="packt_screen">Monitoring</span> tab from the cluster's <span class="packt_screen">Details</span> page.</li>
</ol>
<p style="padding-left: 90px">Here, you can view essential details and status about your cluster, the running nodes, as well as the underlying I/O and data storage.</p>
<ol start="3">
<li>Alternatively, you can also use Amazon CloudWatch to view and monitor the cluster's various metrics. To do so, launch the Amazon CloudWatch dashboard by selecting this URL: <a href="https://console.aws.amazon.com/cloudwatch/home">https://console.aws.amazon.com/cloudwatch/home</a>.</li>
</ol>
<ol start="4">
<li>Next, from the navigation pane, select the <span class="packt_screen">Metrics</span> option to view all the metrics associated with EMR. Use the <kbd>JobFlowID</kbd> dimension to filter the EMR cluster in case you have multiple clusters running in the same environment.</li>
</ol>
<p style="padding-left: 90px">Here is a list of some important EMR metrics worth monitoring:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Metric name</strong></p>
</td>
<td>
<p><strong>Metric description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>AppsFailed</kbd></p>
</td>
<td>
<p>The number of applications submitted to the EMR cluster that have failed to complete. This application status is monitored internally and reported by YARN.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>MRUnhealthyNodes</kbd></p>
</td>
<td>
<p>The number of nodes available to MapReduce jobs marked in an <kbd>UNHEALTHY</kbd> state.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>MRLostNodes</kbd></p>
</td>
<td>
<p>The number of nodes allocated to MapReduce that have been marked in a <kbd>LOST</kbd> state.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>CorruptBlocks</kbd></p>
</td>
<td>
<p>The number of blocks that HDFS reports as corrupted.</p>
</td>
</tr>
</tbody>
</table>
<div style="padding-left: 60px" class="packt_infobox">You can view the complete list of monitored metrics at: <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html</a>.</div>
<ol start="5">
<li>Once a Metric is identified, select the <span class="packt_screen">Metric</span> and click on the <span class="packt_screen">Graphed metrics</span> tab. Here, select the <span class="packt_screen">Create alarm</span> option provided under the <span class="packt_screen">Actions</span> column to create and set an alarm threshold, as well as its corresponding action.</li>
</ol>
<p>In this way, you can also leverage Amazon CloudWatch events to periodically monitor the events generated by the cluster. Remember, however, that EMR tracks and records events only for a period of seven days. With this, we come to the end of this particular section and EMR, as well. In the next section, we will be learning and exploring a bit about yet another awesome analytics service called Amazon Redshift!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introducing Amazon Redshift</h1>
                </header>
            
            <article>
                
<p>Amazon Redshift is one of the <strong>database as a service</strong> (<strong>DBaaS</strong>) offerings from AWS that provides a massively scalable data warehouse as a managed service, at significantly lower costs. The data warehouse is based on the open source PostgreSQL database technology however; not all features offered in PostgreSQL are present in Amazon Redshift. Here's a look at some of the essential concepts and terminologies that you ought to keep in mind when working with Amazon Redshift:</p>
<ul>
<li><strong>Clusters</strong>: Just like Amazon EMR, Amazon Redshift too relies on the concept of clusters. Clusters here are logical containers containing one or more instances or compute nodes, and one leader node that is responsible for the cluster's overall management. Here's a brief look at what each node provides:
<ul>
<li><strong>Leader node</strong>: The leader node is a single node present in a cluster that is responsible for orchestrating and executing various database operations, as well as facilitating communication between the database and associate client programs.</li>
<li><strong>Compute node</strong>: Compute nodes are responsible for executing the code provided by the leader node. Once executed, the compute nodes share the results back to the leader node for aggregation. Amazon Redshift supports two types of compute nodes: dense storage nodes and dense compute nodes. The dense storage nodes provide standard hard disk drives for creating large data warehouses; whereas, the dense compute nodes provide higher performance SSDs. You can start off by using a single node that provides 160 GB of storage and scale up to petabytes by leveraging one or more 16 TB capacity instances as well.</li>
</ul>
</li>
<li><strong>Node slices</strong>: Each compute node is partitioned into one or more smaller chunks or slices by the leader node, based on the cluster's initial size. Each slice contains a portion of the compute nodes memory, CPU and disk resource, and uses these resources to process certain workloads that are assigned to it. The assignment of workloads is again performed by the leader node.</li>
<li><strong>Databases</strong>: As mentioned earlier, Amazon Redshift provides a scalable database that you can leverage for a data warehouse, as well as analytical purposes. With each cluster that you spin in Redshift, you can create one or more associated databases with it. The database is based on the open source relational database PostgreSQL (v8.0.2) and thus, can be used in conjunction with other RDBMS tools and functionalities. Applications and clients can communicate with the database using standard PostgreSQL JDBC and ODBC drivers.</li>
</ul>
<p>Here is a representational image of a working data warehouse cluster powered by Amazon Redshift:</p>
<div class="CDPAlignCenter CDPAlign"><img height="633" width="993" src="Images/1c270279-61dc-49d6-8ebd-1027dd8c20fa.png"/></div>
<p>With this basic information in mind, let's look at some simple and easy to follow steps using which you can set up and get started with your Amazon Redshift cluster.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting started with Amazon Redshift</h1>
                </header>
            
            <article>
                
<p>In this section, we will be looking at a few simple steps which you can take to have a fully functioning Amazon Redshift cluster up and running in a matter of minutes:</p>
<ol>
<li>First up, we have a few prerequisite steps that need to be completed before we begin with the actual set up of the Redshift cluster. From the AWS Management Console, use the <span class="packt_screen">Filter</span> option to filter out <span class="packt_screen">IAM</span>. Alternatively, you can also launch the <span class="packt_screen">IAM</span> dashboard by selecting this URL: <a href="https://console.aws.amazon.com/iam/">https://console.aws.amazon.com/iam/</a>.</li>
</ol>
<ol start="2">
<li>Once logged in, we need to create and assign a role that will grant our Redshift cluster read-only access to Amazon S3 buckets. This role will come in handy later on in this chapter when we load some sample data on an Amazon S3 bucket and use Amazon Redshift's <kbd>COPY</kbd> command to copy the data locally into the Redshift cluster for processing. To create the custom role, select the <span class="packt_screen">Role</span> option from the <span class="packt_screen">IAM</span> dashboards' navigation pane.</li>
<li>On the <span class="packt_screen">Roles</span> page, select the <span class="packt_screen">Create role</span> option. This will bring up a simple wizard using which we will create and associate the required permissions to our role.</li>
<li>Select the <span class="packt_screen">Redshift</span> option from under the <span class="packt_screen">AWS Service</span> group section and opt for the <span class="packt_screen">Redshift - Customizable</span> option provided under the <span class="packt_screen">Select your use case</span> field. Click <span class="packt_screen">Next</span> to proceed with the set up.</li>
<li>On the <span class="packt_screen">Attach permissions policies</span> page, filter and select the <span class="packt_screen">AmazonS3ReadOnlyAccess</span> permission. Once done, select <span class="packt_screen">Next: Review</span>.</li>
<li>In the final <span class="packt_screen">Review</span> page, type in a suitable name for the role and select the <span class="packt_screen">Create Role</span> option to complete the process. Make a note of the role's ARN as we will be requiring this in the later steps. Here is snippet of the role policy for your reference:</li>
</ol>
<pre style="padding-left: 60px">{ 
  "Version": "2012-10-17", 
  "Statement": [ 
    { 
      "Effect": "Allow", 
      "Action": [ 
        "s3:Get*", 
        "s3:List*" 
      ], 
      "Resource": "*" 
    } 
  ] 
} </pre>
<p style="padding-left: 90px">With the role created, we can now move on to creating the Redshift cluster.</p>
<ol start="7">
<li>To do so, log in to the AWS Management Console and use the <span class="packt_screen">Filter</span> option to filter out <span class="packt_screen">Amazon Redshift</span>. Alternatively, you can also launch the Redshift dashboard by selecting this URL: <a href="https://console.aws.amazon.com/redshift/">https://console.aws.amazon.com/redshift/</a>.</li>
<li>Select <span class="packt_screen">Launch Cluster</span> to get started with the process.</li>
</ol>
<ol start="9">
<li>Next, on the <span class="packt_screen">CLUSTER DETAILS</span> page, fill in the required information pertaining to your cluster as mentioned in the following list:
<ul>
<li><span class="packt_screen">Cluster identifier</span>: A suitable name for your new Redshift cluster. Note that this name only supports <em>lowercase</em> strings.</li>
<li><span class="packt_screen">Database name</span>: A suitable name for your Redshift database. You can always create more databases within a single Redshift cluster at a later stage. By default, a database named <kbd>dev</kbd> is created if no value is provided:</li>
</ul>
</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="316" width="626" src="Images/65d00270-7593-4bd4-93a9-0cf6670605a0.png"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li><span class="packt_screen">Database port</span>: The port number on which the database will accept connections. By default, the value is set to <kbd>5439,</kbd> however you can change this value based on your security requirements.</li>
<li><span class="packt_screen">Master user name</span>: Provide a suitable username for accessing the database.</li>
<li><span class="packt_screen">Master user password</span>: Type in a strong password with at least one uppercase character, one lowercase character and one numeric value. Confirm the password by retyping it in the <span class="packt_screen">Confirm password</span> field.</li>
</ul>
</li>
</ul>
<ol start="10">
<li>Once completed, hit <span class="packt_screen">Continue</span> to move on to the next step of the wizard.</li>
<li>On the <span class="packt_screen">NODE CONFIGURATION</span> page, select the appropriate <span class="packt_screen">Node type</span> for your cluster, as well as the <span class="packt_screen">Cluster type</span> based on your functional requirements. Since this particular cluster setup is for demonstration purposes, I've opted to select the <span class="packt_screen">dc2.large</span> as the <span class="packt_screen">Node type</span> and a <span class="packt_screen">Single Node</span> deployment with <em>1</em> compute node. Click <span class="packt_screen">Continue</span> to move on the next page once done.</li>
</ol>
<div class="packt_infobox">It is important to note here that the cluster that you are about to launch will be live and not running in a sandbox-like environment. As a result, you will incur the standard Amazon Redshift usage fees for the cluster until you delete it. You can read more about Redshift's pricing at: <a href="https://aws.amazon.com/redshift/pricing/">https://aws.amazon.com/redshift/pricing/</a>.</div>
<ol start="12">
<li>In the <span class="packt_screen">ADDITIONAL CONFIGURATION</span> page, you can configure add-on settings, such as encryption enablement, selecting the default VPC for your cluster, whether or not the cluster should have direct internet access, as well as any preferences for a particular Availability Zone out of which the cluster should operate. Most of these settings do not require any changes at the moment and can be left to their default values.</li>
<li>The only changes required on this page is associating the previously created IAM role with the cluster. To do so, from the <span class="packt_screen">Available Roles</span> drop-down list, select the custom Redshift role that we created in our prerequisite section. Once completed, click on <span class="packt_screen">Continue</span>.</li>
<li>Review the settings and changes on the <span class="packt_screen">Review</span> page and select the <span class="packt_screen">Launch Cluster</span> option when completed.</li>
</ol>
<p>The cluster takes a few minutes to spin up depending on whether or not you have opted for a single instance deployment or multiple instances. Once completed, you should see your cluster listed on the <span class="packt_screen">Clusters</span> page, as shown in the following screenshot. Ensure that the status of your cluster is shown as <span class="packt_screen">healthy</span> under the <span class="packt_screen">DB Health</span> column. You can additionally make a note of the cluster's endpoint as well, for accessing it programmatically:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/eb5c62d5-ac0c-4a6f-a546-ce35680aca42.png" width="840" height="226"/></div>
<p>With the cluster all set up, the next thing to do is connect to the same. In the next section, we will be looking at a few simple steps you can take to connect to your newly deployed Redshift cluster.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Connecting to your Redshift cluster</h1>
                </header>
            
            <article>
                
<p>You can use a number of tools to connect to your Redshift cluster once its up and running. Most of these tools are PostgreSQL compliant and easily available off the shelf. In this case, we are going to install and use an open source SQL client tool called <strong>SQL Workbench/J</strong>.</p>
<p>To begin with, you will need to have Java runtime installed on your local workstation. The Java runtime version will have to <em>match</em> the requirements of SQL Workbench/J, otherwise it simply won't work. You can check the version of the installed Java runtime on your local desktop by either locating the Java configuration on the <span class="packt_screen">Control Panel</span> or by typing in the following command in a Terminal if you are working with a Linux distribution:</p>
<pre><strong># java --version</strong></pre>
<p>In this case, we are using a simple Windows desktop for installing SQL Workbench/J. Download the correct version of the software from here: <a href="http://www.sql-workbench.net/downloads.html">http://www.sql-workbench.net/downloads.html</a>.<a href="http://www.sql-workbench.net/downloads.html"/></p>
<p>With the software downloaded, the installation is pretty straightforward. Accept the end user license agreement, select a path for the software's installation and that's it! You should have the SQL Workbench/J up and running now:</p>
<ol>
<li>To connect SQL Workbench/J with your Redshift cluster, you will need your newly created database's JDBC URL. You can copy it by selecting the <span class="packt_screen">Connect client</span> option from Redshift's navigation pane and selecting your newly deployed cluster from the <span class="packt_screen">Get cluster connection URL</span> section, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/e45e8ab8-5527-4152-9b9b-54b9f1f3ee12.png" width="910" height="271"/></div>
<ol start="2">
<li>You will also need to download the correct version of the associated Amazon Redshift <span class="packt_screen">JDBC Driver</span> JAR using the same page as well.</li>
</ol>
<ol start="3">
<li>Once completed, from the SQL Workbench/J client, select <span class="packt_screen">File</span>, followed by the <span class="packt_screen">Connect window</span> option.</li>
<li>Here, click on <span class="packt_screen">Create a new connection profile</span> to get started. This will pop up a <span class="packt_screen">New profile</span> box where you will need to enter a name for this new profile.</li>
<li>Once the profile is created; select the <span class="packt_screen">Manage drivers</span> option. This will display the <span class="packt_screen">Manage drivers</span> dialog box, as shown in the following screenshot. Select the <span class="packt_screen">Amazon Redshift</span> option and provide a suitable <span class="packt_screen">Name</span> for your connection driver, as well. Click on the browse icon and select the downloaded Amazon Redshift driver JAR that we downloaded from Redshift a while back. Click on <span class="packt_screen">OK</span> to complete the driver settings:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/73ecfa93-949c-4f41-9f72-33b834760c25.png" width="613" height="386"/></div>
<ol start="6">
<li>With the driver in place, the final thing left to do is connect to the database and test it. For that, select the newly created <span class="packt_screen">Connection profile</span> from SQL Workbench/J and paste the copied database JDBC URL in the <span class="packt_screen">URL</span> field as shown. Provide the database's <span class="packt_screen">Username</span> and <span class="packt_screen">Password</span> as configured during the cluster's setup. Additionally, ensure that the <span class="packt_screen">Autocommit</span> option is checked as shown here:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/c3dba302-05a6-457f-bb9a-5465f415c94b.png" width="783" height="292"/></div>
<ol start="7">
<li>You can also test the connection by selecting the <span class="packt_screen">Test</span> option on the SQL Workbench/J screen. Once completed, click <span class="packt_screen">OK</span> to establish and open the SQL prompt.</li>
</ol>
<p>With this step completed, you should have a running Redshift cluster connected to the SQL Workbench/J client as well. The next and final step left for us is to run a few sample queries and test the cluster's functionality, so let's get started with that right away!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with Redshift databases and tables</h1>
                </header>
            
            <article>
                
<p>Before we start querying the Redshift database, we will first need to upload some same data to it. For this particular scenario, we are going to use a small subset of HTTP request logs that originated from a web server at the NASA Kennedy Space Center in Florida. This data is available for public use and can be downloaded from here: <a href="http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html">http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html</a>.</p>
<p>The log file essentially contains the following set of columns:</p>
<ul>
<li><span class="packt_screen">Host</span>: The host that is making the web request to the web server. This field contains fully qualified hostnames or IP addresses as well.</li>
<li><span class="packt_screen">Timestamp</span>: The timestamp of the particular web request. The format is <kbd>DAY MON DD HH:MM:SS YYYY</kbd>. This timestamp uses a 24-hour clock.</li>
<li><span class="packt_screen">Request</span>: The method used to request the server (<kbd>GET</kbd>/<kbd>HEAD</kbd>/<kbd>POST</kbd>).</li>
<li><span class="packt_screen">URL</span>: The URL of the resource that was requested by the client.</li>
<li><span class="packt_screen">Response</span>: This contains the HTTP response code (<kbd>200</kbd>, <kbd>302</kbd>, <kbd>304</kbd>, and <kbd>404</kbd>).</li>
<li><span class="packt_screen">Bytes</span>: The size of the reply in bytes.</li>
</ul>
<p>Here's a snippet of the data for your reference:</p>
<pre>pppa006.compuserve.com,807256800,GET,/images/launch-logo.gif,200,1713 
vcc7.langara.bc.ca,807256804,GET,/shuttle/missions/missions.html,200,8677 
pppa006.compuserve.com,807256806,GET,/history/apollo/images/apollo-logo1.gif,200,1173 </pre>
<div class="packt_infobox">You can download the sample CSV file (2.14 MB containing 30,970 entries) used for this scenario using the following link:<br/>
<a href="https://github.com/yoyoclouds/Administering-AWS-Volume2">https://github.com/yoyoclouds/Administering-AWS-Volume2.</a></div>
<p>With the file downloaded, all you need to do is upload it to one of your Amazon S3 buckets. Remember, that this bucket should be accessible by Amazon Redshift otherwise you may get a <kbd>S3ServiceException: Access Denied</kbd> exception during execution.</p>
<p>Next, from the SQL Workbench/J client, type in the following code to create a new table within our Redshift database:</p>
<pre>create table apachelogs( 
host varchar(100), 
time varchar(20), 
method varchar(8), 
url varchar(200), 
response varchar(10), 
bytes varchar(10)); </pre>
<div class="packt_infobox"><span>You can find the complete copy of the previous code at: <a href="https://github.com/yoyoclouds/Administering-AWS-Volume2">https://github.com/yoyoclouds/Administering-AWS-Volume2</a>.</span></div>
<p>Select the <span class="packt_screen">Execute Query</span> button. You should receive an output stating that the table is created, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/264efe17-0c6c-4b8d-bb87-952232c6d57a.png" width="690" height="376"/></div>
<p>Next, use the <kbd>COPY</kbd> command to load the contents of the data file stored in Amazon S3 into the newly created Redshift table. The <kbd>COPY</kbd> command is a very versatile command and can be used to load data residing in Amazon S3, Amazon EMR, or even from an Amazon DynamoDB table into Amazon Redshift. To know more about the <kbd>COPY</kbd> command, navigate to this URL: <a href="https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html</a>.<a href="https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html"/></p>
<p>Substitute the values of <kbd>&lt;REDSHIFT_TABLE_NAME&gt;</kbd> with the name of the newly created table, the <kbd>&lt;BUCKET_NAME&gt;</kbd> with the name of the S3 bucket that contains the data file, and <kbd>&lt;REDSHIFT_IAM_ROLE_ARN&gt;</kbd> with the ARN of the IAM read-only access role that we created as a part of Amazon Redshift's prerequisite process:</p>
<pre><strong>copy &lt;REDSHIFT_TABLE_NAME&gt; from 's3://&lt;BUCKET_NAME&gt;/data.csv'  
credentials 'aws_iam_role=&lt;REDSHIFT_IAM_ROLE_ARN&gt;'  
csv;</strong> </pre>
<p>Once the code is pasted into the SQL Workbench/J, click on the <span class="packt_screen">Execute Query</span> button. Here is a snapshot of the command execution from SQL Workbench/J:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/0a1d3eec-116d-4eef-9599-e47775ab0769.png" width="690" height="376"/></div>
<p>With the data loaded, you can now use simple queries to query the dataset, as described in this section. The following command will list all 30,970 records from the table:</p>
<pre><strong>select * from apachelogs;</strong></pre>
<p>The following command will list only those records whose response value was <kbd>404</kbd>:</p>
<pre><strong>select * from apachelogs where response=404;</strong> </pre>
<p>The following command will list all the hosts that have requested for the particular resource:</p>
<pre><strong>select host from apachelogs where url='/images/NASA-logosmall.gif';</strong> </pre>
<p>You can also use the Redshift dashboard to view the performance and runtime of each individual query by first selecting your Redshift cluster name from the <span class="packt_screen">Cluster</span> page. Next, select the <span class="packt_screen">Queries</span> tab to bring up the list of the most recently executed queries, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/6d7a32a0-9ba1-440d-87f7-650ef68454d6.png" width="1123" height="295"/></div>
<p>You can drill down into each query by further selecting the <em>query identification number</em> as well.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Planning your next steps</h1>
                </header>
            
            <article>
                
<p>Before we conclude by summarizing the chapter, there are a few things I highly recommend that you try out with Amazon EMR, as well as with Amazon Redshift. First up, EMRFS.</p>
<p>We briefly touched upon the topic of EMRFS while deciding which filesystem to opt for when it comes to deploying the EMR Cluster. <strong>EMR File System</strong> (<span><strong>EMRFS</strong></span><span>) is an implementation of the traditional HDFS that allows for reading and writing files from Amazon EMR directly to Amazon S3. This essentially allows you to leverage the consistency provided by S3, as well as some of its other feature sets, such as data encryption. To read more about EMRFS and how you can use it for your EMR clusters, visit: <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html</a></span><span>.</span></p>
<p>Secondly, Amazon EMR also provides an enterprise-grade Hadoop distribution in the form of MapR. The MapR distribution of Hadoop provides you with a plethora of features that enhances your overall experience when it comes to building distributed applications, as well as managing the overall Hadoop cluster. For example, selecting MapR as the Hadoop distribution provides support for industry-standard interfaces, such as NFS and ODBC, using which you can connect your EMR cluster with any major BI tool, including Tableau and Toad. MapR internally also provides built-in high availability, data protection, higher performances, and a whole list of additional features. You can read more about the MapR distribution for Hadoop at EMR at: <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-mapr.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-mapr.html</a>.</p>
<p>Last but not the least, I would also recommend that you try out some of Amazon Redshift's advanced features in the form of reserved nodes and parameter groups. Parameter groups are essentially a group of parameters that are applied to the database when it is created. You can find the parameter group for your existing database by selecting the <span class="packt_screen">Parameter Group</span> option from the Redshift's navigation pane. You can use and tweak these parameter groups based on your requirements to fine tune and customize the database. To know how to leverage parameter groups for your database tuning, visit: <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-parameter-groups.html">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-parameter-groups.html</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Well that brings us to the end of yet another amazing chapter. Let's quickly summarize what we have learnt so far!</p>
<p>To start off, we began by learning a bit about the various services offered by AWS for big data analytics followed by a quick getting started with Amazon EMR guide. We learnt about a few of Amazon EMR's concepts as well as launched our very first EMR cluster, as well. We also ran our first simple job on the EMR cluster and learnt how to monitor its performance using the likes of Amazon CloudWatch.</p>
<p>Towards the end of the chapter, we got to know Amazon Redshift along with its core concepts and workings. We also created our first Redshift cluster, connected to it using an open source client and ran a couple of SQL queries, as well.</p>
<p>In the next chapter, we will be learning and exploring yet another AWS service designed for data orchestration so stick around, we still have much to learn!</p>


            </article>

            
        </section>
    </div>



  </body></html>