- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Architecture Overview and Definitions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构概述与定义
- en: Kubernetes is an amazing technology; however, as we saw in the last chapter,
    it is not a simple technology. I consider Kubernetes not only as container orchestration,
    but besides that, it is also a platform with standard interfaces to integrate
    containers with the broader infrastructure, including storage, networks, and hypervisors.
    That said, you must consider all the prerequisites and aspects involved in an
    OpenShift self-managed cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一项了不起的技术；然而，正如我们在上一章所看到的，它并不是一项简单的技术。我认为 Kubernetes 不仅仅是容器编排，除此之外，它还是一个具有标准接口的平台，用于将容器与更广泛的基础设施集成，包括存储、网络和虚拟化技术。因此，你必须考虑到
    OpenShift 自管理集群中的所有先决条件和相关方面。
- en: In this chapter, we will walk through the main concepts related to the Kubernetes
    and OpenShift architecture. The main purpose here is you *think before doing*
    and make important decisions, to avoid rework later.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍与 Kubernetes 和 OpenShift 架构相关的主要概念。这里的主要目的是让你 *在行动之前思考* 并做出重要决策，避免后续的返工。
- en: 'The following main topics will be covered in the chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Understanding the foundational concepts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基础概念
- en: OpenShift architectural concepts and best practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 架构概念与最佳实践
- en: Infrastructure/cloud provider
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施/云提供商
- en: Network considerations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络考虑事项
- en: Other considerations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他考虑事项
- en: OpenShift architectural checklists
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift 架构检查清单
- en: Let's get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: As we are covering the architecture side of OpenShift in this chapter, you still
    don't need access to any specific hardware or software to follow this chapter,
    but this will be expected some chapters ahead. However, it is important you have
    some pre-existing knowledge of OpenShift and Kubernetes for you to achieve the
    best possible result from this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章涉及的是 OpenShift 的架构部分，因此在本章中，你并不需要访问任何特定的硬件或软件，但这在后面的章节中会有所要求。不过，为了能够从本章获得最佳结果，建议你具备一定的
    OpenShift 和 Kubernetes 基础知识。
- en: Prerequisites
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: This chapter is intended to be for **Information Technology** (**IT**) architects
    that already have some basic knowledge of Kubernetes or OpenShift use. That said,
    we are not covering in this chapter basic concepts such as what a Pod, Service,
    or Persistent Volume is. But if you don't know these basic concepts yet, don't
    freak out! We have prepared a list of recommended training and references for
    you in the last chapter of this book. We suggest you to take the Kubernetes Basics
    and Kube by Example before moving forward with this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章面向那些已经具备一定 Kubernetes 或 OpenShift 使用基础知识的 **信息技术** (**IT**) 架构师。因此，本章不涉及诸如
    Pod、Service 或 Persistent Volume 等基础概念的介绍。如果你还不了解这些基础概念，别担心！我们已经在本书最后一章准备了推荐的培训和参考资料。我们建议你在继续本章内容之前，先学习
    Kubernetes 基础和 Kube by Example。
- en: Understanding the foundational concepts
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基础概念
- en: 'Let''s start by understanding the main concepts related to Kubernetes and OpenShift
    components and servers. First, any OpenShift cluster is composed of two types
    of servers: **master** and **worker** nodes.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解 Kubernetes 和 OpenShift 组件与服务器的主要概念开始。首先，任何 OpenShift 集群都由两种类型的服务器组成：**主节点**和
    **工作节点**。
- en: Master nodes
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主节点
- en: 'This server contains the **control plane** of a Kubernetes cluster. Master
    servers on OpenShift run over the **Red Hat Enterprise Linux CoreOS** (**RHCOS**)
    operating system and are composed of several main components, such as the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务器包含 Kubernetes 集群的 **控制平面**。OpenShift 上的主服务器运行在 **Red Hat Enterprise Linux
    CoreOS** (**RHCOS**) 操作系统上，并由多个主要组件组成，例如以下内容：
- en: '`kube-apiserver`): Responsible for exposing all Kubernetes APIs. All actions
    performed on a Kubernetes cluster are done through an API call—whenever you use
    the **command-line interface** (**CLI**) or a **user interface** (**UI**), an
    API call will always be used.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`): 负责暴露所有的 Kubernetes API。所有在 Kubernetes 集群上执行的操作都是通过 API 调用进行的——无论是使用
    **命令行接口** (**CLI**) 还是 **用户界面** (**UI**)，都会使用 API 调用。'
- en: '`etcd`): The database stores all cluster data. `etcd` is a highly available
    distributed key-value database. For in-depth information about `etcd`, refer to
    its documentation here: [https://etcd.io/docs/latest/](https://etcd.io/docs/latest/).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`): 数据库存储所有集群数据。`etcd` 是一个高度可用的分布式键值数据库。有关 `etcd` 的详细信息，请参考其文档：[https://etcd.io/docs/latest/](https://etcd.io/docs/latest/)。'
- en: '`kube-scheduler`): It is the responsibility of `kube-scheduler` to assign a
    node for a Pod to run over. It uses complex algorithms that consider a large set
    of aspects to decide which is the best node to host the Pod, such as computing
    resource available versus required node selectors, affinity and anti-affinity
    rules, and others.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`): `kube-scheduler` 负责为 Pod 分配一个节点来运行。它使用复杂的算法，考虑大量因素来决定哪个节点最适合承载
    Pod，比如可用的计算资源与所需节点选择器、亲和性和反亲和性规则等。'
- en: '`kube-controller-manager`): Controllers are an endless loop that works to ensure
    that an object is always in the desired state. As an illustration, think about
    smart home automation equipment: a controller is responsible for orchestrating
    the equipment to make sure the environment will always be in the desired programmed
    state—for example, by turning the air conditioning on and off from time to time
    to keep the temperature as close as possible to the desired state. Kubernetes
    controllers have the same function— they are responsible for monitoring objects
    and responding accordingly to keep them at the desired states. There are a bunch
    of controllers that are used in a Kubernetes cluster, such as replication controller,
    endpoints controller, namespace controller, and serviceaccounts controller. For
    more information about controllers, check out this page: [https://kubernetes.io/docs/concepts/architecture/controller/](https://kubernetes.io/docs/concepts/architecture/controller/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`): 控制器是一个无尽的循环，负责确保某个对象始终处于期望的状态。例如，可以将其比作智能家居自动化设备：控制器负责协调设备，确保环境始终处于预定的状态——例如，定期开关空调以保持温度尽可能接近期望值。Kubernetes
    控制器具有相同的功能——它们负责监控对象并根据情况做出响应，以保持其处于预期状态。Kubernetes 集群中使用了许多控制器，如副本控制器、端点控制器、命名空间控制器和服务帐户控制器。有关控制器的更多信息，请查看此页面：[https://kubernetes.io/docs/concepts/architecture/controller/](https://kubernetes.io/docs/concepts/architecture/controller/)'
- en: 'These are the components of the Kubernetes control plane that runs on the master
    nodes; however, OpenShift has some additional services to extend Kubernetes functionality,
    as outlined here:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是运行在主节点上的 Kubernetes 控制平面组件；然而，OpenShift 还提供了一些额外的服务来扩展 Kubernetes 的功能，如下所述：
- en: '`openshift-apiserver`): This validates and configures data for OpenShift exclusive
    resources, such as routes, templates, and projects.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openshift-apiserver`): 该服务负责验证和配置 OpenShift 独有资源的数据，如路由、模板和项目。'
- en: '`openshift-controler-manager`): This works to ensure that OpenShift exclusive
    resources reach the desired state.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openshift-controler-manager`): 该服务确保 OpenShift 独有资源达到预期状态。'
- en: '`openshift-oauth-apiserver`): Responsible for validating and configuring data
    to authenticate a user, group, and token with OpenShift.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openshift-oauth-apiserver`): 负责验证并配置数据，用以认证用户、组和 OpenShift 的令牌。'
- en: 'The following figure shows the main control plane components of Kubernetes
    and OpenShift:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Kubernetes 和 OpenShift 的主要控制平面组件：
- en: '![Figure 2.1 – OpenShift control plane components](img/B18015_02_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – OpenShift 控制平面组件](img/B18015_02_01.jpg)'
- en: Figure 2.1 – OpenShift control plane components
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – OpenShift 控制平面组件
- en: 'These components can be found in multiple namespaces, as you can see in the
    following table:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件可以在多个命名空间中找到，您可以在下表中看到：
- en: '![](img/B18015_02_Table_01.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_01.jpg)'
- en: What Are Operators?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 Operators？
- en: 'If you''ve never heard about Operators, you may be thinking: *What are Operators,
    after all?* Operators are nothing more than a standard method to package, deploy,
    and maintain Kubernetes applications and objects. They use **Custom Resource Definitions**
    (**CRDs**) to extend the Kubernetes API functionality and also some standards
    for the application''s life cycle: deploy, patch, keep the desired state, and
    even auto-pilot it (autoscaling, tuning, failure detections, and so on). Check
    out this link for more information: [https://kubernetes.io/docs/concepts/extend-kubernetes/operator/](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从未听说过 Operators，你可能会想：*Operators 究竟是什么？* Operators 其实就是一种标准方法，用于打包、部署和维护
    Kubernetes 应用和对象。它们通过 **自定义资源定义** (**CRDs**) 来扩展 Kubernetes API 功能，并且为应用的生命周期提供一些标准：部署、修补、保持期望状态，甚至自动化操作（如自动扩缩容、调优、故障检测等）。查看更多信息，请访问这个链接：[https://kubernetes.io/docs/concepts/extend-kubernetes/operator/](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)。
- en: Bootstrap node
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引导节点
- en: The bootstrap node is a temporary server—used only during cluster deployment—that
    is responsible for injecting the cluster's components into the control plane.
    It is removed by the installation program when the bootstrap is finished successfully.
    As it is a temporary server that lives only during the deployment, it is usually
    not considered in the OpenShift architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 引导节点是一个临时服务器——仅在集群部署期间使用——它负责将集群的组件注入到控制平面中。当引导过程成功完成时，安装程序会将其移除。由于它是一个只在部署过程中存在的临时服务器，通常不会被视为
    OpenShift 架构的一部分。
- en: Workers
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作节点
- en: 'Workers are the servers where the workloads themselves run. On OpenShift, workers
    can run over RHCOS or **Red Hat Enterprise Linux** (**RHEL**). Although RHEL is
    also supported for OpenShift workers, RHCOS, in general, is preferred for the
    following reasons:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点是承载工作负载的服务器。在 OpenShift 上，工作节点可以运行在 RHCOS 或 **Red Hat 企业 Linux**（**RHEL**）上。虽然
    OpenShift 支持在 RHEL 上运行工作节点，但一般推荐使用 RHCOS，原因如下：
- en: '**Immutable**: RHCOS is a tight operating system designed to be managed remotely
    by OpenShift Container Platform itself. This enables consistency and makes upgrades
    a much easier and safer procedure, as OpenShift will always know and manage the
    actual and desired states of the servers.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可变**：RHCOS 是一个精简的操作系统，旨在通过 OpenShift 容器平台远程管理。这增强了一致性，并使升级过程变得更加简单和安全，因为
    OpenShift 始终能够了解并管理服务器的实际状态和期望状态。'
- en: '`rpm-ostree`: RHCOS uses the `rpm-ostree` system, which enables transactional
    upgrades and adds consistency to the infrastructure. Check out this link for more
    information: [https://coreos.github.io/rpm-ostree/](https://coreos.github.io/rpm-ostree/).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpm-ostree`：RHCOS 使用 `rpm-ostree` 系统，它支持事务性升级并为基础设施增加一致性。有关更多信息，请查看此链接：[https://coreos.github.io/rpm-ostree/](https://coreos.github.io/rpm-ostree/)。'
- en: '`podman` and `skopeo`. During normal functioning, you are not encouraged to
    access and run commands on workers directly (as they are managed by the OpenShift
    platform itself); however, those tools are helpful for troubleshooting purposes—as
    we will see in detail in [*Chapter 6*](B18015_06.xhtml#_idTextAnchor113) of this
    book, *OpenShift Troubleshooting, Performance, and* *Best Practices*.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`podman` 和 `skopeo`。在正常运行过程中，通常不鼓励直接访问和在工作节点上运行命令（因为它们由 OpenShift 平台本身管理）；然而，这些工具在故障排除时非常有用——正如我们将在本书的
    [*第六章*](B18015_06.xhtml#_idTextAnchor113)《OpenShift 故障排除、性能和最佳实践》中详细讨论的那样。'
- en: '`systemd`, which ensures the same level of security and quality you would have
    by using the standard RHEL operating system.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`systemd`，它确保了你使用标准 RHEL 操作系统时同样的安全性和质量。'
- en: '`rpm-ostree` system to make atomic upgrades, which allows safer and easier
    upgrade and rollback (if needed).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `rpm-ostree` 系统进行原子升级，使得升级和回滚（如有需要）变得更加安全和简便。
- en: 'In the following figure, you can view how these objects and concepts are used
    in an OpenShift worker node:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以查看这些对象和概念在 OpenShift 工作节点中的使用方式：
- en: '![Figure 2.2 – RHCOS worker node ](img/B18015_02_02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – RHCOS 工作节点](img/B18015_02_02.jpg)'
- en: Figure 2.2 – RHCOS worker node
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – RHCOS 工作节点
- en: Types of workers
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作节点类型
- en: 'There are some common types of workers, the most usual being these:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些常见类型的工作节点，最常见的包括以下几种：
- en: '**Application workers**: Responsible for hosting the workloads—this is where
    the application containers run.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序工作节点**：负责托管工作负载——即应用程序容器运行的地方。'
- en: '**Infrastructure workers**: This type of server is usually used to host the
    platform infrastructure tools, such as the ingress (routers), internal registry,
    the monitoring stack (Prometheus and Grafana), and also the logging tool (Elasticsearch
    and Kibana).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施工作节点**：这种类型的服务器通常用于托管平台基础设施工具，如入口（路由器）、内部注册表、监控堆栈（Prometheus 和 Grafana），以及日志工具（Elasticsearch
    和 Kibana）。'
- en: '**Storage workers**: Container storage solutions, such as **Red Hat OpenShift
    Data Foundation**, usually require some dedicated worker nodes to host their Pods.
    In such cases, a best practice is to use a dedicated node group for them.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储工作节点**：容器存储解决方案，如**Red Hat OpenShift 数据基础设施**，通常需要一些专用的工作节点来托管其 Pods。在这种情况下，最佳实践是为它们使用专用的节点组。'
- en: In the next section, we will see how to use different types of workers to design
    a highly available and resilient OpenShift cluster.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用不同类型的工作节点来设计一个高度可用且有弹性的 OpenShift 集群。
- en: Highly available cluster
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高可用集群
- en: 'It is not uncommon for OpenShift clusters to become critical for the enterprise—sometimes,
    they start small but become large really quickly. Due to that, you should consider
    in your OpenShift cluster architecture **non-functional requirements** (**NFRs**)
    such as **high availability** (**HA**) from day one. A highly available cluster
    is comprised of the following aspects:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift集群对企业来说变得至关重要并不罕见——有时它们从小规模开始，但很快就变得庞大。因此，你在设计OpenShift集群架构时，应从第一天开始考虑**非功能性需求**（**NFRs**），如**高可用性**（**HA**）。一个高可用集群包含以下方面：
- en: '`etcd` uses a distributed consensus algorithm named **Raft protocol**, which
    requires at least *three nodes to be highly available*. It is not the focus of
    this book to explain the Raft protocol, but if you want to understand it better,
    refer to these links:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd` 使用了一种名为**Raft协议**的分布式共识算法，它要求至少有*三个节点才能保持高可用性*。本书的重点不在于解释Raft协议，但如果你想更好地理解它，可以参考以下链接：'
- en: 'Raft description: [https://raft.github.io/](https://raft.github.io/)'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raft描述：[https://raft.github.io/](https://raft.github.io/)
- en: 'Illustrated example: [http://thesecretlivesofdata.com/raft/](http://thesecretlivesofdata.com/raft/)'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 说明示例：[http://thesecretlivesofdata.com/raft/](http://thesecretlivesofdata.com/raft/)
- en: '**Infrastructure worker nodes**: At least two nodes are required to have ingress
    highly available. We will discuss later in this chapter what you should consider
    for other infrastructure components such as monitoring and logging.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施工作节点**：至少需要两个节点才能确保入口的高可用性。我们将在本章稍后讨论你应该考虑的其他基础设施组件，如监控和日志记录。'
- en: '**Application worker nodes**: At least two nodes are also required to be considered
    highly available; however, you may have as many nodes as required to provide enough
    capacity for expected workloads. In this chapter, we will walk through some sizing
    guidance to determine the number of workers required for a workload, if you have
    an estimated required capacity.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用工作节点**：至少需要两个节点才能被视为高可用；然而，你可以根据预期的工作负载需要，增加足够数量的节点。在本章中，我们将探讨一些容量规划指导，以帮助你确定所需的工作节点数量，如果你已经有了预估的容量需求。'
- en: 'The following figure shows what a highly available cluster architecture looks
    like:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了高可用集群架构的样子：
- en: '![Figure 2.3 – OpenShift highly available cluster  ](img/B18015_02_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – OpenShift高可用集群](img/B18015_02_03.jpg)'
- en: Figure 2.3 – OpenShift highly available cluster
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – OpenShift高可用集群
- en: Now that we are on board with the foundational concepts of Kubernetes and OpenShift,
    let's dive further and look at OpenShift's architecture, along with some best
    practices.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了Kubernetes和OpenShift的基础概念，让我们进一步深入，了解OpenShift的架构及一些最佳实践。
- en: OpenShift architectural concepts and best practices
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift架构概念和最佳实践
- en: 'In this section, we will discuss the main concepts related to the OpenShift
    architecture design and some best practices you should consider. In general, when
    we are designing an architecture for an OpenShift cluster, the aspects in the
    following table need to be defined accordingly:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论与OpenShift架构设计相关的主要概念以及一些你应考虑的最佳实践。一般来说，当我们为OpenShift集群设计架构时，以下表格中的方面需要相应地定义：
- en: '![](img/B18015_02_Table_02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_02.jpg)'
- en: Details of how to address these aspects (deployment, configurations, and so
    on) will be covered from [*Chapter 5*](B18015_05.xhtml#_idTextAnchor090)*,* *OpenShift
    Deployment,* onward. Another key point to note is that we are still focusing on
    one single OpenShift cluster only—the main objective here is to help you to define
    a standard cluster architecture that best fits your case. In the next chapter,
    we will explore aspects you need to consider when working with multiple environments,
    clusters, and providers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解决这些方面（如部署、配置等）的详细内容，将从[*第5章*](B18015_05.xhtml#_idTextAnchor090)*,* *OpenShift部署*开始讨论。另一个关键点是，我们仍然专注于单一的OpenShift集群——这里的主要目标是帮助你定义一个最适合你情况的标准集群架构。在下一章中，我们将探讨在处理多个环境、集群和提供商时需要考虑的方面。
- en: In the following sections, we are going to walk through each of these points,
    highlighting the most important items you need to cover.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐一介绍这些要点，重点讲解你需要涵盖的最重要项目。
- en: Installation mode
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装模式
- en: 'You already know from the previous chapter that we have three installation
    modes with OpenShift, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你在上一章中已经了解了OpenShift的三种安装模式，具体如下：
- en: '**Installer-provisioned** **infrastructure** (**IPI**)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装程序提供的** **基础设施** (**IPI**)'
- en: '**User-provisioned** **infrastructure** (**UPI**)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户提供的基础设施**（**UPI**）'
- en: Provider-agnostic (if you haven't seen it, review the *OpenShift installation
    modes* section from the last chapter)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供商无关（如果你还没看到，回顾一下上一章的*OpenShift安装模式*部分）
- en: Here, we will briefly discuss important aspects you need to consider from each
    option to drive the best decision for your case.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将简要讨论你需要考虑的每个选项中的重要方面，以帮助你做出最佳决策。
- en: IPI
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IPI
- en: This mode is a simplified opinionated method for cluster provisioning and is
    also a fully automated method for installation and upgrades. With this model,
    you can make the operational overhead lower; however, it is less flexible than
    UPI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式是一种简化的、具有指导意见的方法，用于集群的配置，同时也是一种完全自动化的安装和升级方法。使用这种模型，你可以降低操作开销；然而，它的灵活性不如UPI。
- en: 'You can see an example of IPI here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到一个IPI的示例：
- en: '![Figure 2.4 – IPI  ](img/B18015_02_04.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – IPI](img/B18015_02_04.jpg)'
- en: Figure 2.4 – IPI
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – IPI
- en: '*Figure 2**.4* shows all layers that are automated by the installer during
    the cluster deployment.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.4* 显示了安装程序在集群部署过程中自动化的所有层级。'
- en: UPI
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: UPI
- en: In this mode, you provision the servers manually—you are also responsible for
    managing them. As such, you have more flexibility within the infrastructure layer.
    In this mode, OpenShift still has some level of integration with the infrastructure
    or cloud provider to provide storage services for the platform.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，你需要手动配置服务器——你也需要负责管理它们。因此，你在基础设施层面有更多的灵活性。在这种模式下，OpenShift 仍然与基础设施或云提供商有一定的集成，以提供平台的存储服务。
- en: Agnostic installer
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无关安装程序
- en: This mode is similar to UPI; however, there is no integration between OpenShift
    and the infrastructure or cloud provider. Therefore, in this mode, you will not
    have any storage plugins installed with the platform—you will need to deploy an
    in-tree or **Container Storage Interface** (**CSI**) plugin on day two to provide
    persistent volumes to your workloads (we are going to cover storage-related aspects
    later in this chapter).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式与UPI类似；但是，OpenShift与基础设施或云提供商之间没有集成。因此，在这种模式下，你不会在平台中安装任何存储插件——你需要在第二天部署一个内置的或**容器存储接口**（**CSI**）插件，以为你的工作负载提供持久卷（我们将在本章后面讲解与存储相关的内容）。
- en: 'You can see an example of UPI/an agnostic installer here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到一个UPI/无关安装程序的示例：
- en: '![Figure 2.5 – UPI/Agnostic installer  ](img/B18015_02_05.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – UPI/无关安装程序](img/B18015_02_05.jpg)'
- en: Figure 2.5 – UPI/Agnostic installer
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – UPI/无关安装程序
- en: As you can see in *Figure 2**.5*, with UPI or an agnostic installer, there are
    some layers you are responsible for providing, as prerequisites, to deploy a cluster
    (and also maintain it on day two), as opposed to IPI, from *Figure 2**.4*, which
    is fully automated.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，*图2.5* 中，在UPI或无关安装程序的情况下，你需要负责提供一些必备层级来部署集群（并在第二天进行维护），而与*图2.4* 中完全自动化的IPI模式相反。
- en: Computing
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算
- en: 'From a computing perspective, the following are important attributes that must
    be considered during the architecture design:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算角度来看，以下是架构设计中必须考虑的重要属性：
- en: '**Nodes and cluster sizing**: Define the number and size of worker nodes to
    host workloads expected for the platform. Some important factors need to be considered
    here to have a resilient cluster—this topic will be covered later in this chapter.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点和集群规模**：定义工作节点的数量和大小，以承载平台预期的工作负载。在这里需要考虑一些重要因素，以确保集群的弹性——这一主题将在本章后面进行讲解。'
- en: '**Environment segmentation**: It is possible to have one cluster only that
    provides a segregated group of nodes for specific reasons. Sometimes, it makes
    sense to have a dedicated group of nodes to provide services for specific environments
    in one single cluster—it is possible to have one single cluster with nodes dedicated
    for a development environment, another group for staging, and another one for
    production, for instance. That said, this is a crucial decision that needs to
    be made—going for one cluster for each environment or having one single cluster
    that serves multiple environments. We are going to explore this point in the next
    chapter and see what the pros and cons of each case are.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境划分**：可能只需要一个集群，提供为特定目的而隔离的节点组。有时候，使用一个专门的节点组来为特定环境提供服务是有意义的——例如，一个集群中可能有为开发环境提供服务的节点组，为暂存环境提供服务的节点组，和为生产环境提供服务的节点组。因此，这是一个需要做出的关键决策——是为每个环境创建一个集群，还是使用一个集群来服务多个环境。我们将在下一章深入探讨这一点，并查看每种情况的利弊。'
- en: Master nodes' sizing
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主节点的资源配置
- en: 'To define the master nodes'' size, we recommend you follow Red Hat''s benchmark,
    based on expected cluster load and number of nodes, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义主节点的大小，建议根据预期的集群负载和节点数量，遵循Red Hat的基准配置，具体如下：
- en: '![](img/B18015_02_Table_03.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_03.jpg)'
- en: Infrastructure node sizing
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础设施节点的资源配置
- en: 'Similarly, infrastructure nodes'' size also has a benchmark, based on expected
    cluster size, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，基础设施节点的大小也有一个基准，基于预期的集群大小，具体如下：
- en: '![](img/B18015_02_Table_04.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_04.jpg)'
- en: However, the preceding table does not consider OpenShift logging. Therefore,
    if you are planning to use it, add at least four more **virtual CPUs** (**vCPUs**)
    and 16 GB to the nodes on which Elasticsearch instances will be hosted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前述表格并未考虑OpenShift日志记录。因此，如果你计划使用日志功能，请在托管Elasticsearch实例的节点上至少增加四个**虚拟CPU**（**vCPU**）和16GB内存。
- en: Worker nodes' sizing
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作节点的资源配置
- en: There isn't just one algorithm to estimate the size of an OpenShift cluster.
    The sizing algorithm we listed here is based on our personal experience along
    the years working with it, and also great articles and resources we have studied
    so far—some good references on this topic are available at the end of this chapter
    in the *Further* *reading* section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 估算OpenShift集群大小并没有单一的算法。我们在这里列出的资源配置算法，基于我们多年来与OpenShift的实践经验，同时参考了我们所学到的优秀文章和资源——关于这个话题的一些优秀参考资料可在本章的*进一步阅读*部分找到。
- en: Allocatable resources
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可分配资源
- en: 'The sizing estimation rationale for computing resources needs to consider the
    nodes'' allocatable resources. The allocatable resource is the real amount that
    can be used for workloads in a node, considering the number of resources that
    are reserved for the operating system and `kubelet`. The calculation of allocatable
    resources is given by the following formula:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源配置的估算依据需要考虑节点的可分配资源。可分配资源是指节点上实际可用于工作负载的资源量，考虑到操作系统和`kubelet`所保留的资源。可分配资源的计算公式如下：
- en: '![](img/B18015_02_001.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_001.jpg)'
- en: OpenShift Default Values
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 默认值
- en: 'The default values for OpenShift workers are as follows (at the time of this
    writing):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift工作节点的默认值如下（截至本文写作时）：
- en: '**CPU**:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**CPU**：'
- en: '- `system-reserved = 500m (*)`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '- `system-reserved = 500m (*)`'
- en: '- `kube-reserved =` `0m (*)`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '- `kube-reserved =` `0m (*)`'
- en: '- `hard-eviction =` `0m (*)`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '- `hard-eviction =` `0m (*)`'
- en: '**Memory**:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**内存**：'
- en: '- `system-reserved = 1Gi`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '- `system-reserved = 1Gi`'
- en: '- `kube-reserved = 0Gi`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '- `kube-reserved = 0Gi`'
- en: '- `hard-eviction =` `100Mi`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '- `hard-eviction =` `100Mi`'
- en: (*) "`m`" stands for *millicore*, a standard Kubernetes unit that represents
    one vCPU divided into 1,000 parts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (*) "`m`" 表示*毫核*，是Kubernetes的标准单位，表示一个vCPU被划分为1000个部分。
- en: Recommended allocatable resources
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推荐的可分配资源
- en: 'Besides the standard aforementioned allocatable resources, it should also be
    considered as a best practice to keep at least 25% of resources available in a
    node, for resilience purposes. I''ll explain: when one node goes down, the native
    Kubernetes resilience mechanism, after some time, will move the Pods to other
    nodes with available resources—that means if you don''t plan to have extra capacity
    on the nodes, this resilience mechanism is at risk. You should also consider extra
    capacity for autoscaling at peak times and future growth. Therefore, it is recommended
    you consider this extra capacity in the calculation of workers'' computing sizing,
    as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述标准的可分配资源外，作为最佳实践，还应考虑在节点中保留至少25%的资源以增强容错性。我来解释一下：当一个节点发生故障时，Kubernetes的原生容错机制将在一段时间后将Pods迁移到其他有可用资源的节点——这意味着如果你没有为节点预留额外容量，这个容错机制就会面临风险。你还应考虑在高峰时段的自动伸缩和未来的增长。因此，建议在计算工作节点的计算资源时，考虑这些额外容量，具体如下：
- en: '![](img/B18015_02_002.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_002.jpg)'
- en: Important Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: 'Usually, some level of CPU overcommitment is—somewhat—handled well by the operating
    system. That said, the extra capacity mentioned previously doesn''t always apply
    to the CPU. However, this is a workload-dependent characteristic: most container
    applications are more memory- than CPU-bound, meaning that CPU overcommitment
    will not have a great impact on overall application performance, while the same
    does not happen with memory—but again, check your application''s requirement to
    understand that.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，操作系统能较好地处理一定程度的 CPU 超额分配。也就是说，前面提到的额外容量并不总是适用于 CPU。然而，这是一个依赖于工作负载的特性：大多数容器应用程序更多依赖内存而非
    CPU，这意味着 CPU 超额分配不会对整体应用性能产生很大影响，而内存则不同——但请根据你的应用需求进行检查以了解这一点。
- en: Let's use an example to make this sizing logic clear.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来更清楚地说明这个资源分配逻辑。
- en: Example
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Imagine that you use servers with 8 vCPUs and 32 GB random-access memory (RAM)
    as the default size. A worker of this size will have, in the end, the following
    recommended allocatable resources:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用 8 vCPU 和 32 GB 随机访问内存（RAM）的服务器作为默认规格。此规格的工作节点最终将具有以下推荐的可分配资源：
- en: CPU
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU
- en: '![](img/B18015_02_Table_05.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_05.jpg)'
- en: Memory:![](img/B18015_02_Table_06.jpg)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：![](img/B18015_02_Table_06.jpg)
- en: '**Legend**:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**图例**：'
- en: '*ar =* *allocatable resources*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*ar =* *可分配资源*'
- en: '*nc =* *node capacity*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*nc =* *节点容量*'
- en: '*kr =* *kube-reserved*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*kr =* *Kube 保留*'
- en: '*sr =* *system-reserved*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*sr =* *系统保留*'
- en: '*he =* *hard-eviction threshold*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*he =* *硬驱逐阈值*'
- en: Therefore, a worker with 8 vCPUs and 32 GB RAM will have approximately **5 vCPUs
    and 23 GB RAM** considered as the usable capacity for applications. Considering
    an example in which an application Pod requires on average 200 millicores and
    1 GB RAM, a worker of this size would be able to host approximately 23 Pods (limited
    by memory).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个拥有 8 vCPU 和 32 GB RAM 的工作节点大约会有 **5 vCPU 和 23 GB RAM** 被视为可供应用使用的容量。假设某个应用
    Pod 平均需要 200 毫核和 1 GB RAM，那么这样的工作节点大约可以承载 23 个 Pod（以内存为限制）。
- en: Aggregated logging
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合日志
- en: 'You can optionally deploy the **OpenShift Logging** tool that is based on **Elasticsearch**,
    **Kibana**, and **Fluentd**. The following diagram explains how this tool works:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择部署基于 **Elasticsearch**、**Kibana** 和 **Fluentd** 的 **OpenShift Logging**
    工具。下图解释了该工具的工作原理：
- en: '![Figure 2.6 – OpenShift Logging components  ](img/B18015_02_06.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – OpenShift 日志组件  ](img/B18015_02_06.jpg)'
- en: Figure 2.6 – OpenShift Logging components
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – OpenShift 日志组件
- en: You are not required to use OpenShift Logging, though, if you have your logging
    solution and want to keep it. You only need to configure the `ClusterLogForwarder`,
    as you are going to see in later chapters of this book (from [*Chapter 5*](B18015_05.xhtml#_idTextAnchor090),
    *OpenShift* *Deployment,* onward).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你并不需要强制使用 OpenShift Logging，如果你已有日志解决方案并希望继续使用它，只需要配置 `ClusterLogForwarder`，正如本书后面章节（从
    [*第 5 章*](B18015_05.xhtml#_idTextAnchor090)，*OpenShift 部署*）所示。
- en: Monitoring
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: 'Another important tool that any container orchestration platform needs to have
    is a monitoring tool that can monitor your infrastructure and applications. OpenShift
    comes natively with a monitoring solution based on **Prometheus**, **AlertManager**,
    and **Grafana**. The following diagram explains the monitoring components:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 任何容器编排平台都必须配备一个监控工具，以便监控你的基础设施和应用程序。OpenShift 自带一个基于 **Prometheus**、**AlertManager**
    和 **Grafana** 的监控解决方案。下图解释了监控组件：
- en: '![Figure 2.7 – OpenShift monitoring components  ](img/B18015_02_07.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – OpenShift 监控组件  ](img/B18015_02_07.jpg)'
- en: Figure 2.7 – OpenShift monitoring components
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – OpenShift 监控组件
- en: OpenShift monitoring is not optional; it is used by many internal platform components.
    However, if you do not intend to use it in favor of another monitoring tool, you
    may keep it using ephemeral storage. On the other hand, if you are planning to
    use it, we recommend you provide persistent storage to save the monitoring metrics.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 监控并非可选项；许多内部平台组件都在使用它。然而，如果你打算使用其他监控工具而不是 OpenShift，你可以选择使用临时存储来保持其运行。另一方面，如果你计划使用
    OpenShift 监控，我们建议你提供持久化存储以保存监控指标。
- en: Storage
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: Containers are stateless by nature, but this does not mean that it is not possible
    to have stateful containers on OpenShift. There are multiple ways to mount storage
    volumes inside containers and enable stateful workloads. In the following sections,
    we will walk through the common storage requirements of an OpenShift cluster that
    you should consider in your architectural design.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 容器本质上是无状态的，但这并不意味着在 OpenShift 上不可能有有状态的容器。有多种方法可以在容器内挂载存储卷并启用有状态的工作负载。在接下来的几节中，我们将详细介绍您在架构设计中应考虑的
    OpenShift 集群的常见存储需求。
- en: Storage backends
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储后端
- en: 'There are two types of storage implementations: in-tree and CSI plugins.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 存储实现有两种类型：*in-tree* 和 CSI 插件。
- en: In-tree volume plugins
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*in-tree* 卷插件'
- en: 'In-tree plugins are implementations that allow a Kubernetes platform to access
    and use external storage backends. The name *in-tree* comes from the fact that
    these implementations are developed and released in the main Kubernetes repositories,
    as *in-tree* modules. There are several types of supported in-tree plugins with
    OpenShift, as follows (*):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*in-tree* 插件是允许 Kubernetes 平台访问和使用外部存储后端的实现。*in-tree* 的名称源于这些实现是在主要 Kubernetes
    仓库中开发和发布的，作为*in-tree*模块。以下是支持的几种*in-tree*插件类型：'
- en: '![](img/B18015_02_Table_07.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_07.jpg)'
- en: (*) At the time this book was written. Check the currently supported options
    at [https://access.redhat.com/articles/4128421](https://access.redhat.com/articles/4128421).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (*) 本书编写时。请访问[https://access.redhat.com/articles/4128421](https://access.redhat.com/articles/4128421)查看当前支持的选项。
- en: CSI drivers
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CSI 驱动程序
- en: 'With more and more storage providers supporting Kubernetes, the development
    and maintenance of in-tree plugins became difficult and was no longer the most
    efficient model. The CSI has been created in this context: to provide a standard
    way to extend Kubernetes storage capabilities using API interfaces—as such, you
    can easily add new CSI plugins for different storage providers and use them with
    OpenShift. With CSI, it is possible to also have interesting features such as
    **snapshots, resizing, and volume cloning**; however, it is up to the storage
    provider to implement these features or not, so check with them if they have a
    CSI driver implementation available and which operations are implemented and supported.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的存储提供商支持 Kubernetes，开发和维护*in-tree*插件变得困难，并且不再是最有效的模型。在这种背景下创建了 CSI：通过使用
    API 接口提供一种标准方式来扩展 Kubernetes 存储能力—因此，您可以轻松地为不同的存储提供商添加新的 CSI 插件，并在 OpenShift 中使用它们。使用
    CSI，还可以拥有诸如**快照、调整大小和卷克隆**等有趣的功能；然而，是否实现这些功能取决于存储提供商，因此请与他们核实是否有可用的 CSI 驱动程序实现以及支持哪些操作。
- en: Important Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Red Hat supports the CSI APIs and implementation from the OpenShift side; however,
    support of the storage side is a storage vendor's responsibility. Check with your
    storage vendor if there is a supported CSI option for OpenShift.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Red Hat 支持来自 OpenShift 方面的 CSI API 和实现；然而，支持存储端是存储供应商的责任。如果有支持的 OpenShift CSI
    选项，请与您的存储供应商联系。
- en: Storage requirements
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储需求
- en: Now that you have learned about the types of storage plugins available for OpenShift,
    let's review the storage requirements you usually have with an OpenShift cluster.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了可用于 OpenShift 的存储插件类型，让我们来审查一下通常在 OpenShift 集群中具有的存储需求。
- en: Server disks
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务器硬盘
- en: OpenShift servers use one disk with 120 GB by default. Large clusters require
    master nodes with low latency and high throughput disks, which can provide at
    least 500 sequential **input/output operations per second** (**IOPS**) (usually
    **solid-state drive** (**SSD**) or **Non-Volatile Memory Express** (**NVMe**)
    disks). We are also going to see in-depth details about this in [*Chapter 5*](B18015_05.xhtml#_idTextAnchor090),
    *OpenShift Deployment*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，OpenShift 服务器使用一块 120 GB 的硬盘。大型集群需要低延迟和高吞吐量的主节点，这些节点可以提供至少 500 个顺序**输入/输出操作每秒**（**IOPS**）（通常为**固态硬盘**（**SSD**）或**非易失性内存扩展**（**NVMe**）硬盘）。我们还将在[*第
    5 章*](B18015_05.xhtml#_idTextAnchor090)，*OpenShift 部署*中深入探讨这些细节。
- en: OpenShift internal registry
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenShift 内部注册表
- en: This depends on the number and size of application images to be stored on it.
    If you do not have an estimated value for the images, an initial size of 200 GB
    is usually enough for the first few weeks. As a best practice, consider setting
    image pruner policies to automatically delete images that are no longer used—we
    are going to cover these best practices with examples in [*Chapter 5*](B18015_05.xhtml#_idTextAnchor090),
    *OpenShift Deployment*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于要存储的应用程序镜像的数量和大小。如果你没有镜像的预估值，通常 200 GB 的初始大小足够支撑最初的几周。作为最佳实践，考虑设置镜像修剪策略，自动删除不再使用的镜像——我们将在[*第
    5 章*](B18015_05.xhtml#_idTextAnchor090)中通过示例讲解这些最佳实践，*OpenShift 部署*。
- en: 'Volume type used by OpenShift internal registry: **RWX**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 内部注册表使用的卷类型：**RWX**
- en: OpenShift Logging
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenShift 日志
- en: 'This depends on the number of logs generated by the applications. Here is an
    example of the volume size required for an application that generates 10 lines
    of logs per second (lines-per-second); the lines have 256 bytes (bytes-per-line)
    on average, considering a retention period of 7 days for the logs:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于应用程序生成的日志数量。以下是一个示例，展示了每秒生成 10 行日志（每秒行数）的应用程序所需的卷大小；这些日志每行平均为 256 字节（每行字节数），假设日志的保留期为
    7 天：
- en: '![](img/B18015_02_011.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_011.jpg)'
- en: 'This means that one single Pod of that application will consume nearly 1.5
    GB over 7 days (the period for which a log will be stored on Elasticsearch). Another
    important thing to consider is Elasticsearch''s replication factor, which will
    require more storage depending on the replication factor selected. There following
    replication factors are available:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着该应用程序的一个 Pod 在 7 天内将消耗接近 1.5 GB 的存储（这是日志存储在 Elasticsearch 上的时长）。另一个需要考虑的重要因素是
    Elasticsearch 的复制因子，根据选择的复制因子，存储需求会有所不同。以下是可用的复制因子：
- en: '`FullRedundancy`: Replicates the primary shards for each index to every Elasticsearch
    node'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FullRedundancy`：将每个索引的主分片复制到每个 Elasticsearch 节点'
- en: '`MultipleRedundancy`: Replicates the primary shards for each index to 50% of
    the Elasticsearch nodes'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultipleRedundancy`：将每个索引的主分片复制到 50% 的 Elasticsearch 节点'
- en: '`SingleRedundancy`: Makes one copy of the primary shards for each index'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SingleRedundancy`：为每个索引的主分片创建一个副本'
- en: '`ZeroRedundancy`: Does not make a copy of the primary shards'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ZeroRedundancy`：不创建主分片的副本'
- en: 'Volume type used by OpenShift Logging: **RWO**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 日志使用的卷类型：**RWO**
- en: OpenShift monitoring
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenShift 监控
- en: OpenShift monitoring is installed by default with the platform using ephemeral
    storage (also known as **emptyDir**), meaning that, for some reason, when the
    Prometheus pod gets restarted, all metrics data will be lost. To avoid losing
    metrics data, consider a persistent volume for **Prometheus** and **AlertManager**
    Pods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 监控默认情况下使用平台内的临时存储（也称为**emptyDir**），这意味着，如果由于某些原因，Prometheus Pod 被重新启动，所有的指标数据将丢失。为了避免丢失指标数据，考虑为
    **Prometheus** 和 **AlertManager** Pods 配置持久卷。
- en: 'Red Hat has a benchmark based on various tests performed, as represented here.
    This empirical data is good guidance to estimate the volume required for Prometheus:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Red Hat 基于执行的各种测试提供了基准，如图所示。这些经验数据是估算 Prometheus 所需存储的良好指南：
- en: '![](img/B18015_02_Table_08.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_08.jpg)'
- en: (*) 15 days is the default retention period.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: (*) 15 天是默认的保留期。
- en: 'You also need to consider volumes for **AlertManager**: typically, a volume
    size of **20 GB** is enough for most cases.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要考虑 **AlertManager** 的卷：通常情况下，**20 GB** 的卷大小对大多数情况足够。
- en: By default, an HA configuration is composed of **two Prometheus replicas and
    three** **AlertManager replicas**.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，HA 配置由**两个 Prometheus 副本和三个** **AlertManager 副本**组成。
- en: 'Using the preceding reference, we can estimate the volumes required for OpenShift
    monitoring. For example, let''s say that we are planning a cluster that will have
    no more than 50 nodes and 1,800 Pods. In that case, we''d need to use the following
    formula:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述的参考数据，我们可以估算 OpenShift 监控所需的卷大小。例如，假设我们计划部署一个不超过 50 个节点和 1,800 个 Pods 的集群。在这种情况下，我们需要使用以下公式：
- en: '![](img/B18015_02_012.jpg)![](img/B18015_02_013.jpg)![](img/B18015_02_014.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_012.jpg)![](img/B18015_02_013.jpg)![](img/B18015_02_014.jpg)'
- en: 'Volume type used by OpenShift monitoring: **RWO**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 监控使用的卷类型：**RWO**
- en: Note
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The preceding requirements are based on empirical data. The real consumption
    observed can be higher depending on the workloads and resource usage. For more
    information, refer to the official documentation: [https://docs.openshift.com/container-platform/latest/scalability_and_performance/scaling-cluster-monitoring-operator.html](https://docs.openshift.com/container-platform/latest/scalability_and_performance/scaling-cluster-monitoring-operator.html).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 前述要求基于经验数据。实际观察到的消耗可能更高，具体取决于工作负载和资源使用情况。有关更多信息，请参阅官方文档：[https://docs.openshift.com/container-platform/latest/scalability_and_performance/scaling-cluster-monitoring-operator.html](https://docs.openshift.com/container-platform/latest/scalability_and_performance/scaling-cluster-monitoring-operator.html)。
- en: At this time, you don't need to know in-depth details about the OpenShift components
    such as logging or monitoring, as we are only covering the amount of storage required
    (or estimated) for them. These tools will be covered in detail later in this book.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你不需要深入了解OpenShift组件，如日志或监控，因为我们仅在此讨论所需的（或估算的）存储量。有关这些工具的详细信息将在本书后面讲解。
- en: Example
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: 'As we have already addressed sizing guidelines for an OpenShift cluster, let''s
    use an example to make it clearer. Imagine that we are designing an OpenShift
    cluster architecture that is planned to host a three-tier Node.js application
    with the following capacity:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经讨论了OpenShift集群的大小规范，接下来我们使用一个示例来使其更清晰。假设我们正在设计一个OpenShift集群架构，计划托管一个三层Node.js应用，具有以下容量：
- en: Up to 20 Pods on the frontend consume 300 millicores and 1 GB RAM each at peak
    load. Each pod generates 30 lines of logs per second (256 bytes per line). Stateless
    Pods.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端最多20个Pods在峰值负载时每个需要300毫核心和1 GB RAM。每个Pod每秒生成30行日志（每行256字节）。无状态Pods。
- en: Up to 4 Pods on the backend need 500 millicores and 1 GB RAM each at peak load.
    Each pod generates 10 lines of logs per second (256 bytes per line). Stateless
    Pods.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后端最多4个Pods在峰值负载时每个需要500毫核心和1 GB RAM。每个Pod每秒生成10行日志（每行256字节）。无状态Pods。
- en: 1 MongoDB database instance with 8 GB RAM and 2 vCPUs. It generates 1 line of
    logs per second (256 bytes per line). An **RWO** volume is required of 500 GB.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个MongoDB数据库实例，具有8 GB RAM和2个vCPU。它每秒生成1行日志（每行256字节）。需要一个**RWO**卷，容量为500 GB。
- en: Our logging stack is configured with `ZeroRedundancy` (there is no data replication).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的日志堆栈配置为`ZeroRedundancy`（没有数据复制）。
- en: Compute sizing
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算大小
- en: 'First, let''s see the total amount of CPU and memory required (for workloads
    only), as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看所需的CPU和内存总量（仅针对工作负载），如下所示：
- en: CPU
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU
- en: '![](img/B18015_02_015.png) ![](img/B18015_02_016.png) ![](img/B18015_02_017.png)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B18015_02_015.png) ![](img/B18015_02_016.png) ![](img/B18015_02_017.png)'
- en: Memory
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存
- en: '![](img/B18015_02_018.png) ![](img/B18015_02_019.png) ![](img/B18015_02_020.png)
    ![](img/B18015_02_021.png)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B18015_02_018.png) ![](img/B18015_02_019.png) ![](img/B18015_02_020.png)
    ![](img/B18015_02_021.png)'
- en: Volume
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷
- en: '![](img/B18015_02_022.png)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B18015_02_022.png)'
- en: 'We will assume nodes with 4 vCPUs and 16 GB RAM by default. As we saw in this
    chapter, we need to apply the following formula to define the recommended allocatable
    resources:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们默认假设节点具有4个vCPU和16 GB RAM。如本章所示，我们需要应用以下公式来定义推荐的可分配资源：
- en: CPU
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU
- en: '![](img/B18015_02_023.jpg)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B18015_02_023.jpg)'
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are considering, in this case, that some level of CPU overcommit is acceptable,
    and due to that, we are not considering the 25% of extra capacity here (recommended
    allocatable resources).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此假设某些程度的CPU过载是可以接受的，因此我们在此不考虑额外的25%容量（推荐的可分配资源）。
- en: Memory![](img/B18015_02_024.jpg)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存![](img/B18015_02_024.jpg)
- en: '![](img/B18015_02_025.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_025.jpg)'
- en: 'Therefore, three nodes are required to host this workload:'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，需要三个节点来托管此工作负载：
- en: '![](img/B18015_02_026.jpg)![](img/B18015_02_027.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_026.jpg)![](img/B18015_02_027.jpg)'
- en: That means we will need **3 nodes with 4 vCPU and 16 GB RAM** to provide the
    capacity required for this application.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要**3个节点，4个vCPU和16 GB RAM**，以提供此应用所需的容量。
- en: Storage sizing
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储大小
- en: 'Now, let''s calculate the number of volumes required, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算所需的卷数量，如下所示：
- en: '**Virtual machines** (**VMs**): 3 (nodes) * 120 GB (recommended per server)
    = **360** **GB disk**'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟机**（**VMs**）：3（节点） * 120 GB（每台服务器推荐） = **360** **GB磁盘**'
- en: 'Workload: **500** **GB RWO**'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作负载：**500** **GB RWO**
- en: 'Internal registry: **200** **GB RWX**'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部注册表：**200** **GB RWX**
- en: 'Logging: **106 GB RWO (****see next)**'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志：**106 GB RWO（**见下文）**
- en: '**Frontend**:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**前端**：'
- en: '![](img/B18015_02_028.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_028.jpg)'
- en: '**Backend**:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**后端**：'
- en: '![](img/B18015_02_029.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_029.jpg)'
- en: '**MongoDB**:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**MongoDB**：'
- en: '![](img/B18015_02_030.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_030.jpg)'
- en: '**Total**:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**总计**：'
- en: '![](img/B18015_02_031.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_031.jpg)'
- en: '**Monitoring**: 248 GB RWO (as we saw in the previous section about the sizing
    for monitoring in a cluster up to 50 nodes and 1,800 Pods)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：248 GB RWO（如我们在上一节中看到的，针对最多 50 个节点和 1,800 个 Pod 的监控尺寸要求）'
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: The following table summarizes the servers required for this cluster, considering
    three additional servers dedicated to hosting the OpenShift infrastructure components
    (**Logging, Monitoring, Registry,** **and Ingress)**.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了这个集群所需的服务器，考虑到有三台额外的服务器专门用于托管 OpenShift 基础设施组件（**日志、监控、注册中心**和**Ingress**）。
- en: '![](img/B18015_02_Table_09.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_09.jpg)'
- en: In the previous table, the bootstrap node is not being considered as it is a
    temporary node that is removed after cluster installation.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的表格中，bootstrap 节点没有被考虑在内，因为它是一个临时节点，集群安装后会被移除。
- en: 'And finally, the requirements for Persistent Volumes are summarized in the
    following table:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，持久化存储卷的要求在以下表格中进行了总结：
- en: '![](img/B18015_02_Table_10.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_10.jpg)'
- en: Now that we already know some best practices to observe in an OpenShift cluster,
    let's discuss in the next section some surrounding aspects you should also consider
    when designing an OpenShift architecture.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了一些在 OpenShift 集群中需要遵循的最佳实践，接下来让我们讨论在设计 OpenShift 架构时，还需要考虑的一些其他方面。
- en: Infrastructure/cloud provider
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础设施/云提供商
- en: As the OpenShift platform is integrated with the infrastructure or cloud provider,
    some prerequisites are also required, but for now, during the architecture design
    phase, you basically need to define which provider you will go for and be aware
    that they have specific prerequisites. We are not covering these pre requisites
    in this chapter, as this is going to be explained in depth in [*Chapter 5*](B18015_05.xhtml#_idTextAnchor090),
    *OpenShift Deployment*.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 OpenShift 平台与基础设施或云提供商集成，因此也需要一些先决条件，但目前在架构设计阶段，你只需要定义选择哪个提供商，并意识到它们有特定的先决条件。我们在这一章中不会涉及这些先决条件，因为它将在[*第
    5 章*](B18015_05.xhtml#_idTextAnchor090)中深入讲解，*OpenShift 部署*。
- en: In that chapter, we will practice the deployment process itself, starting by
    preparing the infrastructure or cloud prerequisites, setting up installer parameters,
    storage, network, the virtualization/cloud layer, and so on. However, during the
    architecture design phase, in general, you don't need to go deeper into these
    details yet, but just choose which provider to go for and keep in mind some specifications
    you will have to fulfill for the provider you have chosen.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将实际操作部署过程，从准备基础设施或云端先决条件、设置安装程序参数、存储、网络、虚拟化/云层等开始。然而，在架构设计阶段，通常不需要深入这些细节，只需要选择一个提供商，并记住一些你需要满足的提供商要求。
- en: Network considerations
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络考虑事项
- en: 'An OpenShift cluster uses an SDN layer to allow communication between workloads
    and cluster objects. The default plugin used with OpenShift at the time this book
    was written is **OpenvSwitch** (**OvS**), but OpenShift is also compatible (and
    supported) with the **OVN-Kubernetes** plugin. Check this link to better understand
    the differences between the plugins: [https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/about-openshift-sdn.html#nw-ovn-kubernetes-matrix_about-openshift-sdn](https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/about-openshift-sdn.html#nw-ovn-kubernetes-matrix_about-openshift-sdn).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 OpenShift 集群使用 SDN 层来允许工作负载与集群对象之间的通信。本书编写时 OpenShift 使用的默认插件是 **OpenvSwitch**（**OvS**），但
    OpenShift 也兼容（并支持）**OVN-Kubernetes** 插件。查看此链接以更好地理解插件之间的差异：[https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/about-openshift-sdn.html#nw-ovn-kubernetes-matrix_about-openshift-sdn](https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/about-openshift-sdn.html#nw-ovn-kubernetes-matrix_about-openshift-sdn)。
- en: 'Within the SDN, there are two virtual subnets—the first one has the Internet
    Protocol (IP) addresses that a Pod inside the cluster uses, while the second is
    always used when you create a service object. The default values for these subnets
    are listed in the following table:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SDN 中，有两个虚拟子网——第一个包含集群内部 Pod 使用的互联网协议（IP）地址，而第二个在创建服务对象时总是使用。以下表格列出了这些子网的默认值：
- en: '![](img/B18015_02_Table_11.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_11.jpg)'
- en: Important Note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The preceding ranges are customizable during the platform installation process
    only! You cannot modify these after installation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以上范围仅在平台安装过程中可定制！安装后无法修改这些范围。
- en: 'Make sure these two ranges don''t conflict with the existing one in your physical
    infrastructure. If you have conflicts, you may experience routing problems between
    Pods on OpenShift and external services that have a real IP within these ranges.
    The reason is simple: OpenShift SDN will always think that anything with an IP
    within the Pods'' range is a pod inside the cluster—and in this case, the SDN
    will never deliver this package to the external network (network address translation,
    or NAT). Therefore, a pod on OpenShift will never be able to communicate with
    a real service out of the cluster that has an IP within the Pods'' or services''
    range. So, be careful to define these two ranges with ones that will *never* be
    used in your infrastructure.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这两个范围与您物理基础设施中现有的范围不冲突。如果发生冲突，您可能会遇到 OpenShift 中的 Pods 与这些范围内具有真实 IP 的外部服务之间的路由问题。原因很简单：OpenShift
    SDN 始终认为任何在 Pods 范围内的 IP 都是集群内部的 pod——在这种情况下，SDN 永远不会将此数据包传递到外部网络（网络地址转换，NAT）。因此，OpenShift
    上的 pod 永远无法与集群外部的真实服务进行通信，除非该服务的 IP 不在 Pods 或服务的范围内。所以，请小心定义这两个范围，确保它们*永远不会*在您的基础设施中被使用。
- en: Let's move on to some other important aspects you need to consider from the
    network perspective, then!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们继续探讨从网络角度需要考虑的其他重要方面！
- en: VPC/VNet
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VPC/VNet
- en: If you are deploying OpenShift on **Amazon Web Services** (**AWS**), Azure,
    or **Google Cloud Platform** (**GCP**), you may choose to install an OpenShift
    cluster in a new or existing VPC/**virtual network** (**VNet**). If you go for
    existing VPC/VNet components such as subnets, NAT, internet gateways, route tables,
    and others, these will no longer be created automatically by the installer—you
    will need to configure them manually.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在 **Amazon Web Services** (**AWS**)、Azure 或 **Google Cloud Platform** (**GCP**)
    上部署 OpenShift，您可以选择在新的或现有的 VPC/**虚拟网络** (**VNet**) 中安装 OpenShift 集群。如果选择现有的 VPC/VNet
    组件，例如子网、NAT、互联网网关、路由表等，安装程序将不再自动创建它们——您需要手动进行配置。
- en: DNS
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNS
- en: Depending on the installation method and the provider, different DNS requirements
    are needed. Again, we are going to cover this point in detail later in this book,
    but keep in mind that a set of DNS requirements depends on the provider and installation
    method you choose.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 根据安装方法和提供商的不同，DNS 的要求也不同。我们将在本书稍后详细讨论这一点，但请记住，DNS 的要求集取决于您选择的提供商和安装方法。
- en: Load balancers
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: The *IPI* in on-premises environments already comes with an embedded highly
    available load balancer included. In cloud environments, OpenShift uses load balancers
    provided by the cloud provider (for example, AWS Elastic Load Balancing (ELB),
    Azure's Network LB, GCP's Cloud Load Balancing). With *UPI*, you need to provide
    an external load balancer and set it up before cluster deployment.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地环境中，*IPI* 已经包含了一个嵌入式的高可用负载均衡器。在云环境中，OpenShift 使用云提供商提供的负载均衡器（例如，AWS 弹性负载均衡（ELB）、Azure
    的网络负载均衡器（Network LB）、GCP 的云负载均衡（Cloud Load Balancing））。使用 *UPI* 时，您需要提供一个外部负载均衡器并在集群部署之前进行配置。
- en: DHCP/IPMI/PXE
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DHCP/IPMI/PXE
- en: If you go for OpenShift on bare metal, observe other requirements specified
    for this type of environment. DHCP, IPMI, and PXE are optional; however, they
    are recommended to have a higher level of automation. Therefore, consider that
    in your cluster architectural design.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择在裸金属上部署 OpenShift，请遵循此类环境下的其他要求。DHCP、IPMI 和 PXE 是可选的，但建议使用它们以提高自动化水平。因此，请在集群架构设计时考虑这一点。
- en: Internet access
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络访问
- en: 'The OpenShift platform needs download access from a list of websites—the Red
    Hat public registries to download the images used with it, either using a proxy
    or direct access. However, it is possible to install it on restricted networks
    as well. Additional work is required, though: you need to establish an internal
    registry first and mirror all required images from Red Hat''s registries to there.
    If you use a proxy, also check the proxy''s performance to avoid timeout errors
    during the image pulling process with OpenShift.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 平台需要从一系列网站访问以下载所使用的镜像，包括 Red Hat 公共注册表，可以通过代理或直接访问。不过，也可以在受限网络上安装
    OpenShift，但需要做额外的工作：您需要先建立一个内部注册表，并将所有所需镜像从 Red Hat 的注册表镜像到此。如果使用代理，请检查代理的性能，避免在
    OpenShift 拉取镜像时发生超时错误。
- en: Well, we've covered great content so far, from foundation concepts to best practices
    you need to observe related to the installation mode, computing, network, and
    storage. We are almost done with the most important aspects of an OpenShift cluster
    architecture, but we can't miss some considerations related to authentication
    and security. See in the following section some final considerations we brought
    to this chapter to help you with your cluster's architecture design.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了从基础概念到与安装模式、计算、网络和存储相关的最佳实践的大量内容。对于 OpenShift 集群架构的最重要方面，我们几乎完成了所有内容，但我们不能忽略与认证和安全相关的一些考虑。请查看本章后续部分，了解我们为此章节带来的一些最终考虑，以帮助您设计集群的架构。
- en: Other considerations
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他考虑事项
- en: Finally, there are a few more things that you should also consider during the
    design phase of your OpenShift cluster.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在设计您的 OpenShift 集群的过程中，还有一些您应考虑的事项。
- en: SSL certificates
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SSL 证书
- en: OpenShift uses SSL for all cluster communication. During the platform installation,
    self-signed certificates are generated; however, it is possible to replace the
    API and ingress certificates. At this point, you only need to know that this is
    possible; later in this book, you will see how to do it.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 使用 SSL 用于所有集群通信。在平台安装期间，会生成自签名证书；但是，可以替换 API 和入口证书。在这一点上，您只需要知道这是可能的；在本书的后面部分，您将看到如何操作。
- en: IdPs
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IdP
- en: 'OpenShift is deployed using a temporary `kubeadmin` user. It is highly recommended
    you configure new IdPs to allow users to log in to the platform using a convenient
    and safe authentication method. There are several supported IdPs with OpenShift;
    here is a current list of supported options (at the time of writing this book):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 使用临时 `kubeadmin` 用户部署。强烈建议您配置新的 IdP 以允许用户使用便捷和安全的认证方法登录平台。OpenShift
    支持多种 IdP；以下是本书撰写时支持的选项列表：
- en: '![](img/B18015_02_Table_12.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_12.jpg)'
- en: To wrap up this chapter and give you a quick reference guide, look at the OpenShift
    architectural checklists we provide next.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 结束本章并为您提供快速参考指南，查看我们提供的 OpenShift 架构检查表。
- en: OpenShift architectural checklists
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenShift 架构检查表
- en: These checklists will help you define the main decisions you may need to take
    during the OpenShift architecture design and can also be used as a summary of
    the concepts covered in this chapter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这些检查表将帮助您在 OpenShift 架构设计过程中确定可能需要采取的主要决策，并可用作本章节涵盖的概念总结。
- en: 'Here''s a checklist for installation mode and computing:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是安装模式和计算的检查列表：
- en: '![](img/B18015_02_Table_13.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_13.jpg)'
- en: 'Here''s a checklist of additional tools:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是附加工具的检查列表：
- en: '![](img/B18015_02_Table_14.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_14.jpg)'
- en: 'Here''s a checklist for storage:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是存储的检查列表：
- en: '![](img/B18015_02_Table_15.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_15.jpg)'
- en: 'Here''s a checklist for the network:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是网络的检查列表：
- en: '![](img/B18015_02_Table_16.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_16.jpg)'
- en: 'Here''s a checklist for other general considerations:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是其他一些一般考虑事项的检查列表：
- en: '![](img/B18015_02_Table_17.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18015_02_Table_17.jpg)'
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through some of the most important aspects you need
    to consider and define before starting a cluster deployment, at the architectural
    design phase. You now understand the different choices you have with the platform
    and how to estimate the number and size of your nodes and storage.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了在开始集群部署之前，在架构设计阶段需要考虑和定义的一些最重要的方面。您现在了解了在平台上的不同选择以及如何估算节点和存储的数量和大小。
- en: Check the next chapter—[*Chapter 3*](B18015_03.xhtml#_idTextAnchor066), *Multi-Tenant
    Considerations*—to acquire more knowledge about the multi-tenant aspects of the
    OpenShift architecture.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 查看下一章节—[*第三章*](B18015_03.xhtml#_idTextAnchor066)，*多租户考虑*—获取有关 OpenShift 架构多租户方面更多知识。
- en: Further reading
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'If you want to go deeper into the topics we covered in this chapter, look at
    the following references:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解本章涵盖的主题，请查看以下参考资料：
- en: '*etcd* *documentation:* [https://etcd.io/docs/latest/](https://etcd.io/docs/latest/)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*etcd* *文档:* [https://etcd.io/docs/latest/](https://etcd.io/docs/latest/)'
- en: '*Kubernetes official* *documentation:* [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes 官方* *文档:* [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
- en: '*About Kubernetes* *Operators:* [https://kubernetes.io/docs/concepts/extend-kubernetes/operator/](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于 Kubernetes* *操作符（Operators）：* [https://kubernetes.io/docs/concepts/extend-kubernetes/operator/](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)'
- en: '*Documentation about* `rpm-ostree`: [https://coreos.github.io/rpm-ostree/](https://coreos.github.io/rpm-ostree/)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于`rpm-ostree`的文档：* [https://coreos.github.io/rpm-ostree/](https://coreos.github.io/rpm-ostree/)'
- en: '*CSI drivers supported by Open Container Platform (OCP)*: [https://docs.openshift.com/container-platform/4.8/storage/container_storage_interface/persistent-storage-csi.html#csi-drivers-supported_persistent-storage-csi](https://docs.openshift.com/container-platform/4.8/storage/container_storage_interface/persistent-storage-csi.html#csi-drivers-supported_persistent-storage-csi)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Open Container Platform (OCP) 支持的 CSI 驱动程序*：[https://docs.openshift.com/container-platform/4.8/storage/container_storage_interface/persistent-storage-csi.html#csi-drivers-supported_persistent-storage-csi](https://docs.openshift.com/container-platform/4.8/storage/container_storage_interface/persistent-storage-csi.html#csi-drivers-supported_persistent-storage-csi)'
- en: '*Graphical explanation about allocatable* *resources:* [https://learnk8s.io/allocatable-resources](https://learnk8s.io/allocatable-resources)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于可分配资源的图形解释：* [https://learnk8s.io/allocatable-resources](https://learnk8s.io/allocatable-resources)'
- en: '*How to plan your environment according to application* *requirements:* [https://docs.openshift.com/container-platform/latest/scalability_and_performance/planning-your-environment-according-to-object-maximums.html#how-to-plan-according-to-application-requirements_object-limits](https://docs.openshift.com/container-platform/latest/scalability_and_performance/planning-your-environment-according-to-object-maximums.html#how-to-plan-according-to-application-requirements_object-limits)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何根据应用程序* *需求规划您的环境：* [https://docs.openshift.com/container-platform/latest/scalability_and_performance/planning-your-environment-according-to-object-maximums.html#how-to-plan-according-to-application-requirements_object-limits](https://docs.openshift.com/container-platform/latest/scalability_and_performance/planning-your-environment-according-to-object-maximums.html#how-to-plan-according-to-application-requirements_object-limits)'
- en: '*Recommended host practices, sizing, and* *others:* [https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-host-practices.html](https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-host-practices.html)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*推荐的主机实践、大小调整及* *其他事项：* [https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-host-practices.html](https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-host-practices.html)'
