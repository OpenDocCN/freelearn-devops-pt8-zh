<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using Azure Storage - Tables, Queues, Files, and Blobs</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">PaaS in Azure is not only about App Services or containers. This particular cloud offers much more, especially when talking about different options for storage, messaging solutions, or monitoring. With services such as Event Hub, Azure Storage, or Application Insights, we're given a complete set of cloud components that offer great flexibility and simplify developing complete, scalable, and easy-to-maintain applications.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Using Azure Storage solutions</li>
<li>Storing structured data with Azure Storage Tables</li>
<li>Implementing fully managed file shares with Azure Storage Files</li>
<li>Using queues with Azure Storage Queues</li>
<li>Using Azure Storage Blobs for object storage</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To perform the exercises in this chapter, you will need the following:</p>
<ul>
<li>An Azure subscription</li>
<li>Visual Studio 2017</li>
<li>Azure Storage Explorer, available at <a href="https://azure.microsoft.com/en-us/features/storage-explorer/">https://azure.microsoft.com/en-us/features/storage-explorer/</a></li>
<li>Azure Storage Emulator, <span>available at </span><a href="https://azure.microsoft.com/en-us/downloads/">https://azure.microsoft.com/en-us/downloads/</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using Azure Storage in a solution</h1>
                </header>
            
            <article>
                
<p>Most applications cannot work without a storage solution. This can be any kind of database—relational, document, file, or graph. Most of them require some skills to be able to configure and start working with them. For now, we have covered one storage solution available in Azure, namely Azure Cosmos DB, which is a serverless database, where the only thing needed was to set a correct throughput value. Of course, Azure offers much more in the way of storage services, of which the most common is Azure Storage. It is a PaaS cloud component (though some define it as serverless, mostly because of a lack of servers) which can be used in four different ways. In this chapter, we will cover all of them, so you will be familiar with their capabilities and features.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Different Azure Storage services</h1>
                </header>
            
            <article>
                
<p>Azure Storage is consists of four different services:</p>
<ul>
<li>Table Storage</li>
<li>Queue Storage</li>
<li>Blob Storage</li>
<li>Azure Files</li>
</ul>
<p>They all serve different purposes and offer different capabilities and limits. While their names are self-explanatory, you will see that each is a completely different service, and though they can be used in connection with each other, they require a different set of skills to be able to do this efficiently, and you need to use best practices. Additionally, Azure Storage offers an additional service called disk storage, which is a feature used by virtual machines. Because of that, it will not be covered in this book. Nonetheless, you can find a link to documentation in the <em>Further reading </em>section. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Different types of storage account</h1>
                </header>
            
            <article>
                
<p class="mce-root">Azure Storage offers three different types of storage account:</p>
<ul>
<li><strong>General-purpose Standard</strong>: Supporting tables, blobs, files, and queues, and three different types of blob: block blobs, page blobs, and append blobs</li>
<li><strong>General-purpose Premium</strong>: Limited to blobs only, and supporting page blobs</li>
<li><strong>Blob storage with hot/cool access tiers</strong>: Limited to blobs only and supporting block blobs and append blobs</li>
</ul>
<p>You will learn more about different kinds of blob in the next sections. The question, for now, is: what is the difference between the standard and premium accounts—besides pricing of course? You can define them as follows:</p>
<ul>
<li><span class="packt_screen">Standard</span>: The most common choice with reasonable performance and support for all types of data. These accounts use magnetic disks for storage.</li>
<li><span class="packt_screen">Premium</span>: Accounts with better performance, thanks to the use of SSD disks—recommended for VMs and when you require quick access to data stored on them.</li>
</ul>
<p>If you would like to compare performance for both types of accounts, here is a comparison for 128 GB disks:</p>
<ul>
<li><span class="packt_screen">Standard</span>: 500 I/O operations / sec, throughput 50 MB / sec, <span>€3,78 per month</span></li>
<li><span class="packt_screen">Premium</span>: 500 I/<span>O operations / sec, throughput 100 MB / sec, €15,12</span><span> per month</span></li>
</ul>
<p>So, as you can see, the <span class="packt_screen">Premium </span>option offers roughly twice the throughput over <span class="packt_screen">Standard</span>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Securing Azure Storage</h1>
                </header>
            
            <article>
                
<p>In general, there are two ways of securing access to your Storage Accounts:</p>
<ul>
<li>Azure AD with RBAC</li>
<li>SAS tokens</li>
</ul>
<p>Additionally, blobs can be accessed publicly (of course, only if you decide to do so). Depending on your needs, one option or another may cover your requirements—this, of course, depends on the characteristics of your application. The following is the difference between those two methods of securing Azure Storage:</p>
<ul>
<li><strong>RBAC</strong>: This method is used to secure management operations on your accounts. You can restrict access to specific features of a service to only a specific group defined in Azure AD. However, you are unable to use this method to secure a blob or a table (although you can do it indirectly by securing access to an SAS token).</li>
<li><strong>SAS tokens</strong>:<strong> </strong>These are long strings, which store different parameters describing access to a resource. They specify a service type, permissions, and the lifetime of a token, or restrict access to an IP address.</li>
</ul>
<p>Here is an example of an SAS token:</p>
<pre>https://myaccount.blob.core.windows.net/securecontainer/blob.txt?<strong>sv=2015-04-05&amp;st=2015-04-29T22%3A18%3A26Z&amp;se=2015-04-30T02%3A23%3A26Z&amp;sr=b&amp;sp=rw&amp;sip=168.1.5.60-168.1.5.70&amp;spr=https&amp;sig=Z%2FRHIX5Xcg0Mq2rqI3OlWTjEg2tYkboXr1P9ZUXDtkk%3D</strong></pre>
<p>As you can see, it restricts access to a <kbd>blob.txt</kbd><strong> </strong>file stored as a blob in the<strong> </strong><kbd>securecontainer</kbd> container. It defines parameters, such as service version (<kbd>sv</kbd>), expiry time (<kbd>se</kbd>), or the actual signature of a token (<kbd>sig</kbd>).<strong> </strong>In general, with SAS tokens, you are able to restrict access to either an account or a service (and thanks to that also, for example, to a range of entities in Table Storage).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Replication</h1>
                </header>
            
            <article>
                
<p>When using a cloud, you have to expect that any service can be down at anytime. Although Azure Storage is considered one of the most durable services (because many services in Azure rely on it), it is possible that it will face an outage. To mitigate problems related to such failures, it offers four different kinds of replication:</p>
<ul>
<li><strong><span>Locally-redundant storage (</span>LRS)</strong>: Three copies of your data within the same data center</li>
<li><strong><span>Zone-redundant storage (</span>ZRS)</strong>: T<span>hree copies of your data within the same region</span></li>
<li><strong><span>Geo-redundant storage (</span>GRS)</strong>: T<span><span>hree copies of your data within the same data center plus three copies in another region</span></span></li>
<li><strong><span>Read-access geo-redundant storage (</span>RA-GRS)</strong>: T<span>hree copies of your data within the same data center plus three copies in another region with the ability to read from that region</span></li>
</ul>
<p>When architecting an application using Azure Storage, you have to carefully design its availability requirements. Depending on your expectations, a different model may suit you better.</p>
<div class="packt_tip">When using a model that replicates data to another data center (basically GRS and RA-GRS), take into account the cost of transferring data between different regions.</div>
<p>You may wonder how durable LRS is compared to other replication models. To define that, you have to understand how data is stored within a single data center. In fact, disks for Azure Storage are installed within racks, which build a bigger concept known as a stamp. Stamps are configured in such a way that they use different power lines and networks, and thanks to such a setup, it is possible to store copies of your data in different fault domains, ensuring that if one fails, the other two will still work. Microsoft states that LRS is designed to provide at least <span>99.999999999% durability. If that is not enough, you may consider other models.</span></p>
<div class="packt_tip">When using RA-GRS, do not take for granted the ability to easily write to the secondary region if an outage occurs. It is Microsoft's responsibility to initiate a failover (as opposed to, for instance, Azure Cosmos DB, where it was your decision), so <strong>r</strong><span><strong>ecovery time objective</strong> </span>(<strong><span>RTO</span></strong>) consists of both time for Microsoft to make a decision, and time to change DNS entries to point to another region.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Storing data with Azure Storage Tables</h1>
                </header>
            
            <article>
                
<p>We will start our journey with Azure Storage capabilities by learning something about Table Storage. If you want to store unstructured data with almost limitless capacity and with high demands regarding availability and durability, this service is for you. In this section, you will learn how to start developing applications using Table Storage and the best practices for storing data and achieving the best performance for both writing and reading it. You will also see how to efficiently query it, and what is important when designing services using this Azure Storage capability.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating an Azure Storage service</h1>
                </header>
            
            <article>
                
<p>To get started, we have to actually create an instance of Azure Storage. To do so, please following these steps:</p>
<ol>
<li>Go to Azure Portal and click on <span class="packt_screen">+ Create a resource</span>.<strong> </strong>Search for <kbd>storage account</kbd><strong> </strong>and click on the <span class="packt_screen">Create </span>button.</li>
</ol>
<ol start="2">
<li>You will see a typical form, where you have to configure a new instance of a service. The following is an example of what I chose:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a3843f54-4e7d-423a-aa0e-b724dfe360b5.png" style="width:21.83em;height:56.92em;" width="361" height="941"/></p>
<p>Now I would like to describe some more mystique options available here:</p>
<ul>
<li><span class="packt_screen">Deployment model</span>: You can select a different deployment model depending on your current requirements. In general, for almost every new storage account, you will select <span class="packt_screen">Resource manager</span><strong> </strong>as the default option. <span class="packt_screen">Classic </span>mode is designed for legacy deployments, which use classic virtual networks. This choice also limits available options when it comes to selecting <span class="packt_screen">Account kind</span><strong> </strong>and some additional features such as <span class="packt_screen">Performance </span>tier.</li>
<li><span class="packt_screen">Account kind</span>: You have three options available here (general purpose, V1/V2, and blob). If you would like to use your storage account with multiple capabilities (tables, queues, blobs), select storage. Selecting V2 gives you the possibility to define an access tier (cool or hot), which is directly connected to the frequency of accessing data stored within an account.</li>
<li><span class="packt_screen">Secure transfer required</span>: With Azure Storage it is possible to require a secure connection if this option is enabled. Turn it on for your production workloads, so no-one will be able to access data stored within an account using, for example, HTTP instead of HTTPS.</li>
<li><span class="packt_screen">Performance</span>: It is possible to select either the <span class="packt_screen">Standard </span>or <span class="packt_screen">Premium </span>performance tier. As mentioned previously, this impacts hardware used to provision your service with common magnetic disks for the <span class="packt_screen">Standard </span>tier and SSDs for <span class="packt_screen">Premium</span>.</li>
<li><span class="packt_screen">Virtual networks</span>: As in many other services, Azure Storage can be provisioned within a virtual network, limiting access to it even more.</li>
</ul>
<p>When everything is set and ready, you can click on the <span class="packt_screen">Create </span>button and wait a moment—your account will be created and soon you will be able to start working with it.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Managing Table Storage</h1>
                </header>
            
            <article>
                
<p>When you go to the <span class="packt_screen">Overview </span>blade, you will see a dashboard with basic information available regarding your account:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/8c59ccb6-4783-4420-8643-b67536590506.png" width="1009" height="384"/></p>
<p>As you can see, it displays the information you defined while creating it, such as location, performance tier, or replication type. Additionally, when you scroll down, you will see the <span class="packt_screen">Monitoring </span>section, where you can see how the whole service works:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d7c6d840-3b9d-485f-90bd-6431d6bbc62f.png" width="1247" height="498"/></p>
<p>In this section, we are covering Table Storage, so find the <span class="packt_screen">Tables </span>blade on the left and click on it. Initially, you should see no tables at all—of course, this is something we expected as this instance of a service has been just provisioned. Nonetheless, this is one of the methods to check what is actually stored within an account:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4192d265-7acd-4613-b5d8-2bb2a954734b.png" width="656" height="315"/></p>
<p>To create a new table, simply click on the <span class="packt_screen">+ Table</span><strong> </strong>button—you will be asked to provide a table name, which is all that is needed to get started. As you probably remember, I described Table Storage as the capability for storing unstructured data. This is the reason why there are no other options for starting with a table—you simply rely on the internal specification of how this service works. The following shows what it looks like when a new container is created:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/9e23615b-2b65-4680-b3ee-424c792f4a30.png" width="945" height="295"/></p>
<p>The preceding screenshot shows an <kbd>orders</kbd> table and its URL—you may wonder what this URL is all about. As there are multiple ways to manage and use Azure services, Azure Storage allows you to use its capabilities using different methods, such as REST, Powershell, Azure CLI, or Azure Portal. When using SDKs and reading their source code, you could find that they are just wrappers around simple a REST API. This makes this particular service superbly easy to get started and work with on a daily basis. We have talked a little bit about Tables basics—now it is time to describe their schema.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Storing data in Table Storage</h1>
                </header>
            
            <article>
                
<p>Each record in Table Storage has a row structure with multiple columns. Each row has the following base columns:</p>
<ul>
<li><kbd>PartitionKey</kbd>: Identifier of a partition of a row</li>
<li><kbd>RowKey</kbd>: The row's identifier</li>
<li><kbd>Timestamp</kbd>: This column tells you when a row was recently modified</li>
<li><kbd>ETag</kbd>: Table Storage implements the optimistic concurrency model and uses ETags to control whether an entity should be modified or not</li>
</ul>
<p>Of course, you are not limited to the columns listed above—you can create any additional columns you want and give each a specified type. However, before we go any further, you have to fully understand the implications of such a design. Here you can find an example of entities stored within a single table:</p>
<p class="mce-root">2018-07-13T11:56:11.108Z</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><kbd>PartitionKey</kbd></td>
<td><kbd>RowKey</kbd></td>
<td><kbd>Timestamp</kbd></td>
<td><kbd>Name</kbd></td>
<td><kbd>Price</kbd></td>
<td><kbd>Created</kbd></td>
<td><kbd>CustomerId</kbd></td>
<td class="CDPAlignCenter CDPAlign"><strong>Quantity</strong></td>
</tr>
<tr>
<td>
<p>Order</p>
</td>
<td>
<p>16Hbs6gs8s</p>
</td>
<td>
<p>2018-07-13T11:56:11.108Z</p>
</td>
<td/>
<td/>
<td>
<p><span>2018-07-13T11:36:11.108Z</span></p>
</td>
<td>
<p>customer-001</p>
</td>
<td/>
</tr>
<tr>
<td>
<p><span>16Hbs6gs8s</span></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p><span>2018-07-13T11:57:17.108Z</span></p>
</td>
<td>
<p>Sponge</p>
</td>
<td>
<p>3.00</p>
</td>
<td>
<p><span>2018-07-13T11:36:11.108Z</span></p>
</td>
<td/>
<td>
<p>3</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the preceding example, you data is stored within multiple partitions and though a single table is used, multiple schemas still can work, so there is no need to use additional containers. Additionally, I used a simple pattern, which allows you to introduce 1:n relationship—each order has a unique <kbd>RowKey</kbd>, which can be used as a partition key for entities related to it (allowing for really easy querying of data).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">PartitionKey</h1>
                </header>
            
            <article>
                
<p>Table Storage uses partitions to distribute, load, and handle requests. The number of partition keys within a table impacts the ability to balance them. It is possible to use a single partition per table, but in most cases, this is an invalid approach, which will lower the performance of your storage account. Partition keys are limited to 1 KB in size and have to be unique within a table (so once an entity is assigned a partition key, all others that use the same value will be stored in the same storage). They <span>also </span><span>have to be strings.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">RowKey</h1>
                </header>
            
            <article>
                
<p>Each row key is a unique identifier of a row within a partition (so you can have rows using the same <kbd>RowKey</kbd> column value as long they have a different <kbd>PartitionKey</kbd>). More importantly, each table is sorted in ascending using values of row keys. This requires a smart design when you need, for example, to read only a selection of the top rows and do not want to provide their row keys (we will cover that later in this chapter). Like <kbd>PartitionKey</kbd>, <kbd>RowKey</kbd> is also limited to 1 KB and has to be a string.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Timestamp</h1>
                </header>
            
            <article>
                
<p>This column is maintained server-side and is a <kbd>DateTime</kbd> value that is changed each time an entity is modified. It is also internally used to provide optimistic concurrency, and cannot be modified. Even if you set it, the value will be ignored.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">General rules for entities</h1>
                </header>
            
            <article>
                
<p>Table Storage has some hard limitations when it comes to storing data:</p>
<ul>
<li>The maximum number of columns is 255</li>
<li>The maximum size of an entity is 1 MB</li>
<li>By default, each entity column is created as a type string—this can be overridden when it is created</li>
<li>It is not possible to store null as a value—if you do not provide a column value, an entity will be considered as if it does not have it at all</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Querying data in Table Storage</h1>
                </header>
            
            <article>
                
<p>To query data in Table Storage, you will need a simple application (it can be a console application) and an SDK for this service. You will also need an instance of Azure Storage—it can be either the one provisioned in Azure, or a local one, if you installed Storage Emulator.</p>
<div class="packt_infobox">To get started with Storage Emulator, simply search for an executable (for example, <span class="packt_screen">Start |</span> type <kbd>Storage Emulator</kbd>) and run it. It will initially create a database for storing data and run in the background, so you will not have to worry about accidentally closing it.</div>
<p>To get started, we have to install the <kbd>WindowsAzure.Storage</kbd> package using NuGet Package Manager. It has everything that is needed to start working with Azure Storage in .NET. Here you can find an example of code for creating a table:</p>
<pre>using Microsoft.WindowsAzure.Storage;<br/><br/>namespace TableStorage<br/>{<br/>    internal class Program<br/>    {<br/>        private static void Main()<br/>        {<br/>            var storageAccount = CloudStorageAccount.Parse("&lt;connection-string&gt;");<br/>            var tableClient = storageAccount.CreateCloudTableClient();<br/>            var table = tableClient.GetTableReference("orders");<br/>            <br/>            table.CreateIfNotExists();<br/>        }<br/>    }<br/>}</pre>
<p>We can briefly describe what this code does:</p>
<ol>
<li>It parses a connection string so it can be used in the following methods</li>
<li>It creates an instance of <kbd>CloudTableClient</kbd>class, which is the main class for working with Table Storage</li>
<li>It gets a reference to a table <kbd>order</kbd>, whether it exists or not</li>
<li>Finally, it creates an <kbd>orders</kbd> table, if does not exist already</li>
</ol>
<div class="packt_tip">You could also use the <kbd>Create()</kbd><strong> </strong>method instead of <kbd>CreateIfNotExists()</kbd>, although, it could break if a table has been already created.</div>
<p>Now we need to get a connection string, so depending on the storage account you would like to use you either:</p>
<ul>
<li>Have to go to Azure Portal, find your storage account, and copy a connection string from <span class="packt_screen">Access keys</span><strong> </strong>blade</li>
<li>Use the <kbd>UseDevelopmentStorage=true</kbd><strong> </strong>value for connecting with Storage Emulator</li>
</ul>
<p>When you execute an application, a table should be created without a problem. Now, when we have a table, we would like to actually insert something in it. To do so, you will need the following code:</p>
<pre>var op = TableOperation.Insert(new DynamicTableEntity("orders", Guid.NewGuid().ToString(), "*",<br/>  new Dictionary&lt;string, EntityProperty&gt;<br/>  {<br/>    {"Created", EntityProperty.GeneratePropertyForDateTimeOffset(DateTimeOffset.Now)},<br/>    {"CustomerId", EntityProperty.GeneratePropertyForString("Customer-001")}<br/>  }));<br/><br/>table.Execute(op);</pre>
<p>Here we are creating a new <kbd>TableOperation</kbd>, which accepts one argument that is an instance of <kbd>TableEntity</kbd>. <kbd>TableEntity</kbd> is a base class that contains all row properties, and has to be passed to a table (like <kbd>PartitionKey</kbd> or <kbd>RowKey</kbd>). Of course, instead of using <kbd>DynamicTableEntity</kbd>, you can derive from <kbd>TableEntity</kbd>and introduce a custom entity class. </p>
<div class="packt_tip">In the preceding example, we used the <kbd>Insert()</kbd><strong> </strong>operation, which may not be the best choice for concurrent requests. In such a scenario, it is sometimes better to use <kbd>InsertOrReplace()</kbd><strong> </strong>or <kbd>InsertOrMerge()</kbd>.</div>
<p>The last thing to do is to query a table. To do so in .NET, you will need something like this:</p>
<pre>var query = new TableQuery();<br/>var result = table.ExecuteQuery(query);<br/><br/>foreach (var entity in result)<br/>{<br/>  Console.WriteLine($"{entity.PartitionKey}|{entity.RowKey}|{entity.Timestamp}|{entity["Created"].DateTimeOffsetValue}|{entity["CustomerId"].StringValue}");<br/>}</pre>
<p>We just executed a basic query, which will return all rows from a table. While it works now, it is not the best idea to query all data within a table using such a query—in most cases, you will use something like the following:</p>
<pre>var query =<br/>  new TableQuery().Where(<br/>    TableQuery.GenerateFilterCondition("PartitionKey", QueryComparisons.Equal, "orders"));</pre>
<p>The preceding query will return all rows in a table that have an <kbd>orders</kbd> partition key. Such queries can be extended as you wish by generating further filter conditions.</p>
<div class="packt_tip">Remember that to achieve the best performance, your queries should include both <kbd>PartitionKey</kbd> and <kbd>RowKey</kbd>. Using <kbd>PartitionKey</kbd> only leads to worse results, but is <span>still acceptable. Using only</span> <kbd>RowKey</kbd> <span>will result in reading the whole partition anyway. Not using those columns will result in reading the whole table.</span></div>
<p>You can also check what is stored in a table using Azure Storage Explorer:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/3480ecc7-bd3c-4579-9a09-89352ee0fc39.png" width="1167" height="336"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Table API in Azure Cosmos DB</h1>
                </header>
            
            <article>
                
<p>It is possible to leverage the premium offering for Table Storage using Azure Cosmos DB. Using that option has the following advantages:</p>
<ul>
<li>Automatic and manual failovers.</li>
<li>Secondary indexes (the ability to index against all properties inside a row).</li>
<li>Independent scaling across multiple regions.</li>
<li>Different consistency levels.</li>
<li>Dedicated throughput per table.</li>
</ul>
<p>While failover can be achieved using Table Storage only, the rest of the presented features are available only for Azure Cosmos DB, and can be a great solution when you like the simplicity of this service and still want to challenge it against more complicated scenarios.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing fully managed file shares with Azure Files</h1>
                </header>
            
            <article>
                
<p>When in need of creating a file share, which can be accessed by different people, you often have to either buy some hardware, which will be set up and configured for such functionality, or use third-party solutions, which can be hard to customize, or expensive. With Azure Storage, you can quickly develop a solution that is almost limitless in terms of capacity, offers industry standard protocols, and can be quickly provisioned, and ready to use.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Files concepts</h1>
                </header>
            
            <article>
                
<p>Azure Files has some basic concepts that create the whole picture of a service. In fact, it is designed to replace current on-premise file servers in terms of functionality and performance. The main difference between Azure Files and the "old" solution is accessibility (as you can set the access token and make the URL private). What is more, it is OS-agnostic, allowing you to use the very same file share mounted on different machines using Linux, Windows, or macOS. It, of course, shares other Azure Storage concepts, so you can use it with the same reliability and durability assurance. The main feature of Azure Files is support for the SMB protocol. This is a very common <span>protocol</span><span> </span><span>(and a mature one, as it was designed in the mid-1980s) for sharing computer resources, and used also for printers, and other network devices. We could summarize Azure Files as follows:</span></p>
<ul>
<li><strong>Fully managed</strong>: This is a full cloud service, where you do not have to worry about the OS or its configuration.</li>
<li><strong>Durability </strong><strong>and resiliency</strong>: With Azure Files you do not have to worry about not having access to data stored and securing your resources against power failures, and other outages.</li>
<li><strong>Common dev tools</strong>: Accessing Azure Files is easy, thanks to the system I/O APIs, appropriate SDKs, or even REST APIs.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with Azure Files</h1>
                </header>
            
            <article>
                
<p>When you go to Azure Portal and open your Azure Storage instance, you can find the Files blade. It is very similar to the one that will be discussed for Blob Storage. It displays a list of available file shares, as seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/e4d8165d-ecc6-4010-8cb5-97b5386923ba.png" width="1165" height="385"/></p>
<p>From this screen, you have the ability to create a new one by clicking on the <span class="packt_screen">+ File share</span><strong> </strong>button. The important thing here is the value of the <span class="packt_screen">Quota </span>field—it determines the maximum capacity of a file share.</p>
<div class="packt_infobox">The maximum value for the quota of a file share is 5,120 GB.</div>
<p>To get information about how to connect to a file share, you can click on the <span class="packt_screen">more</span> button on the right and select <span class="packt_screen">Connect</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/ef04fe98-5092-4c54-9679-888d21ea26e5.png" width="991" height="474"/></div>
<p>It will display some short instructions about how it is possible to quickly connect from your computer to a specific file share. Here you can find an example of a command for Windows written in PowerShell:</p>
<pre>$acctKey = ConvertTo-SecureString -String "&lt;key&gt;" -AsPlainText -Force<br/>$credential = New-Object System.Management.Automation.PSCredential -ArgumentList "Azure\handsonazurestore", $acctKey<br/>New-PSDrive -Name Z -PSProvider FileSystem -Root "\\handsonazurestore.file.core.windows.net\handsonazure" -Credential $credential -Persist</pre>
<p>You can specify the letter of a drive using the <kbd>-Name</kbd><strong> </strong>parameter (in the preceding example it is <kbd>Z</kbd>).</p>
<div class="packt_infobox">Mapping a drive is an operation that may require additional permissions—make sure you are running all these commands as an administrator.</div>
<p>Now I can compare the contents of my file share displayed in Azure portal:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/d6487b6f-934d-4d69-b010-676d194e71a7.png" style="width:42.25em;height:45.67em;" width="625" height="677"/></div>
<p>With my mounted disk on a virtual machine:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/695b1b20-a5df-42a2-b0e5-3ab4ecee9f02.png" width="877" height="432"/></p>
<p>The whole setup took only a few minutes—this is the strength of this service, as normally I would need many hours to set everything up and achieve the same level of portability and reliability. It <span>also</span><span> </span><span>g</span><span>ives you unlimited storage capacity—nothing blocks you from attaching multiple Azure Files shares and storing all your files on them.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Blob Storage versus Azure Files</h1>
                </header>
            
            <article>
                
<p>In fact, both Azure Blob Storage and Azure Files have a similar purpose—you create them to store and share files. There are, however, some fundamental differences between them when it comes to use cases, for example:</p>
<ul>
<li>If you want to create a common file share space for your company, you will use Azure Files</li>
<li>If you want to have a space for files uploaded by your users via, for example, your website, you will use Blob Storage</li>
<li>If you want to have your files completely private, you will use Azure Files</li>
<li>If you want to configure security on a blob or a containers level, you will use Blob Storage</li>
</ul>
<p>Both services <span>also </span><span>have different pricing models (for example, Azure Files is much more expensive when it comes to paying for each GB of data).</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Queues in Azure Queue Storage</h1>
                </header>
            
            <article>
                
<p>Azure Storage—besides being a service for storing many different kinds of data—can be used also as a queue. Queue Storage is another capability that allows you to quickly develop a solution that requires a simple queue solution, and additionally is able to store in a queue millions of messages without affecting performance. In this section, you will see how to develop applications using Queue Storage and what is important when using this feature. Additionally, I assume that you already have a storage account. If not, take a look at the <strong>Storing data with Azure Storage Tables </strong>section, where I described the process of creating an account.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Queue Storage features</h1>
                </header>
            
            <article>
                
<p>In general, Queue Storage has two use cases:</p>
<ul>
<li>Processing messages asynchronously</li>
<li>Exchanging communications between different services (Azure Functions, legacy Web roles/Worker roles)</li>
</ul>
<p>It is a very simple queue solution, which can store and process messages in any format that are limited to 64 KB. The retention time of a message is seven days—after that, it is lost. The capacity of a queue is basically equal to the capacity of your storage account. In general, you should not worry that you will run out of available space. Queue Storage shares many addition features, such as virtual networks, SAS tokens, and many more, with other Azure Storage capabilities. Therefore, we will not reintroduce them in this section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Developing an application using Queue Storage</h1>
                </header>
            
            <article>
                
<p>For the purpose of presenting Queue Storage, I created two applications:</p>
<ul>
<li>Producer </li>
<li>Consumer</li>
</ul>
<p>Producer will create and push messages, which will <span>then</span><span> </span><span>be consumed by Consumer. Here you can find the code of the </span><kbd>Producer</kbd><strong> </strong><span>app:</span></p>
<pre>using System;<br/>using Microsoft.WindowsAzure.Storage;<br/>using Microsoft.WindowsAzure.Storage.Queue;<br/><br/>namespace QueueStorage.Producer<br/>{<br/>    internal class Program<br/>    {<br/>        private static void Main()<br/>        {<br/>            var storageAccount = CloudStorageAccount.Parse("UseDevelopmentStorage=true");<br/>            var queueClient = storageAccount.CreateCloudQueueClient();<br/>            var queue = queueClient.GetQueueReference("orders");<br/>            <br/>            queue.CreateIfNotExists();<br/><br/>            var message = new CloudQueueMessage($"New order ID: {Guid.NewGuid()}");<br/>            queue.AddMessage(message);<br/>        }<br/>    }<br/>}</pre>
<p>And, of course the <kbd>Consumer</kbd> app:</p>
<pre>using System;<br/>using Microsoft.WindowsAzure.Storage;<br/><br/>namespace QueueStorage.Consumer<br/>{<br/>    internal class Program<br/>    {<br/>        private static void Main()<br/>        {<br/>            var storageAccount = CloudStorageAccount.Parse("UseDevelopmentStorage=true");<br/>            var queueClient = storageAccount.CreateCloudQueueClient();<br/>            var queue = queueClient.GetQueueReference("orders");<br/><br/>            var message = queue.GetMessage();<br/>            Console.WriteLine(message.AsString);<br/>            Console.ReadLine();<br/>        }<br/>    }<br/>}</pre>
<p>When you publish a message to a queue, you can retrieve it at any time—as mentioned previously, you have seven days to fetch it from a queue. Here you can find how a message looks like when stored in a queue:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0dba0131-21e1-425d-952a-e4c890c5919c.png" width="1165" height="385"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Object storage solution – Azure Storage Blobs</h1>
                </header>
            
            <article>
                
<p>The last capability of Azure Storage is Blob Storage. In the previous sections, we were using this service to store unstructured data using Table Storage, push messages to a queue with Queue Storage, and create file shares, thanks to File Storage. In the last section of this chapter, we will focus on developing solutions that store so-called blobs. You may wonder what exactly a blob is—well, there is no single definition for that. In general. blobs are files of different types, such as text files, images, or audio. You will see how to use them in your applications, how to secure them, and how you can achieve the maximum performance.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Blob Storage concepts</h1>
                </header>
            
            <article>
                
<p>Before we go deeper into the service, you will have to understand the basic concepts of Blob Storage. Here you can find a diagram that clearly defines three main topics:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/91943c42-c9e8-41ea-aca9-fd880be20ff3.png" width="376" height="407"/></p>
<p>As you can see, we have three different concepts:</p>
<ul>
<li><strong>Account</strong>: Which is basically your Storage Account and stores all data within a Blob Storage.</li>
<li><strong>Container</strong>: Which is a logical entity holding an unlimited amount of blobs inside it. An account can have an unlimited amount of containers.</li>
<li><strong>Blob</strong>: A file stored within a container.</li>
</ul>
<p>Additionally, there are three different types of blob:</p>
<ul>
<li><strong>Block blob</strong>: Text or binary data with a maximum size of 4.7 TB. Such a blob is made of smaller blocks.</li>
<li><strong>Append blobs</strong>: A more specific type of blob, which is the best for scenarios such as logging data, or storing events or transactional logs. They are optimized for append operations.</li>
<li><strong>Page blobs</strong>: Designed for storing VHD files used by VMs.</li>
</ul>
<p>With the newest version of Storage Accounts (v2), it is possible to use the latest features of this service. One of the most interesting additions is access tiers. Now it is possible to select whether you would like to use a hot or cool tier. The choice depends on the frequency of accessing your data—if you would like to read it often, hot will be the best choice, otherwise, it is better to use the cool tier or a general-purpose account.</p>
<div class="packt_tip">The tiers aforementioned are available when you select Blob as your storage account type. They are not available for general-purpose accounts.</div>
<p>There is also one more tier; Archive—designed for storing blobs that are rarely accessed—although it is available only on the blob level. You probably wonder about the differences between these tiers. Here you can find a table that defines their pricing:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"><strong>Hot</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Archive</strong></td>
<td><strong>Cool</strong></td>
</tr>
<tr>
<td><span>First 50 TB/month</span></td>
<td><span class="price-data">$0.0184</span><span> per GB</span></td>
<td><span class="price-data">$0.01</span><span> per GB</span></td>
<td><span class="price-data">$0.002</span><span> per GB</span></td>
</tr>
<tr>
<td><span>Next 450 TB/month</span></td>
<td><span class="price-data">$0.0177</span><span> per GB</span></td>
<td><span class="price-data">$0.01</span><span> per GB</span></td>
<td><span class="price-data">$0.002</span><span> per GB</span></td>
</tr>
<tr>
<td><span>Over 500 TB/month</span></td>
<td><span class="price-data">$0.017</span><span> per GB</span></td>
<td><span class="price-data">$0.01</span><span> per GB</span></td>
<td><span class="price-data">$0.002</span><span> per GB</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p>In terms of storage, you can see that the hot<strong> </strong>tier is the most expensive and the rest are much cheaper, especially archive. Now let us check the price for 10,000 read operations:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><strong>Hot</strong>: $0.<span>004</span></li>
<li class="CDPAlignLeft CDPAlign"><strong>Cool</strong>: $0<span>.01</span></li>
<li class="CDPAlignLeft CDPAlign"><strong>Archive</strong>: $5</li>
</ul>
<p>Ouch—the difference is huge here! This is why selecting the correct tier is so important—you may end up with a solution that costs many, many dollars, only because you misused the Blob Storage tier.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Inserting data into Blob Storage</h1>
                </header>
            
            <article>
                
<p>Now we will try to actually add something to our Blob Storage. Here you can find a piece of code that allows you to upload a single file to a container:</p>
<pre>using System;<br/>using Microsoft.WindowsAzure.Storage;<br/><br/>namespace BlobStorage<br/>{<br/>    internal class Program<br/>    {<br/>        private static void Main()<br/>        {<br/>            var storageAccount = CloudStorageAccount.Parse("UseDevelopmentStorage=true");<br/>            var cloudBlobClient = storageAccount.CreateCloudBlobClient();<br/>            var container = cloudBlobClient.GetContainerReference("handsonazure");<br/><br/>            container.CreateIfNotExists();<br/><br/>            var blob = container.GetBlockBlobReference("foo.txt"); <br/>            blob.UploadText("This is my first blob!");<br/><br/>            Console.ReadLine();<br/>        }<br/>    }<br/>}</pre>
<p>As in the previous examples, this one looks pretty similar. You will need to follow these steps:</p>
<ol>
<li>Firstly you have to create an instance of <kbd>CloudStorageAccount</kbd></li>
<li>Then you need to obtain a reference to a container, and create it if it does not exist</li>
<li>Finally, you have to get a reference to a blob, and upload some contents</li>
</ol>
<p>If I open Azure Storage Explorer, I can see that a new blob was uploaded to a container:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/984feda9-76bc-48c1-b707-84ca85263ea0.png" width="1095" height="386"/></p>
<p>Of course, if I open the file, I will see that it contains the text that I uploaded:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c88f981d-9174-4377-ac5c-80d51ebc730c.png" style="width:32.75em;height:18.67em;" width="530" height="301"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Containers and permissions</h1>
                </header>
            
            <article>
                
<p>It is possible to select a proper access level when it comes to accessing a container stored within Blob Storage. If you go to Azure Portal and open your Azure Storage service, you can find the <span class="packt_screen">Blobs </span>blade. Inside it, you can click on the <span class="packt_screen">+ Container</span><strong> </strong>button, which will open a small window:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/081f7f18-611f-4311-a615-42f8d9025b0a.png" style="width:43.83em;height:20.58em;" width="747" height="350"/></p>
<p>As you can see, besides providing a name for a container, you can select <span class="packt_screen">Public access level</span>. Currently, you have three different options available:</p>
<ul>
<li><span class="packt_screen">Private</span>: For no anonymous access</li>
<li><span class="packt_screen">Blob</span>: Anonymous access on a blob level</li>
<li><span class="packt_screen">Container</span>: Anonymous access on a container level</li>
</ul>
<p>You can click on a container you created to see another screen, where you can manage it. I will use it to actually upload a file to see what other options become available. Here you can find what it will look in the portal when a file is uploaded and I click on it:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/45bd6c11-2d52-4c9e-8842-a556438b9df2.png" width="1202" height="449"/></p>
<p>Now I can see additional metadata regarding a file, manage like acquiring leases, or generate an SAS token.</p>
<div class="packt_tip">If you want to make a file read-only, click on the <span class="packt_screen">Acquire lease</span><strong> </strong>button—while it will still be possible to change it, such an action will require providing a lease ID.</div>
<p>What is more, there is a <span class="packt_screen">URL </span>property available, which can be used to access a blob directly, for example, using a browser. Here you can find how it looks like in my case:</p>
<p class="mce-root"><kbd>https://handsonazurestore.blob.core.windows.net/blob/11047_01_01.PNG</kbd></p>
<p>Now you may wonder what the difference is between <span class="packt_screen">Blob </span>and <span class="packt_screen">Container </span>access. To find out, we will use the following code:</p>
<pre>using System;<br/>using Microsoft.WindowsAzure.Storage.Blob;<br/><br/>namespace BlobStorage<br/>{<br/>    internal class Program<br/>    {<br/>        private static void Main()<br/>        {<br/>            var container = new CloudBlobContainer(new Uri("&lt;container-uri&gt;"));<br/>            var blobs = container.ListBlobs();<br/><br/>            foreach (var blob in blobs)<br/>            {<br/>                Console.WriteLine(blob.Uri);<br/>            }<br/><br/>            Console.ReadLine();<br/>        }<br/>    }<br/>}</pre>
<p>I already created two different containers—one with <span class="packt_screen">Blob </span>access, one with <span class="packt_screen">Container</span>. If I execute the preceding code for a container with full public access, the following is what I will see:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/36fd3500-e727-4b41-9f2f-6c6f26bd4c53.png" width="627" height="69"/></div>
<p>Now let us run it for a container, which has public access for blobs only:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c6135bf3-10fe-4ad2-b5ae-202df734164a.png" width="639" height="294"/></p>
<p>As you can see, container-level operations are unavailable when its access level is blob or private. Of course, if you authorize using, for instance, an access key, you will list all blobs within a container, even if it is private. Of course, it is also possible to set a container-level permission directly from your code:</p>
<pre>using System;<br/>using Microsoft.WindowsAzure.Storage;<br/>using Microsoft.WindowsAzure.Storage.Blob;<br/><br/>namespace BlobStorage<br/>{<br/>    internal class Program<br/>    {<br/>        private static void Main()<br/>        {<br/>            var storageAccount = CloudStorageAccount.Parse("UseDevelopmentStorage=true");<br/>            var cloudBlobClient = storageAccount.CreateCloudBlobClient();<br/>            var container = cloudBlobClient.GetContainerReference("blob");<br/><br/>            container.CreateIfNotExists(BlobContainerPublicAccessType.Blob);<br/><br/>            var blob = container.GetBlockBlobReference("foo.txt"); <br/>            blob.UploadText("This is my first blob!");<br/><br/>            Console.ReadLine();<br/>        }<br/>    }<br/>}</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Blob Storage: additional features</h1>
                </header>
            
            <article>
                
<p>One of the newest and coolest features of Blob Storage is the <span class="packt_screen">Soft delete</span><strong> </strong>feature. It allows you to perform an operation called a soft delete. What does this mean? In some cases, you may want to delete a file, but have the ability to easily revert the deletion within a fixed time period. In Blob Storage, that option is available via the <span class="packt_screen">Soft delete</span><strong> </strong>blade:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ea110677-c180-478e-9e60-f086e5ce6dbf.png" width="627" height="339"/></p>
<p>If you turn it on, any deleted blob will <span>still </span><span>be available within storage (but not for retrieval or modification) for a set number of days. Blob Storage </span><span>also </span><span>has two additional features, which can be used with two other Azure services:</span></p>
<ul>
<li><span class="packt_screen">Azure CDN</span>: A Content Delivery Network service for serving static content to your customers—we will cover this later in the book.</li>
<li><span class="packt_screen">Azure Search</span>: As already discussed, here you can easily set your Blob Storage as a data source for a search engine.</li>
</ul>
<p>So as you can see, this is a very flexible and useful Azure Storage capability, which can be used for file storage, as an Azure Search document store, a logs database, and much, much more. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you have learned some basics regarding one of the most important services in Azure—Azure Storage. We developed a few solutions for Tables, Queues, Files, and Blobs—each enabling you to do different things, from asynchronous message processing to creating file shares. You also read about different redundancy models and how reliable and durable this particular service is. In the <em>Further reading</em><strong> </strong>section, you will find plenty of additional resources, which will allow you to build even more skills for working with this Azure service, such as Table Storage patterns, performance targets, and a REST API reference. In the following chapters, you will learn something about data processing services, such as Azure Event Hub and Azure Stream Analytics.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What tiers are available during account creation when selecting <span class="packt_screen">Blob </span>as an account type?</li>
<li>What must you include in a query against Table Storage to achieve the maximum performance?</li>
<li>What are the available redundancy models for storage accounts?</li>
<li>What is the difference between blob and file storage?</li>
<li>Can you store binary files using Blob Storage?</li>
<li>How long does a message in Queue Storage live before it is removed?</li>
<li>What is the maximum size of a message in Queue Storage?</li>
<li>What is the maximum size of the <kbd>PartitionKey</kbd> column value?</li>
<li>What concurrency model is implemented in Table Storage?</li>
<li>What is the difference between Azure Files storage and on-premise filesystem?</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>Disk storage: <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/about-disks-and-vhds">https://docs.microsoft.com/en-us/azure/virtual-machines/windows/about-disks-and-vhds</a></li>
<li>SAS token reference: <a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1">https://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1</a></li>
<li>ARM vs classic deployment: <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-deployment-model">https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-deployment-model</a></li>
<li>Table Storage data model: <a href="https://docs.microsoft.com/en-us/rest/api/storageservices/Understanding-the-Table-Service-Data-Model">https://docs.microsoft.com/en-us/rest/api/storageservices/Understanding-the-Table-Service-Data-Model</a></li>
<li>Blob Storage pricing: <a href="https://azure.microsoft.com/en-us/pricing/details/storage/blobs/">https://azure.microsoft.com/en-us/pricing/details/storage/blobs/</a></li>
<li>File Storage performance targets: <a href="https://docs.microsoft.com/en-us/azure/storage/files/storage-files-scale-targets">https://docs.microsoft.com/en-us/azure/storage/files/storage-files-scale-targets</a></li>
<li>Guidelines for Table Storage: <a href="https://docs.microsoft.com/en-us/azure/storage/tables/table-storage-design-guidelines">https://docs.microsoft.com/en-us/azure/storage/tables/table-storage-design-guidelines</a></li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>