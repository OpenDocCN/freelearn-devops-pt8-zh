["```\n$ which docker-compose\n/usr/bin/docker-compose\n```", "```\n$ docker compose --help\nUsage:  docker compose [OPTIONS] COMMAND\nDocker Compose\nOptions:\n...\nCommands:\n  build       Build or rebuild services\n…\n  version     Show the Docker Compose version information\nRun 'docker compose COMMAND --help' for more information on a command.\n```", "```\n    $ pip install docker-compose\n    Defaulting to user installation because normal site-packages is not writeable\n    Collecting docker-compose\n      Downloading docker_compose-1.29.2-py2.py3-none-any.whl (114 kB) ——— 114.8/114.8 KB 3.9 MB/s eta 0:00:00\n    ...\n    Successfully installed attrs-22.2.0 bcrypt-4.0.1 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 docker-6.0.1 docker-compose-1.29.2 dockerpty-0.4.1 docopt-0.6.2 idna-3.4 jsonschema-3.2.0 packaging-23.0 paramiko-3.1.0 pycparser-2.21 pynacl-1.5.0 pyrsistent-0.19.3 python-dotenv-0.21.1 requests-2.28.2 texttable-1.6.7 urllib3-1.26.15 websocket-client-0.59.0\n    ```", "```\n    $ docker-compose --version\n    docker-compose version 1.29.2, build unknown\n    ```", "```\n    $ sudo apt-get install -qq docker-compose\n    Preconfiguring packages ...\n    Selecting previously unselected package pigz.\n    …\n    Setting up docker-compose (1.29.2-1) ...\n    Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n    Processing triggers for man-db (2.10.2-1) .\n    ```", "```\n    $ file /usr/bin/docker-compose\n    /usr/bin/docker-compose: Python script, ASCII text executable\n    ```", "```\n    $ sudo apt-get install docker-compose-plugin -qq\n    Selecting previously unselected package docker-compose-plugin\n    ...\n    Unpacking docker-compose-plugin (2.17.2-1~ubuntu.22.04~jammy) ...\n    Setting up docker-compose-plugin (2.17.2-1~ubuntu.22.04~jammy) ...\n    $ docker compose version\n    docker-compose version compatible with both Docker Compose v2 and v3.\n    ```", "```\nservices:\n      service_name1:\n            <SERVICE_SPECS>\n...\n      service_nameN:\n            <SERVICE_SPECS>\nvolumes:\n      volume_name1:\n            <VOLUME_SPECS>\n…\n      volume_nameN:\n            <VOLUME_SPECS>\nnetworks:\n      network_name1:\n            <NETWORK_SPECS>\n…\n      network_nameN:\n            <NETWORK_SPECS>\n```", "```\nversion: \"3.7\"\nservices:\n  # load balancer\n  lb:\n    build: simplestlb\n    image: myregistry/simplest-lab:simplestlb\n    environment:\n      - APPLICATION_ALIAS=simplestapp\n      - APPLICATION_PORT=3000\n    networks:\n      simplestlab:\n          aliases:\n          - simplestlb\n    ports:\n      - \"8080:80\"\n  db:\n    build: simplestdb\n    image: myregistry/simplest-lab:simplestdb\n    environment:\n        - \"POSTGRES_PASSWORD=changeme\"\n    networks:\n       simplestlab:\n        aliases:\n          - simplestdb\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n  app:\n    build: simplestapp\n    image: myregistry/simplest-lab:simplestapp\n    environment:\n      - dbhost=simplestdb\n      - dbname=demo\n      - dbuser=demo\n      - dbpasswd=d3m0\n    networks:\n       simplestlab:\n        aliases:\n          - simplestapp\n    depends_on:\n      - lb\n      - db\nvolumes:\n  pgdata:\nnetworks:\n  simplestlab:\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.16.0.0/16\n```", "```\n…\nservices:\n  lb:\n    environment:\n      - APPLICATION_ALIAS=simplestapp\n      - APPLICATION_PORT=3000\n…\n  db:\n    environment:\n        - \"POSTGRES_PASSWORD=changeme\"\n…\n  app:\n    environment:\n      - dbhost=simplestdb\n      - dbname=demo\n      - dbuser=demo\n      - dbpasswd=d3m0\n…\n```", "```\n...services:\n  app:\n...\n    configs:\n     - source: appconfig\n        target: /app/config\n        uid: '103'\n        gid: '103'\n        mode: 0440\nvolumes:\n…\nnetworks:\n...\nconfigs:\n  appconfig:\n    file: ./appconfig.txt\n```", "```\n$ printf \"mysecretdbpassword\" | docker secret create postgres_pass -\ndzr8bbh5jqgwhfidpnrq7m5qs\n```", "```\n…\n  db:\n    build: simplestdb\n    image: myregistry/simplest-lab:simplestdb\n    environment:\n        - POSTGRES_PASSWORD_FILE: /run/secrets/postgres_pass\n    secrets:\n    - postgres_pass\n…\nsecrets:\n  postgres_pass:\n     external: true\n```", "```\nsecrets:\n  my_secret_name:\n    file: <FULL_PATH_TO_SECRET_FILE>\n```", "```\nservices:\n  lb:\n    build: simplestlb\n    image: myregistry/simplest-lab:simplestlb\n...\n  db:\n    build: simplestdb\n    image: myregistry/simplest-lab:simplestdb\n...\n  app:\n    build: simplestapp\n    image: myregistry/simplest-lab:simplestapp\n```", "```\n$ docker-compose --project-name test build \\\n--progress quit --quiet\n$ docker image ls\nREPOSITORY                TAG           IMAGE ID       CREATED      SIZE\nmyregistry/simplest-lab   simplestapp   26a95450819f   3 days ago   73.7MB\nmyregistry/simplest-lab   simplestdb    7d43a735f2aa   3 days ago   243MB\nmyregistry/simplest-lab   simplestlb    3431155dcfd0   3 days ago   8.51MB\n```", "```\n$ docker-compose --project-name test build \\\n--progress quit --quiet\n$ docker image ls\nREPOSITORY                TAG           IMAGE ID       CREATED      SIZE\ntest-app                  latest        b1179d0492be   3 days ago   73.7MB\nmyregistry/simplest-lab   simplestapp   b1179d0492be   3 days ago   73.7MB\ntest-db                   latest        8afd263a1e89   3 days ago   243MB\nmyregistry/simplest-lab   simplestdb    8afd263a1e89   3 days ago   243MB\ntest-lb                   latest        4ac39ad7cefd   3 days ago   8.51MB\nlatest was used by default. This is what we expect in such situations, but we could have used any of the following keys to modify the build process:\n\n*   `context`: This key must be included inside the `build` key to identify the context used for each image. All the files included in this `context` directory will be passed to the container runtime for analysis. Take care to remove any unnecessary files in this path.\n*   `dockerfile`: By default, the container runtime will use any existing Dockerfile in your `build` folder, but we can use this key to change this filename and use our own.\n*   `dockerfile_inline`: This key may be very interesting, as it allows us to use `inline` definitions, as we already learned in [*Chapter 2*](B19845_02.xhtml#_idTextAnchor036), *Building Docker Images*. These quick definitions don’t permit any `COPY` or `ADD` keys.\n*   `args`: This key is equivalent to `--build-arg` and it allows us to add any required arguments to our `build` process. Remember that you should include appropriate `ARG` keys in your Dockerfile.\n*   `labels`: We can include labels in our Dockerfile, and we can also add new ones or overwrite those already defined by using the `labels` key. We will include a list with these labels in key-value format.\n*   `targets`: This key will identify which targets should be compiled in the `build` process. This may be of interest when you want to separate the base image and some additional debug ones from the production-ready final container images.\n*   `tags`: We can add more than one tag at a time. This may be pretty interesting for defining a `build` process that creates a container image for different registries. By using this key, you will be able to push to all registries at the same time (you will need to be already logged in or you will be asked for your username and password).\n*   `platforms`: Remember that we learned that `buildx` allowed us to prepare images for different container runtimes architectures (we learned how to create images for ARM64, AMD64, and so on in [*Chapter 2*](B19845_02.xhtml#_idTextAnchor036), *Building Docker Images*). In our Compose file, we can write which architectures must always be included in the `build` process. This is very interesting for automating your software supply chain.\n*   `secrets`: Sometimes, we need to include a token, an authentication file with a username and password, or a certificate for accessing some SSL-protected site during the `build` process. In such situations, we can use a secret to introduce this information only at such a stage. You should always avoid adding sensitive information to your container images. Secrets will only be accessible to the containers created for building the image; thus they will not be present in the final image. We will need to define a `secrets` object in our Compose file, but this time, it will be used in the `build` process instead of the container runtime. Here is an example of adding a certificate required to access a server:\n\n    ```", "```\n\n    We can use the long syntax, which is always recommended because it allows us to set the destination path for the included file (by default, the secrets will be present in `/run/secrets`) and its permissions and ownership:\n\n    ```", "```\n\nThere are some keys that may be interesting to you if you plan on modifying the default caching behavior during the `build` processes. You can find additional information at [https://docs.docker.com/compose/compose-file/build/](https://docs.docker.com/compose/compose-file/build/).\nYou can push images to the registries to share them using `docker-compose` if you included a registry in the `image` key definition; otherwise, images will be local.\nBy default, all images will be pushed when you execute `docker-compose push`, but as with any other Compose action, you may need to pass a service as an argument. In this case, it is useful to use the `--include-deps` argument to push all the images of the services defined in the `depends_on` key. This will ensure that your service will not miss any required images when it is executed:\n\n```", "```\n\n Notice, in this example, that even though we have just pushed the `app` service image, `lb` and `db` are also pushed to our registry because they were declared as dependencies.\nIn this section, we learned how to use `docker-compose` for building and sharing our application’s components. In the next section, we will run containers defined in our Compose YAML files and review their logs and status.\nRunning and debugging multi-container applications\nApplications executed using Docker Compose will be orchestrated but without high availability. This doesn’t mean you can’t use it in production, but you may need additional applications or infrastructure to keep your components always on.\nDocker Compose provides an easy way of running applications using a *single point of management*. You may have more than one YAML file for defining your application’s components, but the `docker-compose` command will merge them into a single definition. We will simply use `docker-compose up` to launch our complete application, although we can manage each component separately by simply adding its service’s name. `docker-compose` will refresh the components’ status and will just recreate those that have stopped or are non-existent. By default, it will attach our terminal to all application containers, which may be very useful for debugging but it isn’t useful for publishing our services. We will use `-d` or `--detach` to launch all container processes in the background.\nThe `docker-compose up` execution will first verify whether all the required images are present on your system. If they aren’t found, the container runtime will receive the order of creating them if a `build` key is found. If this key isn’t present, images will be downloaded from the registry defined in your `image` key. This process will be followed every time you execute `docker-compose up`. If you are changing some code within your application’s component folders, you will need to recreate them by changing your compose YAML `image` tags or the image’s digest.\nImportant note\nYou can use `docker-compose up --d --build` to specifically ask your container runtime to rebuild all the images (or part of them if you specified a service). As you may expect, the runtime will check each image layer (`RUN` and `COPY`/`ADD` keys) and rebuild only those that have changed. This will avoid the use of an intermediate `docker-compose build` process. Remember to maintain your container runtime disk space by pruning old unnecessary images.\nAs we already mentioned in this section, containers will always be created if they don’t exist when we execute `docker-compose up`. But in some cases, you will need to execute a fresh start of all containers (maybe some non-resolved dependencies in your code may need to force some order or reconnection). In such situations, we can use the `--force-recreate` argument to enforce the recreation of your services’ containers.\nWe will use the `entrypoint` and `command` keys to *overwrite* the ones defined in the images used for creating each service container and we will specifically define which services will be available for the users by publishing them. All other services will use the internally defined network for their communications.\nAs you may expect, everything we learned about `<PROJECT_NAME>_<SERVICE_NAME>_<INSTANCE>`); we will instead use the service’s name to locate a defined service. For example, we will use `db` in our `app` component connection string. Take care because your instance name will also be available but shouldn’t be used. This will really break the portability and dynamism of your applications if you move them to clustered environments where instances’ names may not be usable or if you use more than one replica for some services.\nImportant note\nWe can manage the number of container replicas for a service by using the `replicas` key. These replicas will run in isolation, and you will not need a load balancer service to redirect the service requests to each instance. Consider the following example:\n`services:`\n`app:`\n`…`\n`deploy:`\n`replicas: 2`\n`…`\nIn such a situation, two containers of our `app` service will be launched. Docker Swarm and Kubernetes will provide TCP load balancer capabilities. If you need to apply your own balancer rules (such as specific weights), you need to add your own load balancer service. Your container runtime will just manage OSI layer 3 communications ([https://en.wikipedia.org/wiki/OSI_model](https://en.wikipedia.org/wiki/OSI_model)).\nMulti-container applications defined in a Compose file will run after we execute `docker-compose up --detach`, and to review their *state*, we will use `docker-compose ps`. Remember to add your project in all your commands if you need to overwrite the default project’s name (current folder). We can use common `--filter` and `--format` arguments to filter and modify the output of this command. If some of the service’s containers are missing, maybe they didn’t start correctly; by default, `docker-compose ps` will only show the running containers. To review all the containers associated with our project, we will use the `--all` argument, which will show the running and stopped containers.\nIf any issues are found in our project’s containers, we will see them as exited in the `docker-compose ps` command’s output. We will use `docker-compose logs` to review all container logs at once, or we can choose to review only the specific service in error by adding the name of the service to this command.\nWe can use the `--file` (or `-f`) argument to define the complete path to our Compose YAML file. For this to work, it is very useful to first list all the Compose applications running in our system by using `docker-compose ls`. The full path to each application’s Compose YAML file will be shown along with its project’s name, as in this example:\n\n```", "```\n\n In this case, we can add the path to the Compose file to any `docker-compose` action:\n\n```", "```\n\n This will work even with `build` actions, as the Compose YAML file location will be used as a reference for all commands. The `context` key may be included to modify its behavior.\nWe can review the port exposed for the application in the `docker-compose ps` output. To review our application’s logs, we can use `docker-compose logs`, and each service will be represented in a different random color. This is very useful for following the different entries in each service. We can specify a single service by passing its name as an argument.\nThe following screenshot shows the output of `docker-compose logs` using `--tail 5` to only retrieve the latest five lines:\n![Figure 5.2 – Service container logs retrieved by using docker-compose logs](img/B19845_05_02.jpg)\n\nFigure 5.2 – Service container logs retrieved by using docker-compose logs\nNotice that in this simple test, we only have two services, and colors are applied to each one. We retrieved only the latest five lines of each container by adding `--tail 5`. This argument applies to all containers (we didn’t get the latest five lines of all logs merged). It is also important to mention that service names must be used as arguments when we need to use an action in a specific service. We will never use the container names; hence, we need to include the appropriate project name.\nWe can use the same approach to access a container’s namespace by using the `exec` action. Remember that we learned in [*Chapter 4*](B19845_04.xhtml#_idTextAnchor096), *Running Docker Containers*, that we can execute a new process inside our container (it will share all the container’s process kernel namespaces). By using `docker-compose exec <SERVICE_NAME>`, we can execute a new process inside any of our service’s containers:\n\n```", "```\n\n In summary, we will be able to run the same actions for containers by using `docker-compose`.\nFor you as a developer, Docker Compose can really help you develop applications faster. You will be able to run all application containers at once. In the development stage, you can include your code in specific containers by mounting a volume, and you can verify how your changes affect other components. For example, you can mount the code of one application component and change it while other components are running. Of course, you can do this without the `docker-compose` command line, but you will need to automate your deployments with scripts and verify the containers’ state. Docker Compose orchestrates this for you, and you can focus on changing your code. If you work in a team and all other developers provide container images and you share some application information, you can run these images locally while you are still working on your component.\nNow that we know how to run and interact with multi-container applications, we will end this chapter by learning how to use environment variables to deploy your applications under different circumstances.\nManaging multiple environments with Docker Compose\nIn this section, we will learn how to prepare our Compose YAML files as templates for running our applications in different environments and under different circumstances, such as developing or debugging.\nIf you are familiar with the use of environment variables in different operating systems, this section will seem pretty easy. We already learned how to use variables to modify the default behavior of Dockerfiles ([*Chapter 2*](B19845_02.xhtml#_idTextAnchor036), *Building Docker Images*) and containers at runtime ([*Chapter 4*](B19845_04.xhtml#_idTextAnchor096), *Running Docker Containers*). We used variables to overwrite the default values defined and modify the `build` process or the execution of container image processes. We will use the same approach with Compose YAML files. We will now review some of the different options we have to use variables with the `docker-compose` command line.\nWe can define a `.env` file with all the variables we are going to use in a Compose YAML file defined as a template. Docker Compose will search for this file in our project’s root folder by default, but we can use `--env-file <FULL_FILE_PATH>` or the `env_file` key in our Compose YAML file. In this case, the key must be set for each service using the environment file:\n\n```", "```\n\n The environment file will overwrite the values defined in our images. Multiple environment files can be included; thus, the order is critical. The lower ones in your list will overwrite the previous values, but this also happens when we use more than one Compose YAML file. The order of the arguments passed will modify the final behavior of the execution.\nYou, as a developer, must prepare your Compose YAML files with variables to modify the execution passed to your container runtime.  The following example shows how we can implement some variables to deploy applications in different environments:\n\n```", "```\n\n In this example, we can complete our variables with the following `.``env` file:\n\n```", "```\n\n This environment file will help us define a base build and deployment. Different Dockerfiles will be included – `Dockerfile.dev` and `Dockerfile.prod`, for example.\nWe can then verify the actual configuration applied using `docker-compose`:\n\n```", "```\n\n All the values have already been assigned using the `.env` file, but these can be overridden manually:\n\n```", "```\n\n Remember that profiles and targets can also be used to prepare specific images and then run the services completely customized.\nWe can now review some labs that will help us better understand some of the content of this chapter.\nLabs\nThe following labs will help you deploy a simple demo application by using some of the commands learned in this chapter. The code for the labs is available in this book’s GitHub repository at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git). Ensur[e that you have the latest revision available by simply executing `git`](https://github.com/PacktPublishing/Docker-for-Developers-Handbook.git) `clone https://github.com/PacktPublishing/Docker-for-Developers-Handbook.git` to download all its content or `git pull` if you have already downloaded the repository before. All commands and content used in these labs will be located inside the `Docker-for-Developers-Handbook/Chapter5` directory.\nIn this chapter, we learned how to deploy a complete application using `docker-compose`. Let’s put this into practice by deploying a sample application.\nDeploying a simple demo application\nIn this lab, we will learn how to deploy an application with three components: a load balancer, a frontend, and a database.\nThere are hundreds of good Docker Compose examples and, in fact, there are many vendors who provide their applications packaged in the Compose YAML format, even for production. We chose this pretty simple application because we are focusing on the Docker command line and not on the application itself.\nIf you list the content of the `Chapter5` folder, you will see a folder named `simplestapp`. There is a subfolder for each component and a Compose file that will allow us to deploy the full application.\nThe Compose YAML file that defines our application contains the following code:\n\n```", "```\n\n This application is a very simplified demo for showing how various components could be deployed. Never use environment variables for your sensitive data. We already learned how to use `configs` and `secrets` objects in this chapter. It is good to also notice that we didn’t use a non-root user for the database and load balancer components. You, as a developer, should always try to keep security at the maximum on your application components. It is also important to notice the lack of health checks at the Dockerfile and Compose levels. We will learn more about application health checks in Kubernetes later in this book because it may not always be a good idea to include some **Transmission Control Protocol** (**TCP**) check tools in your images. In [*Chapter 8*](B19845_08.xhtml#_idTextAnchor170), *Deploying Applications with the Kubernetes Orchestrator*, we will learn how this orchestration platform provides internal mechanisms for such tasks and how we can enforce better security options.\nIn this lab, only one volume will be used for the database component, and the only service published is the load balancer. We included this service just to let you understand how we can integrate a multilayer application and only share one visible component. All images will be created locally (you may want to upload to your own registry or Docker Hub account). Follow the next steps to deploy the `simplestapp` application described in the `compose` file:\n\n1.  To build all the images for this project, we will use `docker-compose build`:\n\n    ```", "```\n\nImportant note\nIf you reset your Docker Desktop before starting the labs, you may find some errors regarding an old Docker container runtime integration on your WSL environment:\n`$ docker-compose --file simplestlab/docker-compose.yaml --project-name` `chapter5 build`\n`docker endpoint for \"default\"` `not found`\nThe solution is very easy: simply remove your old Docker integration by removing your `.docker` directory, located in your home directory: `$ rm -``rf ~/.docker`.\n\n1.  We can take a look at the images created locally:\n\n    ```", "```\n\n     2.  Let’s now create the container for the database service:\n\n    ```", "```\n\n    All the objects required for the database service are created. It is not running yet, but it is ready for that.\n\n     3.  We run this service alone and review its status:\n\n    ```", "```\n\n    If you omit the Compose filename and the project’s name, we will get neither the services nor the containers:\n\n    ```", "```\n\n    Always ensure you use the appropriate name and Compose file for all the commands related to a project:\n\n    ```", "```\n    $ docker-compose --file \\\n    simplestlab/docker-compose.yaml \\\n    --project-name chapter5 up -d\n    [+] Running 3/3\n     :: Container chapter5-lb-1   Started                                        2.0s\n     :: Container chapter5-db-1   Running                                  0.0s\n    Created to Started.\n    ```", "```\n\n     4.  We can now review the status of all our application components and the ports exposed:\n\n    ```", "```\n\n![Figure 5.3 – The simplestlab application](img/B19845_05_03.jpg)\n\nFigure 5.3 – The simplestlab application\nThis allows us to graphically review how requests are distributed when multiple backends are available.\n\n1.  We can scale our `app` component in this example. This option may be complicated or impossible in other deployments, as it really depends on your own application code and logic. For example, you should scale a database component without appropriate database internal scale logic (you should review the database server vendor’s documentation):\n\n    ```", "```\n    $ docker-compose --file \\\n    simplestlab/docker-compose.yaml \\\n    --project-name chapter5 logs app\n    chapter5-app-1  | dbuser: demo dbpasswd: d3m0\n    …\n    chapter5-app-1  | dbuser: demo dbpasswd: d3m0\n    …\n    chapter5-app-2  | Can use environment variables to avoid '/APP/dbconfig.js' file configurations.\n    ```", "```\n\nFinally, your application is up and running, and we can move on to the next lab, in which we will use the same Compose file to deploy a second project with the same application.\nDeploying another project using the same Compose YAML file\nIn this simple example, we will review and discuss the problems we may encounter by running two projects using the same Compose YAML file. To do this, we will follow these instructions:\n\n1.  Let’s create a new project by using a new project name:\n\n    ```", "```\n    networks:\n      simplestlab:\n        ipam:\n          driver: default\n    ```", "```\n    $ docker-compose --file \\\n    simplestlab/docker-compose.yaml \\\n    --project-name newdemo create\n    [+] Running 5/5\n     :: Network newdemo_simplestlab  Created                           0.9s\n     :: Volume \"newdemo_pgdata\"      Created                         0.0s\n     :: Container newdemo-db-1       Created                         0.2s\n     :: Container newdemo-lb-1       Created                    0.2s\n    volume and network objects were created with the project prefix. We will not be able to reuse the project name because object names must be unique.\n    ```", "```\n\n     2.  Let’s run all the application components now:\n\n    ```", "```\n\n     3.  We change our Compose YAML and add the `LB_PORT` variable as the port for exposing our application:\n\n    ```", "```\n\n    Then, we test it again:\n\n    ```", "```\n\n    Let’s review the component status:\n\n    ```", "```\n    $ docker-compose ls\n    NAME                STATUS              CONFIG FILES\n    chapter5            running(4)          /home/frjaraur/labs/Chapter5/simplestlab/docker-compose.yaml\n    newdemo             running(3)          /home/frjaraur/labs/Chapter5/simplestlab/docker-compose.yaml\n    ```", "```\n\nWith this simple example of the usual problems you may find while preparing your applications, we will end this chapter’s labs by removing all the objects created.\nRemoving all projects\nTo remove all the created projects, we will perform the following steps:\n\n1.  We will remove all the deployed projects by using `docker-compose down`:\n\n    ```", "```\n\n    You may notice that volumes are not listed as removed. We can review the current volumes on your system:\n\n    ```", "```\n    $ docker-compose -f  /home/frjaraur/labs/Chapter5/simplestlab/docker-compose.yaml --project-name newdemo down --volumes\n    WARN[0000] The \"LB_PORT\" variable is not set. Defaulting to a blank string.\n    [+] Running 5/5\n    :: Container newdemo-app-1      Removed                                  0.0s\n    :: Container newdemo-lb-1       Removed                              0.1s\n    :: Container newdemo-db-1       Removed                                0.1s\n    :: Volume newdemo_pgdata        Removed                                       0.1s\n    newdemo project was removed, as we can verify now, but the volume from the Chapter5 project is still present:\n\n    ```", "```\n\n    ```", "```\n\n     2.  We remove the remaining volume manually:\n\n    ```", "```\n\nAdditional labs are included in this book’s GitHub repository.\nSummary\nIn this chapter, we covered the basic usage of Docker Compose and discussed how it will help you develop and run your multi-component applications locally. You will be able to run and review the status of all your application’s components using a single command line. We learned about the syntax of Compose YAML files and how to prepare template-like files for developing your applications using different customizations. You will probably run your applications in production using a clustered orchestrated environment such as Docker Swarm or Kubernetes, but `docker-compose` does also provide a basic orchestration for running your multi-container applications in production using a single server. Understanding the basic orchestration and networking features of Docker Compose will help us introduce more sophisticated orchestration methods, which we will learn about in the next chapters.\nIn the next chapter, we will briefly review orchestration platforms that will help us run our applications cluster-wide.\n\n```"]