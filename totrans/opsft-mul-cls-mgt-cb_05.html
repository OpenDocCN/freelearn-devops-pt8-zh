<html><head></head><body>
		<div id="_idContainer097">
			<h1 id="_idParaDest-82" class="chapter-number"><a id="_idTextAnchor090"/>5</h1>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor091"/>OpenShift Deployment</h1>
			<p>In this chapter, we will navigate through a deployment process for Red Hat OpenShift. A successful implementation case will be only possible when you understand the needs of the architecture, understand the branding technology applied to the underlying infrastructure, and understand the workloads you are supposed to have on top of OpenShift.</p>
			<p>As you have seen in <a href="B18015_01.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Hybrid Cloud Journey and Strategies</em>, there are a lot of options in this multi-cluster universe. So, what are the best options for you? How can you choose? It can be very confusing if you start to prepare the requisites of the deployment without properly preparing yourself for it. That said, we must stress the importance of being aligned with the architecture chosen to ensure the expected deployment will succeed. Runtime changes and unplanned gaps can have unexpected results and create a platform full of flaws, causing the platform to malfunction. Making a comparison with the real world, it is like a ship that cannot be smoothly loaded with containers because it is always being repaired and, as such, is not ready to perform long trips or even depart.</p>
			<p>Considering the explanation in the first part of this book, there are now several options for deploying OpenShift clusters. Next, we'll start with a checklist of questions that will help you make the right decision for your needs. Keep in mind, there are no right or wrong answers, but it will help you decide which is the best for you.</p>
			<p>This chapter covers the following:</p>
			<ul>
				<li>Requirements </li>
				<li>OpenShift installation prerequisites </li>
				<li>Preparing for the installation</li>
				<li>Installation</li>
				<li>What's next?</li>
			</ul>
			<p>Let's get started then!</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor092"/>Requirements</h1>
			<p>This is a practical chapter in which you will deploy an OpenShift cluster using what we consider to be the most complex deployment procedure: the <strong class="bold">User-Provisioned Installer</strong> (<strong class="bold">UPI</strong>)/agnostic installation.</p>
			<p>The source code used in this chapter is available at <a href="https://github.com/PacktPublishing/OpenShift-Multi-Cluster-Management-Handbook/tree/main/chapter05">https://github.com/PacktPublishing/OpenShift-Multi-Cluster-Management-Handbook/tree/main/chapter05</a>.</p>
			<p>As we already covered in this book, there are many different types of installations and supported providers, and it is almost impossible to cover every combination of them – neither is it our intention, as there is plenty of documentation, tutorials, and great references on the internet that will guide you through all types of installations. </p>
			<p>That said, we understand that the most-added-value deployment procedure we can bring to you is the UPI/agnostic one; when you make it, you will be able to understand and easily execute the other types of installations. The reason for this is simple: with the UPI/agnostic installation, you are responsible for providing all the prerequisites that an OpenShift deployment requires, while with the <strong class="bold">Installer-Provisioned Infrastructure</strong> (<strong class="bold">IPI</strong>) deployment, the installer itself will provide the prerequisites for you automatically within the underlying provider. </p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor093"/>OpenShift checklist opt-in</h2>
			<p>Have you seen the checklist we gave to you in <a href="B18015_02.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Architecture Overview and Definitions</em>, and <a href="B18015_03.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Multi-Tenant Considerations</em>. If you are reading this chapter to implement an OpenShift cluster in an enterprise and you haven't read those chapters yet, we strongly recommend you go back and read them, as they contain important aspects you need to think about and consider first, before deploying a cluster.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor094"/>Lab requisites</h2>
			<p>To follow the labs in this chapter, you will need the following:</p>
			<ul>
				<li>A hypervisor or cloud provider in which you can spin up the instances. You can also use bare metal servers if you have them available.</li>
				<li>This is the minimum requirement for the VMs:<ul><li>One temporary server for the Bootstrap node</li>
<li>Three servers for master nodes, with 2 vCPU, 8 GB RAM, and 50 GB of disk (minimum)</li>
<li>Two servers for worker nodes, with 2 vCPU, 8 GB RAM, and 50 GB of disk (minimum)</li>
<li>One server for the bastion node, with 2 vCPU, 4 GB RAM, and 20 GB of disk (minimum)</li>
</ul></li>
			</ul>
			<p>If you don't have enough resources available in your environment, you can also use a three-node cluster, in which masters and workers are co-located in the same nodes. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The requirements listed are valid only for a lab. Refer to <a href="B18015_02.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Architecture Overview and Definitions</em>, to get a reliable sizing for an enterprise installation. </p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor095"/>OpenShift installation prerequisites</h1>
			<p>Before starting your<a id="_idIndexMarker315"/> journey through OpenShift deployment, you<a id="_idIndexMarker316"/> must observe several prerequisites. First, we will explore the options according to the decision that you previously made in the opt-in form.</p>
			<p>As explained in <a href="B18015_01.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Hybrid Cloud Journey and Strategies</em>, OpenShift has three installation modes: <strong class="bold">Installer-Provisioned Infrastructure</strong> (<strong class="bold">IPI</strong>), <strong class="bold">User-Provisioned Infrastructure</strong> (<strong class="bold">UPI</strong>), and <strong class="bold">agnostic</strong> (that is, a bare <a id="_idIndexMarker317"/>metal installer). It is very<a id="_idIndexMarker318"/> important to remember that no option will work well for every case, although the best option is the one that best fits into the architecture designed for you previously.</p>
			<p>This chapter is very focused on deployment and all things related to spawning up a cluster by yourself, so keep that in mind when making your own lab and enjoy the tips and materials on our GitHub repository, which will be a real Swiss Army knife for you.</p>
			<p>The following table shows you which<a id="_idIndexMarker319"/> installation methods you have available, according to the provider chosen (at the time this book was written):</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18015_Table_5.1.jpg" alt=""/>
				</div>
			</div>
			<p>Regarding the terms in the previous table, we classified some of the available options for each infrastructure provider to<a id="_idIndexMarker320"/> give you an overview of the current possibilities (at the time of writing this book). When we say <strong class="bold">Recommended</strong>, we are not only giving our perspective, but we are trying to say this is a <em class="italic">common and best choice</em> for that scenario. <strong class="bold">Possible</strong> indicates a valid option, but you will have some penalties, such <a id="_idIndexMarker321"/>as losing some great automation features the product brings out of the box. For that reason, we classified them as <strong class="bold">Possible</strong>, but not as the best choice. <strong class="bold">Not Available</strong> is self-explanatory.</p>
			<p>As the prerequisites will be different according to the installation method, we prepared a matrix that helps you start the preparation of the underlying infrastructure to begin the cluster deployment:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18015_Table_5.2.jpg" alt=""/>
				</div>
			</div>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor096"/>UPI/agnostic installer </h2>
			<p>Any OpenShift installation must have a valid Red Hat subscription, the OpenShift installer binaries, a<a id="_idIndexMarker322"/> <strong class="bold">pull secret</strong> file, a<a id="_idIndexMarker323"/> public <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) <strong class="bold">key</strong>, and the resources available according to each provider.</p>
			<p>In this section, we will <a id="_idIndexMarker324"/>guide you through a feasible and reliable cluster installation, whatever provider you have chosen. We will also <a id="_idIndexMarker325"/>set up the prerequisites needed using practical examples – feel free to use those configurations as many times as you need.</p>
			<p>Note that the files used in this chapter are also available in our GitHub repository: <a href="https://github.com/PacktPublishing/OpenShift-Multi-Cluster-Management-Handbook/tree/main/chapter05">https://github.com/PacktPublishing/OpenShift-Multi-Cluster-Management-Handbook/tree/main/chapter05</a></p>
			<p>So, let's start with the prerequisite systems that are not part of OpenShift itself, but are indispensable to ensure that everything will work fine during the deployment process. </p>
			<h3>DNS</h3>
			<p>In this section, we<a id="_idIndexMarker326"/> will discuss the <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) requirements to provision an OpenShift<a id="_idIndexMarker327"/> cluster. For demonstrations purposes, we will give the minimum configuration to make everything work; for in-depth settings, check the references we have provided in the last chapter of this book. </p>
			<p>For our lab, we will<a id="_idIndexMarker328"/> use the <strong class="bold">BIND</strong> tool running in a Red Hat Enterprise Linux 8 VM; however, you can use any other DNS server on top of Windows or your preferred Linux distribution. We will refer to this Linux VM from now on by the name <strong class="bold">bastion</strong>, which is a kind of<a id="_idIndexMarker329"/> convention with Red Hat architectures. If you want to strictly follow the instructions in this chapter, we recommend you use a fresh installation of Red Hat Enterprise Linux 8, using the minimum install.</p>
			<p>An OpenShift cluster requires a dedicated subdomain. To facilitate your understanding, we will use a hypothetical <em class="italic">hybrid cloud company</em> that uses <strong class="source-inline">hybridmycloud.com</strong> as its main public domain. The complete subdomain for the OpenShift cluster will be <strong class="source-inline">ocp.hybridmycloud.com</strong>.</p>
			<p>To install BIND, run the following commands on your bastion VM:</p>
			<pre class="source-code">$ sudo yum install bind bind-utils -y
$ sudo systemctl enable --now named
$ sudo firewall-cmd --permanent --add-port=53/tcp
$ sudo firewall-cmd --permanent --add-port=53/udp
$ sudo firewall-cmd --reload</pre>
			<p>Now, we are going to configure <a id="_idIndexMarker330"/>the DNS server to be used with the OpenShift installation and applications. Perform the following steps to accomplish this:</p>
			<ol>
				<li>Create a subdomain zone by adding the following code in your <strong class="source-inline">named.conf</strong> file. You can alternatively download a file ready to be used in our GitHub repository at <strong class="source-inline">chapter05/named.conf</strong>:<p class="source-code">$ sudo cat &lt;&lt; EOF &gt;&gt;  /etc/named.conf</p><p class="source-code">zone "ocp.hybridmycloud.com" IN {</p><p class="source-code">type master;</p><p class="source-code">file "/var/named/ocp.hybridmycloud.com.db";</p><p class="source-code">allow-query { any; };</p><p class="source-code">allow-transfer { none; };</p><p class="source-code">allow-update { none; };</p><p class="source-code">};</p><p class="source-code">zone "1.168.192.in-addr.arpa" IN {  </p><p class="source-code">type master;  </p><p class="source-code">file "/var/named/1.168.192.in-addr.arpa";</p><p class="source-code">allow-update { none; };</p><p class="source-code">};</p><p class="source-code">EOF</p></li>
				<li>Create a forward zone file at <strong class="source-inline">/var/named/ocp.hybridmycloud.com.db</strong>:<p class="source-code">$ sudo cat &lt;&lt;EOF &gt; /var/named/ocp.hybridmycloud.com.db</p><p class="source-code">;<strong class="bold">[1]</strong> Begin Common Header Definition</p><p class="source-code">\$TTL 86400</p><p class="source-code">@ IN SOA bastion.ocp.hybridmycloud.com. root.ocp.hybridmycloud.com. (</p><p class="source-code">202201010001 ;Serial</p><p class="source-code">21600 ;Refresh</p><p class="source-code">3600 ;Retry</p><p class="source-code">604800 ;Expire</p><p class="source-code">86400 ;Minimum TTL</p><p class="source-code">)</p><p class="source-code">;End Common Header Definition</p><p class="source-code">;Name Server Information <strong class="bold">[2]</strong></p><p class="source-code">   IN NS bastion.ocp.hybridmycloud.com.</p><p class="source-code">;IP address of Name Server <strong class="bold">[3]</strong></p><p class="source-code">bastion IN A 192.168.1.200</p><p class="source-code">;api internal and external purposes <strong class="bold">[4]</strong></p><p class="source-code">api        IN    A    192.168.1.200 </p><p class="source-code">api-int    IN    A    192.168.1.200 </p><p class="source-code">;wildcard application <strong class="bold">[5]</strong></p><p class="source-code">*.apps     IN    A    192.168.1.200</p><p class="source-code">;bootstrap node to start cluster install only <strong class="bold">[6]</strong></p><p class="source-code">bootstrap  IN    A    192.168.1.90 </p><p class="source-code">;master nodes <strong class="bold">[7]</strong></p><p class="source-code">master1    IN    A    192.168.1.91 </p><p class="source-code">master2    IN    A    192.168.1.92 </p><p class="source-code">master3    IN    A    192.168.1.93</p><p class="source-code">;worker nodes <strong class="bold">[8]</strong></p><p class="source-code">worker1    IN    A    192.168.1.101</p><p class="source-code">worker2    IN    A    192.168.1.102</p><p class="source-code">EOF</p></li>
			</ol>
			<p>Let's look at this code in <a id="_idIndexMarker331"/>more detail:</p>
			<p><strong class="bold">[1]</strong>: Common DNS zone header.</p>
			<p><strong class="bold">[2]</strong>: The nameserver will be its own Bastion server.</p>
			<p><strong class="bold">[3]</strong>: The IP address from the nameserver (Bastion IP).</p>
			<p><strong class="bold">[4]</strong>: These records are mandatory and need to point to the VIP that will be used for the OpenShift API functions. In our case, we are using the bastion server as the VIP (suitable only for lab environments).</p>
			<p><strong class="bold">[5]</strong>: Wildcard VIP record used for the applications that run on OpenShift. In our case, we are using the bastion server as the VIP (suitable only for lab environments).</p>
			<p><strong class="bold">[6]</strong>: Bootstrap node IP record, used only for the cluster installation and can be removed after it.</p>
			<p><strong class="bold">[7]</strong>: Master node IP records, where the <a id="_idIndexMarker332"/>control plane objects will be hosted.</p>
			<p><strong class="bold">[8]</strong>: Worker node IP records, where the workloads will run. If you go for a three-node cluster, disregard the worker hosts.</p>
			<ol>
				<li value="3">Create a reverse zone file at <strong class="source-inline">/var/named/1.168.192.in-addr.arpa</strong>:<p class="source-code">$ sudo cat &lt;&lt;EOF &gt; /var/named/1.168.192.in-addr.arpa</p><p class="source-code">\$TTL 1W @    IN    SOA    bastion.ocp.hybridmycloud.com.root (     </p><p class="source-code">2019070700 ; serial </p><p class="source-code">3H         ; refresh (3 hours) </p><p class="source-code">30M        ; retry (30 minutes) </p><p class="source-code">2W         ; expiry (2 weeks) </p><p class="source-code">1W )       ; minimum (1 week) </p><p class="source-code">5.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">api.ocp.hybridmycloud.com.;</p><p class="source-code">5.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">api-int.ocp.hybridmycloud.com.;</p><p class="source-code">90.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">bootstrap.ocp.hybridmycloud.com.; </p><p class="source-code">91.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">master1.ocp.hybridmycloud.com.; </p><p class="source-code">92.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">master2.ocp.hybridmycloud.com.; </p><p class="source-code">93.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">master3.ocp.hybridmycloud.com.; </p><p class="source-code">101.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">worker1.ocp. hybridmycloud.com.; </p><p class="source-code">102.1.168.192.in-addr.arpa. IN PTR </p><p class="source-code">worker2.ocp. hybridmycloud.com.;</p><p class="source-code">EOF</p></li>
			</ol>
			<p class="callout-heading">Important Notes</p>
			<p class="callout">Do <em class="italic">not</em> create a reverse zone record for the application's wildcard VIP, as that will lead to the wrong DNS resolution.</p>
			<p class="callout">If you created it for a three-node cluster, disregard the worker A and PTR records.</p>
			<ol>
				<li value="4">Restart the <strong class="source-inline">named</strong> service:<p class="source-code">sudo systemctl restart named</p></li>
				<li>Validate the DNS to ensure that<a id="_idIndexMarker333"/> all DNS records are set up appropriately using the following <strong class="source-inline">dig</strong> commands (replace <strong class="source-inline">192.168.1.200</strong> with your bastion IP):<ol><li value="1">Validate the OpenShift API using the following:</li>
</ol><p class="source-code">dig +short @192.168.1.200 api.ocp.hybridmycloud.com</p><p class="source-code">dig +short @192.168.1.200 api-int.ocp.hybridmycloud.com</p><ol><li value="2">For the BIND samples we described in this section, the output <em class="italic">must</em> be as follows:</li>
</ol><p class="source-code">192.168.1.5</p><p class="source-code">192.168.1.5</p><ol><li value="3">Validate the application's wildcard using the following:</li>
</ol><p class="source-code">dig +short @192.168.1.200 joedoe.apps.ocp.hybridmycloud.com</p><p class="source-code">dig +short @192.168.1.200 whatever.apps.ocp.hybridmycloud.com</p><ol><li value="4">All results <em class="italic">must</em> point to the ingress application's wildcard VIP, as follows:</li>
</ol><p class="source-code">192.168.1.6</p><p class="source-code">192.168.1.6</p><ol><li value="5">Validate the nodes, as follows:</li>
</ol><p class="source-code">dig +short @192.168.1.200 boostrap.ocp.hybridmycloud.com</p><p class="source-code">dig +short @192.168.1.200 master1.ocp.hybridmycloud.com</p><p class="source-code">dig +short @192.168.1.200 master2.ocp.hybridmycloud.com</p><p class="source-code">dig +short @192.168.1.200 master3.ocp.hybridmycloud.com</p><p class="source-code">dig +short @192.168.1.200 worker1.ocp.hybridmycloud.com</p><p class="source-code">dig +short @192.168.1.200 worker2.ocp.hybridmycloud.com</p></li>
			</ol>
			<p>The answer must be the following:</p>
			<p class="source-code">192.168.1.90</p>
			<p class="source-code">192.168.1.91</p>
			<p class="source-code">192.168.1.92</p>
			<p class="source-code">192.168.1.93</p>
			<p class="source-code">192.168.1.101</p>
			<p class="source-code">192.168.1.102</p>
			<ol>
				<li value="6">Finally, let's validate the <a id="_idIndexMarker334"/>reverse records, as follows:</li>
			</ol>
			<p class="source-code">dig +short @192.168.1.200 -x 192.168.1.90</p>
			<p class="source-code">dig +short @192.168.1.200 -x 192.168.1.91</p>
			<p class="source-code">dig +short @192.168.1.200 -x 192.168.1.92</p>
			<p class="source-code">dig +short @192.168.1.200 -x 192.168.1.93</p>
			<p class="source-code">dig +short @192.168.1.200 -x 192.168.1.101</p>
			<p class="source-code">dig +short @192.168.1.200 -x 192.168.1.102</p>
			<p>The results look similar to the following:</p>
			<p class="source-code">bootstrap.ocp.hybridmycloud.com.</p>
			<p class="source-code">master1.ocp.hybridmycloud.com.</p>
			<p class="source-code">master2.ocp.hybridmycloud.com.</p>
			<p class="source-code">master3.ocp.hybridmycloud.com.</p>
			<p class="source-code">worker1.ocp.hybridmycloud.com.</p>
			<p class="source-code">worker2.ocp.hybridmycloud.com.</p>
			<p>Well done! If your DNS server is properly <a id="_idIndexMarker335"/>resolving names, you took a big step in preparing the prerequisites. Now, let's move on to another important piece of an OpenShift installation using the UPI method: the <strong class="bold">Dynamic Host Configuration Protocol</strong> (<strong class="bold">DHCP</strong>).</p>
			<h3>DHCP</h3>
			<p>DHCP is used to provide IP addresses to the<a id="_idIndexMarker336"/> OpenShift nodes. In UPI or<a id="_idIndexMarker337"/> agnostic installation nodes, the IP address needs to be set using static configuration on DHCP (the <strong class="source-inline">fixed-address</strong> parameter).</p>
			<p>Make sure that the IP address and hostname for the nodes in the DNS and DHCP match – each IP address and hostname in the DNS and DHCP need to be the same. In this subsection of prerequisites, we are focusing on creating a simple DHCP setup for later study and laboratory use. As previously stated, DHCP will be configured to provide static IP addresses, under the <strong class="source-inline">192.168.1.x</strong> subnet, so, this configuration uses the <strong class="bold">media access control</strong> (<strong class="bold">MAC</strong>) address of each node's Ethernet interfaces:</p>
			<ol>
				<li value="1">Install DHCP on your bastion VM:<p class="source-code">$ sudo yum install dhcp-server -y</p></li>
				<li>Configure <a id="_idIndexMarker338"/>the <strong class="source-inline">dhcpd.conf</strong> file according to the hostnames and IP addresses used with the <a id="_idIndexMarker339"/>DNS:<p class="source-code">cat &lt;&lt;EOF &gt; /etc/dhcp/dhcpd.conf</p><p class="source-code"># DHCP Server Configuration file.</p><p class="source-code">#<strong class="bold">[1]</strong></p><p class="source-code">ddns-update-style interim;</p><p class="source-code">ignore client-updates;</p><p class="source-code">authoritative;</p><p class="source-code">allow booting;</p><p class="source-code">allow bootp;</p><p class="source-code">allow unknown-clients;</p><p class="source-code">default-lease-time 3600;</p><p class="source-code">default-lease-time 900;</p><p class="source-code">max-lease-time 7200;</p><p class="source-code">#<strong class="bold">[2]</strong></p><p class="source-code">subnet 192.168.1.0 netmask 255.255.255.0 {</p><p class="source-code">option routers 192.168.1.254;</p><p class="source-code">option domain-name-servers 192.168.1.200;</p><p class="source-code">option ntp-servers 192.168.1.200;</p><p class="source-code">next-server 192.168.1.200; <strong class="bold">#[2.1]</strong></p><p class="source-code">#filename "pxelinux.0";<strong class="bold">#[2.2]</strong></p><p class="source-code">#<strong class="bold">[3]</strong></p><p class="source-code">group {</p><p class="source-code">host bootstrap {</p><p class="source-code">hardware ethernet 50:6b:8d:aa:aa:aa;</p><p class="source-code">fixed-address 192.168.1.90;</p><p class="source-code">option host-name "bootstrap.ocp.hybridmycloud.com";</p><p class="source-code">allow booting;</p><p class="source-code">}</p><p class="source-code">host master1 {</p><p class="source-code">hardware ethernet 50:6b:8d:bb:bb:bb;</p><p class="source-code">fixed-address 192.168.1.91;</p><p class="source-code">option host-name "master1.ocp.hybridmycloud.com";</p><p class="source-code">allow booting;</p><p class="source-code">}</p><p class="source-code">host master2 {</p><p class="source-code">hardware ethernet 50:6b:8d:cc:cc:cc;</p><p class="source-code">fixed-address 192.168.1.92 ;</p><p class="source-code">option host-name "master2.ocp.hybridmycloud.com";</p><p class="source-code">allow booting;</p><p class="source-code">}</p><p class="source-code">host master3 {</p><p class="source-code">hardware ethernet 50:6b:8d:dd:dd:dd;</p><p class="source-code">fixed-address 192.168.1.93 ;</p><p class="source-code">option host-name "master3.ocp.hybridmycloud.com";</p><p class="source-code">allow booting;</p><p class="source-code">}</p><p class="source-code">host worker1 {</p><p class="source-code">hardware ethernet 50:6b:8d:11:11:11;</p><p class="source-code">fixed-address 192.168.1.101;</p><p class="source-code">option host-name "worker1.ocp.hybridmycloud.com";</p><p class="source-code">allow booting;</p><p class="source-code">}</p><p class="source-code">host worker2 {</p><p class="source-code">hardware ethernet 50:6b:8d:22:22:22;</p><p class="source-code">fixed-address 192.168.1.102;</p><p class="source-code">option host-name "worker2.ocp.hybridmycloud.com";</p><p class="source-code">allow booting;</p><p class="source-code">}</p><p class="source-code">}</p><p class="source-code">} </p><p class="source-code">EOF</p><p class="source-code">$ sudo systemctl enable --now dhcpd</p><p class="source-code">$ sudo firewall-cmd --add-service=dhcp --permanent</p><p class="source-code">$ sudo firewall-cmd --reload</p></li>
			</ol>
			<p>Let's look at this code in more detail:</p>
			<p><strong class="bold">[1]</strong>: Common settings to define <a id="_idIndexMarker340"/>DHCP as authoritative in that <a id="_idIndexMarker341"/>subnet and times of IP lease.</p>
			<p><strong class="bold">[2]</strong>: Scope subnet definition:</p>
			<ul>
				<li><strong class="bold">[2.1]</strong> and <strong class="bold">[2.2]</strong>: Must be defined when using a PXE server, helpful for bare metal installations. In this lab, we are going to use VMs and, as such, that will not be used; therefore, leave it commented (using the <strong class="source-inline">#</strong> character at the beginning of the line).</li>
			</ul>
			<p><strong class="bold">[3]</strong>: A group with all nodes to lease IP addresses. If you go for a three-node cluster, disregard the worker hosts.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">After you create the VMs in your hypervisor, update <strong class="source-inline">dhcpd.conf</strong> accordingly with the MAC addresses you <a id="_idIndexMarker342"/>get from the network interfaces; otherwise, no IP address will be given to this subnet.</p>
			<h3>Web servers</h3>
			<p>A web server is used to <a id="_idIndexMarker343"/>serve the OS image to install nodes, and also to provide the Ignition files (Ignition files are manifest files encoded on <strong class="source-inline">base64</strong>). In our scenario, we will install and configure an Apache web server, which is a very simple way to provide all the necessary tools for the cluster installation to run adequately.</p>
			<p>Follow this short list of steps to accomplish this task:</p>
			<ol>
				<li value="1">Install an <strong class="bold">httpd</strong> server:<p class="source-code">$ sudo yum install httpd policycoreutils-python-utils –y</p></li>
				<li>Configure <strong class="source-inline">/etc/httpd/conf/httpd.conf</strong> to change the <strong class="bold">Listen directive</strong>:<p class="source-code">$ sudo sed –i 's/80/81/g' /etc/httpd/conf/httpd.conf</p></li>
				<li>Apply <strong class="bold">SELinux</strong> to change the default <strong class="source-inline">httpd</strong> port:<p class="source-code">$ sudo semanage port -a -t http_port_t -p tcp 81</p></li>
				<li>Create a rule on <strong class="bold">firewalld</strong> to allow port <strong class="source-inline">81</strong>:<p class="source-code">$ sudo firewall-cmd --add-port 81/tcp --permanent</p><p class="source-code">$ sudo firewall-cmd --reload</p></li>
				<li>Create a directory for OS image and Ignition files, and a file to test the connectivity:<p class="source-code">$ sudo mkdir –p /var/www/html/images</p><p class="source-code">$ sudo mkdir –p /var/www/html/ignition</p><p class="source-code">$ sudo touch /var/www/html/images/imageFileToTest.txt</p><p class="source-code">$ sudo touch /var/www/html/ignition/ignitionFileToTest.txt</p></li>
				<li>Set permission and owner to the files:<p class="source-code">$ sudo chown –R apache. /var/www/html/</p><p class="source-code">$ sudo chmod 744 –R /var/www/html/</p></li>
				<li>Start and enable the Apache web server:<p class="source-code">$ sudo systemctl enable --now httpd </p></li>
				<li>Test connectivity using the <strong class="source-inline">curl</strong> command:<p class="source-code">$ curl –O http://192.168.1.200:81/images/imageFileToTest.txt</p><p class="source-code">$ curl –O http://192.168.1.200:81/ignition/ignitionFileToTest.txt</p></li>
			</ol>
			<p>If you can see the file downloaded to<a id="_idIndexMarker344"/> your current folder, it<a id="_idIndexMarker345"/> means you have the Apache web server properly configured to serve the OpenShift installation.</p>
			<h3>Load balancer</h3>
			<p>A load balancer is<a id="_idIndexMarker346"/> another important element in an OpenShift cluster architecture. It is responsible for balancing the connection to the pool member in a set of nodes. For performance and resilience reasons, it is recommended that you use dedicated load balancing appliance hardware for production environments.</p>
			<p>For our lab, we will be using HAProxy in our bastion VM to perform the load balancing function, which is a powerful, lightweight, and easy-to-use software load balancer. </p>
			<p>That being said, before we start to configure it, it is important that you understand the basics of load balancing methods and the best practices that fit with the OpenShift platform. </p>
			<p>I suppose the load balancer is like drops of water evenly distributed between a few cups, and each drop must fall into one glass, then the next drop must land in the next glass, and so on. Nonetheless, the cups are periodically dumped to avoid waste or overload. </p>
			<p>So, there are some ways to perform the task; these<a id="_idIndexMarker347"/> ways are known as <strong class="bold">balancing methods</strong>. The following table explains the scenarios that OpenShift will make use of:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B18015_Table_5.3.jpg" alt=""/>
				</div>
			</div>
			<p>A typical load balancer configuration is <a id="_idIndexMarker348"/>composed of four pairs of frontend and backend configurations that will balance the different types of<a id="_idIndexMarker349"/> requests and give a reliable fault tolerance to the platform.</p>
			<p>The first pool members are the master nodes; these are a group of three members and should work with the Least Connection method with a sourced address setup. This setting ensures that, during internal API calls to the load balancer, the request will be handled from the same node that started the call requisition and, as such, gives the proper callback and asynchronous functioning. </p>
			<p>You can find the HAProxy frontend and backend configurations in the sample here: (frontend <strong class="source-inline">openshift-api-server</strong> and backend <strong class="source-inline">openshift-api-server</strong>).</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B18015_05_01.jpg" alt="Figure 5.1 – Master node load balancer "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Master node load balancer</p>
			<p>The second group of load balancer configurations is also used with the master nodes for the OpenShift <strong class="source-inline">machine-config-server</strong> API. See the HAProxy configuration for the <strong class="source-inline">machine-config-server</strong> API in the frontend <strong class="source-inline">machine-config-server</strong> and backend <strong class="source-inline">machine-config-server</strong>.</p>
			<p>The third and fourth groups of load <a id="_idIndexMarker350"/>balancers should be a pool of at least two nodes (worker nodes) that will be responsible for the traffic routing from the outside to the application distributed on the worker nodes of the cluster (one for HTTP and another for HTTPS). </p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B18015_05_02.jpg" alt="Figure 5.2 – Ingress load balancer "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Ingress load balancer</p>
			<p>OpenShift often recommends the<a id="_idIndexMarker351"/> least connections with source addresses running on the transport layer (Layer 4,) which gives good performance to routing applications. However, when using certificates signed by a public <strong class="bold">Certificate Authority</strong> (<strong class="bold">CA</strong>) in the load balancer, instead of the OpenShift Ingress Controller, you must eventually set up this pool to work on the application layer (Layer 7).</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">We strongly recommend disabling the SSL inspection on the load balancer/firewall layers to avoid digital certificate issues and the malfunctioning of the OpenShift cluster. This occurs because the enterprise load balancer/firewall solution, when used with SSL inspection mode enabled, decrypts every TCP payload and re-encapsulates them with a new SSL header. OpenShift interprets it as a certificate error, causing incorrect source/destiny TCP packets, and hanging up to TLS termination.</p>
			<p>In a nutshell, the complete <strong class="source-inline">haproxy.cfg</strong> will be similar<a id="_idIndexMarker352"/> to the following:</p>
			<pre class="source-code">$ sudo yum install haproxy -y
$ sudo cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfg
# Global settings
global
  maxconn 20000
  log /dev/log local0 info
  chroot /var/lib/haproxy
  pidfile /var/run/haproxy.pid
  user haproxy
  group haproxy
  daemon
  # turn on stats unix socket
  stats socket /var/lib/haproxy/stats
defaults
  mode http
  log global
  option httplog
  option dontlognull
  option forwardfor except 127.0.0.0/8
  option redispatch
  retries 3
  timeout http-request 10s
  timeout queue 1m
  timeout connect 10s
  timeout client 300s
  timeout server 300s
  timeout http-keep-alive 10s
  timeout check 10s
  maxconn 20000
# Enable haproxy status endpoint
listen stats
  bind :9000
  mode http
  stats enable
  stats uri /
<strong class="bold"># OpenShift API (port 6443)</strong>
<strong class="bold">frontend openshift-api-server</strong>
  bind *:6443 
  <strong class="bold">default_backend openshift-api-server</strong>
  mode tcp
  option tcplog
<strong class="bold">backend openshift-api-server</strong>
  balance source
  mode tcp
# bootstrap line below can be removed after the cluster is deployed
  server bootstrap 192.168.1.90:6443 check
  server master1 192.168.1.91:6443 check
  server master2 192.168.1.92:6443 check
  server master3 192.168.1.93:6443 check
# machine-config-server API (port 22623)
<strong class="bold">frontend machine-config-server</strong>
  bind *:22623
  <strong class="bold">default_backend machine-config-server</strong>
  mode tcp
  option tcplog
<strong class="bold">backend machine-config-server</strong>
  balance source
  mode tcp
# bootstrap line below can be removed after the cluster is deployed
  server bootstrap 192.168.1.90:22623 check
  server master1 192.168.1.91:22623 check
  server master2 192.168.1.92:22623 check
  server master3 192.168.1.93:22623 check
# Applications HTTP (port 80)
<strong class="bold">frontend ingress-http</strong>
  bind *:80
  <strong class="bold">default_backend ingress-http</strong>
  mode tcp
  option tcplog
<strong class="bold">backend ingress-http</strong>
  balance source
  mode tcp
  server worker1 192.168.1.101:80 check # <strong class="source-inline">[1]</strong>
  server worker2 192.168.1.102:80 check # <strong class="source-inline">[1]</strong>
# Applications HTTPS (port 443)
<strong class="bold">frontend ingress-https</strong>
  bind *:443
  <strong class="bold">default_backend ingress-https</strong>
  mode tcp
  option tcplog
<strong class="bold">backend ingress-https</strong>
  balance source
  mode tcp
  server worker0 192.168.1.101:443 check # <strong class="source-inline">[1]</strong>
  server worker1 192.168.1.102:443 check # <strong class="source-inline">[1]</strong>
EOF</pre>
			<p>Let's look at this code in more detail:</p>
			<p><strong class="bold">[1]</strong>:  If you go for a three-node<a id="_idIndexMarker353"/> cluster, you should also point to the master nodes here.</p>
			<p>After that, apply the HAProxy configuration by starting and enabling the service, as follows:</p>
			<pre class="source-code">$ sudo setsebool -P haproxy_connect_any=1
$ sudo systemctl enable --now haproxy
$ sudo firewall-cmd --add-service=http --permanent
$ sudo firewall-cmd --add-service=https --permanent
$ sudo firewall-cmd --add-port=6443/tcp --permanent
$ sudo firewall-cmd --add-port=22623/tcp --permanent
$ sudo firewall-cmd --reload</pre>
			<p>After it starts, your load balancer can receive incoming connections and give the redirect to one member of the pool.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">As soon as the installation of the OpenShift control plane is finished, you will receive a message saying that it is safe to remove Bootstrap from load balancing; then, you can open the <strong class="source-inline">haproxy.cfg</strong> file and comment, or remove the lines that are referencing the Bootstrap server and restart the HAProxy server to apply the configuration.</p>
			<h3>Firewall</h3>
			<p>As the cluster has a lot of components involved, it is important to think about the security between Red Hat OpenShift and all other systems it integrates with. Unless you are working with a disconnected environment (refer to the <em class="italic">Further reading</em> section of this chapter for more details), you will need to grant access to ports <strong class="source-inline">80</strong> and <strong class="source-inline">443</strong> from certain URLs. These URLs are<a id="_idIndexMarker354"/> needed to download the required container images and others. Therefore, whitelist the following URLs in your network firewall:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B18015_Table_5.4.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Reference</p>
			<p class="callout">Refer to this link for the <a id="_idIndexMarker355"/>latest set of URLs: <a href="B18015_05.xhtml">https://docs.openshift.com/container-platform/4.9/installing/install_config/configuring-firewall.html</a>.</p>
			<h3>PXE server</h3>
			<p>A PXE server is a component that easily allows the boot process to look for installation files. During PXE configuration, you<a id="_idIndexMarker356"/> can create a simple <strong class="bold">Grand Unified Bootloader</strong> (<strong class="bold">GRUB</strong>) menu that works exactly like an OS installer, with all the <a id="_idIndexMarker357"/>kernel parameters you need.</p>
			<p>We will deploy some packages to install the PXE server, create directories to store the configuration files, and develop a simple and useful bootstart menu. Now, SSH to your bastion server and do the following:</p>
			<ol>
				<li value="1">Install these <strong class="source-inline">syslinux</strong> packages:<p class="source-code">$ sudo yum install –y syslinux-tftpboot syslinux-nonlinux syslinux tftp-server</p><p class="source-code">$ firewall-cmd --add-service=tftp --permanent</p><p class="source-code">$ firewall-cmd --reload</p><p class="source-code">$ sudo systemctl enable --now tftp</p></li>
				<li>Create the following directories:<p class="source-code">$ mkdir -p /var/lib/tftpboot/networkboot/coreOS</p><p class="source-code">$ mkdir -p /var/lib/tftpboot/pxelinux.cfg</p></li>
				<li>Copy the required PXE server files, as follows:<p class="source-code">$ cp /usr/share/syslinux/* /var/lib/tftpboot</p></li>
				<li>Copy the Red Hat CoreOS image files, as shown here (files are available for download at this link: <a href="https://console.redhat.com/openshift/install/platform-agnostic/user-provisioned">https://console.redhat.com/openshift/install/platform-agnostic/user-provisioned</a>):<p class="source-code">├── networkboot</p><p class="source-code">│    └── coreOS</p><p class="source-code">│          ├── rhcos-live-initramfs.x86_64.img</p><p class="source-code">│          ├── rhcos-live-kernel-x86_64</p><p class="source-code">│          └── rhcos-live-rootfs.x86_64.img</p></li>
				<li>Finally, create the bootloader menu to assist you in the installation:<p class="source-code">$ sudo cat &lt;&lt;EOF &gt; /var/lib/tftpboot/pxelinux.cfg/default</p><p class="source-code">UI vesamenu.c32</p><p class="source-code">MENU COLOR sel 4 #ffffff std</p><p class="source-code">MENU COLOR title 0 #ffffff</p><p class="source-code">TIMEOUT 120</p><p class="source-code">PROMPT 0</p><p class="source-code">MENU TITLE OPENSHIFT 4.X AGNOSTIC PXE MENU</p><p class="source-code">LABEL BOOTSTRAP NODE</p><p class="source-code">  KERNEL networkboot/coreOS/rhcos-live-kernel-x86_64 </p><p class="source-code">  APPEND initrd=networkboot/coreOS/rhcos-live-initramfs.x86_64.img,networkboot/coreOS/rhcos-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://192.168.1.200:81/ignition/bootstrap.ign </p><p class="source-code">LABEL MASTER NODE</p><p class="source-code">  KERNEL networkboot/coreOS/rhcos-live-kernel-x86_64 </p><p class="source-code">  APPEND initrd=networkboot/coreOS/rhcos-live-initramfs.x86_64.img,networkboot/coreOS/rhcos-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/sdacoreos.inst.ignition_url=http://192.168.1.200:81/ignition/master.ign </p><p class="source-code">LABEL  WORKER NODE  </p><p class="source-code">  KERNEL networkboot/coreOS/rhcos-live-kernel-x86_64 </p><p class="source-code">  APPEND initrd=networkboot/coreOS/rhcos-live-initramfs.x86_64.img,networkboot/coreOS/rhcos-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/sdacoreos.inst.ignition_url=http://192.168.1.200:81/ignition/worker.ign </p><p class="source-code">EOF</p></li>
			</ol>
			<p>Now that we have all the <a id="_idIndexMarker358"/>prerequisite components correctly set, we can start the installation using the UPI or agnostic installation method! So, go ahead and start your engines!</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor097"/>IPI</h2>
			<p>Even though we tried to give <a id="_idIndexMarker359"/>you a smooth demonstration of how to create all the prerequisite systems and servers, it might still look like an exhausting process. It is important to emphasize<a id="_idIndexMarker360"/> that it can be tiring when preparing on your own, but, in large enterprises, those infrastructures are often already working and need only some small setting tweaks to reach the necessary state.</p>
			<p>You must be worn out after going through all the previous steps for the UPI installer. The good news is that the IPI installer is much easier to follow! You probably compared all the things needed on the table <strong class="bold">Preparation Stuff Table</strong>. To accomplish the task using the IPI, you should have<a id="_idIndexMarker361"/> only your cloud credentials, ensure the object limit of your cloud provider is enough for the minimum required in OpenShift, choose the size of cloud instances that best fit your needs, create the <strong class="source-inline">install-config.yaml</strong> file, and run the OpenShift install binary to spawn your cluster.</p>
			<p>This process is simple due to the high level of automation that OpenShift has under the hood, which uses cloud APIs to create all the prerequisites for you, according to the parameters you set in <strong class="source-inline">install-config.yaml</strong>. Obviously, there are some changes from cloud to cloud. In the following code, we show two excerpts that change in <strong class="source-inline">install-config.yaml</strong> when<a id="_idIndexMarker362"/> preparing your file for AWS, Azure, and GCP:</p>
			<p>Here's an AWS <strong class="source-inline">install-config</strong> sample file:</p>
			<pre class="source-code">apiVersion: v1
baseDomain: hybridmycloud.com 
credentialsMode: Mint 
controlPlane: 
  hyperthreading: Enabled 
  name: master
  platform:
    aws:
      zones:
      - us-west-2a
      - us-west-2b
      rootVolume:
        iops: 4000
        size: 500
        type: io1 
        type: m5.xlarge
    replicas: 3
compute: 
  - hyperthreading: Enabled 
  name: worker
  platform:
    aws: 
      rootVolume:
        iops: 2000
        size: 500
        type: io1 
      type: c5.4xlarge
      zones:
      - us-west-2c
    replicas: 3
metadata:
  name: test-cluster 
  networking:
    clusterNetwork:
    - cidr: 10.128.0.0/14
    hostPrefix: 23
    machineNetwork:
    - cidr: 10.0.0.0/16
    networkType: OpenShiftSDN
    serviceNetwork:
    - 172.30.0.0/16
  platform:
    aws:
      region: us-west-2 
      userTags:
      adminContact: jdoe
    costCenter: 7536
    amiID: ami-96c6f8f7 
    serviceEndpoints: 
      - name: ec2
        url: https://vpce-id.ec2.us-west-2.vpce.amazonaws.com
fips: false 
sshKey: ssh-ed25519 AAAA... 
pullSecret: '{"auths": ...}'</pre>
			<p>Next, let's look at a<a id="_idIndexMarker363"/> sample Azure <strong class="source-inline">install-config</strong> file:</p>
			<pre class="source-code">apiVersion: v1 
baseDomain: hybridmycloud.com 
controlPlane: 
  hyperthreading: Enabled 
  name: master 
  platform: 
    azure: 
      osDisk: 
        diskSizeGB: 1024 
        type: Standard_D8s_v3 
        replicas: 3 
    compute: 
  - hyperthreading: Enabled 
  name: worker 
  platform: 
    azure: 
      type: Standard_D2s_v3 
      osDisk: diskSizeGB: 512 
      zones: 
      - "1" 
      - "2" 
      - "3" 
  replicas: 5 
metadata: 
  name: test-cluster 
  networking: 
  clusterNetwork: 
  - cidr: 10.128.0.0/14 
  hostPrefix: 23 
  machineNetwork: 
  - cidr: 10.0.0.0/16 
  networkType: OpenShiftSDN 
  serviceNetwork:
  - 172.30.0.0/16
platform: 
  azure: 
    BaseDomainResourceGroupName: resource_group 
    region: centralus 
    resourceGroupName: existing_resource_group 
    outboundType: Loadbalancer 
    cloudName: AzurePublicCloud 
pullSecret: '{"auths": ...}'</pre>
			<p>Here, we <a id="_idIndexMarker364"/>have a GCP <strong class="source-inline">install-config</strong> sample:</p>
			<pre class="source-code">apiVersion: v1
baseDomain: hybridmycloud.com
controlPlane:
  hyperthreading: Enabled
  name: master
  platform:
  gcp:
    type: n2-standard-4
    zones:
    - us-central1-a
    - us-central1-c
  osDisk:
    diskType: pd-ssd
    diskSizeGB: 1024
    encryptionKey:
      kmsKey:
      name: worker-key
      keyRing: test-machine-keys
      location: global
      projectID: project-id
  replicas: 3
compute:
- hyperthreading: Enabled
  name: worker
  platform:
  gcp:
    type: n2-standard-4
    zones:
    - us-central1-a
    - us-central1-c
    osDisk:
    diskType: pd-standard
    diskSizeGB: 128
    encryptionKey:
    kmsKey:
      name: worker-key
      keyRing: test-machine-keys
      location: global
      projectID: project-id
  replicas: 3
metadata:
name: test-cluster
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
gcp:
  projectID: openshift-production
  region: us-central1
pullSecret: '{"auths": ...}'
fips: false
sshKey: ssh-ed25519 AAAA...</pre>
			<p>Well done! Now you<a id="_idIndexMarker365"/> have the correct <strong class="source-inline">install-config.yaml</strong> files to use with<a id="_idIndexMarker366"/> your cloud provider. Continue with the installation to start OpenShift using your preferred installation method.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor098"/>Preparing for the installation</h1>
			<p>As you have seen in the previous sections, the <a id="_idIndexMarker367"/>prerequisites are very important, and any mistake could be an <em class="italic">Achilles' heel</em> for the OpenShift cluster installation and functioning. Failure to prepare the prerequisites correctly will cause errors during the cluster deployment that are not always easy to troubleshoot to find the root cause. That said, we would like to stress the importance of preparing and validating the pre-requisites correctly before starting the cluster deployment.</p>
			<p>To start the installation using the UPI method, you will need the following:</p>
			<ul>
				<li>An SSH key pair</li>
				<li>A pull secret for the cluster, which you can generate by accessing <a href="B18015_05.xhtml">https://console.redhat.com/openshift/install</a>, with a valid user subscription</li>
				<li>OpenShift installer binary</li>
				<li>OpenShift command-line tools</li>
				<li>Installation configuration file (<strong class="source-inline">install-config.yaml</strong>)</li>
			</ul>
			<p>In the following sections, we will detail all of those steps.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor099"/>An SSH key pair</h2>
			<p>Starting from OpenShift version<a id="_idIndexMarker368"/> 4, Red Hat begins to use Red Hat CoreOS as the main OS due to the container and immutable nature. Red Hat CoreOS needs some Ignition files to provision the OS based on that configuration. This process leads to a secure and reliable way of provisioning OpenShift nodes, allowing a<a id="_idIndexMarker369"/> standard <strong class="bold">zero-touch provisioning</strong> (<strong class="bold">ZTP</strong>) process.</p>
			<p>SSH is used to access the nodes directly and only through a pair of keys assigned to the username <strong class="source-inline">coreos</strong> (it is not possible to access the nodes using a simple username/password combination). It is vital to keep a copy of the SSH key pair used during the cluster deployment in case of a problem with your cluster, as this is the only way to directly access the nodes to collect logs and try to troubleshoot them. Also, the SSH key pair will become part of the Ignition files and the public key pair is distributed across all nodes of the cluster. </p>
			<p>We are going to use our bastion VM to create an SSH key pair, by using the following command:</p>
			<pre class="source-code">$ ssh-keygen -t ecdsa -N '' -f ~/.ssh/clusterOCP_key</pre>
			<p>We will use a public key in the next steps, for example, <strong class="source-inline">clusterOCP_key.pub</strong>.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Never expose or share the SSH private key; any malicious person with the private key could get root access to the nodes and, with some knowledge, escalate privileges as an OpenShift <strong class="source-inline">cluster-admin</strong> user.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor100"/>Pull secret</h2>
			<p>A pull secret is a file that contains <a id="_idIndexMarker370"/>a collection of usernames<a id="_idIndexMarker371"/> and passwords encoded in <strong class="source-inline">Base64</strong> used for authentication in image registries, such as <strong class="source-inline">quay.io</strong> and <strong class="source-inline">registry.redhat.io</strong>. You need to have a valid username at <strong class="source-inline">console.redhat.com</strong> to download or copy the pull secret.</p>
			<p>To do so, complete the following two steps:</p>
			<ol>
				<li value="1">Access https://console.redhat.com/openshift/create and access <strong class="bold">Downloads </strong>in the side menu, as shown in the following figure:</li>
			</ol>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B18015_05_03.jpg" alt="Figure 5.3 – Pull secret, Downloads menu "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Pull secret, Downloads menu</p>
			<ol>
				<li value="2">Scroll down to the <strong class="bold">Tokens</strong> section and click on the <strong class="bold">Copy</strong> or <strong class="bold">Download</strong> buttons to get the<a id="_idIndexMarker372"/> pull secret, as shown here:</li>
			</ol>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B18015_05_04.jpg" alt="Figure 5.4 – Download or copy the pull secret "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Download or copy the pull secret</p>
			<p>On this page, you will also find the command line, developer tools, and the installer binaries to download.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor101"/>OpenShift installer binary</h2>
			<p>To install the cluster, you should <a id="_idIndexMarker373"/>download the installer binary, which can be downloaded from the Red Hat Hybrid Cloud Console, as shown in <em class="italic">Figure 5.3</em>, or you can browse the public repository found here: <a href="B18015_05.xhtml">https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/latest/</a>.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor102"/>OpenShift command-line tools</h2>
			<p>Like the installer binary, you can download the command-line tools under the same public repository<a id="_idIndexMarker374"/> mentioned previously, or through the Red Hat Hybrid Cloud Console from where you downloaded the pull secret.</p>
			<p>OpenShift command-line tools consist of the <strong class="source-inline">oc</strong> and <strong class="source-inline">kubectl</strong> CLIs, which you will use to manage and run commands on the cluster as soon as it has been spawned.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor103"/>Installation configuration file (install-config.yaml)</h2>
			<p>The last step before starting the<a id="_idIndexMarker375"/> installation is to create a manifest file called <strong class="source-inline">install-config.yaml</strong>. Essentially, this file consists of the control plane, workers, and network definitions, along with some metadata, such as the pull secret and the public SSH key.</p>
			<p>Based on our previous configuration, the following is a sample of the <strong class="source-inline">install-config</strong> file that can be used with the installation. You can find this file in our GitHub repository at <strong class="source-inline">chapter05/none-install-config.yaml</strong>:</p>
			<pre class="source-code">$ mkdir ~/ocp
$ cat &lt;&lt;EOF &gt; ~/ocp/install-config.yaml
apiVersion: v1
baseDomain: hybridmycloud.com # <strong class="bold">[1]</strong>
compute:
- hyperthreading: Enabled 
  name: worker
  replicas: 2 # <strong class="bold">[2]</strong>
controlPlane:
  hyperthreading: Enabled 
  name: master 
  replicas: 3 # <strong class="bold">[3]</strong>
metadata:
  name: ocp # <strong class="bold">[4]</strong>
networking:
  clusterNetwork:
  - cidr: 10.148.0.0/14 # <strong class="bold">[5]</strong>
    hostPrefix: 23 
  networkType: OpenShiftSDN # <strong class="bold">[6]</strong>
  serviceNetwork: 
  - 10.153.0.0/16 # <strong class="bold">[7]</strong>
platform:
  none: {} # <strong class="bold">[8]</strong>
fips: false
pullSecret: '&lt;YOUR-PULL-SECRET&gt;' # <strong class="bold">[9]</strong>
sshKey: '&lt;YOUR-SSH-KEY&gt;' # <strong class="bold">[10]</strong>
EOF</pre>
			<p>Let's look at this code in more detail:</p>
			<p><strong class="bold">[1]</strong>: The base domain for your cluster. Needs to match with the one you configured in your DNS in the previous section.</p>
			<p><strong class="bold">[2]</strong>: The initial number of worker<a id="_idIndexMarker376"/> nodes you will be deploying with the cluster. If you go for a three-node cluster, this value must be zero.</p>
			<p><strong class="bold">[3]</strong>: The initial number of master nodes you will be deploying with the cluster. For a highly available cluster, always use three replicas.</p>
			<p><strong class="bold">[4]</strong>: The cluster name. This also needs to match with what you specified in your DNS.</p>
			<p><strong class="bold">[5]</strong>: A block of IP addresses that will be used internally for pods within OpenShift SDN. We explored this concept in <a href="B18015_02.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Architecture Overview and Definitions</em>.</p>
			<p><strong class="bold">[6]</strong>: The type of SDN used. Valid values are <strong class="source-inline">OpenShiftSDN</strong> or <strong class="source-inline">OVNKubernetes</strong>.</p>
			<p><strong class="bold">[7]</strong>: A block of IP addresses that will be used internally for network services within OpenShift SDN. We explored this concept in <a href="B18015_02.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Architecture Overview and Definitions</em>.</p>
			<p><strong class="bold">[8]</strong>: Specific data about the underlying platform. This will be different depending on the platform on which your cluster will be hosted (such as AWS, Azure, GCP, and VMware). For agnostic installation, use <strong class="source-inline">none</strong>.  </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can use also the <strong class="source-inline">openshift-installer</strong> binary to generate a sample <strong class="source-inline">install-config.yaml</strong> file for the provider you are going to work with. Use the following command for that: <strong class="source-inline">./openshift-install create install-config</strong>.</p>
			<p>After this file is created, you<a id="_idIndexMarker377"/> can proceed to the installation steps in the following section.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor104"/>Installation</h1>
			<p>Now, some further steps<a id="_idIndexMarker378"/> should be performed before deploying the cluster. The first step in the installation is to create the manifest files. We strongly recommend you create a backup of the <strong class="source-inline">install-config.yaml</strong> file before running the following command, as this command removes the original file and you will need to create it again from scratch if you need to retry the installation:</p>
			<pre class="source-code">$ ./openshift-install create manifests --dir=home/user/ocp/</pre>
			<p>Open the <strong class="source-inline">~/ocp/manifests/cluster-scheduler-02-config.yml</strong> file in your preferred editor. Change the <strong class="source-inline">mastersSchedulable</strong> parameter to <strong class="source-inline">false</strong> if you go for a regular cluster, or <strong class="source-inline">true</strong> if you decided to provision a three-node cluster.</p>
			<p>You should now generate the Ignition files by running the following command: </p>
			<pre class="source-code">$ ./openshift-install create ignition-configs --dir=home/user/ocp/</pre>
			<p>After the previous command, you should have three new Ignition files: <strong class="source-inline">bootstrap.ign</strong>, <strong class="source-inline">master.ign</strong>, and <strong class="source-inline">worker.ign</strong>.</p>
			<p>Copy these three files to the HTTP server you prepared in the previous section:</p>
			<pre class="source-code">$ sudo cp ~/ocp/*.ign /var/www/html/ignition
$ sudo chown -R apache. /var/www/html
$ sudo chmod 744 /var/www/html</pre>
			<p>Finally, you can <a id="_idIndexMarker379"/>proceed with provisioning the nodes.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor105"/>Phase 1 – Provisioning servers</h2>
			<p>First, you need to <a id="_idIndexMarker380"/>provision the servers. This will vary depending on the underlying<a id="_idIndexMarker381"/> infrastructure but, in general, the process for virtualized environments (for example, VMware, vSphere, and RHV) is as follows:</p>
			<ol>
				<li value="1">Import the Red Hat CoreOS template to the hypervisor.</li>
				<li>Clone it and configure the VM parameters according to the provider.</li>
			</ol>
			<p>On the other hand, the process for bare metal or agnostic installation is performed either by booting using the Red Hat CoreOS ISO or using a PXE. </p>
			<p>In our lab, we are going to boot using the Red Hat CoreOS ISO. Follow these steps to do it:</p>
			<ol>
				<li value="1">Download the ISO file from <strong class="source-inline">console.redhat.com</strong>, as mentioned previously, or directly through this link: <a href="https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/latest/rhcos-live.x86_64.iso">https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/latest/rhcos-live.x86_64.iso</a>.</li>
				<li>In the bastion VM, run the following command to get the <strong class="source-inline">SHA512</strong> digest of the Ignition files (this will be used after booting from the ISO to validate the authenticity of the file):<p class="source-code">$ sha512sum ~/ocp/bootstrap.ign</p><p class="source-code">$ sha512sum ~/ocp/master.ign</p><p class="source-code">$ sha512sum ~/ocp/worker.ign</p></li>
			</ol>
			<p>An example of the output is shown here:</p>
			<p class="source-code">a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b installation_directory/bootstrap.ign</p>
			<ol>
				<li value="3">Boot using the ISO image, but don't specify any options until you see a shell prompt.</li>
				<li>Run the following <strong class="source-inline">coreos-installer</strong> command to start the ignition process and, consequently, the OS provisioning:<p class="source-code">$ sudo coreos-installer install --ignition-url= http://192.168.1.200:81/ignition/bootstrap.ign /dev/sda --ignition-hash=SHA512-</p><p class="source-code">a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b</p></li>
				<li>Repeat the same process for each server, always respecting the following format for the <strong class="source-inline">coreos-intaller</strong> command:<p class="source-code">$ sudo coreos-installer install --ignition-url=http://192.168.1.200:81/&lt;node_type&gt;.ign &lt;device&gt; --ignition-hash=SHA512-&lt;digest&gt;</p></li>
			</ol>
			<p>Where <strong class="source-inline">&lt;node_type&gt;</strong> will be <strong class="source-inline">bootstrap.ign</strong>, <strong class="source-inline">master.ign</strong>, or <strong class="source-inline">worker.ign</strong>, <strong class="source-inline">&lt;device&gt;</strong> is the <a id="_idIndexMarker382"/>disk to be used to install the OS (such as <strong class="source-inline">/dev/sda</strong>), and <strong class="source-inline">&lt;digest&gt;</strong> is the result of the <strong class="source-inline">sha512sum</strong> command mentioned previously.</p>
			<p>After booting the bootstrap and master nodes using this procedure, you can go to the next step to monitor the progress of the installation.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor106"/>Phase 2 – Bootstrap and control plane</h2>
			<p>In this phase, Bootstrap will <a id="_idIndexMarker383"/>download the container images to provision the control plane components. As soon as the containers in each master node are running, the control plane components will start configuring themselves<a id="_idIndexMarker384"/> until the <strong class="bold">etcd</strong> cluster, API, and controllers from OpenShift are synchronized. </p>
			<p>Run the following command from the Bastion VM to monitor the progress of the Bootstrap and control plane deployment:</p>
			<pre class="source-code">./openshift-install wait-for bootstrap-complete --dir= /home/user/ocp/ --log-level=debug</pre>
			<p>Immediately after the command is triggered, you will see some log messages on the console, similar to this sample:</p>
			<pre class="source-code">INFO Waiting up to 30m0s for the Kubernetes API at https://api.ocp.hybridmycloud.com:6443... INFO API v1.22.1 up INFO Waiting up to 30m0s for bootstrapping to complete... INFO It is now safe to remove the bootstrap resources</pre>
			<p>After that, you must remove<a id="_idIndexMarker385"/> Bootstrap from the load balancer and restart the <strong class="source-inline">haproxy</strong> service.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Remember, the Bootstrap server is a one-shot use only; therefore, you can destroy the Bootstrap server completely from the infrastructure provider because it will not be used anymore, even if something goes wrong during the cluster installation.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor107"/>Phase 3 – Check for certificates to sign – For UPI and agnostic installations only</h2>
			<p>When Bootstrap finishes its <a id="_idIndexMarker386"/>process, the Red Hat OpenShift Container Platform creates a series of <strong class="bold">certificate signing requests</strong> (<strong class="bold">CSRs</strong>) for each of the <a id="_idIndexMarker387"/>nodes. During our planning, we attempted to provision two worker nodes, so we must accept the certificates to join the worker nodes to the cluster.</p>
			<p>We need to use the <strong class="source-inline">oc</strong> client to approve the certificates. To do so, run the following command to export the <strong class="source-inline">kubeadmin</strong> credentials and get access to the cluster:</p>
			<pre class="source-code">$ export KUBECONFIG=~/ocp/auth/kubeconfig</pre>
			<p>A simple command can list the pending certificates and approve them until no pending certificates are showing:</p>
			<pre class="source-code">$ oc get csr | grep –i Pending 
NAME      AGE   REQUESTOR                             CONDITION 
csr-bfd72 5m26s system:node:worker0.ocp.hybridmycloud.com   Pending 
csr-c57lv 5m26s system:node:worker1.ocp.hybridmycloud.com   Pending 
...</pre>
			<p>Then, to approve the certificates, run the following command:</p>
			<pre class="source-code">$ oc get csr -o name | xargs oc adm certificate approve
certificatesigningrequest.certificates.k8s.io/csr-bfd72 approved
certificatesigningrequest.certificates.k8s.io/csr-c57lv approved</pre>
			<p>To confirm that everything<a id="_idIndexMarker388"/> worked fine, run the following commands until all nodes remain <strong class="source-inline">Ready</strong>:</p>
			<pre class="source-code">$ oc get nodes
NAME                        STATUS   ROLES    AGE   VERSION
ocp-7m9wx-master-0       Ready    master   77d   v1.21.1+9807387
ocp-7m9wx-master-1       Ready    master   77d   v1.21.1+9807387
ocp-7m9wx-master-2       Ready    master   77d   v1.21.1+9807387
ocp-7m9wx-worker-jds5s   Ready    worker   77d   v1.21.1+9807387
ocp-7m9wx-worker-kfr4d   Ready    worker   77d   v1.21.1+9807387</pre>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor108"/>Phase 4 – Finishing the installation</h2>
			<p>We are almost at the<a id="_idIndexMarker389"/> end of our UPI/agnostic installation! Now, we must check the cluster operators to ensure that all of them are available. </p>
			<p>Using the following command, you will be able to monitor the cluster operators' deployment progress:</p>
			<pre class="source-code">./openshift-install wait-for install-complete --dir= /home/user/ocp/ --log-level=debug
INFO Waiting up to 30m0s for the cluster to initialize...</pre>
			<p>When it finishes, you will receive the <strong class="source-inline">kubeadmin</strong> password to finally get access to your OpenShift cluster.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout"><strong class="source-inline">kubeadmin</strong> is a temporary user with the <strong class="source-inline">cluster-admin</strong> privileges. It is highly recommended that you remove the <strong class="source-inline">kubeadmin</strong> user as soon as you set up a new identity provider, and give proper <strong class="source-inline">cluster-admin</strong> privileges to the cluster administrators.</p>
			<p>Now, you can access the OpenShift Console GUI using your preferred browser. To do so, browse to <a href="B18015_05.xhtml">https://console-openshift-console.apps.ocp.hybridmycloud.com</a> and insert the <strong class="source-inline">kubeadmin</strong> credentials:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B18015_05_05.jpg" alt="Figure 5.5 – Accessing the Console UI "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Accessing the Console UI</p>
			<p>Then, sit back<a id="_idIndexMarker390"/> and enjoy:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B18015_05_06.jpg" alt="Figure 5.6 – The Console UI "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – The Console UI</p>
			<p>Congratulations! You successfully deployed an OpenShift cluster! Celebrate your great success, but remember that your journey is just starting! </p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor109"/>What's next?</h1>
			<p>Now, OpenShift is minimally functional, meaning that you have a control plane that can schedule pods, handle API calls and controllers, and an etcd cluster, which offers key/value storage for objects in the cluster. You also have some worker nodes fully functional that can already host some workloads.</p>
			<p>But that's not all! Now, the activities that demand configuration begin (also known as Day 2, or post-installation activities). In the next few chapters, we will look at Ingress configurations, networks, registry configurations, views for local volumes, and persistent volumes. </p>
			<p>We will also talk about taints and tolerations, security, and best practices – everything it takes for you to go from zero to hero and start acting on more complex implementations.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor110"/>FAQs</h1>
			<p>While in the deployment phase, whatever kind of installation you do, it is common for things to not work as expected. We will look at some error cases you could face during deployment.</p>
			<p><em class="italic">Why did the </em><strong class="source-inline">openshift-install</strong><em class="italic"> execution stick waiting for the API</em>?</p>
			<p>The following message will be displayed:</p>
			<pre class="source-code">Message: "INFO Waiting up to 20m0s for the Kubernetes API at https://api.ocp.hybridmycloud.com:6443..."</pre>
			<p>Sometimes, the <strong class="source-inline">INFO</strong> message can be big trouble. At this point, you do not have much output to investigate, even if you increase the debug level of the messages. In this case, you should look at some of these options:</p>
			<ul>
				<li>Look up the API URL and check the DNS resolution; the query must result in the API IP in <strong class="source-inline">install-config.yaml</strong>.</li>
				<li>Try to ping all three master nodes individually to assure it is already up.</li>
				<li>UPI installation: Check on the hypervisor VM terminal if all masters are up and running and at the login prompt screen. Master nodes on the bootloader menu or whatever different situation of the login prompt screen could result in an API waiting message.</li>
				<li>IPI installation: Check the cloud credentials and permissions related to prerequisites. Your credentials might not have all the necessary permissions to create objects in general. Don't give admin permission to the role attributed by the cluster service user because installation searches for a specific permission name. You can check roles and permission tables at <a href="B18015_05.xhtml">https://docs.openshift.com/container-platform/4.9/installing/installing_vsphere/installing-vsphere-installer-provisioned.html</a>, according to the cloud provider (for example, VMware vCenter).</li>
			</ul>
			<p><em class="italic">Timeout installation waiting for Bootstrap to complete</em></p>
			<p>The following message will be displayed:</p>
			<pre class="source-code">Message: "INFO Waiting up to 30m0s for bootstrapping to complete..."</pre>
			<p>When the <strong class="source-inline">openshift-install</strong> binary freezes while waiting for a Bootstrap process to complete, it means that it is waiting for some cluster operators to become available. In this case, do the following:</p>
			<ul>
				<li>Check whether you have enough worker nodes (at least two) to make the Ingress operator available.</li>
				<li>SSH to a worker node and check that the <strong class="source-inline">crictl</strong> process is still creating pods.</li>
				<li>SSH to a worker node and search for errors related to <strong class="source-inline">kube-apiserver</strong>, <strong class="source-inline">kubelet</strong>, <strong class="source-inline">podman</strong>, or <strong class="source-inline">crictl</strong> using the <strong class="source-inline">journalctl</strong> daemon. </li>
			</ul>
			<p><em class="italic">X509 messages during cluster deployment</em></p>
			<p>The following message will be displayed:</p>
			<pre class="source-code">Message: "x509 certificate signed by unknown authority.."</pre>
			<p>When creating Ignition files, OpenShift automatically creates a self-signed certificate that will be verified with every API call within the cluster. However, even if you have done all the prerequisite processes properly, sometimes you may get similar <strong class="source-inline">x509</strong> messages that will result in the installation process failing and not achieving the expected result. Try checking the following options:</p>
			<ul>
				<li><strong class="bold">VSphere IPI installation</strong>: Make sure you have imported the VMware CA certificate from the cluster that will extend OpenShift to the machine and will start the cluster installation.</li>
				<li><strong class="bold">Bare metal installation</strong>: <strong class="source-inline">master.ign</strong> does not have the same CA certificate configured on the load balancer that must respond in <strong class="source-inline">api-int</strong>. Also, verify that the external load balancer has been configured to use Layer 4/TCP/Passthrough.</li>
			</ul>
			<p>Certificates created by <strong class="source-inline">openshift-install</strong> residing in <strong class="source-inline">master.ign</strong> have an expiration date of 24 hours and cannot be updated. If you tried to install the day before, and it was not successful, delete the installation directory and start creating manifests and Ignition files again.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor111"/>Summary</h1>
			<p>In this chapter, we have examined some options to install and configure your OpenShift Container Platform solution. From public clouds to on-premises, we have navigated through UPI, IPI, and agnostic implementation methods.  </p>
			<p>You now know about the public clouds offering fully supported implementations and documentation to start your cluster.  </p>
			<p>You are invited to look deeper into making your OpenShift cluster stronger, more reliable, and as secure as possible. We encourage you to continue to the next chapter and learn even more with us.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor112"/>Further reading</h1>
			<p>If you want to look at more information related to the concepts we covered in this chapter, check the following references:</p>
			<ul>
				<li><em class="italic">The installation process for disconnected installations:</em> <a href="B18015_05.xhtml">https://docs.openshift.com/container-platform/latest/installing/installing-mirroring-installation-images.html</a></li>
				<li><em class="italic">OpenShift Container Platform 4.x:</em> Tested integrations by Red Hat and partners: <a href="B18015_05.xhtml">https://access.redhat.com/articles/4128421</a></li>
				<li><em class="italic">OpenShift Container Platform IPI: x509 certificate signed by an unknown authority:</em> <a href="B18015_05.xhtml">https://access.redhat.com/solutions/5203431</a></li>
				<li><em class="italic">OpenShift Container Platform bare metal:</em> x509 certificate signed by an unknown authority: <a href="B18015_05.xhtml">https://access.redhat.com/solutions/4271572</a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer098">
			</div>
		</div>
</body></html>