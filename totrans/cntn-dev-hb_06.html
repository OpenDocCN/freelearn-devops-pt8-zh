<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer066">
<h1 class="chapter-number" id="_idParaDest-117"><a id="_idTextAnchor134"/>6</h1>
<h1 id="_idParaDest-118"><a id="_idTextAnchor135"/>Fundamentals of Container Orchestration</h1>
<p>So far, we have learned what software containers are, how they work, and how to create them. We focused on using them, as developers, to create our applications and distribute functionalities into different components running in containers. This chapter will introduce you to a whole new perspective. We will learn how our applications run in production using containers. We will also introduce the concept of container orchestrators and cover what they can deliver and the key improvements we need to include in our applications to run them in a distributed <span class="No-Break">cluster-wide fashion.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Introducing the key concepts <span class="No-Break">of orchestration</span></li>
<li>Understanding stateless and <span class="No-Break">stateful applications</span></li>
<li>Exploring <span class="No-Break">container orchestrators</span></li>
</ul>
<p>We will then go on to study how to leverage Docker Swarm and Kubernetes orchestrators’ features in <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating with Swarm</em>, and <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the Kubernetes Orchestrator</em>. This chapter does not include any labs as it is intended to teach you the theory behind Docker Swarm <span class="No-Break">and Kubernetes.</span></p>
<h1 id="_idParaDest-119"><a id="_idTextAnchor136"/>Introducing the key concepts of orchestration</h1>
<p>Running an <a id="_idIndexMarker669"/>application on a host may be complicated, but executing this same application on a distributed environment composed of multiple hosts would be very tedious. In this section, we will review some of the key concepts regarding the orchestration of application components, regardless of whether they are run using containers or as different <span class="No-Break">virtual machines.</span></p>
<p>Orchestrators are special software components that help us manage the different interactions and dependencies between our application components. As you can imagine, if you divide your application into its many different functionalities, each with its own entity, orchestrating them together is key. We have to say here that some special functionalities, such as dependency management, may not be available in your orchestrator and therefore you will need to manage them by yourself. This gives rise to an important question: what do we need to know about orchestrators to prepare our applications for them? Orchestrators keep our processes up and running, manage the communications between our applications’ components, and attach the storage required for <span class="No-Break">these processes.</span></p>
<p>Focusing specifically on container-based applications, it is easy to understand that container runtimes will be part of the orchestration infrastructure as they are required to run containers. The biggest challenge when working with containers is the intrinsic dynamism associated with their networking features. This will probably not be a problem in virtual machine environments, but containers commonly use different IP addresses on each execution (and although we can manually assign IP addresses to containers, it is not good practice). Note that in the previous chapters’ labs (<a href="B19845_04.xhtml#_idTextAnchor096"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Running Docker Containers</em>, and <a href="B19845_05.xhtml#_idTextAnchor118"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Creating Multi-Container Applications</em>), we used service names instead of container <span class="No-Break">IP addresses.</span></p>
<p>Let’s review the key<a id="_idIndexMarker670"/> concepts to bear in mind when designing an application to run its components in a distributed <span class="No-Break">cluster-wide fashion:</span></p>
<ul>
<li><strong class="bold">Dependencies resolution</strong>: Some dependencies may exist in your application. This means<a id="_idIndexMarker671"/> that some services are required <em class="italic">before</em> others to enable your application’s functionality. Some orchestration solutions such as Docker Compose (standalone orchestration) include this feature, but most others usually don’t. It is up to you, as a developer, to resolve any issues arising from these dependencies in your code. A simple example is a database connection. It is up to you to determine what to do if the connection from some components is lost due to a database failure. Some components may function correctly while others may need to reconnect. In such situations, you should include a verification of the connectivity before any transaction and prepare to manage queued transactions that may accrue before your component realizes that the database <span class="No-Break">isn’t working.</span></li>
<li><strong class="bold">Status</strong>: Knowing the current status of each component is critical at any time. Some orchestrators have their own features to check and verify the status of each component, but you, as the developer of your application, know best which paths, processes, ports, and so on are required and how to test whether they are alive. In <a href="B19845_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Building Docker Images</em>, we discussed how the simpler the content on your container images, the better the security. If your application requires some additional software for testing its health, you should include it; however, it may be a better idea to include some test endpoints or testing functions that can be called when required to verify the health of the <span class="No-Break">application’s components.</span></li>
<li><strong class="bold">Circuit breakers</strong>: If we have already successfully managed our application’s dependencies, circuit breakers will allow us to identify any problems and make appropriate decisions when some components are down. Orchestrators don’t provide any circuit breakers natively; you will need additional software or infrastructure components to implement such solutions. Depending on the complexity of your application, it can be beneficial to integrate some circuit breakers. For example, we can stop all the components that require a healthy database whenever this is not available, while other components can continue running and providing their functionality <span class="No-Break">as usual.</span></li>
<li><strong class="bold">Scalability</strong>: Perhaps some of your application’s components run with more than one replica. Scalability should be built in by design. Orchestrators allow you to execute more<a id="_idIndexMarker672"/> than one replica of any component, but it is up to you to manage their co-existence. A database component running with more than one replica will corrupt your data if it is not prepared for such a situation. In this example, you would <a id="_idIndexMarker673"/>need to use a master-slave or distributed database architecture. Other issues regarding data transactions may appear, such as in your frontends, if you don’t manage user sessions. In such situations, session integrity may require additional components to ensure that all transactions follow a coordinated workflow. Orchestrators do not know about any of your application’s components’ functionality and as such, it is up to you to determine whether some components can scale up or down. Some orchestrators will provide you with rules for deciding when this should occur, but they just trigger the actions for managing your component’s replicas. All replicas will be treated in the same way and you will need to add additional components if you require weight distribution functionality to ensure some replicas receive more data <span class="No-Break">than others.</span></li>
<li><strong class="bold">High availability</strong>: The concept of high availability may vary depending on whether you ask someone from the infrastructure team or the application side, but both will <a id="_idIndexMarker674"/>agree that it should be transparent for the end user. In terms of infrastructure, we can think of high availability at <span class="No-Break">various levels:</span><ul><li><strong class="bold">Duplicity of infrastructure</strong>: When physical or virtual machines are used to provide high availability for our applications, we need to have all our infrastructure duplicated, including the underlying communications layers and storage. Most devices know how to manage the special configurations that must be <a id="_idIndexMarker675"/>changed when a replicated device acts as a master. Servers should maintain quorum or master-slave relations to decide which of them will handle user requests. This will be required even if we plan to have active-passive (where only one instance handles all requests and the others take over only if any error is found) or active-active responses (all instances serve at the <span class="No-Break">same time).</span></li><li><strong class="bold">Routes to healthy components</strong>: Duplicated infrastructure requires a layer of extra components to route the user’s request. Load balancers help present the different applications’ endpoints in a completely transparent way for <span class="No-Break">the users.</span></li><li><strong class="bold">Storage backends</strong>: As we would expect, storage not only has to maintain the data safely and securely, but also attach it to any running and serving instance. In active-passive environments, storage will switch from a damaged instance to a healthy one. You, as the developer, may need to ensure data integrity after <span class="No-Break">a switchover.</span></li></ul><p class="list-inset">High availability means that services will never be interrupted, even when maintenance tasks require one instance to be stopped. Container orchestrators don’t provide high availability by themselves; it must be factored in as part of your <span class="No-Break">application design.</span></p></li>
<li><strong class="bold">Service state definition</strong>: We usually define how an application is to be deployed, including the number of replicas that must be available for the service to be considered healthy. If any replica fails, the orchestrator will create or start a new one for you. We don’t need to trigger any event, just define the monitors to allow the orchestrator to review the status of each <span class="No-Break">application component.</span></li>
<li><strong class="bold">Resilience</strong>: Applications may fail, and the orchestrator will try to start them again. This is the concept of <em class="italic">resilience</em>, which orchestrators provide by default. When your application is run in containers, the container runtimes keep the application alive by starting them again when something goes wrong. Orchestrators interact with container runtimes to manage the start and stop processes for your containers cluster-wide, trying to mitigate the impact of the failures in your applications. Therefore, you must design your applications to stop and start fast. Usually, applications running in containers don’t take more than a few seconds to get running, so your users may not even notice an outage. So, to avoid<a id="_idIndexMarker676"/> significant issues for users, we must provide high availability for our <span class="No-Break">application’s components.</span></li>
<li><strong class="bold">Distributed data</strong>: When your applications run in a distributed cluster-wide fashion, different <a id="_idIndexMarker677"/>hosts will run your application’s components, hence the data required needs to be made available when needed. The orchestrator will interact with container runtimes to mount container volumes. You can mount your data in all the possible hosts in anticipation that the containers will use it, and this may seem a good idea at first. However, managing an application’s data permissions may leave you with unexpected consequences. For example, you can misconfigure the permissions for a root directory where different applications are storing their data, allowing some applications to read others’ data files. It is usually better to entrust the volume management to the orchestrator itself, using the features it makes available <span class="No-Break">for this.</span></li>
<li><strong class="bold">Interoperability</strong>: Communications between your application components can be complicated, but orchestration provides you with a simplified network layer. In <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating with Swarm</em>, and <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the Kubernetes Orchestrator</em>, we will see how both of these orchestrators provide a different layer of communications for your applications. It is very important to learn and understand how they deploy your application’s communications and design your application to avoid any network lock-in from the beginning. This will allow your applications to work on any of the <span class="No-Break">available orchestrators.</span></li>
<li><strong class="bold">Dynamic addressing</strong>: Container environments are based on dynamic objects managed by the container runtime. Some of these objects can have static properties such as IP addresses within containers. Managing static properties in very dynamic infrastructures can be difficult and is not recommended. Orchestrators will manage dynamism for you if you follow <span class="No-Break">their rules.</span></li>
<li><strong class="bold">Service discoverability</strong>: Services published within orchestrators will be announced internally and can be reached from any other application component. By default, such services only work internally, which improves the security of your <a id="_idIndexMarker678"/>full application as you only externally publish those frontend services that must be reached <span class="No-Break">by users.</span></li>
<li><strong class="bold">Multisite</strong>: Having<a id="_idIndexMarker679"/> multiple data centers for follow-the-sun or disaster recovery architectures is common in big enterprises. You must always ensure that your applications can run anywhere. We can go further than this because some companies may have cloud-provisioned infrastructure as well as on-premises services – in such situations, your applications may run some components on cloud infrastructure while others run locally. Apart from the infrastructure synchronization challenges that this scenario may create, if you design your applications with these advanced environments in mind and you understand the breakpoints to avoid (for example, quorum between components if communications are lost), you will still be able to run your applications in <span class="No-Break">extreme situations.</span></li>
</ul>
<p>One of the key aspects of deploying applications cluster-wide is the management of component <a id="_idIndexMarker680"/>status. In the next section, we will review the importance of setting a component's status outside the container’s life cycle when designing <span class="No-Break">our applications.</span></p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor137"/>Understanding stateless and stateful applications</h1>
<p>Previously, we provided a brief description of the key points and concepts regarding orchestrating your applications and running them distributed across a cluster, in which you may have noticed that providing a service to your users without outages can be complex. We reviewed how orchestrators help us deliver resilient processes and saw how high availability has to be designed into applications. One of the key aspects of such a design concerns how your application manages the processes’ state <span class="No-Break">over time.</span></p>
<p>Applications fall into two different categories, depending on how they manage their processes’ state: <strong class="bold">stateful</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">stateless</strong></span><span class="No-Break">.</span></p>
<p>Before learning about each one, it is important to understand what the state of an application or process means. The state of a system is the condition in which it is at a specific time. This system can be running or stopped, or even in between both when it is either starting or stopping. It is important to be able to identify and manage the status of the application’s components. To manage the state of a system, such as triggering an action to start or stop the system, we must know how to retrieve the state. In many cases, this is not simple, and particularly complex situations may require managing some dependencies or interactions with other <span class="No-Break">external systems.</span></p>
<p>Now that we have a definition for the state of a system, let’s describe stateful and stateless applications with <span class="No-Break">some examples.</span></p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor138"/>Stateful applications</h2>
<p>Imagine a situation where <a id="_idIndexMarker681"/>a process needs some data to be loaded before it can start. If the process reads all its configurations when this starts, we will need to restart it for any change to take effect. But if this process reads part of the configuration (or the full content) whenever it is needed for some functions or actions, a restart may not be needed. That’s why we will need to know whether the process has already started or not to load the required data. In some cases, we can design a full process that loads the data whenever the process starts without reviewing whether it was started before. However, other times, this can’t be done because we can’t replace the data or load it more than once. A simple file can be used as a flag to specify whether the load process was already executed or if we need to load the <span class="No-Break">data again.</span></p>
<p>This can be managed locally quite easily if our application is running on a host as a simple process but isn’t easy when working with containers. When a container runs in a host, it uses its own storage layers unless we specify a volume to store some data. A new container running the same process can reuse a previous volume if your application stores this flag file outside the container’s life cycle by design. This appears easy enough in a standalone host where all the processes run in the <span class="No-Break">same host.</span></p>
<p>The volume can be either a bind mount (a directory from the host’s filesystem) or a named volume (a volume with a known and reusable name). When you run your application cluster-wide, this approach may not work correctly. This is because a bind mount is attached to a host, and that directory will not exist on other hosts. Remote filesystems can be used to persist the flag and make it available in other hosts. In this case, we use a volume and the orchestrator will manage the mounting of the <span class="No-Break">required filesystem.</span></p>
<p>However, managing an application’s state is more difficult when more than one process is involved. In such situations, it is recommended to take this requirement into account at the very beginning of your application’s design process. If we were designing a web application, for example, we would need to store some user data to identify who made a given request. In this scenario, we don’t just have to manage the process state – we also need to manage <a id="_idIndexMarker682"/>users’ data, so more than one file will be required and we will have to use a database to store this data. We usually say that such an application is <strong class="bold">stateful</strong> and<a id="_idIndexMarker683"/> requires <span class="No-Break"><strong class="bold">persistent data</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor139"/>Stateless applications</h2>
<p>Stateless <a id="_idIndexMarker684"/>applications, on the other hand, do not require any data persistence. We can restart these components whenever needed without persisting any data. The application itself contains all the required information. Imagine a service that receives some data, and if the service doesn’t respond, we send the data again until it gives a response. This service can do some operations with the received data and send a response without needing to save any data. In such a situation, this service is <strong class="bold">stateless</strong>. It may require some external data to do operations, but if something goes wrong and we need to restart the service, we aren’t concerned with the status of any pending operations. We will simply send the data again until we get a valid response. When some operations are still pending, we have both a stateful process, which requires us to load some pending requests, and a stateless process, because it doesn’t store the requests by itself. The service sending the request may need to store it while the one processing the <span class="No-Break">operation doesn’t.</span></p>
<p>As you may imagine, stateless applications are easier to manage in distributed environments. We don’t need to manage process states and their associated data in different locations <a id="_idIndexMarker685"/>across <span class="No-Break">a cluster.</span></p>
<p>In the next section, we will review some of the most popular <span class="No-Break">container orchestrators.</span></p>
<h1 id="_idParaDest-123"><a id="_idTextAnchor140"/>Exploring container orchestrators</h1>
<p>Now that we know<a id="_idIndexMarker686"/> what to expect from any container orchestrator, let’s review some of the most important and technically relevant ones available. We will also take a quick look at the strengths and weaknesses of each <span class="No-Break">option presented.</span></p>
<p>We will start with the currently most popular and widely extended container <span class="No-Break">orchestrator, Kubernetes.</span></p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor141"/>Kubernetes</h2>
<p><strong class="bold">Kubernetes</strong> is an open <a id="_idIndexMarker687"/>source container orchestration<a id="_idIndexMarker688"/> platform, fast becoming the de facto standard for running microservices on cloud providers and local data centers. It started as a Google project for managing the company’s internal applications back in 2003. This project was initially called Borg and was created for deploying workloads distributed across different nodes and clusters. This project evolved into a more complex orchestration platform called Omega, which focused on bigger clusters running thousands of workloads for very different applications. In 2014, Google published Borg’s code to the open source community and it finally became Kubernetes that same year. In 2015, the first release, Kubernetes 1.0, was published after Red Hat, IBM, Microsoft, and Docker joined the community project. The Kubernetes community is now huge and a very important part of why this orchestrator has become so <span class="No-Break">popular nowadays.</span></p>
<p>The most important feature of Kubernetes is that its core is focused on executing a few tasks and delegating more complicated tasks to external plugins or controllers. It is so extensible that many contributors add new features daily. Nowadays, as Kubernetes is the most popular and extended container orchestrator, it is quite common for software vendors to provide their own Kubernetes definitions for their applications when you ask them for high availability. Kubernetes does not provide container runtimes, cluster network capabilities, or clustered storage by default. It is up to us to decide which container runtime to use for running the containers to be deployed and maintained by <span class="No-Break">the orchestrator.</span></p>
<p>On the network side, Kubernetes defines a list of rules that must be followed by any <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) we<a id="_idIndexMarker689"/> want to be included in our platform to deliver inter-container communications cluster-wide, as we will learn in <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the Kubernetes Orchestrator</em>. The Kubernetes network model differs from other orchestration solutions in the way it presents a plain or flat network (no routing is required between containers), where all containers are reachable by default. Many open source and proprietary options for deploying the Kubernetes network are also available, including Flannel, Weave, Cilium, and Calico. These network providers define the overlay networks and IPAM configurations for our Kubernetes cluster and even encrypt communications <span class="No-Break">between nodes.</span></p>
<p>Kubernetes provides<a id="_idIndexMarker690"/> many cloud provider integrations because it <a id="_idIndexMarker691"/>was designed to be cloud-ready. A cloud controller is available for managing integrations with publishing applications or using some special cloud-provided storage backends. As mentioned earlier in this section, Kubernetes does not provide a solution for deploying any cluster-wide storage backend, but you can integrate NFS and some AWS, Google, and Azure storage backends for your applications. To extend your storage possibilities, you can use <strong class="bold">Container Storage Interfaces</strong> (<strong class="bold">CSI</strong>), which <a id="_idIndexMarker692"/>are different vendor or community-driven storage backends that can easily be integrated into Kubernetes to provide different storage solutions for our <span class="No-Break">orchestrated containers.</span></p>
<p>Many cloud providers and software vendors package and share or sell their own Kubernetes flavors. For example, Red Hat/IBM provide their own Kubernetes platform inside their OpenShift product. Microsoft, Amazon, Google, and indeed almost all cloud service providers have their own Kubernetes implementations ready for use. In these Kubernetes platforms, you don’t even have to manage any of the control plane features – the platforms are offered as Kubernetes-managed solutions for you, as the developer, to use for delivering your applications. These solutions are known as <strong class="bold">Kubernetes-as-a-Service</strong> platforms <a id="_idIndexMarker693"/>and are where you pay for your workloads and the bandwidth used in <span class="No-Break">your applications.</span></p>
<p>The Kubernetes project publishes a release approximately every 4 months and maintains three minor releases at a time (thus providing almost a year of patches and support for each release). Some changes and deprecations are always expected between releases, so it is very important to review the change notes for <span class="No-Break">each release.</span></p>
<p>Kubernetes clusters have nodes with different roles: <strong class="bold">master</strong> nodes <a id="_idIndexMarker694"/>create the control plane for delivering <a id="_idIndexMarker695"/>containers, while <strong class="bold">worker</strong> nodes execute the workloads assigned to them. This model allows us to deploy a Kubernetes cluster with high availability by replicating some of the master node’s services. An open source key-value database, called etcd, is used for managing all objects’ (known in<a id="_idIndexMarker696"/> Kubernetes as <strong class="bold">resources</strong>) references <span class="No-Break">and states.</span></p>
<p>Kubernetes has evolved so fast that nowadays, we can even manage and integrate virtual machines into Kubernetes clusters by using operators such as KubeVirt. An additional great aspect of Kubernetes is that you can create your own resources (<strong class="bold">Kubernetes Custom Resource Definitions</strong>) for<a id="_idIndexMarker697"/> your application when some special requirements<a id="_idIndexMarker698"/> for your application are not met by the<a id="_idIndexMarker699"/> Kubernetes <span class="No-Break">core resources.</span></p>
<p>Let’s quickly summarize the pros of <span class="No-Break">using Kubernetes:</span></p>
<ul>
<li>Available for <a id="_idIndexMarker700"/>customers of many cloud providers and from many on-premises  software solutions for <span class="No-Break">container providers</span></li>
<li>Very thoroughly documented with lots of examples and guides for learning <span class="No-Break">the basics</span></li>
<li>Highly extensible via standardized interfaces such as the CNI <span class="No-Break">and CSI</span></li>
<li>Lots of objects or resources that will meet most of your application’s requirements for <span class="No-Break">running cluster-wide</span></li>
<li>Comes with many security features included, such as role-based access control, service accounts, security contexts, and <span class="No-Break">network policies</span></li>
<li>Offers different methods for publishing <span class="No-Break">your applications</span></li>
<li>Used as a standard deployment method for many software vendors, you may easily find your applications packaged in Kubernetes <span class="No-Break">manifest format</span></li>
</ul>
<p>However, it does have some disadvantages as well. These include <span class="No-Break">the following:</span></p>
<ul>
<li>It is not easy to <a id="_idIndexMarker701"/>master due to its continuous evolution and many resource types. The learning curve may seem higher compared to other <span class="No-Break">orchestration solutions.</span></li>
<li>The many releases per year may necessitate a lot of effort to maintain <span class="No-Break">the platform.</span></li>
<li>Having many flavors can be a problem when each vendor introduces their own particularities on <span class="No-Break">their platforms.</span></li>
<li>You will never use Kubernetes for just one application or a few small ones because it takes a lot of maintenance and deploying efforts. Indeed, Kubernetes-as-a-Service providers such as Microsoft’s Azure Kubernetes Service will help you with minimal <span class="No-Break">maintenance efforts.</span></li>
</ul>
<p>We will learn about <a id="_idIndexMarker702"/>all the Kubernetes features and <a id="_idIndexMarker703"/>how we will prepare and deploy our applications in this orchestrator in more detail in <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the </em><span class="No-Break"><em class="italic">Kubernetes Orchestrator</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor142"/>Docker Swarm</h2>
<p><strong class="bold">Docker Swarm</strong> is a container <a id="_idIndexMarker704"/>orchestration solution <a id="_idIndexMarker705"/>created by Docker Inc. It is intended to provide a simple orchestration platform that includes everything needed to run our containerized applications cluster-wide by default. This includes overlay networking (which can be encrypted) and isolation by creating different networks for each project <span class="No-Break">if required.</span></p>
<p>Different objects can be used to deploy our <a id="_idIndexMarker706"/>applications, such <a id="_idIndexMarker707"/>as <strong class="bold">global</strong> or <strong class="bold">replicated</strong> services, each with its own properties for managing how the containers are to be spread cluster-wide. As we have seen with Kubernetes, a master-worker or (master-slave) model is also used. The master node creates the full control plane and the worker nodes execute <span class="No-Break">your containers.</span></p>
<p>It is important to mention a big difference in the way changes are managed within the cluster. While Kubernetes uses etcd as its key-value database, Docker Swarm manages its own object database solution using the Raft Consensus Algorithm with a complete command-line interface for this. Docker Engine installation is enough to get working with Docker Swarm as the container runtime binaries also include SwarmKit features. Moby is the open source project behind Docker Inc., having created kits for delivering and improving container communications (VPNKit) and improving the default <strong class="source-inline">docker build</strong> features (BuildKit), some of which we covered in <a href="B19845_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Building Docker Images</em>, with the <strong class="source-inline">buildx</strong> extended build command-line. SwarmKit is the Moby project behind Docker Swarm and provides the cluster functionality, security, and simplicity of its model. Docker Swarm is quite simple, but this doesn’t mean it isn’t production-ready. It provides the minimum features required to deploy your applications with <span class="No-Break">high availability.</span></p>
<p>It is important to mention that Compose YAML files allow us to deploy our applications using a set of manifests for creating and managing all our application objects in Docker Swarm. Some of the keys we learned about in <a href="B19845_05.xhtml#_idTextAnchor118"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Creating Multi-Container Applications</em>, will not work here, such as <strong class="source-inline">depends_on</strong>, hence application dependency management <a id="_idIndexMarker708"/>must <a id="_idIndexMarker709"/>be covered in your <span class="No-Break">code itself.</span></p>
<p> Here are some of the advantages of <span class="No-Break">Docker Swarm:</span></p>
<ul>
<li>Easier to learn<a id="_idIndexMarker710"/> than other <span class="No-Break">container orchestrators</span></li>
<li>Integrated inside Docker Engine and managed using the Docker <span class="No-Break">command line</span></li>
<li><span class="No-Break">Single-binary deployment</span></li>
<li>Compatible with Compose <span class="No-Break">YAML files</span></li>
</ul>
<p>Some of the disadvantages are <span class="No-Break">as follows:</span></p>
<ul>
<li>There are fewer <a id="_idIndexMarker711"/>objects or resources for deploying our applications, which may affect the logic applied in our applications. It is important to understand that you, as a developer, can implement the logic for your application inside your code and avoid any possible issues associated with <span class="No-Break">the orchestration.</span></li>
<li>It only works with the Docker container runtime, so vendor lock-in is present and the security improvements offered by other container runtimes can’t be <span class="No-Break">used here.</span></li>
<li>Although Docker Swarm provides some plugins associated with the container runtime, it isn’t as open and extensible <span class="No-Break">as Kubernetes.</span></li>
<li>Publishing applications is easier, but this means that we can’t apply any advanced features <a id="_idIndexMarker712"/>without <span class="No-Break">external tools.</span></li>
</ul>
<p>We will learn <a id="_idIndexMarker713"/>more <a id="_idIndexMarker714"/>about Docker Swarm in <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating </em><span class="No-Break"><em class="italic">with Swarm</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor143"/>Nomad</h2>
<p>HashiCorp <strong class="bold">Nomad</strong> is a<a id="_idIndexMarker715"/> platform <a id="_idIndexMarker716"/>that allows us to run containers, virtual machines (using <strong class="bold">QEMU</strong>, which<a id="_idIndexMarker717"/> is a well-known open source virtualization engine), and Java applications. It focuses on scheduling application workloads and checking which services, such as discovery, health check monitoring, DNS, and secrets, are delivered by other HashiCorp tools, including <strong class="bold">Consul</strong> and <strong class="bold">Vault</strong>. Nomad<a id="_idIndexMarker718"/> bases <a id="_idIndexMarker719"/>its<a id="_idIndexMarker720"/> security on <strong class="bold">Access Control Lists</strong> (<strong class="bold">ACLs</strong>), including tokens, policies, roles, and capabilities. In terms of networking, it uses CNI plugins for the bridge working mode. Its multi-site features allow us to run applications in different regions from a single-orchestration perspective <a id="_idIndexMarker721"/><span class="No-Break">using </span><span class="No-Break"><strong class="bold">federation</strong></span><span class="No-Break">.</span></p>
<p>Nomad adopts some of the architecture features mentioned with Kubernetes and Docker Swarm, where some nodes act as the control plane (servers) while others (clients) execute all the workloads. Servers accept jobs from users, manage clients, and determine the <span class="No-Break">workload placements.</span></p>
<p>HashiCorp provides a Community <a id="_idIndexMarker722"/>Edition and <strong class="bold">Software-as-a-Service</strong> (<strong class="bold">SaaS</strong>) platform in its cloud. It can be integrated via an API with some CI/CD environments and scripts for infrastructure automation can be included. Some of the advantages of this are <span class="No-Break">as follows:</span></p>
<ul>
<li>Simplicity in <a id="_idIndexMarker723"/>usage <span class="No-Break">and maintenance</span></li>
<li><span class="No-Break">Single-binary deployment</span></li>
<li>Flexibility to deploy and manage virtual machines, along with containerized and <span class="No-Break">non-containerized applications</span></li>
</ul>
<p>Here are some of <span class="No-Break">its limitations:</span></p>
<ul>
<li>Although HashiCorp <a id="_idIndexMarker724"/>provides good documentation, they don’t have many users and thus less of a community behind <span class="No-Break">the project.</span></li>
<li>Nomad appeared at the same time as Docker Swarm (a legacy platform) and Kubernetes were starting out, but Nomad initially focused on virtual machines and applications. Container orchestration became associated with Docker Swarm and Kubernetes, both of which gained popularity in this field as <span class="No-Break">a result.</span></li>
<li>With fewer associated projects, Nomad was left in the hands of a single company, which may have been more mature. This makes the evolution of the product or the<a id="_idIndexMarker725"/> addition of new features slower than <span class="No-Break">community-driven projects.</span></li>
</ul>
<h2 id="_idParaDest-127"><a id="_idTextAnchor144"/>Apache Mesos</h2>
<p><strong class="bold">Mesos</strong> is a project<a id="_idIndexMarker726"/> created by the Apache<a id="_idIndexMarker727"/> organization in 2009 for running cluster-wide workloads. This happened before containers became widely used and as a result, containers were only integrated into the project when most of the architecture’s logic had already been designed. This means that Mesos can run containers and normal application workloads cluster-wide, as <span class="No-Break">Nomad can.</span></p>
<p>Hadoop and other big data workload managers are the main frameworks that are managed by Apache Mesos, while its usage for containers is quite limited or at least less popular than the other solutions listed in this section. The benefit of Mesos is that integrating Apache projects’ workloads, such as those for Spark, Hadoop, and Kafka, is easy because it was designed <span class="No-Break">for them.</span></p>
<p>However, the disadvantages include <span class="No-Break">the following:</span></p>
<ul>
<li>Special packages <a id="_idIndexMarker728"/>for workloads or frameworks may require manual configuration and thus are not as standardized as Kubernetes or Docker Compose <span class="No-Break">YAML files</span></li>
<li>This orchestrator is not very popular and thus has a smaller community, with only a few tutorial<a id="_idIndexMarker729"/> examples available compared to Kubernetes or <span class="No-Break">Docker Swarm</span></li>
</ul>
<h2 id="_idParaDest-128"><a id="_idTextAnchor145"/>Cloud vendor-specific orchestration platforms</h2>
<p>Now that <a id="_idIndexMarker730"/>we have had a<a id="_idIndexMarker731"/> quick overview of the most important on-premises orchestrators (some of which are also available as cloud solutions), let’s examine some of the orchestration solutions created specifically by the cloud vendors for their own platforms. You can expect a degree of vendor lock-in when using their specific features. Let’s look at the most <span class="No-Break">important ones:</span></p>
<ul>
<li><strong class="bold">Amazon Elastic Container Service</strong> (<strong class="bold">ECS</strong>) and <strong class="bold">Fargate</strong>: Amazon ECS is an Amazon-managed<a id="_idIndexMarker732"/> container <a id="_idIndexMarker733"/>orchestration <a id="_idIndexMarker734"/>service. ECS relies on your EC2 contracted resources (storage, virtual networks, and load balancers) and as such, you can increase or decrease the hardware available for the platform by adding more resources or nodes. Amazon <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) is completely<a id="_idIndexMarker735"/> different from this simplified option as it deploys a complete Kubernetes cluster for you. On the other hand, AWS Fargate is a simpler technology that allows you to run containers without having to manage servers or clusters on the Amazon compute platform. You simply package your application in containers and specify the base operating system and the CPU and memory requirements. You will just need to configure a few networking settings and<a id="_idIndexMarker736"/> the <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) to secure your access. These are all the requirements so that you can finally run <span class="No-Break">your application.</span></li>
<li><strong class="bold">Google Anthos</strong> and <strong class="bold">Google Cloud Run</strong>: Although<a id="_idIndexMarker737"/> Google Cloud <a id="_idIndexMarker738"/>Platform offers its own Kubernetes-as-a-Service <a id="_idIndexMarker739"/>platform, <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), it also<a id="_idIndexMarker740"/> provides Anthos and Cloud Run. Anthos is a hybrid and cloud-agnostic container management platform that allows the integration of applications running in containers in Google cloud and on your data center. It focuses on preparing and running applications in containers used as virtual machines. On the other hand, Google Cloud Run is more flexible, offering the ability to scale workloads on demand and integrate CI/CD tools and different <span class="No-Break">container runtimes.</span></li>
<li><strong class="bold">Azure Service Fabric</strong> and <strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>): Microsoft provides<a id="_idIndexMarker741"/> different<a id="_idIndexMarker742"/> solutions for running simple containers while <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) is also<a id="_idIndexMarker743"/> available. Azure Service Fabric provides a complete microservice-ready platform for your applications, while ACI is a simplified version in which you run containers as if they were small virtual machines. You simply <a id="_idIndexMarker744"/>code and build your container images for your applications instead of managing the infrastructure that <span class="No-Break">runs them.</span></li>
</ul>
<p>All these cloud platforms allow you to test and even run your applications in production without having to manage any of the underlying orchestration. Just using a simple web UI, you determine what should be done when your application’s components fail and can add as many resources as needed. The cloud storage services available from each provider can be used by your application and you can monitor the complete cost of your application thanks to the reports available for your cloud platform. Here are some of the pros of these <a id="_idIndexMarker745"/><span class="No-Break">cloud-vendor</span><span class="No-Break"><a id="_idIndexMarker746"/></span><span class="No-Break"> platforms:</span></p>
<ul>
<li>Easier to use than any other <span class="No-Break">container orchestration</span></li>
<li>A full Kubernetes cluster may not be necessary for testing or even executing your applications, making cloud-vendor solutions a <span class="No-Break">reasonable choice</span></li>
</ul>
<p>Their disadvantages include <span class="No-Break">the following:</span></p>
<ul>
<li>Vendor lock-in is <a id="_idIndexMarker747"/>always present with these platforms as you will use many cloud <span class="No-Break">vendor-embedded services</span></li>
<li>They can be used effectively for testing or even publishing some simple applications, but when using microservices, some issues may arise, depending on the complexity <a id="_idIndexMarker748"/>of your <span class="No-Break">application’s components</span></li>
</ul>
<h1 id="_idParaDest-129"><a id="_idTextAnchor146"/>Summary</h1>
<p>In this chapter, we reviewed the common orchestration concepts we will employ in our applications and the different platforms available. We learned about the most important features on offer to help us decide which options are more suited for our application. Learning how orchestrators work will be a big boon to you, as a developer, in designing your applications to run cluster-wide and with high availability in production, thanks to the unique features that orchestration offers. In the following chapters, we will delve deeper into Kubernetes, which is the most popular and widely extended container orchestration platform, and Docker Swarm, both of which are available in the cloud <span class="No-Break">and on-premises.</span></p>
</div>
</div></body></html>