<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Workflow</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll discuss the workflow in Puppet. We'll cover what makes a good technical workflow, how to apply that to Puppet, and how to use the <strong>Puppet Development Kit</strong> (<strong>PDK</strong>) to improve our workflow. We'll investigate the following qualities of a good workflow: ease of use, rapid feedback, ease of onboarding, and quality control. We'll use Puppet Git repositories to provide a basic Puppet workflow that can be tuned to any system of management. We'll also explore the new PDK released by Puppet, which can improve our workflow.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Puppet workflow</li>
<li>Designing a Puppet workflow</li>
<li>Using the PDK</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Puppet workflow</h1>
                </header>
            
            <article>
                
<p>A workflow is a series of processes that work flows through, from initiation to completion. As the Puppet environments become more complex in an organization, a trusted and shared workflow will make sharing work easier. A Puppet workflow should allow us to access code, edit code, test our code, and, eventually, deploy our code back to the Puppet Master. Although it is not required, it is highly recommended that an organization or group of workers adopt a shared workflow. A shared workflow possesses a few main benefits, as follows:</p>
<ul>
<li>A measurable ease of use</li>
<li>Rapid feedback</li>
<li>Ease of onboarding</li>
<li>Quality control</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ease of use</h1>
                </header>
            
            <article>
                
<p>The primary reason to design and begin a workflow is to provide for ease of use. A team should design a workflow around their code base, allowing them to understand how to retrieve specific code, how to edit that code, and the impacts of the new edits. A workflow also provides a standardized way of packaging the code, to be delivered and used by the existing code base. Each step in the workflow should be clear, concise, communicated, and repeatable. It is important that everyone on the team understands not only how the workflow works, but why each step of the workflow exists, so that they can troubleshoot and contribute to the workflow, should something change in the organization.</p>
<p>One of the primary benefits of a shared workflow, as opposed to individualized workflows, is the ability to measure the impact of the workflow on the organization. To measure our workflow, we first separate standard and nonstandard units of work. The edits that we make to our code often vary in size and complexity, and are not easy to measure in standard units. On the other hand, code is generally checked out, tested, and deployed in the same way every time, leaving us with a good estimate of how long it will take to go through our workflow, minus the code edits. </p>
<p>If our workflow takes about 30 seconds to clone the code repository, an unknown amount of time to edit code, 5 minutes to run a test, and another 30 seconds to deploy the code in our environment, our workflow, with a single test, will take about 6 minutes. If we have eight members of our team, who each run through this workflow 10 times a day on average, our workflow actually constitutes about 8 hours a week of our combined work (<em>8 x 10 x 6</em> = <em>480</em> minutes, or 8 hours). Cutting this testing time in half reduces our total time as a team spent on the workflow by about 3 1/3 hours per week. Because of this measurable amount of time that can be saved in a workflow, a team should consider optimizing their workflow whenever possible.</p>
<p>Generally, you won't need more than a rough estimate of the time it takes to perform the standard functions of the workflow, but you will need to know which pieces might be performed more than once. With Puppet, a user will likely write, push, and test code more than they will pull it down. You can inspect each piece of the workflow separately and seek to improve a part of the process, but you should consider the ramifications of a change to the rest of the workflow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rapid feedback</h1>
                </header>
            
            <article>
                
<p>A good workflow should provide constant feedback to its users. Each step should be clearly defined, with strict pass or fail criteria. For example, Git will warn a user whenever it detects a problem, such as being unable to pull code or push code back to the origin repository. We can extend this with Git commit hooks, both server-side and client-side, which perform checks to ensure that the code is in a proper state before being accepted into an organizational Git server from the local repository. Running Puppet itself within our test criteria, we expect clean and idempotent runs. The Puppet catalog should not produce failing resources, nor should it manage the same resource with every Puppet run.</p>
<p>The time it takes to solve problems with Puppet shrinks as more feedback is provided by a workflow to the engineer. If you work in a workflow that requires pushing code to an environment on the Puppet Master, and you are testing on a true agent, a simple run of <kbd>puppet parser validate</kbd> can save a lot of time. The parser validation will quickly tell you if Puppet code can be compiled, rather than what it will do. This simple command can reduce the number of times that we <kbd>git commit</kbd> on the code, push it to the Git repository, deploy it to an environment, log in to the test machine, and wait for the Puppet agent to trigger a catalog error. We can even ensure that this command is run before every commit with a precommit Git hook. Automated testing tools, such as RSpec and Beaker, can extend this methodology, and, combined with a CI/CD pipeline (discussed in the next chapter), can provide even more rapid feedback to code developers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ease of onboarding</h1>
                </header>
            
            <article>
                
<p>A well-built workflow naturally facilitates the ease of adding new members to a project, whether open source or a part of an organization. A simple tool suite and guide can be invaluable to those new members, and can help them to get over the hurdle of the first commit. Even a simple getting started <kbd>README</kbd> can go a long way, if properly maintained. Onboarding new members to a project is costly, and quality workflow can minimize the time spent by the new member. Bringing on new project members also requires some information and time from existing project members. If your project is an ongoing development effort, it's highly likely that you'll have some turnover, and saving time for existing members while shortening the time for new members to reach effectiveness should be a priority in your workflow.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quality control</h1>
                </header>
            
            <article>
                
<p>A good workflow should always seek to reduce mistakes and increase code quality. Every built-in safety mechanism in a workflow allows a team to iterate over more complex features more quickly. Simple things, such as preventing pushes directly to production branches and basing production environments on semantically versioned code, allow for rapid development, without any worries about toppling critical infrastructure. </p>
<p>The following lists a few examples of workflow improvements designed around security and stability:</p>
<ul>
<li>Preventing direct code pushes to production on the control repository</li>
<li>Preventing direct code pushes to masters on individual modules</li>
<li>Running Puppet parser validation on all manifests prior to a push back to the  repository of <span>origin</span></li>
<li>Running code reviews prior to merging into a master or <kbd>production-like</kbd> branches of the control repository</li>
<li>Automated testing</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing a Puppet workflow</h1>
                </header>
            
            <article>
                
<p>Puppet has undergone a lot of changes in code management since its beginnings. Even the general workflow has changed drastically. This section will help you to understand some of the history of code management in Puppet, some of the challenges, and, most importantly, some of the solutions for designing and working with a strong Puppet workflow.</p>
<p>Originally, we wrote Puppet manifests directly to the disk. We logged on to the Puppet Master via SSH and edited our manifests directly, treating most of our code like configuration files for remote machines. This model required custom backups and recovery for code applied to agents, and did not provide easy rollbacks. If something went wrong in a deployment, you were forced to take snippets of code from a backup <span>manually</span> and deploy it to a system. Some members of the community took to storing their Puppet code in Git. As the number of individual repositories grew in organizations, manually bringing in Git repositories individually became more troublesome, and some community open source projects formed that were focused on staging Git code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Components of the Puppet workflow</h1>
                </header>
            
            <article>
                
<p>Although r10k is not the only Puppet Code Manager, it has become the standard Code Manager deployed to enterprise organizations. We'll break the work down into tasks and repositories, as follows:</p>
<ul>
<li>Repositories:
<ul>
<li>Control repository</li>
<li>Module repositories</li>
</ul>
</li>
<li>Tasks:
<ul>
<li>Clone</li>
<li>Create new branch</li>
<li>Edit relevant code</li>
<li>Add and commit</li>
<li>Push</li>
<li>Puppet login and deploy</li>
<li>Classify</li>
<li>Test (automatic or manual)</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Repositories</h1>
                </header>
            
            <article>
                
<p>Code management requires that all code be stored in Git. Splitting your code up into multiple repositories and placing the code on the master allows for references to different versions of code. Each of your modules should reside in a separate repository, allowing for versioning and governance on a per-module basis. The <kbd>Puppetfile</kbd> will call these repositories by using the Puppet Forge, or pointers at your own local Git instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Control repository</h1>
                </header>
            
            <article>
                
<p>Our control repository, as described in the previous chapter, is nothing more than a Git code repository. The only unique quality that you need to keep in mind when working with it, is that branch names correspond to Puppet environments. If you create a Git branch named <kbd>feature</kbd> and deploy the code, the Puppet Master will deploy that code to <kbd>/etc/puppetlabs/code/environments/feature</kbd>. Generally, the Master branch is replaced with another protected branch named <kbd>production</kbd> in the control repository, so that agents can check in to a production branch by default.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Module repository</h1>
                </header>
            
            <article>
                
<p>Module repositories are standard Git repositories. Generally, we want to protect the master branch and keep it from receiving direct commits. Contributors to component modules should instead submit pull requests to the repository and allow for a code review before accepting the code into the master branch. The master branch should be a functional version of the module at all times, although it need not be a version ready to be deployed into production. Treating the master as stable code allows non-production environments to point reliably at the master branch of all repositories, to get the latest accepted code during development. When it comes to deploying to production, we'll actually use a Git tag to create a version, such as 1.2.0. We can then deploy our latest code into non-production and formally accept code into production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tasks</h1>
                </header>
            
            <article>
                
<p>The primary driver of the workflow in a Code Manager or r10k-based system is a Git workflow. There are multiple models of Git workflows, such as GitHub flow and Git flow, but the primary focus of this book isn't on Git, so we'll start with a minimal set of commands and procedures. The most effective way to get started is to work on the temporary environments provided by our control repository. In this workflow, we assume that a Git solution is already implemented on-site, or is provided by a managed service provider, and the Puppet Master is using Code Manager to deploy environments.</p>
<p>The first step of the workflow is to identify the components that need to change. In this workflow example, we'll assume that we're performing a change on a component module and a profile embedded in the control repository. We'll include remediation steps during the manual test phase, to include new code deployments and new pushes to the Git repository. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clone and edit the component repositories</h1>
                </header>
            
            <article>
                
<p><span>First, we'll clone the component module, change to a new feature branch, and perform edits on the files in the repository. We'll ensure that we use a Git branch during development, so that we can send our code to the upstream Git repository without impacting the original code. We'll end this step with a new snapshot of code on a separate branch of an existing module, so that we can test this code in isolation. This set of steps is the general workflow for the following:</span></p>
<ol>
<li>Making a copy of the upstream repository for an individual module (<kbd>git clone</kbd>/<kbd>pull</kbd>)</li>
<li>Creating a branch of the module, separate from the Master (<kbd>git checkout</kbd>)</li>
<li>Making any and all edits to the code (IDE of choice)</li>
<li>Creating a snapshot of the current state of the code (<kbd>git add</kbd> and <kbd>commit</kbd>)</li>
<li>Sending the snapshot back to the upstream repository (<kbd>git push</kbd>)</li>
</ol>
<p><span>In action, the code is as follows:</span></p>
<pre><strong># Clone the remote git repository for the module. You can skip this step if the</strong><br/><strong># repository is already present on your local system</strong><br/><strong>git clone git@gitserver.com:puppet/module.git</strong><br/><br/><strong># If the repository is already local on the system, we'll just want to update our</strong><br/><strong># local master branch</strong><br/><strong>git pull origin master</strong><br/><br/><strong># Check out a new environment based on the existing master branch, which is the</strong><br/><strong># default branch of a git repository, and the branch we should start on on a clone.</strong><br/><strong>git checkout -b new_feature</strong><br/><br/><strong># We'll edit some files to add new features</strong><br/><br/><strong># Adding new paramters to init</strong><br/><strong>vim manifests/init.pp - Adding new parameters to init</strong><br/><strong># Adding a new feature to config</strong><br/><strong>vim manifests/config.pp</strong><br/><strong># Ensuring the new feature is represented in the deployed template</strong><br/><strong>vim templates/file.epp</strong><br/><br/><strong># Add all edited files to git staging, assuming you're at the base of the repository</strong><br/><strong>git add .</strong><br/><br/><strong># Add an atomic commit, not only describing what the commit is, but why it was done</strong><br/><strong>git commit -m 'Added new code to support feature requested by client'</strong><br/><br/><strong># Push this code back to the origin repository as the new branch</strong><br/><strong>git push origin new_feature</strong></pre>
<p>Our edits are now in the upstream repository, in a <kbd>new_feature</kbd> branch. The master branch will continue to serve as a reference point for further development for others, and for testing in a staging environment. So that we can begin to test this code, we'll create a new Puppet environment, designed specifically for testing and iteration over this code set.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloning the control repository</h1>
                </header>
            
            <article>
                
<p>The first step starts like the last one: cloning the Git repository. One thing to remember about Puppet environments is that a branch of this repository corresponds to a Puppet environment. Most users of Puppet don't have a master environment, but rather, the production environment that Puppet places nodes into by default. If your organization has any environments prior to production, as many do, you'll want to make sure that you begin on the existing branch before creating a new branch. The <kbd>git checkout -b</kbd> command creates a new branch, starting from the branch that you are currently on. The following are the steps for creating a new environment, modeled after an existing environment:</p>
<ol>
<li>Make a copy of the control repository from the upstream repository (<kbd>git clone</kbd>).</li>
<li>Check out the environment that you want to write new code against (<kbd>git checkout</kbd>).</li>
<li>Check out a new branch, based on the current branch (<kbd>git checkout -b</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong># This step is not needed if the repository is already on the local file system</strong><br/><strong>git clone git@gitserver.com:puppet/control-repo.git</strong><br/><br/><strong># We'll assume integration is the pre-production branch used by the organization</strong><br/><strong># to stage changes before moving into production-like branches</strong><br/><strong># Remember, there usually is no master branch in a control repository, so we want</strong><br/><strong># to target a specific branch to work against.</strong><br/><strong>git checkout integration</strong><br/><br/><strong># If this repo has been freshly cloned, git pull shouldn't provide any new updates,</strong><br/><strong># but it's safe to run either way. If the repository has already been cloned in the</strong><br/><strong># past, you definitely want to run this command to pull the latest commits from </strong><br/><strong># upstream.</strong><br/><strong>git pull origin integration</strong><br/><br/><strong># We'll perform a second checkout, with the -b flag to indicate a new branch based on the existing branch</strong><br/><strong>git checkout -b new_feature</strong><br/><br/></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Like the steps we took for our component module repository, this set of commands ensures that we have a local copy of the repository with the latest commits to the integration branch, and that we started a new branch based on the existing code. We're in a state to edit files found directly in our control repository, such as the <kbd>Puppetfile</kbd>, <kbd>hieradata</kbd>, and embedded <kbd>roles</kbd> and <kbd>profiles</kbd> (if you keep them in the control repository, rather than as separate, individual repositories). Once we have the code, we will want to edit the relevant files, create a new commit, push the code back to the origin repository, and deploy the environment. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Editing the control repository</h1>
                </header>
            
            <article>
                
<p>Once we're inside of the local copy of the intended environment, it will be the right time to make changes to the code. We generally spawn these additional short-lived environments so that simple commands can be used to deploy new code. We have a few files to target, because we think of the control repository as a configuration file for the rest of the environment. The <kbd>Puppetfile</kbd> is used to manage dependencies, including any component modules (from the Forge or your own environment). <kbd>roles</kbd> and <kbd>profiles</kbd> are often kept in the control repository, as well, and code can be edited directly in these environments. The workflow for making changes in the control repository is as follows:</p>
<ol>
<li>Edit the files (in the IDE of your choice).</li>
<li>Make a snapshot of the current state of the code (<kbd>git add</kbd> and <kbd>commit</kbd>).</li>
<li>Send the environment back to the remote repository (<kbd>git push</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong># Edit our files</strong><br/><br/><strong># Change the branch of the component module to new_feature</strong><br/><strong>vim Puppetfile</strong><br/><br/><strong>mod 'module',</strong><br/><strong>  git =&gt; git@gitserver.com:puppet/module.git,</strong><br/><strong>  branch =&gt; 'new_feature'</strong><br/><br/><strong># Make a change in the profile that utilizes the component modules</strong><br/><strong>vim site/profiles/manifests/baseline.pp</strong><br/><br/><strong># Add our new changes, to be staged for a commit</strong><br/><strong>git add .</strong><br/><br/><strong># Commit our changes</strong><br/><strong>git commit -m 'Supporting new Feature to support &lt;effort&gt;'</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre style="padding-left: 60px"><br/><strong># Push our code back to the control repository as a new branch intended to be</strong><br/><strong># realized as a new environment on the Puppet Master</strong><br/><strong>git push origin new_feature</strong></pre>
<p>At this point, we've edited a module and files in the control repository and pushed them back to the origin. We'll now deploy the branch we made in the preceding code, and we will tweak our profile to use the module changes. Unless you have set up Git hooks or a CI/CD solution, you'll also have to trigger an environment deployment on the Puppet Master.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the new environment on the Puppet Master</h1>
                </header>
            
            <article>
                
<p>Puppet provides the PE Client Tools, as described in <a href="6c818865-77fb-4527-a3a5-922db1301217.xhtml">Chapter 5</a>, <em>Managing Code</em>, specifically for deploying code. If these tools are not available on your workstation, you can also log in to the Puppet Master, where they are already available for use. Assuming that you are using Code Manager, the following steps remain the same whether you are on a local workstation or a remote server:</p>
<ol>
<li>Retrieve the login token from Puppet Enterprise (<kbd>puppet-access login</kbd>).</li>
<li>Deploy an environment from the upstream repository branch (<kbd>puppet-code deploy</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong># If PE Client Tools are not installed locally, the Puppet Master comes with them</strong><br/><strong># installed by default. We'll assume that the PE client tools are not already</strong><br/><strong># installed and log in to the Puppet Master</strong><br/><strong>ssh user@puppet.org.net</strong><br/><br/><strong># Generate an authorization token to allow your PE Console user to deploy code</strong><br/><strong>puppet-access login</strong><br/><br/><strong># Use our access token to deploy our new environment. Notice the -w flag, which</strong><br/><strong># triggers the client tools to wait and give you a pass or fail message on the</strong><br/><strong># status of the deployment.</strong><br/><strong>puppet-code deploy new_feature -w</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now our code has been deployed as a fresh environment on the Puppet Master. We're still missing a step to classify our test system and ensure that it is placed in the proper environment. For a Puppet Enterprise user, you can both classify and declare an environment by using a node classifier group in the PE console. To create a new node group, select an environment, check the environment group box, name it, and click <span class="packt_screen">Create</span>. Enter your new environment group, pin your test node to the group, and add any relevant classes to the classification page.</p>
<p>You can also classify via <kbd>manifests/site.pp</kbd> in the control repository, as follows:</p>
<pre>node 'test.node' {<br/>  include relevant_role_or_profile<br/>  include new_feature<br/>}</pre>
<p>The code for classification via Hiera is as follows:</p>
<pre># data/host/test.node.yaml<br/>---<br/>classification:<br/>  - relevant_role_or_profile<br/>  - new_feature<br/><br/># manifests/site.pp<br/><br/># Notice the lack of a node group around the include statement<br/>include $::classification</pre>
<p>There are multiple ways to classify that are commonly used by Puppet users, but without automated testing, we'll have to do some classification and run the agent to check the results of our tests. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the changes</h1>
                </header>
            
            <article>
                
<p>After your test node is properly attached to the environment group, you can log in to the node and trigger an agent run with puppet <kbd>agent -t</kbd>. Alternatively, you can run the Puppet agent through the PE console and read the log there. If you don't see any changes, there are a few possible reasons, as follows:</p>
<ul>
<li>The agent has already run, between when you classified the node in the console and ran the Puppet agent.</li>
<li>A step was missed and the code was not properly deployed.</li>
<li>Your code does not trigger any new changes on the system, and you should modify the system to see if Puppet corrects the change.</li>
</ul>
<p>Ensure that you check the resources targeted by your change to see whether the agent has already deployed the new changes. You might also want to verify that the code deployment was done properly, and that you pushed your code back to the Git repository. If your code does not trigger any changes on the system, or if it triggers undesired changes, you can perform the following shorter workflow until the code is resolved properly:</p>
<ol>
<li>Edit the code in the target repository: the control repository or the module repository (with the IDE of choice)</li>
<li>Make a snapshot of the code (<kbd>git add</kbd> and <kbd>commit</kbd>)</li>
<li>Push the code back to the remote repository (<kbd>git push</kbd>)</li>
<li>Redeploy the environment (<kbd>puppet-access login</kbd> and <kbd>puppet-code deploy</kbd>)</li>
<li>Trigger an agent run on the test machine (<kbd>puppet agent</kbd> or PE console)</li>
<li>Check for changes on the target system</li>
<li>Repeat until the desired state is achieved:</li>
</ol>
<pre style="padding-left: 60px"><strong># Start in the repository with the change. This could be a component module</strong><br/><strong># or the control repository. We're assuming each repository is still on the</strong><br/><strong># branch from the last step, and no pulls or branch changes are necessary.</strong><br/><br/><strong># Edit the file with the targeted changes</strong><br/><strong>vim manifests/manifest.pp</strong><br/><br/><strong># Add the file to the git staging area</strong><br/><strong>git add manifests/manifest.pp</strong><br/><br/><strong># Commit the file to the repository</strong><br/><strong>git commit -m 'Fixing specific bug'</strong><br/><br/><strong># Push the repository back to upstream origin</strong><br/><strong>git push origin new_feature</strong><br/><br/><strong># From the Puppet Master, or a workstation with PE Client Tools</strong><br/><br/><strong># Log in with RBAC</strong><br/><strong>puppet-access login</strong><br/><br/><strong># Deploy the environment</strong><br/><strong>puppet-code deploy new_feature -w</strong><br/><br/><strong># On the test node</strong><br/><br/><strong># Run the agent, observe the results</strong><br/><strong>puppet agent -t</strong><br/><br/><strong># Repeat as necessary until issues are solved</strong></pre>
<p>Once our code is in the desired state, we will be ready to begin placing it back into a long-lived environment on the Puppet Master. Modules should have their code merged back to the master, and changes to the control repository will need to be merged with a longer lived branch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Merging branches</h1>
                </header>
            
            <article>
                
<p>In our earlier steps, we isolated our working code into a feature-branch and a short-lived, non-production environment. While teams and organizations should select some merging safeguards and strategies, such as peer code reviews and automated testing, this section will focus on the steps required to merge branches into master or long-lived branches in Puppet. Enterprise and open source web-based Git solutions usually contain some extra controls to indicate who can merge into a repository, and to which branches. The general best practice is to allow for a peer review of code, and the reviewer can accept the code into the long-lived branch or master branch. Merging our code via the command line is a simple process, as follows:</p>
<ol>
<li>Switch to the branch that you want to merge to (<kbd>git checkout</kbd>)</li>
<li>Merge another branch into this one (<kbd>git merge</kbd>)</li>
<li>Push the merged branch into upstream repository (<kbd>git push</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong># Many Enterprise-focused git repositories have built in merge features, that ar</strong><br/><strong># likely more robust and easier to use than a simple git merge. If you have an in</strong><br/><strong># house git solution, follow the program documentation on a merge request</strong><br/><br/><strong># On Module</strong><br/><strong># We'll change to target branch, in this case master</strong><br/><strong>$ git checkout master</strong><br/><br/><strong>$ git merge feature_branch</strong><br/><strong>Updating 0b3d899..227a02e</strong><br/><strong>Fast-forward</strong><br/><strong> README.md | 1 +</strong><br/><strong> 1 file changed, 1 insertion(+)</strong><br/><strong> create mode 100644 README.md</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre style="padding-left: 60px"><br/><strong># Push the branch to upstream repository so Puppet can find it.</strong><br/><strong>$ git push origin master</strong></pre>
<p>Merging in the control repository can sometimes be troublesome, due to the <kbd>Puppetfile</kbd> being (intentionally) different between versions. Our <kbd>production-like</kbd> branches should use Git tags to declare the intended version of the code to be deployed and promoted up the series of environments. Our <kbd>non-production-like</kbd> environments are generally pointed to the master branch of each module, providing the latest accepted stable code to the environment for testing and development. Merging is performed in the same way as with a component module; just ensure that you don't overwrite the <kbd>Puppetfile</kbd> on a <kbd>production-like</kbd> branch with the less controlled <kbd>Puppetfile</kbd> in a <kbd>non-production-like</kbd> branch. Production branches should refer to Git tags for deploying code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Git tags and versioning</h1>
                </header>
            
            <article>
                
<p>Git tags are used to create a permanent state of code and separate it from the existing branches. Tags are not intended to be iterated upon, but rather, should be used as a marker in time for the state of the code. This makes tags a perfect fit for the release versioning of Puppet code. We can create tags from any branch, but the master is the most common branch to cut release tags from. We can simply use the <kbd>git tag</kbd> command on our module repository to create a snapshot with a semantic version number and push it to the origin repository, to be called on by r10k or Code Manager. The workflow for a Git tag is also short, as follows:</p>
<ol>
<li>Check out the target branch (usually the master) for the tag (<kbd>git checkout</kbd>)</li>
<li>Version the code (<kbd>git tag</kbd>)</li>
<li>Push the tag to the remote repository (<kbd>git push</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong>rary at Ryans-MBP in ~/workspace/packt/module (master)</strong><br/><strong>$ git tag 'v1.4'</strong><br/><br/><strong>$ git tag -l</strong><br/><strong>v1.4</strong><br/><br/><strong>$ git push origin v1.4</strong></pre>
<p>After our module has been properly versioned, we can edit the <kbd>production-like</kbd> Puppetfile to utilize our tag, rather than point to a particular development or master branch:</p>
<pre><strong># Production-like branch, tagged with a solid version number</strong><br/><strong>forge https://forge.puppetlabs.com</strong><br/><br/><strong>mod 'module',</strong><br/><strong>  git =&gt; 'git@gitserver.com:puppet/module.git',</strong><br/><strong>  tag =&gt; 'v1.4'</strong></pre>
<p>This is a simple version of the Puppet workflow, but it still leaves room for improvement. Puppet recently released a tool called the PDK, to help facilitate quality Puppet tooling into your workflow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the PDK</h1>
                </header>
            
            <article>
                
<p>A good workflow should provide ease of use, rapid feedback, ease of onboarding, and quality control. The PDK aims to increase productivity across this space. Many tools in the PDK have existed for quite some time, but they were often difficult to use and configure for workstation development.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PDK</h1>
                </header>
            
            <article>
                
<p>Puppet makes the PDK freely available on their website, and it has a release for each major operating system. It uses a fully isolated environment to provide Puppet binaries and RubyGems that make development much simpler. Tools included in the PDK, as of version 1.5.0, are as follows:</p>
<ul>
<li>Create new Puppet artifacts:
<ul>
<li>Modules</li>
<li>Classes</li>
<li>Defined types</li>
<li>Tasks</li>
<li>Puppet Ruby providers</li>
</ul>
</li>
<li>PDK validate—simple health checks:
<ul>
<li>Puppet parser validate (Puppet syntax)</li>
<li>Puppet lint (Puppet style)</li>
<li>Puppet metadata syntax</li>
<li>Puppet metadata style</li>
<li>RuboCop (Ruby style)</li>
</ul>
</li>
<li>PDK test unit (Puppet RSpec—unit testing)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating new Puppet artifacts</h1>
                </header>
            
            <article>
                
<p>The PDK allows users to create new artifacts, using best practices. Each <kbd>pdk new</kbd> command builds an artifact already structured for Puppet. These artifacts are intended to conform to Puppet's best practices. If you're testing the PDK in an isolated environment for the first time, starting with a new module is the easiest method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The pdk new command</h1>
                </header>
            
            <article>
                
<p>The command <kbd>pdk new module</kbd> brings the user to a prompt, requesting that the user specify the Puppet Forge username, the author's full name, the module license, and the supported operating systems. If you do not have a Forge username or a module license, you can enter in any value. After the prompt, you'll find a new directory that contains code. If you want to send this code to an upstream repository, follow these steps on the command line:</p>
<pre><strong># From directory pdk new module was run in, enter the module, create a</strong><br/><strong># git repository and add all files to staging</strong><br/><strong>$ cd module</strong><br/><strong>$ git init</strong><br/><strong>$ git add .</strong><br/><br/><strong># Initial Commit is a good common message as a starting point</strong><br/><strong>$ git commit -m 'Initial Commit'</strong><br/><br/><strong># Add the upstream remote</strong><br/><strong>$ git remote add origin git@gitserver.com:puppet/module.git</strong><br/><br/><strong># Push to master and begin regular module development workflow</strong><br/><strong>$ git push origin master</strong></pre>
<p>If you're working with a previously created module, you can use the <kbd>pdk convert</kbd> command to place any items missing from the template into the existing module. By default, the PDK deploys the templates found at <a href="https://github.com/puppetlabs/pdk-templates">https://github.com/puppetlabs/pdk-templates</a>. If you need to change any of the files found here, you can clone a copy of <kbd>pdk-templates</kbd> from the official repository and send it to a central Git repository. You'll need to use the <kbd>pdk convert --template-url &lt;https&gt; </kbd> to select the new template and deploy it to the existing module. The <kbd>--template-url flag </kbd> command will also set the new URL as the default URL on the workstation.</p>
<p>You should feel free to make your own copy of this template, as the one provided by Puppet is fairly extensive and rather opinionated. It even includes some ways to get started with CI/CD systems, such as <kbd>gitlab-ci</kbd>. Trim the files for systems that you don't use, and make sure that everything provided by the template makes sense for your organization.</p>
<p class="mce-root"/>
<p>The template repository provides three directories and configuration files to the PDK, as follows:</p>
<ul>
<li><kbd>moduleroot</kbd>: The Ruby templates in this directory will be placed on top of existing files. This is useful when you want to enforce a particular file, like a CI/CD pipeline.</li>
<li><kbd>moduleroot_init</kbd>: The Ruby templates in this directory will not override existing files. This is great for starter files, like module templates.</li>
<li><kbd>object_templates</kbd>: The Ruby templates that determine the output of the file on commands like <kbd>pdk new class</kbd>.</li>
<li><kbd>config_defaults.yaml</kbd>: This provides defaults and variables to be used for all Ruby templates in the PDK template.</li>
</ul>
<p>Once you have your new module template, you can begin to create manifests inside of the module for Puppet code with the PDK. From inside of the new module, we can use <kbd>pdk new class</kbd> to begin making manifests. The command creates manifests according to an autoload layout, so running <kbd>pdk new class server::main</kbd> would create a file at <kbd>manifests/server/main.pp</kbd>. The class created with the default template will start as an empty, non-parameterized class, with Puppet string-style documentation at the top of the file. The <kbd>pdk new defined_type </kbd> command will make a similar file, but will use the defined declaration instead of the class declaration:</p>
<pre><strong>$ pdk new class config</strong><br/><strong>pdk (INFO): Creating '/Users/rary/workspace/packt/module/manifests/config.pp' from template.</strong><br/><strong>pdk (INFO): Creating '/Users/rary/workspace/packt/module/spec/classes/config_spec.rb' from template</strong><br/><br/><strong># Sample with folders</strong><br/><strong>$ pdk new class server::main</strong><br/><strong>pdk (INFO): Creating '/Users/rary/workspace/packt/module/manifests/server/main.pp' from template.</strong><br/><strong>pdk (INFO): Creating '/Users/rary/workspace/packt/module/spec/classes/server/main_spec.rb' from template.</strong></pre>
<p>The <kbd>pdk new task</kbd> command will create files in the <kbd>tasks</kbd> directory, based on the template for use with Puppet tasks. Puppet tasks are a way to automate ad hoc scripts and commands across your infrastructure, using Puppet. <kbd>pdk new provider</kbd> is an experimental feature for designing new custom Ruby providers to Puppet. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Once the new objects are created and developed against, the PDK will also provide a tool suite for syntax and style, with <kbd>pdk validate</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The pdk validate command</h1>
                </header>
            
            <article>
                
<p>The PDK provides <kbd>pdk validate</kbd> to check both syntax and style. Syntax checks make sure that your code can compile, and that you're not missing things such as commas or closing braces in manifests or JSON metadata. Syntax checks can also be performed manually on manifests with <kbd>puppet parser validate</kbd>. Style checking looks at the code to make sure that it adheres to a standard style guide. Puppet-lint is used to provide style checks to Puppet, and all of the rules can be found at <a href="http://puppet-lint.com/">http://puppet-lint.com/</a>. When a module is healthy, the PDK will return check marks against all tasks:</p>
<pre><strong>$ pdk validate</strong><br/><strong>pdk (INFO): Running all available validators...</strong><br/><strong>pdk (INFO): Using Ruby 2.4.4</strong><br/><strong>pdk (INFO): Using Puppet 5.5.1<br/></strong><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking metadata syntax (metadata.json tasks/*.json).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking module metadata style (metadata.json).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking task metadata style (tasks/*.json).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Puppet manifest syntax (**/**.pp).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Puppet manifest style (**/*.pp).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Ruby code style (**/**.rb).</strong><br/><br/></pre>
<p>An invalid <kbd>metadata.json</kbd> will prevent the uploading of modules to the Forge and the running of RSpec tests. This file details the author of the module, and other information, such as dependencies and supported operating systems:</p>
<pre><strong>#Invalid Metadata.json</strong><br/><br/><strong>$ pdk validate</strong><br/><strong>/opt/puppetlabs/pdk/private/ruby/2.4.4/lib/ruby/gems/2.4.0/gems/pdk-1.5.0/lib/pdk/module/metadata.rb:142:in `validate_name': Invalid 'name' field in metadata.json: Field must be a dash-separated user name and module name. (ArgumentError)</strong></pre>
<p><kbd>pdk validate</kbd> also runs Puppet parser validation across every manifest in the module. In the following example, a curly brace was forgotten at the end of <kbd>init.pp</kbd>, and the PDK is informing us that the code will not compile:</p>
<pre><strong># Failed Parser Validation</strong><br/><strong># Can be ran alone with puppet parser validate</strong><br/><br/><strong>$ pdk validate</strong><br/><strong>pdk (INFO): Running all available validators...</strong><br/><strong>pdk (INFO): Using Ruby 2.4.4</strong><br/><strong>pdk (INFO): Using Puppet 5.5.1</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking metadata syntax (metadata.json tasks/*.json).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking module metadata style (metadata.json).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Puppet manifest syntax (**/**.pp).</strong><br/><strong>[</strong><strong><img class="inline-image" src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Ruby code style (**/**.rb).</strong><br/><strong>info: task-metadata-lint: ./: Target does not contain any files to validate (tasks/*.json).</strong><br/><strong>Error: puppet-syntax: manifests/init.pp:9:1: Could not parse for environment production: Syntax error at '}'</strong></pre>
<p>If the Puppet parser validation passes, <kbd>puppet-lint</kbd> will run on all manifests. It will print out errors and warnings in the code, based on the Puppet Style Guide. In the following example, we run pdk validate against a manifest has a line that continues beyond 140 characters on line 10 and trailing whitespace after line 9:</p>
<pre><strong>$ pdk validate</strong><br/><strong>pdk (INFO): Running all available validators...</strong><br/><strong>pdk (INFO): Using Ruby 2.4.4</strong><br/><strong>pdk (INFO): Using Puppet 5.5.1</strong><br/><strong>[<img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/>] Checking metadata syntax (metadata.json tasks/*.json).</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking module metadata style (metadata.json).<br/></strong><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>]</strong><strong>Checking Puppet manifest syntax (**/**.pp).</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Puppet manifest style (**/*.pp).</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Ruby code style (**/**.rb).</strong><br/><strong>info: task-metadata-lint: ./: Target does not contain any files to validate (tasks/*.json).</strong><br/><strong>warning: puppet-lint: manifests/init.pp:10:140: line has more than 140 characters</strong><br/><strong>error: puppet-lint: manifests/init.pp:9:28: trailing whitespace found</strong></pre>
<p>In some cases, rather than print out a warning or error, we want to disable it. A list of checks can be found at <a href="http://puppet-lint.com/checks/">http://puppet-lint.com/checks/</a>, and can be used to disable individual checks. In the following example, notice the comment after the message statement, telling lint to ignore the 140-character limit:</p>
<pre># A description of what this class does<br/>#<br/># @summary A short summary of the purpose of this class<br/>#<br/># @example<br/># include module<br/>class module {<br/><br/>  notify {'String-trigger':<br/>    message =&gt;'This is the string that never ends. Yes it goes on and on my friends. Some developer just started writing without line breaks not knowing what they do, so this string will go on forever just because...' # lint:ignore:140chars<br/>  }<br/><br/>}</pre>
<p>If we have multiple places in a single manifest that we'd like to ignore, we can use the lint block <kbd>ignore</kbd> by placing the comment on a line alone and ending it with <kbd># lint:endignore</kbd>. In the following example, we have two large strings that won't be alerted on <kbd>puppet-lint</kbd>:</p>
<pre>class module::strings {<br/><br/># lint:ignore:140chars<br/>  notify {'Long String A':<br/>    message =&gt;'This is the string that never ends. Yes it goes on and on my friends. Some developer just started writing without line breaks not knowing what they do, so this string will go on forever just because this is the string that never ends...'<br/>  }<br/><br/>  notify {'Long String B':<br/>    message =&gt;'This is another string that never ends. Yes it goes on and on my friends. Some developer just started writing without line breaks not knowing what they do, so this string will go on forever just because this is the string that never ends...'<br/>  }<br/><br/># lint:endignore<br/><br/>}</pre>
<p>If you have a check that you'd like to disable, you can also create a <kbd>puppet-lint.rc</kbd> file. This file can be placed in <kbd>/etc</kbd> for a global config, as <kbd>.puppet-lint.rc</kbd> in the home directory for a user config, or at the base of a module, as <kbd>.puppet-lint.rc</kbd>. If your team uses local development workstations, consider adding a <kbd>.puppet-lint.rc</kbd> to your PDK template, to enforce a standard on each repository:</p>
<pre><strong># Permanently ignore ALL 140 character checks</strong><br/><strong>$ cat puppet-lint.rc</strong><br/><strong>--no-140chars-check</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, any Ruby code will be validated by RuboCop. RuboCop will check the style of all Ruby files in a module. This provides style checking to custom facts, types, providers, and even tasks written in Ruby:</p>
<pre><strong>$ pdk validate</strong><br/><strong>pdk (INFO): Running all available validators...</strong><br/><strong>pdk (INFO): Using Ruby 2.4.4</strong><br/><strong>pdk (INFO): Using Puppet 5.5.1</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking metadata syntax (metadata.json tasks/*.json).</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking module metadata style (metadata.json).</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Puppet manifest syntax (**/**.pp).</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Puppet manifest style (**/*.pp).</strong><br/><strong>[</strong><strong><img src="assets/eb51e960-235c-4a31-90ab-ed603c863429.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Checking Ruby code style (**/**.rb).</strong><br/><strong>info: task-metadata-lint: ./: Target does not contain any files to validate (tasks/*.json).</strong><br/><strong>error: rubocop: spec/classes/config_spec.rb:8:38: unexpected token tRCURLY</strong><br/><strong>(Using Ruby 2.1 parser; configure using `TargetRubyVersion` parameter, under `AllCops`)</strong></pre>
<p><kbd>pdk validate</kbd> provides a quick check of the style and syntax of your code. It does not check the functionality of your code. The PDK also provides a boiler template for RSpec tests out of the box, so that when a new class is created with <kbd>pdk new class</kbd>, a simple corresponding RSpec test is created along with it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The pdk test unit command</h1>
                </header>
            
            <article>
                
<p>New manifests built with <kbd>pdk new class</kbd> are also provided with a default RSpec test. Unit tests are written to ensure that a manifest performs what is expected as it is running. The default unit test provided by Puppet ensures that the code compiles successfully on every operating system listed in the <kbd>metadata.json</kbd>, with default facts for those operating systems. This can be expanded to create more robust unit tests. In the following example, a check has been added that states that the <kbd>init.pp</kbd> of the module should provide a file called <kbd>/etc/example</kbd> that is not provided by the manifest:</p>
<pre><strong>$ pdk test unit</strong><br/><strong>pdk (INFO): Using Ruby 2.4.4</strong><br/><strong>pdk (INFO): Using Puppet 5.5.1</strong><br/><strong>[</strong><strong><img src="assets/44e8371e-c57d-4389-a19e-b575881608ba.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Preparing to run the unit tests.</strong><br/><strong>[</strong><strong><img src="assets/44e8371e-c57d-4389-a19e-b575881608ba.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Running unit tests.</strong><br/><strong> Evaluated 45 tests in 2.461011 seconds: 9 failures, 0 pending.</strong><br/><strong>[</strong><strong><img src="assets/44e8371e-c57d-4389-a19e-b575881608ba.png" style="width:1.25em;height:1.25em;"/></strong><strong>] Cleaning up after running unit tests.</strong><br/><strong>failed: rspec: ./spec/classes/module_spec.rb:9: expected that the catalogue would contain File[test]</strong><br/><strong> module on centos-7-x86_64 should contain File[test]</strong><br/><strong> Failure/Error:</strong><br/><br/><strong> it { is_expected.to compile }</strong><br/><strong> it { is_expected.to contain_file('/etc/example') }</strong><br/><strong> end</strong><br/><strong> end</strong></pre>
<p>The simple test provided by the default PDK only provides <kbd>it { is_expected.to compile }</kbd> as an RSpec test for each module. In the next chapter, we'll expand upon our initial RSpec module, as we cover unit tests and provide some basic code coverage testing to our Puppet modules.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We started this chapter by detailing what makes for a good workflow. Much of the workflow becomes easier when combined with continuous integration and continuous delivery strategies, which will be covered in the next chapter. We'll expand upon the RSpec tests built by the Puppet PDK, and we'll discuss acceptance test strategies. We'll also cover some new workflows and tools to provide more immediate feedback during the development of Puppet code and manifests.</p>


            </article>

            
        </section>
    </body></html>