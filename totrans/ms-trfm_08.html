<html><head></head><body>
<div id="_idContainer103">
<h1 class="chapter-number" id="_idParaDest-164"><a id="_idTextAnchor402"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-165"><a id="_idTextAnchor403"/><span class="koboSpan" id="kobo.2.1">Containerize with AWS – Building Solutions with AWS EKS</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we built and automated our solution on AWS while utilizing </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Elastic Cloud Compute</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">EC2</span></strong><span class="koboSpan" id="kobo.7.1">). </span><span class="koboSpan" id="kobo.7.2">We built VM images with Packer and provisioned our VMs using Terraform. </span><span class="koboSpan" id="kobo.7.3">In this chapter, we’ll follow a similar path, but instead of working with VMs, we’ll look at hosting our application in containers within a </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">Kubernetes cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">To achieve this, we’ll</span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.10.1"> need to alter our approach by ditching Packer and replacing it with Docker to create a deployable artifact for our application. </span><span class="koboSpan" id="kobo.10.2">Once again, we will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.11.1">aws</span></strong><span class="koboSpan" id="kobo.12.1"> provider for Terraform, but this time, we’ll be introducing something new: the </span><strong class="source-inline"><span class="koboSpan" id="kobo.13.1">kubernetes</span></strong><span class="koboSpan" id="kobo.14.1"> provider for Terraform, which will provision to the Kubernetes cluster after our AWS infrastructure has been provisioned using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.15.1">aws</span></strong><span class="koboSpan" id="kobo.16.1"> provider </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">for Terraform.</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">Again, with this approach, we will only focus on the new and different. </span><span class="koboSpan" id="kobo.18.2">I’ll call out where we are building on previous chapters and when something is </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">legitimately new.</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.22.1">Laying </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">the foundation</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Designing </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">Building </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Automating </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">the deployment</span></span><a id="_idTextAnchor404"/></li>
</ul>
<h1 id="_idParaDest-166"><a id="_idTextAnchor405"/><span class="koboSpan" id="kobo.30.1">Laying the foundation</span></h1>
<p><span class="koboSpan" id="kobo.31.1">Our story </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.32.1">continues through the lens of Söze Enterprises, founded by the enigmatic Turkish billionaire Keyser Söze. </span><span class="koboSpan" id="kobo.32.2">Our team has been hard at work building the next-generation autonomous vehicle orchestration platform. </span><span class="koboSpan" id="kobo.32.3">Previously, we had hoped to leapfrog the competition by leveraging Amazon’s rock-solid platform, leveraging our team’s existing skills, and focusing on feature development. </span><span class="koboSpan" id="kobo.32.4">The team was just getting into their groove when a curveball came down </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">from above.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">It turns </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.35.1">out, over the weekend, our elusive executive was influenced by a rendezvous with Andy Jassy, the CEO of AWS, while scuba diving amid the rare and exotic marine life off the coast of the Galápagos. </span><span class="koboSpan" id="kobo.35.2">Keyser heard about the more efficient resource utilization leading to improved cost optimization and faster deployment and rollback times, and he was hooked. </span><span class="koboSpan" id="kobo.35.3">His new autonomous vehicle platform needed to harness the power of the cloud, and container-based architecture was the way to do it. </span><span class="koboSpan" id="kobo.35.4">So, he decided to accelerate his plans to adopt </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">cloud-native architecture!</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">The news of transitioning to a container-based architecture means reevaluating their approach, diving into new technologies, and possibly even reshuffling team dynamics. </span><span class="koboSpan" id="kobo.37.2">For the team, containers were always the long-term plan, but now, things need to be sped up, which will require a significant investment in time, resources, </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">and training.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">As the team scrambles to adjust their plans, they can’t help but feel a mix of excitement and apprehension. </span><span class="koboSpan" id="kobo.39.2">They know that they are part of something groundbreaking under Keyser’s leadership. </span><span class="koboSpan" id="kobo.39.3">His vision for the future of autonomous vehicles is bold and transformative. </span><span class="koboSpan" id="kobo.39.4">And while his methods may be unconventional, they have learned that his instincts are often right. </span><span class="koboSpan" id="kobo.39.5">In this chapter, we’ll explore this transformation from VMs to containers </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">using AW</span><a id="_idTextAnchor406"/><span class="koboSpan" id="kobo.41.1">S.</span></span></p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor407"/><span class="koboSpan" id="kobo.42.1">Designing the solution</span></h1>
<p><span class="koboSpan" id="kobo.43.1">As we</span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.44.1"> saw in the previous chapter, where we built our solution using VMs using AWS EC2, we had full control over the operating system configuration through the VM images we provisioned with Packer. </span><span class="koboSpan" id="kobo.44.2">Now that we will be </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.45.1">transitioning to hosting our solution on AWS </span><strong class="bold"><span class="koboSpan" id="kobo.46.1">Elastic Kubernetes Service</span></strong><span class="koboSpan" id="kobo.47.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.48.1">EKS</span></strong><span class="koboSpan" id="kobo.49.1">), we’ll need to introduce a new tool to replace VM images with </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.50.1">container images – </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.51.1">Docker</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<span class="koboSpan" id="kobo.53.1"><img alt="Figure 8.1 – Logical architecture for the autonomous vehicle platform" src="image/B21183_08_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.54.1">Figure 8.1 – Logical architecture for the autonomous vehicle platform</span></p>
<p><span class="koboSpan" id="kobo.55.1">Our</span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.56.1"> application architecture, comprising a frontend, a backend, and a database, will remain the same, but we will need to provision different resources with Terraform and harness new tools from Docker and Kubernetes to automate the deployment of our solution to this </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">new infrastructure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<span class="koboSpan" id="kobo.58.1"><img alt="Figure 8.2 – Source control structure of our repository" src="image/B21183_08_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.59.1">Figure 8.2 – Source control structure of our repository</span></p>
<p><span class="koboSpan" id="kobo.60.1">In this solution, we’ll</span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.61.1"> have seven parts. </span><span class="koboSpan" id="kobo.61.2">We still have the application code and Dockerfiles (replacing the Packer-based VM images) for both the frontend and backend. </span><span class="koboSpan" id="kobo.61.3">We still have GitHub Actions to implement our CI/CD process, but now we have two Terraform code bases – one for provisioning the underlying infrastructure to AWS and another for provisioning our application to the Kubernetes cluster hosted on EKS. </span><span class="koboSpan" id="kobo.61.4">Then, we have the two code bases for our application’s frontend </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">and bac</span><a id="_idTextAnchor408"/><span class="koboSpan" id="kobo.63.1">kend.</span></span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor409"/><span class="koboSpan" id="kobo.64.1">Cloud architecture</span></h2>
<p><span class="koboSpan" id="kobo.65.1">In</span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.66.1"> the previous </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.67.1">chapter, our cloud-hosting solution was a set of dedicated VMs. </span><span class="koboSpan" id="kobo.67.2">In this chapter, our objective is to leverage AWS EKS to use a shared pool of VMs that are managed by Kubernetes to host our application. </span><span class="koboSpan" id="kobo.67.3">To achieve this, we’ll be using some new resources that are geared toward container-based workloads. </span><span class="koboSpan" id="kobo.67.4">However, much </span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.68.1">of the networking, load balancing, and other components will largely be </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">t</span><a id="_idTextAnchor410"/><span class="koboSpan" id="kobo.70.1">he same.</span></span></p>
<h3><span class="koboSpan" id="kobo.71.1">Virtual network</span></h3>
<p><span class="koboSpan" id="kobo.72.1">Recalling our work in </span><a href="B21183_07.xhtml#_idTextAnchor365"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.73.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.74.1"> with EC2</span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.75.1"> instances and virtual networks, setting up a </span><strong class="bold"><span class="koboSpan" id="kobo.76.1">virtual private cloud</span></strong><span class="koboSpan" id="kobo.77.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.78.1">VPC</span></strong><span class="koboSpan" id="kobo.79.1">) for </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.80.1">AWS EKS follows a similar process. </span><span class="koboSpan" id="kobo.80.2">The core network is still there, with all the pomp and circumstance, from subnets – both public and private – to all the minutia of route tables, internet gateways, and NAT gateways, the virtual network we’ll build for our EKS cluster will largely be the same as the one we created previously. </span><span class="koboSpan" id="kobo.80.3">The only difference will be how we </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">use it:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<span class="koboSpan" id="kobo.82.1"><img alt="Figure 8.3 – AWS virtual network architecture" src="image/B21183_08_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.83.1">Figure 8.3 – AWS virtual network architecture</span></p>
<p><span class="koboSpan" id="kobo.84.1">Previously, we used the public subnets for our frontend VMs and the private subnets for our backend. </span><span class="koboSpan" id="kobo.84.2">As we learned in </span><a href="B21183_05.xhtml#_idTextAnchor278"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.85.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.86.1">, when we introduce Kubernetes into the mix, we’ll be transitioning to a shared pool of VMs that host our application as pods. </span><span class="koboSpan" id="kobo.86.2">These VMs will be hosted in the private subnets and a load balancer will be hosted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">public</span><a id="_idTextAnchor411"/><span class="koboSpan" id="kobo.88.1"> subnets.</span></span></p>
<h3><span class="koboSpan" id="kobo.89.1">Container registry</span></h3>
<p><span class="koboSpan" id="kobo.90.1">Building </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.91.1">on our exploration of container architecture in </span><a href="B21183_05.xhtml#_idTextAnchor278"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.92.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.93.1">, we know that we need to build container</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.94.1"> images and we need to store them in a container registry. </span><span class="koboSpan" id="kobo.94.2">For that purpose, AWS offers </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">Elastic Container Registry</span></strong><span class="koboSpan" id="kobo.96.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.97.1">ECR</span></strong><span class="koboSpan" id="kobo.98.1">). </span><span class="koboSpan" id="kobo.98.2">This is a private container registry, unlike public registries such as Docker Hub, which we looked at in </span><a href="B21183_05.xhtml#_idTextAnchor278"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.99.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.100.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.101.1">We’ll need to utilize the Docker command-line utility to build and push images to ECR. </span><span class="koboSpan" id="kobo.101.2">To be able to do that, we need to grant an identity the necessary permissions. </span><span class="koboSpan" id="kobo.101.3">As we saw in the previous chapter, when we build VM images using Packer, we’ll likely have a GitHub Actions workflow that builds and pushes the container images to ECR. </span><span class="koboSpan" id="kobo.101.4">The identity that the GitHub Actions workflow executes under will need permission to do that. </span><span class="koboSpan" id="kobo.101.5">Once these Docker images are in ECR, the final step is to grant our cluster access to pull images from </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">the registry:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<span class="koboSpan" id="kobo.103.1"><img alt="Figure 8.4 – IAM policy giving a group access to push container images to ECR" src="image/B21183_08_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.104.1">Figure 8.4 – IAM policy giving a group access to push container images to ECR</span></p>
<p><span class="koboSpan" id="kobo.105.1">We’ll set up an IAM group that we’ll grant this permission to. </span><span class="koboSpan" id="kobo.105.2">This will allow us to add the user for the GitHub Action, as well as any other human users who want to push images directly from the command line. </span><span class="koboSpan" id="kobo.105.3">In AWS, IAM policies are extremely flexible; they can be declared independently or inline with the identity they are being attached to. </span><span class="koboSpan" id="kobo.105.4">This</span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.106.1"> allows us to create reusable policies that can be attached to multiple identities. </span><span class="koboSpan" id="kobo.106.2">In this case, we’ll define the policy that grants access to push images to this ECR and then attach it to the group. </span><span class="koboSpan" id="kobo.106.3">Then, membership in the group will grant users access to </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">these permissions.</span></span></p>
<p><span class="koboSpan" id="kobo.108.1">The final step is to grant access to the cluster such that it can pull images from our ECR when it schedules pods within the nodes. </span><span class="koboSpan" id="kobo.108.2">To do that, we can use a built-in AWS policy called </span><strong class="source-inline"><span class="koboSpan" id="kobo.109.1">AmazonEC2ContainerRegistryReadOnly</span></strong><span class="koboSpan" id="kobo.110.1">. </span><span class="koboSpan" id="kobo.110.2">We’ll need to reference it using its fully qualified ARN, which is </span><strong class="source-inline"><span class="koboSpan" id="kobo.111.1">arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly</span></strong><span class="koboSpan" id="kobo.112.1">. </span><span class="koboSpan" id="kobo.112.2">Built-in policies have a common </span><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">arn:aws:iam::aws:policy</span></strong><span class="koboSpan" id="kobo.114.1"> prefix that identifies them as published by AWS and not published by any specific user within their AWS account. </span><span class="koboSpan" id="kobo.114.2">When we publish our own policies, the fully qualified ARN will include our </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">acco</span><a id="_idTextAnchor412"/><span class="koboSpan" id="kobo.116.1">unt number.</span></span></p>
<h3><span class="koboSpan" id="kobo.117.1">Load balancing</span></h3>
<p><span class="koboSpan" id="kobo.118.1">Unlike in the</span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.119.1"> previous chapter, where we provisioned and configured our own AWS </span><strong class="bold"><span class="koboSpan" id="kobo.120.1">Application Load Balancer</span></strong><span class="koboSpan" id="kobo.121.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.122.1">ALB</span></strong><span class="koboSpan" id="kobo.123.1">), when </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.124.1">using Amazon EKS, one of the advantages is that EKS takes on much of the responsibility of provisioning and configuring load balancers. </span><span class="koboSpan" id="kobo.124.2">We can direct and influence its actions using Kubernetes annotations but this is largely taken care of for us. </span><span class="koboSpan" id="kobo.124.3">In our solution, to keep things simple, we’ll be using NGINX as our ingress controller and configuring it to set up an</span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.125.1"> AWS </span><strong class="bold"><span class="koboSpan" id="kobo.126.1">Network Load Balancer</span></strong><span class="koboSpan" id="kobo.127.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.128.1">NLB</span></strong><span class="koboSpan" id="kobo.129.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">for us:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<span class="koboSpan" id="kobo.131.1"><img alt="Figure 8.5 – Elastic load balancer working with an NGINX ingress controller to route traffic to our application’s pods" src="image/B21183_08_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.132.1">Figure 8.5 – Elastic load balancer working with an NGINX ingress controller to route traffic to our application’s pods</span></p>
<p><span class="koboSpan" id="kobo.133.1">To </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.134.1">delegate this responsibility to EKS, we need to grant it the necessary IAM permissions to provision and manage these resources. </span><span class="koboSpan" id="kobo.134.2">Therefore, we’ll need to provision an IAM policy and attach it to the EKS cluster. </span><span class="koboSpan" id="kobo.134.3">We can do this using an IAM role that has been assigned to the cluster’s </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">node group:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<span class="koboSpan" id="kobo.136.1"><img alt="Figure 8.6 – IAM policy allowing EKS to provision and manage elastic load balancers" src="image/B21183_08_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.137.1">Figure 8.6 – IAM policy allowing EKS to provision and manage elastic load balancers</span></p>
<p><span class="koboSpan" id="kobo.138.1">Then, we provision Kubernetes resources (for example, services and ingress controllers) and annotate them to inform the specific configuration of our elastic load balancers that we want EKS to enact o</span><a id="_idTextAnchor413"/><span class="koboSpan" id="kobo.139.1">n </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">our behalf.</span></span></p>
<h3><span class="koboSpan" id="kobo.141.1">Network security</span></h3>
<p><span class="koboSpan" id="kobo.142.1">There are </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.143.1">many ways to host services on Kubernetes and make them accessible outside of the cluster. </span><span class="koboSpan" id="kobo.143.2">In our solution, we’ll be using an AWS elastic load balancer to allow external traffic into our cluster through our NGINX controller. </span><span class="koboSpan" id="kobo.143.3">There are other options, such as NodePort, which allow you to access a pod directly through an exposed port on the node. </span><span class="koboSpan" id="kobo.143.4">This would require public access to the cluster’s nodes and is not the preferred method from both security and </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">scalability perspectives.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">If we want access to the cluster using </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">kubectl</span></strong><span class="koboSpan" id="kobo.147.1">, then we need to turn on public endpoint access. </span><span class="koboSpan" id="kobo.147.2">This is useful when you’re developing something small on your own but not ideal when you’re working in an enterprise context. </span><span class="koboSpan" id="kobo.147.3">You will most likely have the private network infrastructure in place so that you never have to enable the </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">pu</span><a id="_idTextAnchor414"/><span class="koboSpan" id="kobo.149.1">blic endpoint.</span></span></p>
<h3><span class="koboSpan" id="kobo.150.1">Secrets management</span></h3>
<p><span class="koboSpan" id="kobo.151.1">Incorporating</span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.152.1"> secrets into pods within an Amazon EKS cluster can be achieved through various methods, each with its advantages and disadvantages. </span><span class="koboSpan" id="kobo.152.2">As we did with VMs in the previous chapter, the method that we will explore is using AWS Secrets Manager secrets. </span><span class="koboSpan" id="kobo.152.3">Kubernetes has a built-in approach using Kubernetes Secrets. </span><span class="koboSpan" id="kobo.152.4">This method is straightforward and integrated directly into Kubernetes, but it has limitations in terms of security since secrets are encoded in Base64 and can be accessed by anyone with </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">cluster access.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">Integration with AWS Secrets Manager can help solve this problem but to access our secrets stored there, we need to enable our Kubernetes deployments to authenticate with </span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.155.1">AWS </span><strong class="bold"><span class="koboSpan" id="kobo.156.1">Identity and Access Management</span></strong><span class="koboSpan" id="kobo.157.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.158.1">IAM</span></strong><span class="koboSpan" id="kobo.159.1">). </span><span class="koboSpan" id="kobo.159.2">This is often referred to as Workload Identity and it is an approach that is relatively common across </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">cloud platforms:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<span class="koboSpan" id="kobo.161.1"><img alt="Figure 8.7 – AWS EKS with Workload Identity" src="image/B21183_08_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.162.1">Figure 8.7 – AWS EKS with Workload Identity</span></p>
<p><span class="koboSpan" id="kobo.163.1">To set up </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.164.1">Workload Identity on EKS, we need to configure the cluster with an </span><strong class="bold"><span class="koboSpan" id="kobo.165.1">OpenID Connect</span></strong><span class="koboSpan" id="kobo.166.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.167.1">OIDC</span></strong><span class="koboSpan" id="kobo.168.1">) provider. </span><span class="koboSpan" id="kobo.168.2">Then, we</span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.169.1"> must set up an IAM role that has a policy that allows a Kubernetes service account to assume the role. </span><span class="koboSpan" id="kobo.169.2">This IAM role can then be granted access to any AWS permissions and resources that the Kubernetes deployment needs access to, including Secrets Manager secrets. </span><span class="koboSpan" id="kobo.169.3">The last thing we need to do is provision a Kubernetes service account by the same name within Kubernetes and give it a special annotation to connect it to the </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">IAM role.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">Once this is done, our Kubernetes deployments will be allowed to access our AWS Secrets Manager secrets but they won’t be using that access. </span><span class="koboSpan" id="kobo.171.2">The final step is to configure the Kubernetes deployment to pull in the secrets and make them accessible to our application code running in </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">the pods:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<span class="koboSpan" id="kobo.173.1"><img alt="Figure 8.8 – AWS EKS Secrets Manager integration" src="image/B21183_08_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.174.1">Figure 8.8 – AWS EKS Secrets Manager integration</span></p>
<p><span class="koboSpan" id="kobo.175.1">Kubernetes</span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.176.1"> has a common practice of doing this using volume mounts. </span><span class="koboSpan" id="kobo.176.2">As a result, there is a common Kubernetes provider known as the secrets store </span><strong class="bold"><span class="koboSpan" id="kobo.177.1">Container Storage Interface</span></strong><span class="koboSpan" id="kobo.178.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.179.1">CSI</span></strong><span class="koboSpan" id="kobo.180.1">) provider. </span><span class="koboSpan" id="kobo.180.2">This</span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.181.1"> is a cloud-agnostic technique that integrates Kubernetes with external secret stores, such as AWS Secrets Manager. </span><span class="koboSpan" id="kobo.181.2">This method offers enhanced security and scalability, but it requires more setup </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">and maintenance.</span></span></p>
<p><span class="koboSpan" id="kobo.183.1">To get this working, we need to deploy two components to our EKS cluster: the secrets store CSI driver and then the AWS provider for this driver that will allow it to interface with AWS Secrets Manager. </span><span class="koboSpan" id="kobo.183.2">Both of these components can be deployed to our EKS cluster </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.184.1">with </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">Helm</span></strong><span class="koboSpan" id="kobo.186.1">. </span><span class="koboSpan" id="kobo.186.2">Once these important subsystems are in place, we can set up a special Kubernetes resource called </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">SecretProviderClass</span></strong><span class="koboSpan" id="kobo.188.1">. </span><span class="koboSpan" id="kobo.188.2">This is a type of resource that connects to AWS Secrets Manager through the CSI driver to access specific secrets. </span><span class="koboSpan" id="kobo.188.3">It connects to specific secrets in Secrets Manager using the service account that we granted access to via the IAM role a</span><a id="_idTextAnchor415"/><span class="koboSpan" id="kobo.189.1">nd </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">its permissions.</span></span></p>
<h3><span class="koboSpan" id="kobo.191.1">Kubernetes cluster</span></h3>
<p><span class="koboSpan" id="kobo.192.1">Amazon EKS </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.193.1">offers a managed Kubernetes service that streamlines the deployment and management of containerized applications on AWS. </span><span class="koboSpan" id="kobo.193.2">The EKS cluster is the central figure of this architecture. </span><span class="koboSpan" id="kobo.193.3">EKS handles the heavy lifting of setting up, operating, and maintaining the Kubernetes control plane and nodes, which are essentially EC2 instances. </span><span class="koboSpan" id="kobo.193.4">When setting up an EKS cluster, users define node groups, which manifest as collections of EC2 instances that the EKS service is responsible for provisioning </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">and managing.</span></span></p>
<p><span class="koboSpan" id="kobo.195.1">There are several options for node groups that can host your workloads. </span><span class="koboSpan" id="kobo.195.2">The most common examples are AWS-managed and self-managed node groups. </span><span class="koboSpan" id="kobo.195.3">AWS-managed node groups are essentially on-demand EC2 instances that are allocated for the EKS cluster. </span><span class="koboSpan" id="kobo.195.4">AWS simplifies the management of these nodes but this imposes some restrictions on what AWS features can be used. </span><span class="koboSpan" id="kobo.195.5">Self-managed nodes are also essentially on-demand EC2 instances but they provide greater control over the features and configuration options available </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">to them.</span></span></p>
<p><span class="koboSpan" id="kobo.197.1">A great way to optimize for cost is to use a Fargate node group. </span><span class="koboSpan" id="kobo.197.2">This option takes advantage of AWS’ serverless compute engine and removes the need to provision and manage EC2 instances. </span><span class="koboSpan" id="kobo.197.3">However, this is probably more suitable for unpredictable workloads rather than those that require a steady state. </span><span class="koboSpan" id="kobo.197.4">In those situations, you can take advantage of a combination of autoscaling and spot and reserved instances to reap significant discounts and </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">cost reduction:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<span class="koboSpan" id="kobo.199.1"><img alt="Figure 8.9 – Anatomy of an AWS EKS cluster" src="image/B21183_08_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.200.1">Figure 8.9 – Anatomy of an AWS EKS cluster</span></p>
<p><span class="koboSpan" id="kobo.201.1">IAM policies </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.202.1">are a major part of the configuration of EKS due to the nature of the service and how we delegate responsibility to it to manage AWS resources. </span><span class="koboSpan" id="kobo.202.2">This is similar to what we do with AWS Auto Scaling groups but even more so. </span><span class="koboSpan" id="kobo.202.3">IAM policies are attached to the cluster and individual node groups. </span><span class="koboSpan" id="kobo.202.4">Depending on the capabilities you want to enable within your cluster and your node groups, you might need </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">additional policies.</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.205.1">AmazonEKSClusterPolicy</span></strong><span class="koboSpan" id="kobo.206.1"> policy grants the cluster access to control the internal workings of the cluster itself, including node groups, CloudWatch logging, and access control within </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">the cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.208.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.209.1">AmazonEKSVPCResourceController</span></strong><span class="koboSpan" id="kobo.210.1"> policy grants the cluster access to manage network resources such as network interfaces, IP address assignment, and security group attachments to </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">the VPC.</span></span></p>
<p><span class="koboSpan" id="kobo.212.1">There are four policies (</span><strong class="source-inline"><span class="koboSpan" id="kobo.213.1">AmazonEKSWorkerNodePolicy</span></strong><span class="koboSpan" id="kobo.214.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">AmazonEKS_CNI_Policy</span></strong><span class="koboSpan" id="kobo.216.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.217.1">AmazonEC2ContainerRegistryReadOnly</span></strong><span class="koboSpan" id="kobo.218.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">CloudWatchAgentServerPolicy</span></strong><span class="koboSpan" id="kobo.220.1">) that are essential for the operation of EKS worker nodes. </span><span class="koboSpan" id="kobo.220.2">These policies absolutely must be attached to the IAM role that you assign to your </span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.221.1">EKS node group. </span><span class="koboSpan" id="kobo.221.2">They grant access to the EKS cluster’s control plane and let nodes within the node group integrate with the core infrastructure provided by the cluster, including the network, container registries, and CloudWatch. </span><span class="koboSpan" id="kobo.221.3">As described previously, we also added an optional policy to allow the EKS cluster to manage el</span><a id="_idTextAnchor416"/><span class="koboSpan" id="kobo.222.1">astic </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">load balancers.</span></span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor417"/><span class="koboSpan" id="kobo.224.1">Deployment architecture</span></h2>
<p><span class="koboSpan" id="kobo.225.1">Now that</span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.226.1"> we have a good idea</span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.227.1"> of what our cloud architecture is going to look like for our solution on AWS, we need to come up with a plan on how to provision our environmen</span><a id="_idTextAnchor418"/><span class="koboSpan" id="kobo.228.1">ts and deploy </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">our code.</span></span></p>
<h3><span class="koboSpan" id="kobo.230.1">Cloud environment configuration</span></h3>
<p><span class="koboSpan" id="kobo.231.1">Building </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.232.1">upon the methodology we established in </span><a href="B21183_07.xhtml#_idTextAnchor365"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.233.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.234.1"> for provisioning EC2 instances, our approach to provisioning the AWS EKS environment will follow a similar pattern. </span><span class="koboSpan" id="kobo.234.2">The core of this process lies in utilizing GitHub Actions, which will remain unchanged in its fundamental setup </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">and operation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<span class="koboSpan" id="kobo.236.1"><img alt="Figure 8.10 – The Terraform code provisions the environment on AWS" src="image/B21183_08_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.237.1">Figure 8.10 – The Terraform code provisions the environment on AWS</span></p>
<p><span class="koboSpan" id="kobo.238.1">However, instead of provisioning EC2 instances as we did previously, the Terraform code will be tailored to set up the necessary components for an EKS environment. </span><span class="koboSpan" id="kobo.238.2">This includes the creation of an EKS cluster and an ECR. </span><span class="koboSpan" id="kobo.238.3">The GitHub Action will automate the execution of this Terraform code, following the same workflow pattern we </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">used before.</span></span></p>
<p><span class="koboSpan" id="kobo.240.1">By reusing </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.241.1">the GitHub Actions workflow with different Terraform scripts, we maintain consistency in our deployment process while adapting to the different infrastructure requirements of the EKS environment. </span><span class="koboSpan" id="kobo.241.2">This step will need to be executed in a standalone mode to ensure certain prerequisites are there, such as the container registry. </span><span class="koboSpan" id="kobo.241.3">Only once the container registry is provisioned can we build and push container images to it for our frontend and backend </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">application components.</span></span></p>
<p><span class="koboSpan" id="kobo.243.1">This step will also provision the EKS cluster that hosts the Kubernetes control plane. </span><span class="koboSpan" id="kobo.243.2">We’ll use this in the final step in conjunction with the container images to</span><a id="_idTextAnchor419"/><span class="koboSpan" id="kobo.244.1"> deploy </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">our application.</span></span></p>
<h3><span class="koboSpan" id="kobo.246.1">Container configuration</span></h3>
<p><span class="koboSpan" id="kobo.247.1">Unlike Packer, which </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.248.1">doesn’t rely on any existing infrastructure to provision the application deployment artifacts (for example, the AMIs built by Packer), our container images need to have a container registry before they can </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">be provisioned:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<span class="koboSpan" id="kobo.250.1"><img alt="Figure 8.11 – Docker pipeline to build a container image for the frontend" src="image/B21183_08_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.251.1">Figure 8.11 – Docker pipeline to build a container image for the frontend</span></p>
<p><span class="koboSpan" id="kobo.252.1">The workflow is very similar to that of Packer in that we combine the application code and a template that stores the operating system configuration. </span><span class="koboSpan" id="kobo.252.2">In this case, it stores a Dockerfile rather</span><a id="_idTextAnchor420"/><span class="koboSpan" id="kobo.253.1"> than a </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">Packer template.</span></span></p>
<h3><span class="koboSpan" id="kobo.255.1">Kubernetes configuration</span></h3>
<p><span class="koboSpan" id="kobo.256.1">Once we’ve published container images for both the frontend and backend, we’re ready to complete</span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.257.1"> the deployment by adding a final step that executes Terraform using the Kubernetes provider so that it will deploy our application to the </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">EKS cluster:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<span class="koboSpan" id="kobo.259.1"><img alt="Figure 8.12 – Container images as inputs to terraform code, which provisions the environment on EKS’ Kubernetes control plane" src="image/B21183_08_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.260.1">Figure 8.12 – Container images as inputs to terraform code, which provisions the environment on EKS’ Kubernetes control plane</span></p>
<p><span class="koboSpan" id="kobo.261.1">We will output key pieces of information from the previous Terraform step that provisioned the AWS infrastructure. </span><span class="koboSpan" id="kobo.261.2">This will include details about the ECR repositories and the EKS cluster. </span><span class="koboSpan" id="kobo.261.3">We can use these as inputs for the final Terraform execution step where we use the Kubernetes provider. </span><span class="koboSpan" id="kobo.261.4">We have separated this step into separate Terraform workspaces to decouple it from the AWS infrastructure. </span><span class="koboSpan" id="kobo.261.5">This recognizes the hard dependency between the Kubernetes control plane layer and the underlying infrastructure. </span><span class="koboSpan" id="kobo.261.6">It allows us to independently manage the underlying infrastructure without making changes to the Kubernetes deployments, as well as make changes that are isolated within the Kubernetes control plane that will speed</span><a id="_idTextAnchor421"/><span class="koboSpan" id="kobo.262.1"> up the </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">release process.</span></span></p>
<p><span class="koboSpan" id="kobo.264.1">In this section, we reviewed the key changes in our architecture as we transitioned from VM-based architecture to container-based architecture. </span><span class="koboSpan" id="kobo.264.2">In the next section, we’ll get tactical in building the solution, but we’ll be careful to build on the foundations we built in the previous chapter when we first set up our solution on AWS using VMs powered </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">by EC2.</span></span></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor422"/><span class="koboSpan" id="kobo.266.1">Building the solution</span></h1>
<p><span class="koboSpan" id="kobo.267.1">In this section, we’ll </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.268.1">be taking our theoretical knowledge and applying it to a tangible, functioning solution while harnessing the power of Docker, Terraform, and Kubernetes on the AWS platform. </span><span class="koboSpan" id="kobo.268.2">Some parts of this process will require significant change, such as when we provision our AWS infrastructure using Terraform; other parts will have minor changes, such as the Kubernetes configuration that we use to deploy our application to our Kubernetes cluster; and some will be completely new, such as the process to build and push our Docker images t</span><a id="_idTextAnchor423"/><span class="koboSpan" id="kobo.269.1">o our </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">container registry.</span></span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor424"/><span class="koboSpan" id="kobo.271.1">Docker</span></h2>
<p><span class="koboSpan" id="kobo.272.1">As </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.273.1">we saw in the previous chapter, where we built VM images in Packer, there is a certain amount of operating system configuration that needs to be set up. </span><span class="koboSpan" id="kobo.273.2">With Docker, we are doing largely the same thing but we are doing it for a specific process. </span><span class="koboSpan" id="kobo.273.3">This means much of the work that we did in setting up the service in Linux is eliminated because the container runtime controls when the application is running or not. </span><span class="koboSpan" id="kobo.273.4">This is fundamentally different than configuring the Linux operating system to run an executable as a service. </span><span class="koboSpan" id="kobo.273.5">As a result, much of this boilerplate </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">is eliminated.</span></span></p>
<p><span class="koboSpan" id="kobo.275.1">Another major difference is that with the Packer image, we build the application outside of Packer and we drop a zipped artifact containing the application as part of the Packer build. </span><span class="koboSpan" id="kobo.275.2">With Docker, we’ll build the application and produce the artifact within the container build process. </span><span class="koboSpan" id="kobo.275.3">After this process is complete, we’ll follow a similar process where we drop the deployment package into a clean container image layer to eliminate any residual </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">build artifacts:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.277.1">
FROM mcr.microsoft.com/dotnet/sdk:6.0 AS build-env
WORKDIR /app</span></pre> <p><span class="koboSpan" id="kobo.278.1">The following line sets the base image for the build stage. </span><span class="koboSpan" id="kobo.278.2">It uses the official Microsoft .NET SDK image (version </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">6.0</span></strong><span class="koboSpan" id="kobo.280.1">) from </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.281.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.282.1">Microsoft Container </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.283.1">Registry</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.284.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.285.1">MCR</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.287.1">
COPY ./FleetPortal/FleetPortal.csproj ./FleetPortal/
RUN dotnet restore ./FleetPortal/FleetPortal.csproj</span></pre> <p><span class="koboSpan" id="kobo.288.1">Before we build the project, we need to resolve its dependencies. </span><span class="koboSpan" id="kobo.288.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">dotnet restore</span></strong><span class="koboSpan" id="kobo.290.1"> command will do this by pulling all the dependencies from NuGet (the .NET </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">package manager):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.292.1">
COPY . </span><span class="koboSpan" id="kobo.292.2">./
RUN dotnet publish ./FleetPortal/FleetPortal.csproj -c Release -o out</span></pre> <p><span class="koboSpan" id="kobo.293.1">Here, we </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.294.1">execute the </span><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">dotnet publish</span></strong><span class="koboSpan" id="kobo.296.1"> command, which creates the binaries for the project. </span><span class="koboSpan" id="kobo.296.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">-c Release</span></strong><span class="koboSpan" id="kobo.298.1"> option specifies that the build should be optimized for production. </span><span class="koboSpan" id="kobo.298.2">We drop the files into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">out</span></strong><span class="koboSpan" id="kobo.300.1"> folder to be picked up by a </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">future step:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.302.1">
FROM mcr.microsoft.com/dotnet/aspnet:6.0
WORKDIR /app
COPY --from=build-env /app/out .</span></pre> <p><span class="koboSpan" id="kobo.303.1">We start a new build stage with the .NET runtime image as the base and we copy the binaries that we built from the previous stage to this new one. </span><span class="koboSpan" id="kobo.303.2">This will ensure that any intermediate build artifacts are not layered into the </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">container image:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.305.1">
ENTRYPOINT [“dotnet”, “FleetPortal.dll”]</span></pre> <p><span class="koboSpan" id="kobo.306.1">Finally, we set the startup command for the container. </span><span class="koboSpan" id="kobo.306.2">When the container starts, it will run </span><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">dotnet FleetPortal.dll</span></strong><span class="koboSpan" id="kobo.308.1">, which starts our ASP.NET application. </span><span class="koboSpan" id="kobo.308.2">It will start listening for </span><a id="_idTextAnchor425"/><span class="koboSpan" id="kobo.309.1">incoming web </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">server traffic.</span></span></p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor426"/><span class="koboSpan" id="kobo.311.1">Terraform</span></h2>
<p><span class="koboSpan" id="kobo.312.1">As we </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.313.1">discussed in our design, our solution is made up of two application components: the frontend and the backend. </span><span class="koboSpan" id="kobo.313.2">Each has a code base </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.314.1">of application code that needs to be deployed. </span><span class="koboSpan" id="kobo.314.2">However, with a Kubernetes solution, the infrastructure is simplified in that we only need a Kubernetes cluster (and a few other things). </span><span class="koboSpan" id="kobo.314.3">The important piece is the configuration within the Kubernetes </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">platform itself.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">As a result, much of the Terraform setup is very similar to what we did in the previous chapter, so we will only focus on new resources needed for our solution. </span><span class="koboSpan" id="kobo.316.2">You can check the full source code for this book on GitHub if you want to work with the </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">complete solution.</span></span></p>
<h3><span class="koboSpan" id="kobo.318.1">Container registry</span></h3>
<p><span class="koboSpan" id="kobo.319.1">First, we’ll </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.320.1">set up repositories for both the frontend and backend of our application using AWS ECR. </span><span class="koboSpan" id="kobo.320.2">To simplify the dynamic creation of our ECR repositories, we can set up a local variable called </span><strong class="source-inline"><span class="koboSpan" id="kobo.321.1">repository_list</span></strong><span class="koboSpan" id="kobo.322.1"> that has constants for the two container images we need </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">repositories for:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.324.1">
locals {
  repository_list = [“frontend”, “backend”]
  repositories    = { for name in local.repository_list : name =&gt; name }
}</span></pre> <p><span class="koboSpan" id="kobo.325.1">Then, we’ll use a </span><strong class="source-inline"><span class="koboSpan" id="kobo.326.1">for</span></strong><span class="koboSpan" id="kobo.327.1"> expression to generate a map from this list that we can then use to create a corresponding ECR repository using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.328.1">for_each</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.329.1"> iterator:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.330.1">
resource “aws_ecr_repository” “main” {
  for_each = local.repositories
  name                 = “ecr-${var.application_name}-${var.environment_name}-${each.key}”
  image_tag_mutability = “MUTABLE”
}</span></pre> <p><span class="koboSpan" id="kobo.331.1">Next, we’ll set up an IAM group that we can grant access to push container </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">images to:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.333.1">
resource “aws_iam_group” “ecr_image_pushers” {
  name = “${var.application_name}-${var.environment_name}-ecr-image-pushers”
}</span></pre> <p><span class="koboSpan" id="kobo.334.1">Now, we need to generate an IAM policy that grants access to each of the ECR repositories</span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.335.1"> and attach it to the IAM group we </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">created previously:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.337.1">
resource “aws_iam_group_policy” “ecr_image_pushers” {
  for_each = local.repositories
  name  = “${var.application_name}-${var.environment_name}-${each.key}-ecr-image-push-policy”
  group = aws_iam_group.ecr_image_pushers.name
  policy = jsonencode({
    Version = “2012-10-17”,
    Statement = [
      {
        Effect = “Allow”,
        Action = [
          “ecr:GetDownloadUrlForLayer”,
          “ecr:BatchGetImage”,
          “ecr:BatchCheckLayerAvailability”,
          “ecr:PutImage”,
          “ecr:InitiateLayerUpload”,
          “ecr:UploadLayerPart”,
          “ecr:CompleteLayerUpload”
        ],
        Resource = aws_ecr_repository.main[each.key].arn
      }
    ]
  })
}</span></pre> <p><span class="koboSpan" id="kobo.338.1">Finally, we</span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.339.1"> must grant access to this group. </span><span class="koboSpan" id="kobo.339.2">We’ll be granting access to the identities of developers on our team or the GitHub Actions workflows that will be pushing new images as part of our </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">CI/CD process:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.341.1">
resource “aws_iam_group_membership” “ecr_image_pushers” {
  name  = “${var.application_name}-${var.environment_name}-ecr-image-push-membership”
  users = var.ecr_image_pushers
  group = aws_iam_group.ecr_image_pushers.name
}</span></pre> <h3><span class="koboSpan" id="kobo.342.1">Kubernetes cluster</span></h3>
<p><span class="koboSpan" id="kobo.343.1">Now that </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.344.1">our container registry is all set up and we can push images to it, we need to set up our Kubernetes cluster. </span><span class="koboSpan" id="kobo.344.2">That’s where AWS EKS comes in. </span><span class="koboSpan" id="kobo.344.3">The cluster’s configuration is relatively simple but there’s quite a bit of work we need to do with IAM to make it </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">all work.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">Before we provision our EKS cluster, we need to set up the IAM role that it will use to interact with the rest of the AWS platform. </span><span class="koboSpan" id="kobo.346.2">This is not a role that our nodes or Kubernetes deployments will use. </span><span class="koboSpan" id="kobo.346.3">It’s the role that EKS will use to enact configuration changes made to the cluster across all the AWS resources that are </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">being used:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.348.1">
data “aws_iam_policy_document” “container_cluster_assume_role” {
  statement {
    effect = “Allow”
    principals {
      type        = “Service”
      identifiers = [“eks.amazonaws.com”]
    }
    actions = [“sts:AssumeRole”]
  }
}</span></pre> <p><span class="koboSpan" id="kobo.349.1">As a </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.350.1">result, the EKS service will assume this role. </span><span class="koboSpan" id="kobo.350.2">Hence, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">assume</span></strong><span class="koboSpan" id="kobo.352.1"> policy needs to allow a principal of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">Service</span></strong><span class="koboSpan" id="kobo.354.1"> type with </span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">eks.amazonaws.com</span></strong><span class="koboSpan" id="kobo.356.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">its identifier:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.358.1">
resource “aws_iam_role” “container_cluster” {
  name               = “eks-${var.application_name}-${var.environment_name}-cluster-role”
  assume_role_policy = data.aws_iam_policy_document.container_cluster_assume_role.json
}</span></pre> <p><span class="koboSpan" id="kobo.359.1">With this role, we are going to enable EKS to provision and manage the resources that it needs within our AWS account. </span><span class="koboSpan" id="kobo.359.2">As a result, we need to attach the built-in </span><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">AmazonEKSClusterPolicy</span></strong><span class="koboSpan" id="kobo.361.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">AmazonEKSVPCResourceController</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.363.1"> policies:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.364.1">
resource “aws_iam_role_policy_attachment” “eks_cluster_policy” {
  policy_arn = “arn:aws:iam::aws:policy/AmazonEKSClusterPolicy”
  role       = aws_iam_role.container_cluster.name
}</span></pre> <p><span class="koboSpan" id="kobo.365.1">The preceding code is an example of how to do this for one of the policies. </span><span class="koboSpan" id="kobo.365.2">You could create an </span><strong class="source-inline"><span class="koboSpan" id="kobo.366.1">aws_iam_role_policy_attachment</span></strong><span class="koboSpan" id="kobo.367.1"> resource for each of the policies or use an iterator over a collection of the policies that we need </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">to attach.</span></span></p>
<p><span class="koboSpan" id="kobo.369.1">Now that</span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.370.1"> this IAM role is ready, we can set up our cluster using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.371.1">aws_eks_cluster</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.372.1"> resource:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.373.1">
resource “aws_eks_cluster” “main” {
  name                      = local.cluster_name
  role_arn                  = aws_iam_role.container_cluster.arn
  vpc_config {
    security_group_ids = [
      aws_security_group.cluster.id,
      aws_security_group.cluster_nodes.id
    ]
    subnet_ids              = local.cluster_subnet_ids
    endpoint_public_access  = true
    endpoint_private_access = true
  }
  // Other configurations like logging, encryption, etc.
</span><span class="koboSpan" id="kobo.373.2">}</span></pre> <p><span class="koboSpan" id="kobo.374.1">A significant portion of the configuration is done within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.375.1">vpc_config</span></strong><span class="koboSpan" id="kobo.376.1"> block, which references many of the same structures that we provisioned in the </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">previous chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.378.1">One thing that you might want to keep in mind is how important the IAM policies are for enabling this EKS cluster to be successfully provisioned. </span><span class="koboSpan" id="kobo.378.2">Since there is no direct relationship</span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.379.1"> between the IAM role’s policy attachments, you should ensure that IAM role permissions are created before we attempt to provision the EKS cluster. </span><span class="koboSpan" id="kobo.379.2">The following code demonstrates the use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">depends_on</span></strong><span class="koboSpan" id="kobo.381.1"> attribute, which allows us to define this </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">relationship explicitly:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.383.1">
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_vpc_controller_policy,
    aws_cloudwatch_log_group.container_cluster
  ]</span></pre> <p><span class="koboSpan" id="kobo.384.1">The EKS cluster is just the control plane. </span><span class="koboSpan" id="kobo.384.2">For our cluster to have utility, we need to add worker nodes. </span><span class="koboSpan" id="kobo.384.3">We can do this by adding one or more node groups. </span><span class="koboSpan" id="kobo.384.4">These node groups will be composed of a collection of EC2 instances that will be enlisted as worker nodes. </span><span class="koboSpan" id="kobo.384.5">These nodes also need their own </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">IAM role:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.386.1">
data “aws_iam_policy_document” “container_node_group” {
  statement {
    sid     = “EKSNodeAssumeRole”
    actions = [“sts:AssumeRole”]
    principals {
      type        = “Service”
      identifiers = [“ec2.amazonaws.com”]
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.387.1">A key difference is that because this role will be assumed by the worker nodes, which are EC2 instances, the IAM role’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">assume</span></strong><span class="koboSpan" id="kobo.389.1"> policy needs to align with </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">this fact.</span></span></p>
<p><span class="koboSpan" id="kobo.391.1">Just as before</span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.392.1"> with our EKS cluster, which needed an IAM role to be set up as a prerequisite, the same is true for our node group. </span><span class="koboSpan" id="kobo.392.2">Now that the node group’s IAM role is ready, we can use the following code to create an EKS node group associated with the previously defined cluster. </span><span class="koboSpan" id="kobo.392.3">It specifies the desired, minimum, and maximum sizes of the node group, along with other configurations, such as the AMI type and </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">disk size:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.394.1">
resource “aws_eks_node_group” “main” {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = “ng-user”
  node_role_arn   = aws_iam_role.container_node_group.arn
  subnet_ids      = local.cluster_subnet_ids
  scaling_config {
    desired_size = 3
    min_size     = 1
    max_size     = 4
  }
  ami_type       = var.node_image_type
  instance_types = [var.node_size]
}</span></pre> <p><span class="koboSpan" id="kobo.395.1">Again, just like with the EKS cluster, the IAM role’s policy attachments are critical to making the node group functional. </span><span class="koboSpan" id="kobo.395.2">Therefore, you need to make sure that all policy attachments are attached to the IAM role before you start provisioning our node group. </span><span class="koboSpan" id="kobo.395.3">As we discussed in the previous section, there are four policies (, </span><strong class="source-inline"><span class="koboSpan" id="kobo.396.1">AmazonEKSWorkerNodePolicy</span></strong><span class="koboSpan" id="kobo.397.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">AmazonEKS_CNI_Policy</span></strong><span class="koboSpan" id="kobo.399.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">AmazonEC2ContainerRegistryReadOnly</span></strong><span class="koboSpan" id="kobo.401.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">CloudWatchAgentServerPolicy</span></strong><span class="koboSpan" id="kobo.403.1">) that are essential for the operation of EKS </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">worker nodes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.405.1">
  depends_on = [
    aws_iam_role_policy_attachment.eks_worker_node_policy,
    aws_iam_role_policy_attachment.eks_cni_policy,
    aws_iam_role_policy_attachment.eks_ecr_policy,
    aws_iam_role_policy_attachment.eks_cloudwatch_policy
  ]</span></pre> <p><span class="koboSpan" id="kobo.406.1">As you </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.407.1">add additional features to your EKS cluster, you may introduce additional IAM policies that grant the cluster and its worker nodes different permissions within AWS. </span><span class="koboSpan" id="kobo.407.2">When you do, don’t forget to also include these policies in these </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">depends_on</span></strong> <a id="_idTextAnchor427"/><span class="koboSpan" id="kobo.409.1">attributes to ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">smooth operations.</span></span></p>
<h3><span class="koboSpan" id="kobo.411.1">Logging and monitoring</span></h3>
<p><span class="koboSpan" id="kobo.412.1">We can</span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.413.1"> enable CloudWatch logging on the cluster by simply adding the </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">enabled_cluster_log_types</span></strong><span class="koboSpan" id="kobo.415.1"> attribute to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">aws_eks_cluster</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.417.1"> resource:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.418.1">
enabled_cluster_log_types = [“api”, “audit”]</span></pre> <p><span class="koboSpan" id="kobo.419.1">This attribute</span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.420.1"> takes one or more different log types. </span><span class="koboSpan" id="kobo.420.2">I’d recommend checking the documentation for all the different options supported. </span><span class="koboSpan" id="kobo.420.3">Next, we need to provision a CloudWatch log group for </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">the cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.422.1">
resource “aws_cloudwatch_log_group” “container_cluster” {
  name              = “/aws/eks/${local.cluster_name}/cluster”
  retention_in_days = 7
}</span></pre> <p><span class="koboSpan" id="kobo.423.1">This requires a specific naming convention and it needs to match the name you use for your cluster. </span><span class="koboSpan" id="kobo.423.2">Therefore, it’s a good idea to extract the value you pass to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">name</span></strong><span class="koboSpan" id="kobo.425.1"> attribute of</span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.426.1"> the </span><strong class="source-inline"><span class="koboSpan" id="kobo.427.1">aws_eks_cluster</span></strong><span class="koboSpan" id="kobo.428.1"> resource as a local vari</span><a id="_idTextAnchor428"/><span class="koboSpan" id="kobo.429.1">able so that you can use it in </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">two places.</span></span></p>
<h3><span class="koboSpan" id="kobo.431.1">Workload identity</span></h3>
<p><span class="koboSpan" id="kobo.432.1">With the </span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.433.1">cluster provisioned, we need to get the OIDC issuer certificate from the cluster so that we can use it to configure the OpenID Connect provider with AWS IAM. </span><span class="koboSpan" id="kobo.433.2">The following code uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">tls_certificate</span></strong><span class="koboSpan" id="kobo.435.1"> data source from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.436.1">tls</span></strong><span class="koboSpan" id="kobo.437.1"> utility provider, which we covered in </span><a href="B21183_03.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.438.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.439.1">, to obtain additional metadata about </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">the certificate:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.441.1">
data “tls_certificate” “container_cluster_oidc” {
  url = aws_eks_cluster.main.identity[0].oidc[0].issuer
}</span></pre> <p><span class="koboSpan" id="kobo.442.1">With this additional metadata, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">aws_iam_openid_connect_provider</span></strong><span class="koboSpan" id="kobo.444.1"> resource to connect the cluster to the AWS IAM OIDC provider by </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">referencing </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.446.1">sts.amazonaws.com</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.448.1">
resource “aws_iam_openid_connect_provider” “container_cluster_oidc” {
  client_id_list  = [“sts.amazonaws.com”]
  thumbprint_list = [data.tls_certificate.container_cluster_oidc.certificates[0].sha1_fingerprint]
  url             = data.tls_certificate.container_cluster_oidc.url
}</span></pre> <p><span class="koboSpan" id="kobo.449.1">We’ve already set up several IAM roles, including one for the EKS cluster and another for the worker nodes of the cluster. </span><span class="koboSpan" id="kobo.449.2">Therefore, I won’t reiterate the creation of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">aws_iam_role</span></strong><span class="koboSpan" id="kobo.451.1"> resource for the workload identity. </span><span class="koboSpan" id="kobo.451.2">However, this new role does need to have a very distinct assumption policy. </span><span class="koboSpan" id="kobo.451.3">The workload identity IAM role needs to reference the OIDC provider and a yet-to-be-provisioned Kubernetes </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">service account:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.453.1">
data “aws_iam_policy_document” “workload_identity_assume_role_policy” {
  statement {
    actions = [“sts:AssumeRoleWithWebIdentity”]
    effect  = “Allow”
    condition {
      test     = “StringEquals”
      variable = “${replace(aws_iam_openid_connect_provider.container_cluster_oidc.url, “https://”, “”)}:sub”
      values   = [“system:serviceaccount:${var.k8s_namespace}:${var.k8s_service_account_name}”]
    }
    principals {
      identifiers = [aws_iam_openid_connect_provider.container_cluster_oidc.arn]
      type        = “Federated”
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.454.1">As you</span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.455.1"> can see, in the preceding code, the service account follows a very specific naming convention: </span><strong class="source-inline"><span class="koboSpan" id="kobo.456.1">system:serviceaccount:&lt;namespace&gt;:&lt;service-account-name&gt;</span></strong><span class="koboSpan" id="kobo.457.1">. </span><span class="koboSpan" id="kobo.457.2">We replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.458.1">&lt;namespace&gt;</span></strong><span class="koboSpan" id="kobo.459.1"> with the name of the Kubernetes namespace and likewise, we replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.460.1">&lt;service-account-name&gt;</span></strong><span class="koboSpan" id="kobo.461.1"> with the name of the service account. </span><span class="koboSpan" id="kobo.461.2">It’s important to point out that we are referencing resources that do not exist yet. </span><span class="koboSpan" id="kobo.461.3">As such, the reference to them within the workload identity IAM role’s assumption policy is a pointer or a placeholder to this yet-to-be-created resource. </span><span class="koboSpan" id="kobo.461.4">Both the Kubernetes namespace and the </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.462.1">service account are resources that will need to be created within the Kubernetes control plane. </span><span class="koboSpan" id="kobo.462.2">We’ll tackle that in the next section using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">kubernetes</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.464.1">Terraform provider.</span></span></p>
<h3><span class="koboSpan" id="kobo.465.1">Secrets management</span></h3>
<p><span class="koboSpan" id="kobo.466.1">Now </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.467.1">that we have an IAM role for our workload identity, we simply need to grant it access to the AWS resources we want it to use. </span><span class="koboSpan" id="kobo.467.2">Therefore, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">aws_iam_policy_document</span></strong><span class="koboSpan" id="kobo.469.1"> data source once more to generate an IAM policy that we will attach to the workload identity’s IAM role. </span><span class="koboSpan" id="kobo.469.2">This is where we have the opportunity to grant it access to any resource in AWS that our application code will need. </span><span class="koboSpan" id="kobo.469.3">For our solution, we’ll start with access to AWS Secrets Manager secrets by granting it access to read secrets using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.470.1">secretsmanager:GetSecretValue</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.471.1"> action:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.472.1">
data “aws_iam_policy_document” “workload_identity_policy” {
  statement {
    effect = “Allow”
    actions = [
      “secretsmanager:GetSecretValue”,
      “secretsmanager:DescribeSecret”,
    ]
    resources = [
      “arn:aws:secretsmanager:${var.primary_region}:${data.aws_caller_identity.current.account_id}:secret:*”,
    ]
  }
}</span></pre> <p><span class="koboSpan" id="kobo.473.1">This policy will grant the IAM role access to the secrets within this account. </span><span class="koboSpan" id="kobo.473.2">We could further</span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.474.1"> refine its access by enhancing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">*</span></strong><span class="koboSpan" id="kobo.476.1"> wildcard path to ensure that it has access to only certain secrets. </span><span class="koboSpan" id="kobo.476.2">This can be done by implementing a naming convention that uses a unique prefix for your secrets. </span><span class="koboSpan" id="kobo.476.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">application_name</span></strong><span class="koboSpan" id="kobo.478.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1">environment_name</span></strong><span class="koboSpan" id="kobo.480.1"> variables are a perfect way to implement this naming convention and to tighten access to your Kubernetes workloads to AWS </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">Secrets Manager.</span></span></p>
<p><span class="koboSpan" id="kobo.482.1">Now, we just need to provision secrets to Secrets Manager with the right </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">naming convention:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.484.1">
resource “aws_secretsmanager_secret” “database_connection_string” {
  name        = “${var.application_name}-${var.environment_name}-connection-string”
  description = “Database connection string”
}</span></pre> <p><span class="koboSpan" id="kobo.485.1">AWS Secrets Manager uses a parent resource called </span><strong class="source-inline"><span class="koboSpan" id="kobo.486.1">aws_secretsmanager_secret</span></strong><span class="koboSpan" id="kobo.487.1"> as a logical placeholder for the secret itself but recognizes that the secret’s value might change </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">over time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.489.1">
resource “aws_secretsmanager_secret_version” “database_connection_string” {
  secret_id     = aws_secretsmanager_secret.database_connection_string.id
  secret_string = random_password.database_connection_string.result
}</span></pre> <p><span class="koboSpan" id="kobo.490.1">Those different values for the secret are stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">aws_secretsmanager_secret_version</span></strong><span class="koboSpan" id="kobo.492.1"> resources. </span><span class="koboSpan" id="kobo.492.2">You can generate complex secrets using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">random</span></strong><span class="koboSpan" id="kobo.494.1"> provider but it’s probably more common to obtain </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">sec</span><a id="_idTextAnchor429"/><span class="koboSpan" id="kobo.496.1">ret_string</span></strong><span class="koboSpan" id="kobo.497.1"> from the outputs of </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">other resources.</span></span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor430"/><span class="koboSpan" id="kobo.499.1">Kubernetes</span></h2>
<p><span class="koboSpan" id="kobo.500.1">In </span><a href="B21183_05.xhtml#_idTextAnchor278"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.501.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.502.1">, we </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.503.1">introduced Kubernetes </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.504.1">architecture and automation techniques using YAML and </span><strong class="bold"><span class="koboSpan" id="kobo.505.1">HashiCorp Configuration Language</span></strong><span class="koboSpan" id="kobo.506.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.507.1">HCL</span></strong><span class="koboSpan" id="kobo.508.1">). </span><span class="koboSpan" id="kobo.508.2">In our </span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.509.1">solutions in this book, we will be using the Terraform provider for Kubernetes to automate our application’s deployment. </span><span class="koboSpan" id="kobo.509.2">This allows us to both parameterize the Kubernetes configurations that would otherwise be trapped in hard-coded YAML files and provision a combination of Kubernetes primitives</span><a id="_idTextAnchor431"/><span class="koboSpan" id="kobo.510.1"> and Helm charts with the same </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">deployment process.</span></span></p>
<h3><span class="koboSpan" id="kobo.512.1">Provider setup</span></h3>
<p><span class="koboSpan" id="kobo.513.1">Ironically, the</span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.514.1"> first thing we need to do to set up the </span><strong class="source-inline"><span class="koboSpan" id="kobo.515.1">kubernetes</span></strong><span class="koboSpan" id="kobo.516.1"> provider is initialize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.517.1">aws</span></strong><span class="koboSpan" id="kobo.518.1"> provider so that we can get information about our EKS cluster. </span><span class="koboSpan" id="kobo.518.2">We can do that using the data sources provided and a single input variable: the cluster’s name. </span><span class="koboSpan" id="kobo.518.3">Of course, the AWS region is also an implied parameter to this operation but it is part of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.519.1">aws</span></strong><span class="koboSpan" id="kobo.520.1"> provider configuration rather than inputs to the data </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">sources themselves:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.522.1">
data “aws_eks_cluster” “cluster” {
  name = var.eks_cluster_name
}</span></pre> <p><span class="koboSpan" id="kobo.523.1">We’ll use both the </span><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">aws_eks_cluster</span></strong><span class="koboSpan" id="kobo.525.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.526.1">aws_eks_cluster_auth</span></strong><span class="koboSpan" id="kobo.527.1"> data sources to grab the data we need to initialize the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">kubernetes</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.529.1"> provider:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.530.1">
provider “kubernetes” {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
  token                  = data.aws_eks_cluster_auth.cluster.token
  load_config_file       = false
}</span></pre> <p><span class="koboSpan" id="kobo.531.1">Interestingly, the</span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.532.1"> Helm provider setup is pretty much identical to the Kubernetes provider configuration. </span><span class="koboSpan" id="kobo.532.2">It seems a bit redundant, but it’s </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">relatively straightforward:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.534.1">
provider “helm” {
  kubernetes {
    host                   = data.aws_eks_cluster.main.endpoint
    cluster_ca_certificate = base64decode(data.aws_eks_cluster.main.certificate_authority[0].data)
    token          </span><a id="_idTextAnchor432"/><span class="koboSpan" id="kobo.535.1">        = data.aws_eks_cluster_auth.main.token
  }
}</span></pre> <h3><span class="koboSpan" id="kobo.536.1">Namespace</span></h3>
<p><span class="koboSpan" id="kobo.537.1">Creating the</span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.538.1"> Kubernetes namespace is </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">extremely simple:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.540.1">
resource “kubernetes_namespace” “main” {
  metadata {
    name = var.k8s_namespace
    labels = {
      name = var.k8s_namespace
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.541.1">This will act as the logical container for all of the Kubern</span><a id="_idTextAnchor433"/><span class="koboSpan" id="kobo.542.1">etes resources that we provision for </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">our application.</span></span></p>
<h3><span class="koboSpan" id="kobo.544.1">Service account</span></h3>
<p><span class="koboSpan" id="kobo.545.1">In the</span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.546.1"> previous section, we built one-half of this bridge when we set up the OpenID Connect provider configuration within AWS and we specified the Kubernetes namespace and service account name ahead of time. </span><span class="koboSpan" id="kobo.546.2">Now, we’ll finish constructing this bridge by provisioning </span><strong class="source-inline"><span class="koboSpan" id="kobo.547.1">kubernetes_service_account</span></strong><span class="koboSpan" id="kobo.548.1"> and ensuring that </span><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">namespace</span></strong><span class="koboSpan" id="kobo.550.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.551.1">name</span></strong><span class="koboSpan" id="kobo.552.1"> match our </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">AWS configuration:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.554.1">
resource “kubernetes_service_account” “workload_identity” {
  metadata {
    name      = var.k8s_service_account_name
    namespace = var.k8s_namespace
    annotations = {
      “eks.amazonaws.com/role-arn” = var.workload_identity_role
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.555.1">We also need to add an annotation that references the unique identifier (or ARN) for the workload identity’s IAM role. </span><span class="koboSpan" id="kobo.555.2">We can set this up as an output variable in our Terraform workspace that provisions the AWS infrastructure and routes its value to an input variable on the Terraform workspace for our Kubernetes configuration. </span><span class="koboSpan" id="kobo.555.3">This is a great example of how the </span><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">kubernetes</span></strong><span class="koboSpan" id="kobo.557.1"> provider for Terraform can be a useful way of configuring Kubernetes resource</span><a id="_idTextAnchor434"/><span class="koboSpan" id="kobo.558.1">s that require tight coupling with the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">cloud platform.</span></span></p>
<h3><span class="koboSpan" id="kobo.560.1">Secrets store CSI driver</span></h3>
<p><span class="koboSpan" id="kobo.561.1">With the service account set up, our application is one step closer to being able to access our </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.562.1">secrets in AWS Secrets Manager. </span><span class="koboSpan" id="kobo.562.2">However, before we can do that, we need to set up the secrets store CSI driver. </span><span class="koboSpan" id="kobo.562.3">As we discussed previously, this is a common Kubernetes component that provides a standard mechanism for using volume mounts as a way to distribute remotely managed secrets to workloads running in Kubernetes. </span><span class="koboSpan" id="kobo.562.4">The driver is extremely flexible and can be extended through providers that act as adapters for different external secret </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">management systems.</span></span></p>
<p><span class="koboSpan" id="kobo.564.1">First, we need to install the secrets store CSI driver </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">Helm chart:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.566.1">
resource “helm_release” “csi_secrets_store” {
  name       = “csi-secrets-store”
  repository = “https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts”
  chart      = “secrets-store-csi-driver”
  namespace  = “kube-system”
  set {
    name  = “syncSecret.enabled”
    value = “true”
  }
}</span></pre> <p><span class="koboSpan" id="kobo.567.1">We can optionally enable secret synchronization by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">syncSecret.enabled</span></strong><span class="koboSpan" id="kobo.569.1"> attribute to make the secrets accessible from Kubernetes secrets. </span><span class="koboSpan" id="kobo.569.2">This makes it extremely convenient to inject the secrets into our application’s pods without customized code to retrieve them from the </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">mounted volume.</span></span></p>
<p><span class="koboSpan" id="kobo.571.1">Next, we need to install the AWS provider for the </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">CSI driver:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.573.1">
resource “helm_release” “aws_secrets_provider” {
  name       = “secrets-provider-aws”
  repository = “https://aws.github.io/secrets-store-csi-driver-provider-aws”
  chart      = “secrets-store-csi-driver-provider-aws”
  namespace  = “kube-system”
}</span></pre> <p><span class="koboSpan" id="kobo.574.1">Both of </span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.575.1">these Helm charts provision several different Kubernetes resources to your cluster under the </span><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">kube-system</span></strong><span class="koboSpan" id="kobo.577.1"> namespace. </span><span class="koboSpan" id="kobo.577.2">If you encounter errors, interrogating the pods hosting these components is a g</span><a id="_idTextAnchor435"/><span class="koboSpan" id="kobo.578.1">ood place for you to start debugging </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">your configuration.</span></span></p>
<h3><span class="koboSpan" id="kobo.580.1">Secret provider class</span></h3>
<p><span class="koboSpan" id="kobo.581.1">Once we’ve</span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.582.1"> installed both the CSI driver and its AWS provider, we are ready to connect to AWS Secrets Manager. </span><span class="koboSpan" id="kobo.582.2">So far, we have only enabled this ability; we haven’t exercised it by </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">accessing secrets.</span></span></p>
<p><span class="koboSpan" id="kobo.584.1">That’s what the </span><strong class="source-inline"><span class="koboSpan" id="kobo.585.1">SecretProviderClass</span></strong><span class="koboSpan" id="kobo.586.1"> resource is for. </span><span class="koboSpan" id="kobo.586.2">It connects to a specific set of secrets within AWS Secrets Manager. </span><span class="koboSpan" id="kobo.586.3">You’ll notice that the way this type of resource is provisioned is different than other resources in Kubernetes. </span><span class="koboSpan" id="kobo.586.4">While other resource types have a corresponding Terraform resource, </span><strong class="source-inline"><span class="koboSpan" id="kobo.587.1">SecretProviderClass</span></strong><span class="koboSpan" id="kobo.588.1"> uses a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">kubernetes_manifest</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.590.1"> resource.</span></span></p>
<p><span class="koboSpan" id="kobo.591.1">That’s because this resource type is managed through a Kubernetes </span><strong class="bold"><span class="koboSpan" id="kobo.592.1">custom resource definition</span></strong><span class="koboSpan" id="kobo.593.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.594.1">CRD</span></strong><span class="koboSpan" id="kobo.595.1">); it’s </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.596.1">not a built-in type </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">within Kubernetes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.598.1">
resource “kubernetes_manifest” “secret_provider_class” {
  manifest = {
    apiVersion = “secrets-store.csi.x-k8s.io/v1”
    kind       = “SecretProviderClass”
    metadata = {
      name      = “${var.application_name}-${var.environment_name}-secret-provider-class”
      namespace = var.k8s_namespace
    }
    spec = {
      provider = “aws”
      parameters = {
        objects = yamlencode([ ... </span><span class="koboSpan" id="kobo.598.2">])
      }
      secretObjects = [ ... </span><span class="koboSpan" id="kobo.598.3">]
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.599.1">The</span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.600.1"> structure of </span><strong class="source-inline"><span class="koboSpan" id="kobo.601.1">SecretProviderClass</span></strong><span class="koboSpan" id="kobo.602.1"> has two parts. </span><span class="koboSpan" id="kobo.602.2">First, </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">parameters</span></strong><span class="koboSpan" id="kobo.604.1"> is where we declare what secrets we want to </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">bring in:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.606.1">
{
  objectName         = “fleet-portal-dev-connection-string”
  objectType         = “secretsmanager”
  objectVersionLabel = “AWSCURRENT”
}</span></pre> <p><span class="koboSpan" id="kobo.607.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.608.1">objectName</span></strong><span class="koboSpan" id="kobo.609.1"> corresponds to either the relative name of the Secrets Manager secret or a fully qualified ARN for the secret. </span><span class="koboSpan" id="kobo.609.2">Next, </span><strong class="source-inline"><span class="koboSpan" id="kobo.610.1">objectType</span></strong><span class="koboSpan" id="kobo.611.1"> indicates what CSI driver provider should be used to access the secret, while </span><strong class="source-inline"><span class="koboSpan" id="kobo.612.1">objectVersionLabel</span></strong><span class="koboSpan" id="kobo.613.1"> allows us to select a specific version of the secret within Secrets Manager. </span><span class="koboSpan" id="kobo.613.2">For AWS, to access the latest version (probably the most common use case), you need to specify </span><strong class="source-inline"><span class="koboSpan" id="kobo.614.1">AWSCURRENT</span></strong><span class="koboSpan" id="kobo.615.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">the value.</span></span></p>
<p><span class="koboSpan" id="kobo.617.1">Next, there is a collection of </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">secretObjects</span></strong><span class="koboSpan" id="kobo.619.1"> that’s used to define corresponding Kubernetes </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">secret objects:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.621.1">
{
  data = [
    {
      key        = “fleet-portal-dev-connection-string”
      objectName = “fleet-portal-dev-connection-string”
    }
  ]
  secretName = “fleet-portal-dev-connection-string”
  type       = “Opaque”
}</span></pre> <p><span class="koboSpan" id="kobo.622.1">These </span><strong class="source-inline"><span class="koboSpan" id="kobo.623.1">secretObjects</span></strong><span class="koboSpan" id="kobo.624.1"> will later be used in the deployment specification of our application</span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.625.1"> to c</span><a id="_idTextAnchor436"/><span class="koboSpan" id="kobo.626.1">reate environment variables for each secret within </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">the pods.</span></span></p>
<h3><span class="koboSpan" id="kobo.628.1">Deployment</span></h3>
<p><span class="koboSpan" id="kobo.629.1">The Kubernetes </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.630.1">deployment is one of the most significant resources that we have to provision within Kubernetes. </span><span class="koboSpan" id="kobo.630.2">As a result, it can be rather intimidating as there are several rather complex nested sections. </span><span class="koboSpan" id="kobo.630.3">The most important thing going on in the deployment is the container specification. </span><span class="koboSpan" id="kobo.630.4">This sets up the actual runtime environment for </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">our pods.</span></span></p>
<p><span class="koboSpan" id="kobo.632.1">The most important piece of information is the container image we want to use in our pods. </span><span class="koboSpan" id="kobo.632.2">To configure this, we need to construct the fully qualified path to the container image stored in our ECR. </span><span class="koboSpan" id="kobo.632.3">To do that, we need two pieces of information. </span><span class="koboSpan" id="kobo.632.4">First, we need the AWS account number and second, we need the AWS region name where our ECR repository is </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">provisioned to:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.634.1">
locals {
  account_id         = data.aws_caller_identity.current.account_id
  container_registry = “${local.account_id}.dkr.ecr.${var.primary_region}.amazonaws.com/”
}</span></pre> <p><span class="koboSpan" id="kobo.635.1">The</span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.636.1"> AWS account number can easily be obtained from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.637.1">aws_caller_identity</span></strong><span class="koboSpan" id="kobo.638.1"> data source. </span><span class="koboSpan" id="kobo.638.2">This is an extremely simple data source that provides contextual information about the AWS account and IAM identity that Terraform is using with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">aws</span></strong><span class="koboSpan" id="kobo.640.1"> provider. </span><span class="koboSpan" id="kobo.640.2">As a result, to create this data source, you simply create it without </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">any parameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.642.1">
data “aws_caller_identity” “current” {}</span></pre> <p><span class="koboSpan" id="kobo.643.1">This is a common pattern for accessing Terraform provider authentication context and cloud platform provisioning scope – in this case, what AWS account and what region we are </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">provisioning to.</span></span></p>
<p><span class="koboSpan" id="kobo.645.1">Here is the version of the same YAML code converted into HCL using an input variable to set different attributes on </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">the entity:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.647.1">
resource “kubernetes_deployment” “web_app” {
  metadata {
    name      = local.web_app_name
    namespace = var.k8s_namespace
  }
  spec {
    replicas = 3
    selector {
      match_labels = {
        app = local.web_app_name
      }
    }
    template {
      metadata {
        labels = {
          app = local.web_app_name
        }
      }
      spec {
        service_account_name = kubernetes_service_account.workload_identity.metadata[0].name
        container {
          image = local.web_app_image_name
          name  = local.web_app_name
          port {
            container_port = 5000
          }
          env_from {
            config_map_ref {
              name = kubernetes_config_map.web_app.metadata.0.name
            }
          }
        }
      }
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.648.1">The</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.649.1"> local variable we use for the container image name is the fully qualified path to our container image within ECR. </span><span class="koboSpan" id="kobo.649.2">It follows the </span><strong class="source-inline"><span class="koboSpan" id="kobo.650.1">&lt;account&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository&gt;:&lt;tag&gt;</span></strong><span class="koboSpan" id="kobo.651.1"> structure. </span><span class="koboSpan" id="kobo.651.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">&lt;account&gt;</span></strong><span class="koboSpan" id="kobo.653.1"> is the AWS account number, which can be accessed using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">aws_caller_identity</span></strong><span class="koboSpan" id="kobo.655.1"> data source. </span><span class="koboSpan" id="kobo.655.2">Then, </span><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">&lt;region&gt;</span></strong><span class="koboSpan" id="kobo.657.1"> is the AWS region, which is accessible from the input variables. </span><span class="koboSpan" id="kobo.657.2">Finally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.658.1">&lt;repository&gt;</span></strong><span class="koboSpan" id="kobo.659.1"> is the ECR repository name and </span><strong class="source-inline"><span class="koboSpan" id="kobo.660.1">&lt;version&gt;</span></strong><span class="koboSpan" id="kobo.661.1"> is the tag for the specific version of the </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">container image.</span></span></p>
<p><span class="koboSpan" id="kobo.663.1">We can set </span><strong class="source-inline"><span class="koboSpan" id="kobo.664.1">service_account_name</span></strong><span class="koboSpan" id="kobo.665.1"> by referencing other Kubernetes resources provisioned within this Terraform workspace. </span><span class="koboSpan" id="kobo.665.2">This is a key difference between using YAML and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">kubernetes</span></strong><span class="koboSpan" id="kobo.667.1"> provider for Terraform. </span><span class="koboSpan" id="kobo.667.2">If we were using YAML this, would have to be hard-coded, whereas with HCL, we can reference other resources within the </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">Terraform workspace.</span></span></p>
<p><span class="koboSpan" id="kobo.669.1">To reference an AWS Secrets Manager secret, we need to modify the </span><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">container</span></strong><span class="koboSpan" id="kobo.671.1"> block so that it includes another </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.672.1">env</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.673.1"> block:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.674.1">
env {
  name = “DB_CONNECTION_STRING”
  value_from {
    secret_key_ref {
      name = “fleet-portal-dev-connection-string”
      key  = “fleet-portal-dev-connection-string”
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.675.1">This </span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.676.1">allows us to reference one of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">secretObjects</span></strong><span class="koboSpan" id="kobo.678.1"> objects we declared within </span><strong class="source-inline"><span class="koboSpan" id="kobo.679.1">SecretProviderClass</span></strong><span class="koboSpan" id="kobo.680.1"> and give it an environment variable na</span><a id="_idTextAnchor437"/><span class="koboSpan" id="kobo.681.1">me that our application code can reference to access </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">the secret.</span></span></p>
<h3><span class="koboSpan" id="kobo.683.1">Service</span></h3>
<p><span class="koboSpan" id="kobo.684.1">The </span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.685.1">Kubernetes service is primarily a network routing mechanism. </span><span class="koboSpan" id="kobo.685.2">It defines the port on which the service should be exposed to external clients and what port the network traffic should be forwarded to on </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">the pods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.687.1">
resource “kubernetes_service” “web_app” {
  metadata {
    name      = “${local.web_app_name}-service”
    namespace = var.k8s_namespace
  }
  spec {
    type = “ClusterIP”
    port {
      port        = 80
      target_port = 5000
    }
    selector = {
      app = local.web_app_name
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.688.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.689.1">selector</span></strong><span class="koboSpan" id="kobo.690.1"> specifies </span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.691.1">which pods traffic should be forwarded to and it should match the corresponding pods, w</span><a id="_idTextAnchor438"/><span class="koboSpan" id="kobo.692.1">ith the </span><strong class="source-inline"><span class="koboSpan" id="kobo.693.1">app</span></strong><span class="koboSpan" id="kobo.694.1"> label set to the same value as the </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">service’s selector.</span></span></p>
<h3><span class="koboSpan" id="kobo.696.1">ConfigMap</span></h3>
<p><span class="koboSpan" id="kobo.697.1">As we </span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.698.1">know from </span><a href="B21183_05.xhtml#_idTextAnchor278"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.699.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.700.1">, the ConfigMap resource is a great way to pass non-sensitive configuration settings to </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">your pods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.702.1">
resource “kubernetes_config_map” “web_app” {
  metadata {
    name      = “${local.web_app_name}-config”
    namespace = var.k8s_namespace
  }
  data = {
    BackendEndpoint = “”
  }
}</span></pre> <p><span class="koboSpan" id="kobo.703.1">Often, the Terraform workspace that provisions the infrastructure will output several different values that need to be i</span><a id="_idTextAnchor439"/><span class="koboSpan" id="kobo.704.1">ncluded in a Kubernetes ConfigMap (URIs, AWS ARNs, DNS, and </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">so on).</span></span></p>
<h3><span class="koboSpan" id="kobo.706.1">Ingress</span></h3>
<p><span class="koboSpan" id="kobo.707.1">The </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.708.1">ingress controller is a component of Kubernetes that routes external network traffic into the cluster. </span><span class="koboSpan" id="kobo.708.2">It works in conjunction with a Kubernetes ingress, which defines specific rules that route traffic for specific services. </span><span class="koboSpan" id="kobo.708.3">This is very similar to the structure of the CSI driver and </span><strong class="source-inline"><span class="koboSpan" id="kobo.709.1">SecretProviderClass</span></strong><span class="koboSpan" id="kobo.710.1">. </span><span class="koboSpan" id="kobo.710.2">One provides the foundational subsystem, thus enabling the capability, while the other implements a specific configuration using that </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">underlying subsystem.</span></span></p>
<p><span class="koboSpan" id="kobo.712.1">One of </span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.713.1">the most popular ingress controllers is a load balancer called NGINX. </span><span class="koboSpan" id="kobo.713.2">We can set up the NGINX ingress controller using a Helm chart. </span><span class="koboSpan" id="kobo.713.3">The components that are deployed by this Helm chart are why we needed an additional IAM policy that allows our EKS cluster to configure AWS ELB resources. </span><span class="koboSpan" id="kobo.713.4">That’s because the Kubernetes configurations of the ingress controller and ingress resources will be interpreted by EKS and manifested as the provisioning and configuration of AWS ELB resources. </span><span class="koboSpan" id="kobo.713.5">This means that instead of explicitly configuring ELB resources using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.714.1">aws</span></strong><span class="koboSpan" id="kobo.715.1"> Terraform provider, you will be annotating Kubernetes deployments and the necessary ELB resources will be provisioned and configured on </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">your behalf.</span></span></p>
<p><span class="koboSpan" id="kobo.717.1">The first thing we need to do is install the NGINX ingress controller using a </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">Helm chart:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.719.1">
resource “helm_release” “ingress” {
  name       = “ingress”
  repository = “https://charts.bitnami.com/bitnami”
  chart      = “nginx-ingress-controller”
  create_namespace = true
  namespace        = “ingress-nginx”
  set {
    name  = “service.type”
    value = “LoadBalancer”
  }
  set {
    name  = “service.annotations”
    value = “service.beta.kubernetes.io/aws-load-balancer-type: nlb”
  }
}</span></pre> <p><span class="koboSpan" id="kobo.720.1">This will</span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.721.1"> install NGINX and deploy a Kubernetes service for NGINX running under the namespace we specified. </span><span class="koboSpan" id="kobo.721.2">The next step is to configure an ingress for </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">our application:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.723.1">
resource “kubernetes_ingress_v1” “ingress” {
  metadata {
    name      = “${local.web_app_name}-ingress”
    namespace = var.k8s_namespace
    annotations = {
      “kubernetes.io/ingress.class” = “nginx”
    }
  }
  spec {
    rule {
      http { ... </span><span class="koboSpan" id="kobo.723.2">}
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.724.1">An ingress resource is pretty simple. </span><span class="koboSpan" id="kobo.724.2">You need to set the namespace and specify what ingress controller you want to use. </span><span class="koboSpan" id="kobo.724.3">Then, you need to specify paths so that you can route network traffic to the correct </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">Kubernetes services:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.726.1">
path {
  path      = “/”
  path_type = “Prefix”
  backend {
    service {
      name = kubernetes_service.web_app.metadata[0].name
      port {
        number = 80
      }
    }
  }
} </span></pre> <p><span class="koboSpan" id="kobo.727.1">It’s also</span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.728.1"> pretty important to establish explicit </span><strong class="source-inline"><span class="koboSpan" id="kobo.729.1">depends_on</span></strong><span class="koboSpan" id="kobo.730.1"> statements for the Kubernetes services for the frontend and backend application deployments as well as the ingress controller since we don’t reference it directly within the </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">HCL configuration:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.732.1">
depends_on = [
  kubernetes_service.web_app,
  kubernetes_service.web_api,
  helm_release.ingress
]</span></pre> <p><span class="koboSpan" id="kobo.733.1">Now that we’ve built out the three components of our architecture, in the next section, we’ll move on to how we can automate the deployment using Docker to build and publish the container images and then Terraform to provision our infrastructure and deploy our solution </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">to Kubernetes.</span></span></p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor440"/><span class="koboSpan" id="kobo.735.1">Automating the deployment</span></h1>
<p><span class="koboSpan" id="kobo.736.1">In this</span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.737.1"> section, we’ll shift our focus from building our application and its environment to implementing deployment </span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.738.1">automations to efficiently provision our solution to AWS. </span><span class="koboSpan" id="kobo.738.2">Container-based architectures involve three core deployment motions. </span><span class="koboSpan" id="kobo.738.3">First, we must create and publish container images to a container registry. </span><span class="koboSpan" id="kobo.738.4">Next, we must provision the Kubernetes cluster environment where containers will be hosted. </span><span class="koboSpan" id="kobo.738.5">Finally, we must deploy the Kubernetes resources that will create the containers within Kubernetes pods and reference the container images </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">we published.</span></span></p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor441"/><span class="koboSpan" id="kobo.740.1">Docker</span></h2>
<p><span class="koboSpan" id="kobo.741.1">Like </span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.742.1">the VM image that we built with Packer in the previous chapter, the container image acts as an immutable artifact that contains a versioned copy of the application code and operating system configuration. </span><span class="koboSpan" id="kobo.742.2">We need to update this artifact every time something changes in either the application code or the operating </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">system configuration:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.744.1">
on:
  push:
    branches: 
    - main
    paths:
    - ‘src/dotnet/frontend/**’</span></pre> <p><span class="koboSpan" id="kobo.745.1">Just like with Packer, we need to trigger a new container image to be built every time the application code and the operating system are configured within the Dockerfile itself. </span><span class="koboSpan" id="kobo.745.2">With GitHub Actions, we can add a list of </span><strong class="source-inline"><span class="koboSpan" id="kobo.746.1">paths</span></strong><span class="koboSpan" id="kobo.747.1"> that will trigger </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">our workflow:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<span class="koboSpan" id="kobo.749.1"><img alt="Figure 8.13 – VM image versioning" src="image/Image97144.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.750.1">Figure 8.13 – VM image versioning</span></p>
<p><span class="koboSpan" id="kobo.751.1">Now that we have the triggers and some variables set for our workflow, we need to structure </span><strong class="source-inline"><span class="koboSpan" id="kobo.752.1">jobs</span></strong><span class="koboSpan" id="kobo.753.1">. </span><span class="koboSpan" id="kobo.753.2">For </span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.754.1">each Packer template, we will have two jobs: one that builds the C# .NET application code and produces a deployment package and another that runs </span><strong class="source-inline"><span class="koboSpan" id="kobo.755.1">packer build</span></strong><span class="koboSpan" id="kobo.756.1"> to produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">VM image:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.758.1">
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      ...
  </span><span class="koboSpan" id="kobo.758.2">packer:
    runs-on: ubuntu-latest
    steps:
      ...</span></pre> <p><span class="koboSpan" id="kobo.759.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.760.1">build</span></strong><span class="koboSpan" id="kobo.761.1"> job performs a pretty standard .NET build process, which includes restoring package dependencies from NuGet (the .NET package manager), building the code, running unit and integration tests, publishing a deployable artifact, and storing that artifact so that it can be used by future jobs within </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">the pipeline:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<span class="koboSpan" id="kobo.763.1"><img alt="Figure 8.14 – Docker workflow" src="image/B21183_08_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.764.1">Figure 8.14 – Docker workflow</span></p>
<p><span class="koboSpan" id="kobo.765.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.766.1">docker</span></strong><span class="koboSpan" id="kobo.767.1"> job immediately runs Terraform to obtain outputs of the ECR container repository that we </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.768.1">want to target. </span><span class="koboSpan" id="kobo.768.2">We don’t have to run Terraform here but we could explicitly specify the ECR repository’s fully </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">qualified path.</span></span></p>
<p><span class="koboSpan" id="kobo.770.1">Then, it generates a unique version of the name for the container image that will be produced if successful. </span><span class="koboSpan" id="kobo.770.2">We’ll generate this image version based on the current date and the GitHub Action’s run number. </span><span class="koboSpan" id="kobo.770.3">This will guarantee that the image version is unique so that we don’t have to manually set it or worry about conflicts when pushing to </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">the repository:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.772.1">
    - id: image-version
      name: Generate Version Number
      run: |
        echo “version=$(date +’%Y.%m’).${{ github.run_number }}” &gt;&gt; “$GITHUB_OUTPUT”</span></pre> <p><span class="koboSpan" id="kobo.773.1">Next, we need to set </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">up Docker:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.775.1">
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1</span></pre> <p><span class="koboSpan" id="kobo.776.1">Now, we must configure our AWS credentials using an official AWS GitHub Action. </span><span class="koboSpan" id="kobo.776.2">We’ll use an AWS access key and secret access key specified by the GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">environment settings:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.778.1">
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ vars.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ vars.PRIMARY_REGION }}</span></pre> <p><span class="koboSpan" id="kobo.779.1">Once </span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.780.1">the credential has been configured, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">amazon-ecr-login</span></strong><span class="koboSpan" id="kobo.782.1"> action to connect </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">to ECR:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.784.1">
    - name: Log in to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2</span></pre> <p><span class="koboSpan" id="kobo.785.1">Finally, we’ll build and push the image using an official Docker GitHub Action. </span><span class="koboSpan" id="kobo.785.2">It’s important to note that this action is not specific to AWS. </span><span class="koboSpan" id="kobo.785.3">It uses standard container registry protocols to communicate with ECR using the fully qualified path to the ECR repository that we specify in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.786.1">tags</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.787.1"> parameter:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.788.1">
    - name: Build and push Docker image to ACR
      uses: docker/build-push-action@v5
      with:
        context: ${{ env.DOCKER_WORKING_DIRECTORY }}
        push: true
        tags: ${{ steps.terraform.outputs.registry_endpoint }}:${{ steps.image-version.outputs.version }}</span></pre> <p><span class="koboSpan" id="kobo.789.1">Both of our application components (the frontend and the backend) will have a repository, so the registry endpoint will be different depending on which container image </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">we’re pushing.</span></span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor442"/><span class="koboSpan" id="kobo.791.1">Terraform</span></h2>
<p><span class="koboSpan" id="kobo.792.1">In </span><a href="B21183_07.xhtml#_idTextAnchor365"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.793.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.794.1">, we </span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.795.1">comprehensively covered the process of creating a Terraform GitHub Action that authenticates with AWS. </span><span class="koboSpan" id="kobo.795.2">Therefore, we won’t be delving into it any further. </span><span class="koboSpan" id="kobo.795.3">I encourage you to refer back to </span><a href="B21183_07.xhtml#_idTextAnchor365"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.796.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.797.1"> to review </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">the process.</span></span></p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor443"/><span class="koboSpan" id="kobo.799.1">Kubernetes</span></h2>
<p><span class="koboSpan" id="kobo.800.1">When we automate </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.801.1">Kubernetes with Terraform, we are just running </span><strong class="source-inline"><span class="koboSpan" id="kobo.802.1">terraform apply</span></strong><span class="koboSpan" id="kobo.803.1"> again with a different root module. </span><span class="koboSpan" id="kobo.803.2">This time, the root module will configure the </span><strong class="source-inline"><span class="koboSpan" id="kobo.804.1">kubernetes</span></strong><span class="koboSpan" id="kobo.805.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.806.1">helm</span></strong><span class="koboSpan" id="kobo.807.1"> providers in addition to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.808.1">aws</span></strong><span class="koboSpan" id="kobo.809.1"> provider. </span><span class="koboSpan" id="kobo.809.2">However, we won’t create new resources with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.810.1">aws</span></strong><span class="koboSpan" id="kobo.811.1"> provider; we will only obtain data sources from existing resources we provisioned in the previous </span><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">terraform apply</span></strong><span class="koboSpan" id="kobo.813.1"> command that provisioned the infrastructure </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">to AWS.</span></span></p>
<p><span class="koboSpan" id="kobo.815.1">As a result, the GitHub Action that executes this process will look strikingly similar to how we executed Terraform with AWS. </span><span class="koboSpan" id="kobo.815.2">Some of the variables might change t</span><a id="_idTextAnchor444"/><span class="koboSpan" id="kobo.816.1">o include things such as the container image details and </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">cluster information.</span></span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor445"/><span class="koboSpan" id="kobo.818.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.819.1">In this chapter, we designed, built, and automated the deployment of a complete and end-to-end solution using container-based architecture. </span><span class="koboSpan" id="kobo.819.2">We built onto the foundations from </span><a href="B21183_07.xhtml#_idTextAnchor365"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.820.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.821.1">, where we worked with the foundational infrastructure of AWS VPCs but layered on AWS EKS to host our application in containers. </span><span class="koboSpan" id="kobo.821.2">In the next and final step in our AWS journey, we’ll be looking at serverless architecture, moving beyond the underlying infrastructure, and letting the platform itself take our solution to </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">new heights.</span></span></p>
</div>
</body></html>