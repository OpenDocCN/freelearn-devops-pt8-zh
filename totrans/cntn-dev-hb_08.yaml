- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Applications with the Kubernetes Orchestrator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing containers for your applications on your workstation or laptop really
    improves your development process by running other applications’ components while
    you focus on your own code. This simple standalone architecture works perfectly
    in your development stage, but it does not provide **high availability** (**HA**)
    for your applications. Deploying container orchestrators cluster-wide will help
    you to constantly keep your applications running healthy. In the previous chapter,
    we briefly reviewed Docker Swarm, which is simpler and can be a good introductory
    platform before moving on to more complex orchestrators. In this chapter, we will
    learn how to prepare and run our applications on top of **Kubernetes**, which
    is considered a standard nowadays for running containers cluster-wide.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the main features of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Kubernetes’ HA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with Kubernetes using `kubectl`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a functional Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Pods and Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying orchestrated resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving your applications’ security with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the labs for this chapter at https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter8,
    where you will find some extended explanations, omitted in the chapter’s content
    to make it easier to follow. The *Code In Action* video for this chapter can be
    found at [https://packt.link/JdOIY](https://packt.link/JdOIY).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s start this chapter by learning about the main features of Kubernetes
    and why this orchestrator has become so popular.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the main features of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can say without any doubt that Kubernetes is the new standard for deploying
    applications based on containers. However, its success didn’t happen overnight;
    Kubernetes started in 2015 as a community project based on Google’s own workload
    orchestrator, **Borg**. The first commit in Kubernetes’ GitHub repository occurred
    in 2014, and a year later, the first release was published. Two years later, Kubernetes
    went mainstream thanks to its great community. I have to say that you will probably
    not use Kubernetes alone; you will deploy multiple components to achieve a fully
    functional platform, but this isn’t a bad thing, as you can customize a Kubernetes
    platform to your specific needs. Also, Kubernetes by default has a lot of integrations
    with cloud platforms, as it was designed from the very beginning with them in
    mind. For example, cloud storage solutions can be used without additional components.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to briefly compare Kubernetes’ features with those of Docker
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Kubernetes and Docker Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have to declare that, personally, my first impressions of Kubernetes weren’t
    good. For me, it provided a lot of features for many simple tasks that I was able
    to solve with Docker Swarm at that time. However, the more complex your applications
    are, the more features you require, and Docker Swarm eventually became too simple
    as it hasn’t evolved too much. Docker Swarm works well with simple projects, but
    microservices architecture usually requires complex interactions and a lot of
    portability features. Kubernetes has a really steep learning curve, and it’s continuously
    evolving, which means you need to follow the project almost every day. Kubernetes’
    core features are usually improved upon in each new release, and lots of pluggable
    features and side projects also continuously appear, which makes the platform’s
    ecosystem grow daily.
  prefs: []
  type: TYPE_NORMAL
- en: We will see some differences between the Docker Swarm orchestration model and
    the Kubernetes model. We can start with the definition of the workloads within
    a cluster. We mentioned in [*Chapter 7*](B19845_07.xhtml#_idTextAnchor147), *Orchestrating
    with Swarm*, that Docker Swarm doesn’t schedule containers; in fact, it schedules
    **Services**. In Kubernetes, we schedule **Pods**, which is the minimum scheduling
    unit in this orchestrator. A Pod can contain multiple Pods, although most of them
    will just run one. We will explore and learn more about Pods later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the control plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Container orchestrators should provide a **control plane** for all management
    tasks, providing us with scheduling capabilities to execute our application workloads
    on a data plane and cluster-wide networking features. The Kubernetes control plane
    components are designed to manage every cluster component, schedule workloads,
    and review events that emerge in the platform. It also manages the node components,
    which really execute containers for us thanks to their container runtimes. Kubernetes
    follows the manager-worker model, as with Docker Swarm, in which two different
    node roles are defined. Manager nodes will manage the control plane components,
    while worker nodes will execute the tasks assigned by the control plane nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s review some key processes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the key processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is a list of the key processes that run in the Kubernetes control
    plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kube-apiserver**: The API server is a component that interacts with all other
    components and the user. There isn’t any direct communication between components;
    hence, kube-apiserver is essential in every Kubernetes cluster. All the cluster
    management is provided by exposing this component’s API, and we can use different
    clients to interact with the cluster. Different endpoints allow us to retrieve
    and set Kubernetes resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd**: This is a component that provides data storage for all cluster components.
    It is a key-value store that can be consumed via its HTTP REST API. This reliable
    key-value store contains sensitive data, but, as mentioned before, only the kube-apiserver
    component can access it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-scheduler**: This is in charge of allocating workloads to the container
    runtimes deployed in the nodes. To decide which nodes will run the different containers,
    kube-scheduler will ask kube-apiserver for the hardware resources and availability
    of all nodes included in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-controller-manager**: Different controller processes run inside the
    Kubernetes cluster to maintain the status of the platform and the applications
    running inside. The kube-controller-manager is responsible for managing these
    controllers, and different tasks are delegated to each controller:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **node controller** manages nodes’ statuses
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **job controller** is responsible for managing workloads’ tasks and creating
    Pods to run them
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **endpoint controller** creates endpoint resources to expose Pods
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **service account controller** and **token controller** manage accounts
    and API access token authorizations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cloud-controller-manager**: This is a separate component that manages different
    controllers that talk with underlying cloud providers’ APIs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **node controller** manages the state and health of nodes deployed in your
    cloud provider.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **route controller** creates routes in the cloud provider, using its specific
    API to access your deployed workloads.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **service controller** manages cloud providers’ load balancer resources.
    You will never deploy this component in your own local data center because it
    is designed for cloud integrations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s review **node components**, which are in charge of executing and
    giving visibility to the workload processes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding node components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Node components run on worker nodes (as in Docker Swarm, manager nodes can
    also have the worker role). Let’s take a closer look at them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Container runtime**: The runtime for running containers is key, as it will
    execute all the workloads for us. The Kubernetes orchestrator schedules Pods on
    each host, and they run the containers for us.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kubelet**: We can consider kubelet as the Kubernetes integration agent. All
    nodes with the worker role have to run kubelet in order to communicate with the
    control plane. In fact, the control plane will manage communications to receive
    the health of each worker node and the status of their running workloads. kubelet
    will only manage containers deployed in the Kubernetes cluster; in other words,
    you can still execute containers in the workers’ container runtime, but those
    containers will not be managed by Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-proxy**: This component is responsible for Kubernetes communications.
    It is important to mention here that Kubernetes does not really provide full networking
    capabilities by itself, and this component will only manage the integration of
    Kubernetes Service resources within a cluster. Additional communications components
    will be required to have a fully functional cluster. It is fair to say that kube-proxy
    works at the worker-node level, publishing the applications within the cluster,
    but more components will be needed in order to reach other Services deployed in
    other cluster nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A **Service resource** (or simply **Service**) is designed to make your applications’
    Pods accessible. Different options are available to publish our applications either
    internally or externally for users. Service resources will get their own IP address
    to access the associated Pods’ endpoints. We can consider Service resources as
    logical components. We will use Services to access our applications because Pods
    can die and be recreated, acquiring new IP addresses, but Services will remain
    visible with their specified IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes can be replaced when necessary; we can perform maintenance tasks
    whenever it’s required, moving workloads from one node to another. However, control
    plane components can’t be replaced. To achieve Kubernetes’ HA, we need to execute
    more than one replica of control plane components. In the case of etcd, we must
    have an odd number of replicas, which means that at least three are required for
    HA. This requirement leads us to a minimum of three manager nodes (or master nodes,
    in Kubernetes nomenclature) to deploy a Kubernetes cluster with HA, although other
    components will provide HA with only two replicas. Conversely, the number of worker
    nodes may vary. This really depends on your application’s HA, although a minimum
    of two workers is always recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Networking in Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is important to remember that Kubernetes networking differs from the Docker
    Swarm model. By itself, Kubernetes does not provide cluster-wide communications,
    but a standardized interface, the **Container Network Interface** (**CNI**), is
    provided. Kubernetes defines a set of rules that any project integrating a communications
    interface must follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '`localhost` to resolve their internal communications. This really simplifies
    communications when multiple containers need to work together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Host-to-container communications**: Each host can communicate with Pods running
    locally using its container runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod-to-Pod communications**: These communications will work locally but not
    cluster-wide, and Kubernetes imposes that communications must be provided without
    any **network address translation** (**NAT**). This is something the CNI must
    resolve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod-to-Service interactions**: Pods will never consume other Pods as their
    IP address can change over time. We will use Services to expose Pods, and Kubernetes
    will manage their IP addresses, but the CNI must manage them cluster-wide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Publishing Services**: Different approaches exist to publish our applications,
    but they are resolved by Service types and Ingress resources, and cluster-wide
    communications must be included in the CNI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because NAT isn’t allowed, this model declares a flat network, where Pods can
    see each other when the CNI is included in the Kubernetes deployment. This is
    completely different from Docker Swarm, where applications or projects can run
    in isolated networks. In Kubernetes, we need to implement additional mechanisms
    to isolate our applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of CNI plugins available to implement these cluster-wide communications.
    You can use any of them, but some are more popular than others; the following
    list shows recommended ones with some of their key features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flannel** is a simple overlay network provider that works very well out of
    the box. It creates a VXLAN between nodes to propagate the Pods’ IP address cluster-wide,
    but it doesn’t provide network policies. These are Kubernetes resources that can
    drop or allow Pods’ connectivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calico** is a network plugin that supports different network configurations,
    including non-overlay and overlay networks, with or without **Border Gateway Protocol**
    (**BGP**). This plugin provides network policies, and it’s adequate for almost
    all small environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Canal** is used by default in SUSE’s Rancher environments. It combines Flannel’s
    simplicity and Calico’s policy features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cilium** is a very interesting network plugin because it integrates **extended
    Berkeley Packet Filter** (**eBPF**) Linux kernel features in Kubernetes. This
    network provider is intended for multi-cluster environments or when you want to
    integrate network observability into your platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multus** can be used to deploy multiple CNI plugins in your cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud providers offer their own cloud-specific CNIs that allow us to implement
    different network scenarios and manage Pods’ IP addresses within our own private
    cloud infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CNI plugin should always be deployed once the Kubernetes control plane has
    started because some components, such as the internal DNS or kube-apiserver, need
    to be reachable cluster-wide.
  prefs: []
  type: TYPE_NORMAL
- en: Namespace scope isolation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes provides project or application isolation by using **namespaces**,
    which allow us to group resources. Kubernetes provides both cluster-scoped and
    namespace-scoped resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster-scoped** resources are resources available cluster-wide, and we can
    consider most of them as cluster management resources, owned by the cluster administrators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Namespace-scoped** resources are those confined at the namespace level. Services
    and Pods, for example, are defined at the namespace level, while node resources
    are available cluster-wide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespace resources are key to isolating applications and restricting users
    from accessing resources. Kubernetes provides different authentication and authorization
    methods, although we can integrate and combine additional components such as an
    external **Lightweight Directory Access Protocol** (**LDAP**) or Microsoft Active
    Directory.
  prefs: []
  type: TYPE_NORMAL
- en: Internal resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The internal DNS is based on the `SERVICE_NAME.NAMESPACE.svc.cluster.local`.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching data to containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes includes different resource types to attach storage to our workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '`emptyDir`), host storage, and **Network File System** (**NFS**), among other
    remote storage solutions. Other very important volume-like resources are Secrets
    and ConfigMaps, which can be used to manage sensitive data and configurations
    cluster-wide, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Persistent volumes** are the preferred solution when you work on production
    in a local data center. Storage vendors provide their own drivers to integrate
    **network-attached storage** (**NAS**) and **storage area network** (**SAN**)
    solutions in our applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projected volumes** are used to map several volumes inside a unique Pod container’s
    directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing persistent storage to our applications is key in container orchestrators,
    and Kubernetes integrates very well with different dynamic provisioning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we will introduce the concept of **Ingress** resources. These resources
    simplify and secure the publishing of applications running in Kubernetes by linking
    Service resources with specific applications’ URLs. An Ingress controller is required
    to manage these resources, and we can integrate into this component many different
    options, such as NGINX, Traefik, or even more complex solutions, such as Istio.
    It is also remarkable that many network device vendors have also prepared their
    own integrations with Kubernetes platforms, improving performance and security.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have been quickly introduced to Kubernetes, we can take a deep dive
    into the platform’s components and features.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Kubernetes’ HA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying our applications with HA requires a Kubernetes environment with HA.
    At least three replicas of etcd are required and two replicas of other control
    plane components. Some production architectures deploy etcd externally in dedicated
    hosts, while other components are deployed in additional master nodes. This isolates
    completely the key-value store from the rest of the control plane components,
    improving security, but it adds additional complexity to the environment. You
    will usually find three master nodes and enough worker nodes to deploy your production
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Kubernetes installation configures and manages its own internal **certificate
    authority** (**CA**) and then deploys certificates for the different control plane
    and kubelet components. This ensures TLS communications between kube-apiserver
    and other components. The following architecture diagram shows the different Kubernetes
    components in a single-master node scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Kubernetes cluster architecture with HA](img/B19845_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Kubernetes cluster architecture with HA
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes are those designated to run workloads. Depending on the Kubernetes
    installation method, you will be able to run specific workloads on master nodes
    if they also run the kubelet and kube-proxy components. We can use different affinity
    and anti-affinity rules to identify which nodes should finally execute a container
    in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: However, simply replicating the control plane does not provide HA or resilience
    to your applications. You will need a CNI to manage communications between your
    containers cluster-wide. Internal load balancing will route requests to deployed
    Pods within your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Running your applications on different hosts requires appropriate storage solutions.
    Whenever a container starts with a container runtime, the required volumes should
    be attached. If you work on-premises, you will probably use a **Container Storage
    Interface** (**CSI**) in your infrastructure. However, as a developer, you should
    consider your storage requirements, and your infrastructure administrators will
    provide you with the best solution. Different providers will present filesystems,
    blocks, or object storage, and you can choose which best fits your application.
    All of them will work cluster-wide and help you provide HA.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you have to think about how your application’s components work with
    multiple replicas. Your infrastructure provides resilience to your containers,
    but your application’s logic must support replication.
  prefs: []
  type: TYPE_NORMAL
- en: Running a production cluster can be hard, but deploying our own cluster to learn
    how Kubernetes works is a task that I really recommend to anyone who wants to
    deploy applications on these container architectures. Using **kubeadm** is recommended
    to create Kubernetes clusters for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeadm Kubernetes deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubeadm is a tool that can be used to easily deploy a fully functional Kubernetes
    cluster. In fact, we can even use it to deploy production-ready clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We will initialize a cluster from the first deployment node, executing `kubeadm
    init`. This will create and trigger the bootstrapping processes to deploy the
    cluster. The node in which we execute this action will become the cluster leader,
    and we will join new master and worker nodes by simply executing `kubeadm join`.
    This really simplifies the deployment process; every step required for creating
    the cluster is automated. First, we will create the control plane components;
    hence, `kubeadm join` will be executed in the rest of the designated master nodes.
    And once the master nodes are installed, we will join the worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeadm is installed as a binary in your operating system. It is important to
    note here that the Kubernetes master role is only available on Linux operating
    systems. Therefore, we can’t install a Kubernetes cluster only with Microsoft
    Windows or macOS nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This tool does not only install a new cluster. It can be used to modify current
    kubeadm-deployed cluster configurations or upgrade them to a newer release. It
    is a very powerful tool, and it is good to know how to use it, but unfortunately,
    it is out of the scope of this book. Suffice it to say that there are many command-line
    arguments that will help us to fully customize Kubernetes deployment, such as
    the IP address to be used to manage communications within the control plane, the
    Pods’ IP address range, and the authentication and authorization model to use.
    If you have time and hardware resources, it is recommended to create at least
    a two-node cluster with kubeadm to understand the deployment process and the components
    deployed by default on a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the link for a Kubernetes deployment process with the kubeadm tool:
    [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm).
    We will not use kubeadm to deploy Kubernetes in this book. We will use Docker
    Desktop, Rancher Desktop, or Minikube tools, which provide fully automated deployments
    that work out of the box on our laptop or desktop computers.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker or any other container runtime just cares about containers. We learned
    in [*Chapter 7*](B19845_07.xhtml#_idTextAnchor147), *Orchestrating with Swarm*,
    how Docker provides the command line to manage Docker Swarm clusters, but this
    doesn’t work with Kubernetes, as it is a completely different platform. The kube-apiserver
    component is the only component accessible to administrators and end users. The
    Kubernetes community project provides its own tool to manage Kubernetes clusters
    and the resources deployed on them. Thus, in the next subsection, we will learn
    the basics of `kubectl`, the tool that we will use in a lot of examples in this
    book to manage configurations, content, and workloads on clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with Kubernetes using kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn the basics of the `kubectl` command line. It
    is the official Kubernetes client, and its features can be extended by adding
    plugins.
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation process for this tool is quite simple, as it is a single binary
    written in the Go language; therefore, we can download it from the official Kubernetes
    binaries repository. To download from this repository, you must include the release
    to use in the URL. For example, [https://dl.k8s.io/release/v1.27.4/bin/linux/amd64/kubectl](https://dl.k8s.io/release/v1.27.4/bin/linux/amd64/kubectl)
    will link you to the `kubectl` Linux binary for Kubernetes 1.27.4\. You will be
    able to manage Kubernetes clusters using binaries from a different release, although
    it is recommended to maintain alignment with your client and Kubernetes server
    releases. How to install the tool for each platform is described in https://kubernetes.io/docs/tasks/tools/#kubectl.
    As we will use Microsoft Windows in the *Labs* section, we will use the following
    link to install the tool’s binary: [https://kubernetes.io/docs/tasks/tools/install-kubectl-windows](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with `kubectl` by learning the syntax for executing commands with
    this tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`TYPE` indicates that `kubectl` can be used with many different Kubernetes
    resources. We can use the singular, plural, or abbreviated forms, and we will
    use them in case-insensitive format.'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to know before learning some of the uses of `kubectl` is how
    to configure access to any cluster. The `kubectl` command uses, by default, a
    `config` configuration file in the home of each user, under the `.kube` directory.
    We can change which configuration file to use by adding the `--kubeconfig <FILE_PATH_AND_NAME>`
    argument or setting the `KUBECONFIG` variable with the configuration file location.
    By changing the content of the `kubeconfig` file, we can easily have different
    cluster configurations. However, this path change is not really needed because
    the configuration file structure allows different contexts. Each context is used
    to uniquely configure a set of user and server values, allowing us to configure
    a context with our authentication and a Kubernetes cluster endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You will usually access a Kubernetes cluster using an FQDN (or its resolved
    IP address). This name or its IP address will be load-balanced to all Kubernetes
    clusters’ available instances of kube-apiserver; hence, a load balancer will be
    set in front of your cluster servers. In our local environments, we will use a
    simple IP address associated with our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what a configuration file looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can add multiple servers and users and link them in multiple contexts. We
    can switch between defined contexts by using `kubectl config` `use-context CONTEXT_NAME`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `kubectl api-resources` to retrieve the type of resources available
    in the defined cluster. This is important because the `kubectl` command line retrieves
    data from a Kubernetes cluster; hence, its behavior changes depending on the endpoint.
    The following screenshot shows the API resources available in a sample Kubernetes
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Kubernetes API resources in a sample cluster](img/B19845_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Kubernetes API resources in a sample cluster
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is a column that indicates whether a Kubernetes resource
    is namespaced or not. This shows the scope where the resource must be defined.
    Resources can have a cluster scope, be defined and used at the cluster level,
    or be namespace-scoped, in which case they exist grouped inside a Kubernetes namespace.
    Kubernetes namespaces are resources that allow us to isolate and group resources
    within a cluster. The resources defined inside a namespace are unique within the
    namespace, as we will use the namespaces to identify them.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many commands available for `kubectl`, but we will focus in this
    section on just a few of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create`: This action allows us to create Kubernetes resources from a file
    or our terminal `stdin`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apply`: This creates and updates resources in Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete`: We can remove already created resources using `kubectl delete`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run`: This action can be used to quickly deploy a simple workload and define
    a container image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get`: We can retrieve any Kubernetes resource definition by using `kubectl
    get`. A valid authorization is required to either create or retrieve any Kubernetes
    object. We can also use `kubectl describe`, which gives a detailed description
    of the cluster resource retrieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`edit`: We can modify some resources’ properties in order to change them within
    the cluster. This will also modify our applications’ behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can configure Kubernetes resources by using an **imperative** or **declarative**
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: In an imperative configuration, we describe the configuration of the Kubernetes
    resource in the command line, using our terminal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using a declarative configuration, we will create a file describing the configuration
    of a resource, and then we create or apply the content of the file to the Kubernetes
    cluster. This method is reproducible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a basic idea of Kubernetes components, the installation process,
    how to interact with a cluster, and the requirements to run a functional Kubernetes
    platform, let’s see how to easily deploy our own environment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a functional Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review different methods to deploy Kubernetes for different
    purposes. As a developer, you do not need to deploy a production environment,
    but it’s important to understand the process and be able to create a minimal environment
    to test your applications. If you are really interested in the full process, it’s
    recommended to take a look at Kelsey Hightower’s GitHub repository, *Kubernetes
    the Hard Way* (https://github.com/kelseyhightower/kubernetes-the-hard-way). In
    this repository, you will find a step-by-step complete process to deploy manually
    a Kubernetes cluster. Understanding how a cluster is created really helps solve
    problems, although it’s out of the scope of this book. Here, we will review automated
    Kubernetes solutions in which you can focus on your code and not on the platform
    itself. We will start this section with the most popular container desktop solution.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Desktop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have used Docker Desktop in this book to create and run containers using
    a **Windows Subsystem for Linux** (**WSL**) terminal. Docker Desktop also includes
    a one-node Kubernetes environment. Let’s start using this by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Settings** | **Enable Kubernetes**. The following screenshot shows
    how Kubernetes can be set up in your Docker Desktop environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The Docker Desktop Settings area where a Kubernetes cluster
    can be enabled](img/B19845_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – The Docker Desktop Settings area where a Kubernetes cluster can
    be enabled
  prefs: []
  type: TYPE_NORMAL
- en: 'After the Kubernetes cluster is enabled, Docker Desktop starts the environment.
    The following screenshot shows the moment when Kubernetes starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Kubernetes starting in the Docker Desktop environment](img/B19845_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Kubernetes starting in the Docker Desktop environment
  prefs: []
  type: TYPE_NORMAL
- en: Once started, we can access the cluster from our WSL terminal by using the `kubectl`
    command line. As you may have noticed, we haven’t installed any additional software.
    Docker Desktop integrates the commands for us by attaching the required files
    to our WSL environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The status of the Kubernetes cluster is shown in the lower-left side of the
    Docker Desktop GUI, as we can see in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![ Figure 8.5 – Kubernetes’ status shown in Docker Desktop](img/B19845_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Kubernetes’ status shown in Docker Desktop
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the number of nodes included in the deployed Kubernetes cluster
    by executing `kubectl` `get nodes`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that a Kubernetes configuration file was also added for this environment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That was quite easy, and now we have a fully functional Kubernetes cluster.
    This cluster isn’t configurable, but it is all you need to prepare your applications’
    deployments and test them. This solution doesn’t allow us to decide which Kubernetes
    version to deploy, but we can reset the environment at any time from the Docker
    Desktop Kubernetes **Settings** page, which is very useful when we need to start
    the environment afresh. It can be deployed on either Microsoft Windows (using
    a **virtual machine** (**VM**) with Hyper-V or WSL, which is recommended as it
    consumes fewer resources), macOS (on Intel and Apple silicon architectures), or
    Linux (using a VM with **Kernel-Based Virtual** **Machine** (**KVM**)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker prepares some images with all Kubernetes components and deploys them
    for us when we enable Kubernetes in Docker Desktop. By default, all containers
    created for such a purpose are hidden in the Docker Desktop GUI, but we can review
    them from the Docker command line. This Kubernetes solution is really suitable
    if you use the Docker command line to create your applications because everything
    necessary, from building to orchestrated execution, is provided. We can use different
    Kubernetes volume types because a `storageClass` resources in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231),
    *Leveraging Application Data Management* *in Kubernetes*. However, a few things
    are omitted in this Kubernetes deployment, which may impact your work, so it’s
    good to understand its limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Environment internal IP addresses can’t be changed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You cannot ping containers; this is due to the network settings of Docker Desktop,
    and it also affects the container runtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No CNI is provided; hence, no network policies can be applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Ingress resource is provided by default. This is not really an issue because
    other desktop Kubernetes environments wouldn’t provide it either, but you may
    need to deploy your own and modify your `/etc/hosts` file (or Microsoft Windows’s
    equivalent `C:\Windows\system32\drivers\etc\hosts` file) to access your applications.
    We will learn about Ingress resources and controllers in [*Chapter 11*](B19845_11.xhtml#_idTextAnchor244),
    *Publishing Applications*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the more important issues you will find by using Docker Desktop for
    Kubernetes deployment. It is important to understand that performance will be
    impacted when you enable Kubernetes, and you will need at least 4 GB of RAM free
    and four **virtual** **cores** (**vCores**).
  prefs: []
  type: TYPE_NORMAL
- en: As specified in the official documentation, Docker Desktop is not an open source
    project, and it is licensed under the *Docker Subscription Service Agreement*.
    This means that it is free for small businesses (that is, fewer than 250 employees
    and less than $10 million in annual revenue), personal use, education, and non-commercial
    open source projects; otherwise, it requires a paid subscription for professional
    use.
  prefs: []
  type: TYPE_NORMAL
- en: We will now review another desktop solution to deploy a simplified Kubernetes
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Rancher Desktop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This solution comes from SUSE, and it really provides you with the Kubernetes
    Rancher deployment experience on your laptop or desktop computer. Rancher Desktop
    can be installed on Windows systems, using WSL or VMs, or macOS and Linux, using
    only VMs. It is an open source project and includes Moby components, `containerd`,
    and other components that leverage the experience of Rancher, allowing the development
    of new and different projects such as **RancherOS** (a container-oriented operating
    system) or **K3s** (a lightweight certified Kubernetes distribution). There are
    some interesting features on Rancher Desktop:'
  prefs: []
  type: TYPE_NORMAL
- en: We can choose which container runtime to use for the environment. This is a
    key difference and makes it important to test your applications using `containerd`
    directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can set the Kubernetes version to deploy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to define the resources used for the VM (on Mac and Linux).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides the Rancher Dashboard, which combines perfectly with your infrastructure
    when your server’s environments also run Kubernetes and Rancher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows how can we set up a Kubernetes release from
    the Rancher Desktop GUI, in the **Preferences** area. This way, we will be able
    to test our applications using different API releases, which may be very interesting
    before moving our applications to the staging or production stages. Each Kubernetes
    release provides its own set of API resources; you should read each release note
    to find out changes in the API versions and resources that may affect your project
    – for example, if some *beta* resources are now included in the release, or some
    are deprecated. The following screenshot shows the Kubernetes releases available
    for deploying in Rancher Desktop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Different Kubernetes releases can be chosen](img/B19845_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Different Kubernetes releases can be chosen
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen in Docker Desktop, Rancher Desktop also provides a simple button
    to completely reset the Kubernetes cluster. The following screenshot shows the
    **Troubleshooting** area, where we can reset the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The Troubleshooting area from the Rancher Desktop GUI](img/B19845_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The Troubleshooting area from the Rancher Desktop GUI
  prefs: []
  type: TYPE_NORMAL
- en: Rancher Desktop also deploys its own Ingress controller based on Traefik. This
    controller will help us to publish our applications, as we will learn in [*Chapter
    11*](B19845_11.xhtml#_idTextAnchor244), *Publishing Applications*. We can remove
    this component and deploy our own Ingress controller by unselecting the **Traefik**
    option in the **Kubernetes** **Preferences** section, but it is quite interesting
    to have one by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Rancher Dashboard is accessible by clicking on the Rancher Desktop notification
    icon and selecting **Open cluster dashboard**. Rancher Dashboard provides access
    to many Kubernetes resources graphically, which can be very useful for beginners.
    The following screenshot shows the Rancher Dashboard main page, where you can
    review and modify different deployed Kubernetes resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – The Rancher Dashboard main page](img/B19845_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – The Rancher Dashboard main page
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the Kubernetes environment from a WSL terminal by checking its
    version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We can build images for the Kubernetes environment without using an external
    registry by using `nerdctl` with the `-–namespace k8s.io` argument. This way,
    images will be available directly for our deployments.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting that this Kubernetes implementation is aligned with the expected
    network features from Kubernetes; hence, we can ping Pods and Services, or even
    access the Service ports from the WSL environment. It also makes our applications
    accessible by adding a `.localhost` suffix to our host definitions (we will deep
    dive into this option in [*Chapter 11*](B19845_11.xhtml#_idTextAnchor244), *Publishing
    Applications*). However, this cluster is still a standalone node, and we can’t
    test the behavior of our applications under certain failures or movements between
    nodes. If you really need to test these features, we need to go further and deploy
    additional nodes with other solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Both Docker Desktop and Rancher Desktop provide GUI-based Kubernetes deployments,
    but usually, if you don’t need any GUI, we can even deploy more lightweight solutions.
  prefs: []
  type: TYPE_NORMAL
- en: We will now review Minikube, which may be the most complete and pluggable solution.
  prefs: []
  type: TYPE_NORMAL
- en: Minikube
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Minikube Kubernetes environment is very configurable and consumes considerably
    fewer hardware resources than other solutions, allowing us to deploy more than
    one node per cluster, or even multiple clusters on one single computer host. We
    can create a Kubernetes cluster by using either Docker, QEMU, Hyperkit, Hyper-V,
    KVM, Parallels, Podman, VirtualBox, or VMware Fusion/Workstation. We can use many
    different virtualization solutions or even container runtimes, and Minikube can
    be deployed on Microsoft Windows, macOS, or Linux operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some of the features of Minikube:'
  prefs: []
  type: TYPE_NORMAL
- en: It supports different Kubernetes releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different container runtimes can be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A direct API endpoint improves image management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced Kubernetes customization such as the addition of **feature gates**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a pluggable solution, so we can include add-ons such as Ingress for extended
    Kubernetes features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports integration with common CI environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes deployment is easy, and you just require a single binary for Linux
    systems. Different arguments can be used to set up the environment. Let’s review
    some of the most important:'
  prefs: []
  type: TYPE_NORMAL
- en: '`start`: This action creates and starts a Kubernetes cluster. We can use the
    argument `--nodes` to define the number of nodes to deploy and `--driver` to specify
    which method to use to create a cluster. Virtual hardware resources can also be
    defined by using `--cpu` and `--memory`; by default, 2 CPUs and 2 GB of memory
    will be used. We can even choose a specific CNI to deploy with the `--cni` argument
    (`auto`, `bridge`, `calico`, `cilium`, `flannel`, and `kindnet` are available,
    but we can add our own path to a CNI manifest).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`status`: This action shows the status of the Minikube cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop`: This stops a running Minikube cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delete`: This action deletes a previously created cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dashboard`: An open source Kubernetes Dashboard can be deployed as an add-on,
    which can be accessed by using `minikube dashboard`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service`: This option can be very interesting to expose a deployed application
    Service. It returns the Service URL that can be used to access it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mount`: We can mount host directories into the Minikube nodes with this option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssh`: We can access Kubernetes deployed hosts by using `minikube` `ssh <NODE>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node`: This action allows us to manage cluster nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl`: This runs a `kubectl` binary matching the cluster version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`addons`: One of the best features of Minikube is that we can extend its functionality
    with plugins to manage additional storage options for the cluster (for example,
    to define a specific a `csi-hostpath-driver`, or specify the default storage class
    to use, `default-storageclass`, or a dynamic `storage-provisioner`, among other
    options), Ingress controllers (`ingress`, `ingress-dns`, `istio`, and `kong`),
    and security (`pod-security-policy`). We can even deploy the Kubernetes Dashboard
    or the metrics server automatically, which recover metrics from all running workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To create a cluster with two nodes (a master and a worker) we can simply execute
    `minikube start --nodes 2`. Let’s see this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once deployed, we can review the cluster state using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Minikube is a very configurable solution that provides common Kubernetes features.
    In my opinion, it is the best in terms of performance and features.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Minikube Kubernetes deployments in Microsoft Windows require administrator privileges
    if you use Hyper-V. Therefore, you need to open PowerShell or Command Prompt as
    administrator, but this may not be enough. PowerShell Hyper-V must also be included,
    and we will need to execute `Enable-WindowsOptionalFeature -Online -FeatureName
    Microsoft-Hyper-V-Tools-All –All` on a PowerShell console to enable it.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative Kubernetes desktop deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nowadays, there are other interesting options that use even fewer hardware
    resources, but they don’t provide as many features as Minikube. Let’s discuss
    some good candidates to try:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kind**: This solution takes advantage of a Docker runtime installation to
    deploy Kubernetes using containers and their own custom images. It is based on
    kubeadm deployment, and it really works very nicely on Linux desktop systems in
    which you don’t usually install Docker Desktop to run containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K3s**: This Kubernetes deployment is the basis for the Rancher Desktop Kubernetes
    feature. It deploys a lightweight environment using less memory with customized
    binaries. This may impact your application’s deployment if you use bleeding-edge
    features, as they will probably not be available. This solution comes from Rancher-SUSE,
    which also provides K3D to deploy Kubernetes using containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`containerd`. It is in its early stages, but it currently seems a good solution
    if you don’t use any Docker tool in your application development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now continue and deep dive into the concepts of Pods and Services.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Pods and Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about the resources we will use in the Kubernetes
    orchestration platform to deploy our applications. We will start by learning how
    containers are implemented inside Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A `localhost`; hence, we can only use ports once. Volumes associated with a
    Pod are also shared between containers. We can consider a Pod as a small VM in
    which different processes (containers) run together. Pods are considered in a
    healthy state when all their containers run correctly.
  prefs: []
  type: TYPE_NORMAL
- en: As Pods can contain many containers, we can think of using a Pod to deploy applications
    with multiple components. All containers associated with a Pod run on the same
    cluster host. This way, we can ensure that all application components run together,
    and their intercommunications will definitely be faster. Because Pods are the
    smallest unit in Kubernetes, we are only able to scale up or down complete Pods;
    hence, all the containers included will also be executed multiple times, which
    probably isn’t what we need. Not all applications’ components should follow the
    same scaling rules; hence, it is better to deploy multiple Pods for an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a Pod. Two containers are included, and thus, they
    share the IP address and volumes of the same Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – A schema of a Pod with two containers included](img/B19845_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – A schema of a Pod with two containers included
  prefs: []
  type: TYPE_NORMAL
- en: We can run Pods that share the host’s resources by using the host’s namespaces
    (the host’s network, **inter-process communications** or **IPCs**, processes,
    and so on). We should limit this type of Pod because they have direct access to
    a host’s processes, interfaces, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows a declarative file to execute an example Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can use JSON or YAML files to define Kubernetes resources, but YAML files
    are more popular; hence, you have to take care of indentation when preparing your
    deployment files. To deploy this Pod on our Kubernetes cluster, we will simply
    execute `kubectl create -``f <PATH_TO_THE_FILE>`.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'When you access a Kubernetes cluster, a namespace is associated with your profile
    or context. By default, the namespace associated is `default`; therefore, if we
    don’t specify any Kubernetes namespace as an argument to create a resource, the
    `default`namespace will be used. We can change the namespace for the current context
    to be applied to all commands hereafter, by executing `kubectl config use-context
    --current --namespace <NEW_NAMESPACE>`. The namespace for each resource can be
    included under the `metadata` key in the resource’s YAML manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '`...`'
  prefs: []
  type: TYPE_NORMAL
- en: '`metadata:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`name: podname`'
  prefs: []
  type: TYPE_NORMAL
- en: '`namespace: my-namespace`'
  prefs: []
  type: TYPE_NORMAL
- en: '`...`'
  prefs: []
  type: TYPE_NORMAL
- en: We can modify any aspect used for the container by modifying the container image’s
    behavior, as we learned with the Docker command line in [*Chapter 4*](B19845_04.xhtml#_idTextAnchor096),
    *Running* *Docker Containers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not sure of the available keys or are learning how to use a new
    Kubernetes resource, you can use `kubectl explain <RESOURCE>` to retrieve an accurate
    description of the keys available and the expected values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can continue adding keys to obtain more specific definitions – for example,
    we can retrieve the keys under `pod.spec.containers.resources`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Each description shows extended information with links to the Kubernetes documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We can retrieve all available keys at once for a specific resource by using
    `kubectl explain pod --recursive`. This option really helps us to fully customize
    the resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test a real Pod deployment using an NGINX web server. To do this, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the imperative mode with `kubectl run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now list the Pods to verify whether the webserver is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see, it is starting. After a few seconds, we can verify that our
    web server is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can get into the `minikube` node and verify the Pod’s connectivity. The
    following screenshot shows the interaction with the cluster node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Testing connectivity from the minikube node](img/B19845_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Testing connectivity from the minikube node
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now delete the Pod and create a new one, we can easily see that that
    new Pods can receive a new IP address, and thus, our application may need to change
    the IP addresses continuously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a real problem, and that’s why we never use Pod IP addresses. Let’s
    talk now about Services and how they can help us to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Services** are abstract objects in Kubernetes; they are used to expose a
    set of Pods and, thus, they serve an application component. They will get an IP
    address from the internal Kubernetes IPAM system, and we will use this to access
    the associated Pods. We can also associate Services with external resources to
    make them accessible to users. Kubernetes offers different types of Services to
    be published either internally or outside a cluster. Let’s quickly review the
    different Service types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ClusterIP`: This is the default Service type. Kubernetes associates an IP
    address from the defined Service’s IP address range, and containers will be able
    to access this Service by either its IP address or its name. Containers running
    in the same namespace will be able to simply use the Service’s name, while other
    containers will need to use its Kubernetes FQDN (`SERVICE_NAME.SERVICE_NAMESPACE.svc.cluster.local`,
    where `cluster.local` is the FQDN of the Kubernetes cluster itself). This is due
    to Kubernetes’ internal **service discovery** (**SD**), which creates DNS entries
    for all Services cluster-wide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Headless`: These Services are a variant of the `ClusterIP` type. They don’t
    receive an IP address, and the Service’s name will resolve all the associated
    Pods’ IP addresses. We commonly use headless Services to interact with non-Kubernetes
    SD solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NodePort`: When we use a `NodePort` Service, we associate a set of hosts’
    ports with the Service `clusterIP`’s defined IP address. This makes the Service
    accessible from outside a cluster. We can connect from a client computer to any
    of the cluster hosts in the defined port, and Kubernetes will route requests to
    the Service, associated with the `ClusterIP` address via internal DNS, no matter
    which node received the request. Thus, the Pods associated with the Service receive
    network traffic from the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LoadBalancer`: The `LoadBalancer` Service type is used to publish a defined
    Service in an external load balancer. It uses the external load balancer’s API
    to define the required rules to reach the cluster, and indeed, this model uses
    a `NodePort` Service type to reach the Service cluster-wide. This Service type
    is mainly used to publish Services in cloud providers’ Kubernetes clusters, although
    some vendors also provide this feature on-premises.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have seen how Services can be published outside the Kubernetes cluster to
    be consumed by other external applications or even our users, but we can also
    do the opposite. We can include external Services, available in our real network,
    inside our Kubernetes clusters by using the `External` Service type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following schema represents a `NodePort` Service in which we publish port
    `7000`, attached to port `5000`, and exposed in the containers in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – NodePort Service schema](img/B19845_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – NodePort Service schema
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the external requests from users are load-balanced to port
    `7000`, listening on all cluster hosts. All traffic from the users will be internally
    load-balanced to port `5000`, making it available on all Services’ assigned Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the manifest of a Kubernetes Service, obtained
    by using the **imperative method** to retrieve the output only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the Service resource isn’t created because we added the `-o
    yaml` argument to show the output in the YAML format and `–dry-run=client`. This
    option shows the output of the creation command executed against kube-apiserver.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on now to learn how to deploy workloads in a cluster because Pods
    don’t provide resilience; they run as unmanaged standalone workloads without a
    controller.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying orchestrated resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to deploy Pods using imperative and declarative modes,
    we will define new resources that can manage the Pods’ life cycle. Pods executed
    directly with the `kubectl` command line are not recreated if their containers
    die. To control the workloads within a Kubernetes cluster, we will need to deploy
    additional resources, managed by Kubernetes controllers. These controllers are
    control loops that monitor the state of different Kubernetes resources and make
    or request changes when needed. Each controller tracks some resources and tries
    to maintain their defined state. Kubernetes’ kube-controller-manager manages these
    controllers that maintain the overall desired state of different cluster resources.
    Each controller can be accessed via an API, and we will use `kubectl` to interact
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn the basics of Kubernetes controllers and dive
    deep into how to use them in [*Chapter 9*](B19845_09.xhtml#_idTextAnchor202),
    *Implementing* *Architecture Patterns*.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most simple resource that allows us to maintain a defined state for our
    application is a ReplicaSet. It will keep a set of replica Pods running. To create
    a ReplicaSet, we will use a Pod manifest as a template to create multiple replicas.
    Let’s see a quick example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This ReplicaSet will run three Pods with the same `docker.io/nginx:alpine` image;
    the `template` section defines the specifications for these three Pod resources,
    with one container each. The ReplicaSet identifies Pods to manage by using the
    defined `application` label and its `webserver` value, defined in the Pod’s `template`
    section of the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: When we deploy this ReplicaSet, the cluster creates these three Pods, and whenever
    any of them dies, the controller manages this change and triggers the creation
    of a new one.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to review more resources, but keep in mind that the basic idea
    of a `template` section embedded inside a more general definition applies to all
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We may think of a Deployment as an evolution of a ReplicaSet. It allows us to
    update them because a deployment manages a set of replicas but only runs one.
    Every time we create a new deployment, we create an associated ReplicaSet. And
    when we update this deployment, a new ReplicaSet is created with a new definition,
    reflecting the changes from the previous resource.
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a DaemonSet, we can ensure that all cluster nodes get one replica of our
    workload, but we cannot define the number of replicas in a DaemonSet.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A StatefulSet allows more advanced features in our workloads. It allows us to
    manage the order and uniqueness of Pods, ensuring that each replica gets its own
    unique set of resources, such as volumes. Although Pods are created from the same
    template section, a StatefulSet maintains a different identity for each Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Job creates one or more Pods at a time, but it will continue creating them
    until a defined number of them terminate successfully. When a Pod exits, the controller
    verifies whether the number of required completions was reached, and if not, it
    creates a new one.
  prefs: []
  type: TYPE_NORMAL
- en: CronJobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can schedule Pods by using CronJobs, as they schedule jobs. When the execution
    time comes, a Job is created and triggers the creation of defined Pods. As we
    will learn in [*Chapter 9*](B19845_09.xhtml#_idTextAnchor202), *Implementing Architecture
    Patterns*, CronJobs manifests include two `template` sections – one to create
    jobs and another one to define how Pods will be created.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicationControllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can consider ReplicationControllers a previous version of the current ReplicaSet
    resource types. They work similarly to how we keep a number of Pod replicas alive,
    but they differ in how they group the monitored Pods because ReplicationControllers
    do not support set-based selectors. This selector method allows ReplicaSets to
    acquire the state management of Pods created outside of their own manifest; hence,
    if a Pod already running matches the ReplicaSet label’s selection, it will be
    automatically included in the pool of replicated Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an overview of different resources that allow us to create
    orchestrated resources cluster-wide, we can learn some of the Kubernetes features
    that can improve our applications’ security.
  prefs: []
  type: TYPE_NORMAL
- en: Improving your applications’ security with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications running in containers offer many new different features. We can
    run multiple applications’ releases at a time in a host; they start and stop in
    seconds. We can scale components easily, and different applications can coexist
    without even interaction between them. An application’s resilience is also inherited
    from the container runtime features (exited containers autostarting).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can also improve our applications by running them in Kubernetes.
    Each Kubernetes cluster is composed of multiple container runtimes running together
    and in coordination. Container runtimes isolate the hosts’ resources thanks to
    kernel namespaces and **control groups** (**cgroups**), but Kubernetes adds some
    interesting features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Namespaces**: Namespaces are Kubernetes resources that group other resources
    and are designed to distribute Kubernetes resources between multiple users, grouped
    in teams or projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication strategies**: Requests from Kubernetes clients may use different
    authentication mechanisms, such as client certificates, bearer tokens, or an authenticating
    proxy to authenticate them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorization requests**: Users request the Kubernetes API after passing
    authentication, authorization, and different admission controllers. The authorization
    phase involves granting permission to access Kubernetes’ resources and features.
    Requests’ attributes are evaluated against policies, and they are allowed or denied.
    The user, group, API, request path, namespace, verb, and so on that are provided
    in the requests are used for these validations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`root`, or simply change the final resulting user to a non-privileged user
    in order to maintain cluster security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl` to check whether some verbs are available for us or even for another
    user, by using impersonation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Secrets and ConfigMaps**: We already learned how to deploy certain configurations
    in orchestrated environments in [*Chapter 7*](B19845_07.xhtml#_idTextAnchor147),
    *Orchestrating with Swarm*. In Kubernetes, we also have Secrets and ConfigMaps
    resources, but they can be retrieved by users if they are allowed (RBAC). It is
    important to understand that Secrets are packaged in the Base64 format; hence,
    sensitive data can be accessed if we don’t prepare appropriate roles. The kubelet
    Kubernetes component will mount Secrets and ConfigMaps automatically for you,
    and we can use them as files or environment variables in our application deployments.
    Kubernetes can encrypt Secrets at rest to ensure that operating systems administrators
    can’t retrieve them from the etcd database files, but this capability is disabled
    by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`securityContext` profiles, and this is essential because we can ensure that
    a Pod doesn’t run as root, privileged, or use non-read-only containers on any
    defined namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network policies**: Kubernetes deploys a flat network by default; hence,
    all containers can reach each other. To avoid such a situation, Kubernetes also
    provides NetworkPolicies and GlobalNetworkPolicies (applied at the cluster level).
    Not all CNIs are able to implement this feature. Kubernetes only provides **custom
    resource** (**CR**) types, which will be used to implement the **network policies**.
    Verify that your network provider can implement them to be able to use this feature
    (lots of popular CNI plugins such as Calico, Canal, and Cilium are completely
    capable). It is a good recommendation to implement some default global policies
    to drop all external accesses and allow the required communications for each application
    at the namespace level. Network policies define both Ingress and Egress rules.
    These rules work at the connectivity level; hence, we don’t have raw packet logging
    (although some CNI plugins provide some logging features). We will learn how to
    implement rules following best practices in [*Chapter 11*](B19845_11.xhtml#_idTextAnchor244),
    *Publishing Applications*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have an overview of the most important features available in Kubernetes
    that help us to protect both our applications and the entire cluster, we can continue
    by creating some simple labs that will cover the basic usage of a Kubernetes environment.
  prefs: []
  type: TYPE_NORMAL
- en: Labs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will have a short lab section that will help us to learn and understand
    the basics of deploying a local Kubernetes environment with Minikube, testing
    some of its resource types to validate the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the labs is available in this book’s GitHub repository at https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git.
    Ensure you have the latest revision available by simply executing `git clone https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git`
    to download all its content or `git pull` if you already downloaded the repository
    before. All commands and content used in these labs will be located inside the
    `Containers-for-Developers-Handbook/Chapter8` directory.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by deploying a Minikube cluster with two nodes (one master and
    one worker). We will deploy them with 3 GB of RAM each, which will be more than
    enough to test application behavior when some of the cluster node dies, but you
    will probably not need two nodes for your daily usage.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Minikube cluster with two nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this lab, we will deploy a fully functional Kubernetes cluster locally,
    for testing purposes. We will continue working on a Windows 10 laptop with 16
    GB of RAM, which is enough for the labs in this book. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Minikube. First, download it from https://minikube.sigs.k8s.io/docs/start/,
    choose the appropriate installation method, and follow the simple installation
    steps for your operating system. We will use Hyper-V; hence, it must be enabled
    and running on your desktop computer or laptop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once Minikube is installed, we will open an administrator PowerShell terminal.
    Minikube deployments using Hyper-V require execution with administrator privileges.
    This is due to the Hyper-V layer; hence, admin privileges won’t be required if
    you use VirtualBox as your hypervisor or Linux as your operating system (other
    hypervisors can be used, such as KVM, which works very nicely with Minikube).
    Admin rights are also required to remove the Minikube cluster. Once the PowerShell
    terminal is ready, we execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: PS C:\> kubectl get nodes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME           STATUS   ROLES           AGE   VERSION
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: minikube       Ready    control-plane   23m   v1.26.3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: minikube-m02 node does not show any role. This is due to the fact that everything
    in Kubernetes is managed by labels. Remember that we saw how selectors are used
    to identify which Pods belong to a specific ReplicaSet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can review the node labels and create a new one for the worker node. This
    will show us how we can modify the resource’s behavior by using labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now add a new label to the worker node by using a `kubectl` label, `<``RESOURCE>
    <LABEL_TO_ADD>`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will now show you how you can use the deployed Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with the Minikube deployed cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this lab, we will interact with the current cluster, reviewing and creating
    some new resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by listing all Pods deployed in the cluster using `kubectl get
    pods --A`. This will list all Pods in the cluster. We are able to list them after
    the Minikube installation because we connect as administrators. The following
    screenshot shows the output of `kubectl get pods -A`, followed by a list of the
    current namespaces, using `kubectl` `get namespaces`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.12 – The output of the kubectl get pods –A and kubectl get namespace
    commands](img/B19845_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – The output of the kubectl get pods –A and kubectl get namespace
    commands
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a new namespace, `chapter8`, by using `kubectl create` `ns chapter8`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: PS C:\> kubectl get pods -n chapter8
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'ingress-nginx namespace. We will list all the resources deployed in this namespace
    using kubectl get all, as we can see in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 8.13 – The output of kubectl get all –n ingress-nginx](img/B19845_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – The output of kubectl get all –n ingress-nginx
  prefs: []
  type: TYPE_NORMAL
- en: Now, we know how we can filter resources associated with a specific namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now create a simple Pod in the `chapter8` namespace by using the imperative
    format. We will execute `kubectl run webserver --image=nginx:alpine` to run a
    Pod with one container using the `docker.io/nginx:alpine` image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 8.14 – The output of kubectl get pods --namespace chapter8 -o yaml
    webserver](img/B19845_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – The output of kubectl get pods --namespace chapter8 -o yaml webserver
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see which node executes our Pod by using either `kubectl get pods -o
    wide`, which shows extended information, or by filtering the `hostIP` key from
    the YAML output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This can also be done by using the JSON path template (https://kubernetes.io/docs/reference/kubectl/jsonpath/):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the node name is also available in the `spec` section (`spec.nodeName`),
    but this section is where Pod specifications are presented. We will learn in the
    next chapter how we can change the workload behavior by changing the specifications
    from the online manifests, directly in Kubernetes. The `status` section is read-only
    because it shows the actual state of the resource, while some of the sections
    in either the `metadata` or `spec` sections can be modified – for example, by
    adding new labels or annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Before ending the labs from this chapter, we will expose the deployed Pod by
    adding a `NodePort` Service, which will guide our requests to the running web
    server Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing a Pod with a NodePort Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this quick lab, we will use the imperative model to deploy a `NodePort`
    Service to expose the already deployed web server Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we haven’t defined the container port in the `webserver` Pod, Kubernetes
    will not know which port must be associated with the Service; hence, we need to
    pass the `--target-port 80` argument to specify that the Service should link the
    NGINX container port that is listening. We will use port `8080` for the Service,
    and we will let Kubernetes choose one `NodePort` port for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: PS C:\> kubectl get all -n chapter8
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME            READY   STATUS    RESTARTS   AGE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pod/webserver   1/1     Running   0          50m
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 32317 is associated with the Service’s port, 8080, which is associated with
    the webserver Pod’s port, 80 (the NGINX container listens on that port).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can now access the published `NodePort` port on any host, even if it does
    not run any Service-related Pod. We can use the IP address of any of the Minikube
    cluster nodes or use `minikube service -n chapter8 webserver` to automatically
    open our default web browser in the associated URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output in both cases. First, we obtained
    the host’s IP addresses by using `kubectl get nodes –o wide`. We used PowerShell’s
    `Invoke-WebRequest` command to access a combination of IP addresses of the nodes
    and the `NodePort`-published port. Then, we used Minikube’s built-in DNS to resolve
    the Service’s URL by using the `minikube` Service:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15 – The output of kubectl get nodes, different tests using the
    cluster nodes, and the minikube Service URL resolution](img/B19845_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – The output of kubectl get nodes, different tests using the cluster
    nodes, and the minikube Service URL resolution
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we used the IP addresses of both the master and worker nodes
    for the tests, and they worked, even though the Pod only ran on the worker node.
    This output also shows how easy it is to test Services by using Minikube’s integrated
    Services resolution. It automatically opened our default web browser, and we can
    access our Service directly, as we can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – The default web browser accessing the webserver Service’s NodePort
    port](img/B19845_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – The default web browser accessing the webserver Service’s NodePort
    port
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now remove all the resources created in this chapter. It is important
    to first remove the Pod, the Service, and then the namespace. Removing the namespace
    first triggers the removal of all associated resources in cascade, and there may
    be some issues if Kubernetes isn’t able to remove some resources. It will never
    happen in this simple lab, but it is a good practice to remove resources inside
    a namespace before deleting the namespace itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You are now ready to learn more advanced Kubernetes topics in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored Kubernetes, the most popular and extended container
    orchestrator. We had an overall architecture review, describing each component
    and how we can implement an environment with HA, and we learned the basics of
    some of the most important Kubernetes resources. To be able to prepare our applications
    to run in Kubernetes clusters, we learned some applications that will help us
    to implement fully functional Kubernetes environments on our desktop computers
    or laptops.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will deep dive into the resource types we will use to
    deploy our applications, reviewing interesting use cases and examples and learning
    different architecture patterns to apply to our applications’ components.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3:Application Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will describe how applications run in production, and we will use
    different models and Kubernetes features to help us deliver reliable applications
    securely.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19845_09.xhtml#_idTextAnchor202), *Implementing Architecture
    Patterns*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19845_10.xhtml#_idTextAnchor231), *Leveraging Application Data
    Management in Kubernetes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19845_11.xhtml#_idTextAnchor244), *Publishing Applications*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19845_12.xhtml#_idTextAnchor267), *Gaining Application Insights*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
