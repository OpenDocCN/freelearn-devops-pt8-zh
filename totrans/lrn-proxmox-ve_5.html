<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Working with Virtual Disks"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Working with Virtual Disks</h1></div></div></div><p>
<a class="link" href="ch04.html" title="Chapter 4. Creating Virtual Machines">Chapter 4</a>, <span class="emphasis"><em>Creating a Virtual Machine</em></span>, introduced the creation of virtual machines using the Proxmox VE management interface as well as the command line. After an outline of common steps in the procedure, we quickly glossed over creating two virtual machines with two network operating systems: <span class="strong"><strong>Windows Server 2012r2</strong></span> and <span class="strong"><strong>Fedora 23 Server</strong></span>.</p><p>Our most fundamental goal with this chapter is to empower ourselves to make more informed and fully-deliberated decisions affecting the efficiency and reliability of a Proxmox VE virtual machine guest based on its specific use case.</p><p>We will accomplish this goal by achieving the following concrete objectives:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Choose deliberately from among virtual disk image formats available for use through the Proxmox VE interface</li><li class="listitem" style="list-style-type: disc">Choose an appropriate bus/interface by which a virtual disk will connect to a guest</li><li class="listitem" style="list-style-type: disc">Choose an appropriate cache setting for our use cases</li></ul></div><p>In this chapter, then, we elaborate on one critical, and potentially the most valuable, virtual machine component: the <span class="strong"><strong>virtual disk</strong></span> that provides <span class="strong"><strong>secondary storage</strong></span>.</p><p>We will then revisit the virtual machine creation process to elaborate on specific options glossed over in <a class="link" href="ch04.html" title="Chapter 4. Creating Virtual Machines">Chapter 4</a>, <span class="emphasis"><em>Creating a Virtual Machine</em></span>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Choosing a virtual disk format</li><li class="listitem" style="list-style-type: disc">Choosing an interface</li><li class="listitem" style="list-style-type: disc">Selecting a cache option</li></ul></div><div class="section" title="Understanding virtual disks"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Understanding virtual disks</h1></div></div></div><p>This section focuses first on the terms we should be familiar with to build on our understanding of virtual disks.</p><p>After we've agreed on terms, we'll explore virtual disk configuration options that we saw in <a class="link" href="ch04.html" title="Chapter 4. Creating Virtual Machines">Chapter 4</a>, <span class="emphasis"><em>Creating a Virtual Machine</em></span>, but did not explore: virtual disk image formats,bus/interface options, and disk <span class="strong"><strong>cache</strong></span> options.</p><div class="section" title="Coming to terms"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec28"/>Coming to terms</h2></div></div></div><p>Secondary storage is as integral to virtual machines as it is to physical computers. While <span class="strong"><strong>hard disk drives</strong></span> (<span class="strong"><strong>HDDs</strong></span>) are hardly the most expensive hardware in PCs, we could compellingly argue that they are the most valuable, so far as we rely on them to store and provide access to our data, often the unique fruit of our hard labor.</p><p>Throughout this chapter, the term Virtual Disk will refer to a file or set of files that, to a virtual machine, represent a hard disk drive and behave just as a physical hard disk drive or <span class="strong"><strong>solid-state drive</strong></span> (<span class="strong"><strong>SSD</strong></span>) does for a physical computer.</p><p>The similarities between a physical hard disk drive and a virtual disk are, as you might expect, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Identical file system options</li><li class="listitem" style="list-style-type: disc">The same strong understanding of partitions and partition tables is required of administrators with fluency in using the same partition editing tools we'd use on physical machines</li><li class="listitem" style="list-style-type: disc">Identical formatting procedures and options</li><li class="listitem" style="list-style-type: disc">The same support for LVM</li></ul></div><p>However, the striking differences must be made explicit. A physical hard disk drive unit includes more than just the media that stores data.</p><p>The media component of HDDs is constituted by a stack of double-sided, physical platters rotating around a common spindle within a vacuum-sealed structure.</p><p>Also within the vacuum seal is the physical, mechanical apparatus that reads and writes data on the platters. It includes an armature that moves read/write heads to specific locations on the platters.</p><p>A virtual disk has a storage capacity and can be written to or read from like a hard disk; however, the media is simply a file or series of files on the host that uses one of three disk image formats compatible with Proxmox VE: <span class="strong"><strong>QCOW2</strong></span>, <span class="strong"><strong>RAW</strong></span>, and <span class="strong"><strong>VMDK</strong></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip45"/>Tip</h3><p>A fourth option is available when using iSCSI shared storage: RAW as LVM partitions. In this section, we'll address only RAW, qcow2, and VMDK images.</p></div></div><p>As you'll learn later, each of the three virtual disk image formats explicitly supported by the Proxmox VE management interface provides slightly different advantages and disadvantages. For us, this provides extended flexibility.</p><p>For example, in addition to the backup methods an administrator would use to ensure the safety and integrity of the host and the guest data, it can be a simple matter to make backups or snapshots of the state of one or more virtual machines. Restoring an image to a previous state is simple too.</p><p>Another difference between the virtual disk and a hard disk drive is that the bus, the interface between the motherboard and the disk, is physically incorporated into a hard disk unit.</p><p>This is handled quite differently on a Proxmox VE virtual machine, on which you choose the type of bus that's used to communicate with the virtual disk based on your preferences for the specific virtual machine.</p><p>The bus, then, isn't at all part of the virtual disk image; instead, it's part of the virtual machine configuration. On Proxmox VE, there are bus choices available through a simple drop-down box.</p><p>Another subcomponent of the physical hard disk drive that is not represented in a virtual disk is the cache subsystem (or, more accurately, <span class="emphasis"><em>disk buffer</em></span>). The cache, designed to speed up the retrieval of data, constitutes, like the bus interface, a part of a virtual machine's configuration, rather than part of the virtual disk.</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_001.png" alt="Coming to terms"/><div class="caption"><p>Visualizing a hard disk with a SATA interface and disk buffer on the control board</p></div></div><p>
</p><p>In this section, we determined that <span class="emphasis"><em>virtual disk</em></span> will be used to describe the file or group of files that serve as virtual machines' secondary storage devices. In addition, we articulated similarities between physical drives and virtual drives. We then contrasted the physical components of a hard disk drive and how virtualization with Proxmox VE abstracts these components.</p><p>Proxmox VE's configuration process completely divorces the storage media from both the bus and the disk cache or disk buffer, which again frees us to make more deliberate choices about which combination of virtual disk image format, bus, and cache to choose.</p><p>The remaining subsections articulate the features of each image format, bus option, and disk cache option available to us through Proxmox VE.</p></div><div class="section" title="Understanding virtual disk configuration"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec29"/>Understanding virtual disk configuration</h2></div></div></div><p>Recall from <a class="link" href="ch04.html" title="Chapter 4. Creating Virtual Machines">Chapter 4</a>, <span class="emphasis"><em>Creating a Virtual Machine</em></span> that we created new virtual machines from the Proxmox VE interface by clicking on the <span class="strong"><strong>Create VM</strong></span> button toward the top of the page and running through the new VM's configuration options.</p><p>Regarding the fourth tab in the configuration dialog, <span class="strong"><strong>Hard Disk</strong></span>, <a class="link" href="ch04.html" title="Chapter 4. Creating Virtual Machines">Chapter 4</a>, <span class="emphasis"><em>Creating a Virtual Machine</em></span> restricted its concern to defining the size of the virtual disk.</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_002.png" alt="Understanding virtual disk configuration"/><div class="caption"><p>The Hard Disk tab in the Create: Virtual Machine dialog</p></div></div><p>
</p><p>In this section, we elaborate on three additional characteristics that can be defined through the <span class="strong"><strong>Hard Disk</strong></span> tab:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Virtual disk format</li><li class="listitem" style="list-style-type: disc">Bus/Device (interface)</li><li class="listitem" style="list-style-type: disc">Cache (disk buffer)</li></ul></div><p>With at least three configuration options available for each, let's explore how your choices can affect performance and features.</p><div class="section" title="Choosing a virtual disk format"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec4"/>Choosing a virtual disk format</h3></div></div></div><p>According to <span class="emphasis"><em>Mastering Proxmox</em></span>, Proxmox VE's preferred image format for virtual disks is RAW. However, it also supports KVM's <code class="literal">qcow2</code> format and VMDK images commonly associated with VMware products.</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_003.png" alt="Choosing a virtual disk format"/></div><p>
</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip46"/>Tip</h3><p>To learn more about image formats and their manipulation, visit <a class="ulink" href="https://en.wikibooks.org/wiki/QEMU/Images">https://en.wikibooks.org/wiki/QEMU/Images</a>.</p></div></div><div class="section" title="QCOW2"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec8"/>QCOW2</h4></div></div></div><p>QCOW2 is the second release of QEMU's copy-on-write image format. Since Proxmox VE relies on KVM-QEMU for its virtual machine features, QCOW2 is its native and default format.</p><p>As its name suggests, this format supports <span class="strong"><strong>copy on write</strong></span>. This feature allows the VM to store changes made to a base image in a separate QCOW2 file. The metadata (data about data) of the new QCOW2 file includes, for example, the path to the base image.</p><p>When the VM seeks to retrieve data, it checks first to see whether the specific data can be retrieved from the new image; if it is not in the new image, the data is retrieved from the base image referred to by the metadata.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note47"/>Note</h3><p>To learn more about the QCOW2 image structure, visit <a class="ulink" href="https://people.gnome.org/~markmc/qcow-image-format.html">https://people.gnome.org/~markmc/qcow-image-format.html</a>.</p></div></div><p>QCOW2 images also grow as needed (<span class="strong"><strong>thin provisioning</strong></span>), a feature that distinguishes them from RAW images, for which all the space requested at their creation is immediately allocated to a file (<span class="strong"><strong>thick provisioning</strong></span>).</p><p>Consequently, QCOW2 images will be smaller than RAW images in almost any case even when the host's file system doesn't support sparse files. However, the RAW image will have better throughput since it doesn't have to grow as data is written and because it doesn't depend on an intermediary software layer.</p><p>We should take note however, that if PVE has plenty of fast RAM and runs on a recent SSD drive instead of a hard drive, the difference in throughput between a RAW image and a QCOW2 image is much less visible. As we continue to explore image types, keep in mind the impact newer hardware, with ever-decreasing prices, can have on performance.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note48"/>Note</h3><p>A sparse file is one that attempts to use file system space more efficiently when the file itself is mostly empty. This is achieved by writing metadata representing the empty blocks to disk, rather than the actual empty space which makes up the block. Consequently, less disk space is used. Only when the block contains real data is the full block size written to disk as its literal size (<a class="ulink" href="https://en.wikipedia.org/wiki/Sparse_file">https://en.wikipedia.org/wiki/Sparse_file</a>).For a list of file systems with support for sparse files, visit <a class="ulink" href="https://en.wikipedia.org/wiki/Comparison_of_file_systems#Allocation_and_layout_policies">https://en.wikipedia.org/wiki/Comparison_of_file_systems#Allocation_and_layout_policies</a>.</p></div></div><p>QCOW2's snapshot and temporary snapshot support allows an image to contain multiple snapshots from prior moments in the image's history.</p><p>Temporary snapshots store changes until the VM powers off, at which point the snapshot is discarded. Standard snapshots, in contrast, allow us to return to prior states in an image's history.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note49"/>Note</h3><p>To learn more about QCOW2 snapshots with Proxmox VE, visit their wiki page at <a class="ulink" href="https://pve.proxmox.com/wiki/Live_Snapshots">https://pve.proxmox.com/wiki/Live_Snapshots</a>.For more about the mechanics behind QCOW2's snapshot support, visit <a class="ulink" href="https://kashyapc.fedorapeople.org/virt/lc-2012/snapshots-handout.html">https://kashyapc.fedorapeople.org/virt/lc-2012/snapshots-handout.html</a>.</p></div></div></div><div class="section" title="RAW"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec9"/>RAW</h4></div></div></div><p>RAW and QCOW2 are the two most supported Proxmox VE formats discussed in Proxmox VE forums.</p><p>Compared to both QCOW2 and VMDK images, RAW virtual disks are quite simple; and unlike the other formats supported by Proxmox VE, RAW doesn't rely on an intermediary software layer.</p><p>Consequently, RAW is a more efficient option and should be given all due consideration when the performance of a virtual machine is of supreme importance—particularly RAW on LVM.</p><p>Moreover, RAW images can be directly and simply mounted on the Proxmox VE host for direct manipulation without requiring access through the guest.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip50"/>Tip</h3><p>
<span class="strong"><strong>Mounting a RAW file</strong></span>To walk through this procedure, consider visiting <a class="ulink" href="http://equivocation.org/node/107">http://equivocation.org/node/107</a> or <a class="ulink" href="http://forensicswiki.org/wiki/Mounting_Disk_Images#To_mount_a_disk_image_on_Linux">http://forensicswiki.org/wiki/Mounting_Disk_Images#To_mount_a_disk_image_on_Linux</a>; both pages recommend using the <code class="literal">kpartx</code> utility, available in the default Debian repositories.</p></div></div><p>However, unlike QCOW2 images, RAW images are not feature rich; there's no inherent support for snapshots, no thin provisioning, and so on. They are composed of raw data, built sector by sector until they reach their fixed capacity.</p><p>
<span class="emphasis"><em>Performance</em></span>, then, is the real boon of RAW images as virtual drives; if you're relying on a hard disk drive rather than a solid state drive, and reliable, snappy performance is critical, as it might be for a database server, for example, then choose RAW and forego the rich feature set offered by QCOW2.</p><p>Although pre-allocating storage for accumulating VM guests can be an unnecessary strain on resources, keep in mind that RAW virtual drives too can be resized.</p><p>As is the case with QCOW2, only part of the resizing process can be accomplished through the management interface Proxmox VE provides.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note51"/>Note</h3><p>The process for resizing both RAW and QCOW2 images is documented on the Proxmox VE wiki page at <a class="ulink" href="https://pve.proxmox.com/wiki/Resizing_disks#Enlarge_the_virtual_disk.28s.29_in_Proxmox">https://pve.proxmox.com/wiki/Resizing_disks#Enlarge_the_virtual_disk.28s.29_in_Proxmox</a>.</p></div></div></div><div class="section" title="VMDK"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec10"/>VMDK</h4></div></div></div><p>
<span class="strong"><strong>Virtual machine disk</strong></span> (<span class="strong"><strong>VMDK</strong></span>) is the preferred virtual disk format for VMware virtualization products. Having said that, the format was subsequently opened up to other developers and vendors, and has become a popular virtual disk format to support.</p><p>While KVM-QEMU currently supports versions 3, 4, and 6 of the format, and Proxmox VE can create VMs with VMDK images, it's recommended that PVE users rely on QEMU native formats—QCOW2 and RAW—whenever circumstances allow.</p><p>Realistically, however, circumstances aren't always ideal; so before we address bus types, let's touch on a few relevant points regarding VMDK virtual disks.</p><p>Like QCOW2, VMDK is a complex format with a rich feature set (in fact, it has four sub-formats).</p><p>For example, the VMDK format supports thin and thick provisioning. Thin VMDK images, like their QCOW2 counterparts, are slower than preallocated, or thick-provisioned, VMDK images. As we'd expect, they are significantly smaller.</p><p>Likewise, both VMDK and QCOW2 formats support multiple snapshots that enable administrators to restore a virtual machine to a prior state.</p><p>While the format's feature set is rich, not all its features are supported by the Proxmox VE interface, even when the underlying virtualization layer can handle them.</p><p>For example, the VMDK format includes a subformat that splits a virtual disk into 2 GB chunks, essentially to support mobility. This subformat isn't supported by Proxmox VE.</p><p>While Proxmox VE does invite us to create virtual machines with VMDK images, rely as much as possible on QCOW2 and RAW virtual disk formats:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Assuming your PVE is built around a traditional hard disk drive, RAW is ideal for database applications, for example, because it has the performance advantage.</li><li class="listitem" style="list-style-type: disc">QCOW2 offers an extremely powerful feature set that could come at the cost of performance if you're not relying on SSD storage. In addition, QCOW2 is more conservative in its use of hardware resources.</li></ul></div><p>If you've inherited a virtual disk in the VMDK format, it can be converted to either the QCOW2 or RAW format using the <code class="literal">qemu-img</code> command.</p><p>On the other hand, if you're creating a Proxmox VE , reliance on the VMDK format should be reserved for very deliberate and purposeful edge cases.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip52"/>Tip</h3><p>Full documentation for the <code class="literal">qemu-img</code>  command is available on the Web at <a class="ulink" href="https://www.suse.com/documentation/sles11/book_kvm/data/cha_qemu_guest_inst_qemu-img.html">https://www.suse.com/documentation/sles11/book_kvm/data/cha_qemu_guest_inst_qemu-img.html</a> (SUSE) and <a class="ulink" href="https://docs.fedoraproject.org/en-US/Fedora/18/html/Virtualization_Administration_Guide/sect-Virtualization-Tips_and_tricks-Using_qemu_img.html">https://docs.fedoraproject.org/en-US/Fedora/18/html/Virtualization_Administration_Guide/sect-Virtualization-Tips_and_tricks-Using_qemu_img.html</a> (Fedora).The sparse but comprehensive GNU/Linux main page is available at <a class="ulink" href="http://linux.die.net/man/1/qemu-img">http://linux.die.net/man/1/qemu-img</a>.</p></div></div></div></div><div class="section" title="Choosing a bus"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec5"/>Choosing a bus</h3></div></div></div><p>Along with <span class="strong"><strong>Format</strong></span> options, the <span class="strong"><strong>Hard Disk</strong></span> tab of the <span class="strong"><strong>Create: Virtual Machine</strong></span> dialog offers a drop-down menu for the <span class="strong"><strong>Bus/Device</strong></span> with which to interface the virtual disk.</p><p>As illustrated here, four options are available:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>IDE</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SATA</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VIRTIO</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SCSI</strong></span></li></ul></div><p>
</p><div class="mediaobject"><img src="graphics/image_05_004.png" alt="Choosing a bus"/><div class="caption"><p>Bus/Device options</p></div></div><p>
</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note53"/>Note</h3><p>Note that with the exception of I/O performance, the characteristics of the virtual SATA, IDE, and SCSI buses will be the same as their physical counterparts. For details about each, visit Wikipedia:<span class="strong"><strong>SATA</strong></span>: <a class="ulink" href="https://en.wikipedia.org/wiki/Serial_ATA">https://en.wikipedia.org/wiki/Serial_ATA</a>
<span class="strong"><strong>IDE/PATA</strong></span>: <a class="ulink" href="https://en.wikipedia.org/wiki/Parallel_ATA">https://en.wikipedia.org/wiki/Parallel_ATA</a>
<span class="strong"><strong>SCSI</strong></span>: <a class="ulink" href="https://en.wikipedia.org/wiki/SCSI">https://en.wikipedia.org/wiki/SCSI</a>
</p></div></div><p>Of the four options, two are provided as convenient support for compatibility with legacy systems: <span class="strong"><strong>IDE</strong></span> and <span class="strong"><strong>SCSI</strong></span>. (IDE is thus the dialog's default option.)</p><p>The <span class="strong"><strong>SATA</strong></span> option has more currency, and behaves as one would expect a SATA interface to behave—with one significant exception: the <span class="strong"><strong>SATA</strong></span> option does not provide a performance boost over <span class="strong"><strong>IDE</strong></span>, for example.</p><p>I/O performance, in this case, is determined almost entirely by the host's hardware configuration. The VM will not be able to read and write faster than the physical hardware permits.</p><p>Since the limits of the host's hardware I/O performance can't be overcome, KVM-QEMU addresses, instead, the overhead attached to the virtualization process, the other factor affecting performance in this case.</p><p>KVM-QEMU provides a paravirtualization solution called <span class="strong"><strong>virtio</strong></span> that allows the guest and hypervisor to work more cooperatively and efficiently with one another without the virtualization overhead.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note54"/>Note</h3><p>
<span class="strong"><strong>Paravirtualization</strong></span> refers to software components that are aware they are running in a VM. Virtio drivers for use with KVM-QEMU VMs communicate directly with the Proxmox VE host in our case.</p><p>Typical of paravirtualized drivers, virtio drivers are optimized to share queues, buffers, and other data with Proxmox VE to improve throughput and to reduce latency (vTerminology: A Guide to Key Virtualization  Terminology is available at <a class="ulink" href="http://www.globalknowledge.com">http://www.globalknowledge.com</a>).</p><p>We'll return to virtio drivers again in the context of network interfaces in <a class="link" href="ch06.html" title="Chapter 6. Networking with Proxmox VE">Chapter 6</a>, <span class="emphasis"><em>Networking with Proxmox VE</em></span>.</p></div></div><p>This solution, <span class="strong"><strong>VIRTIO</strong></span>, is the only <span class="strong"><strong>Bus/Device</strong></span> option that affects the I/O performance of the VM.</p><p>The ability to take advantage of a guest's virtio devices requires that drivers are available for the guest's OS. As the Proxmox VE wiki page points out, recent Linux kernels already include the virtio drivers; therefore, any recent GNU/Linux distribution running on a Proxmox VE VM "should recognize virtio devices exposed by the KVM hypervisor" (<a class="ulink" href="https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers">https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers</a>).</p><p>VM guests running GNU/Linux, therefore, do not require any additional explicit configuration steps.</p><p>VM guests running a Microsoft Windows OS will need signed drivers installed before the virtio device will be recognized. As you may suspect, there's a hitch here, since the OS install process needs the device driver in order to recognize and install to the virtual disk.</p><p>We can employ any of several tactics to overcome this problem.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip55"/>Tip</h3><p>For additional information on Microsoft operating systems and virtio devices, visit <a class="ulink" href="https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers">https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers</a>.</p></div></div><p>This subsection puts a significant and just emphasis on the performance increase virtio paravirtualization supported by KVM-QEMU provides.</p><p>To summarize, the only time we shouldn't use virtio paravirtualization is when the required drivers aren't available for the OS intended for the virtual machine. Because it will significantly improve I/O throughput and alleviate some of the overhead associated with full virtualization, we should rely on virtio whenever it is a realistic alternative.</p><p>However, let's keep in mind that IDE and SCSI are also viable bus alternatives, but provided primarily for legacy devices and to serve the interests of compatibility and flexibility. SATA, however, has significant currency at this point, so it's a viable alternative if circumstances just don't allow you to take advantage of the virtio solution.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip56"/>Tip</h3><p>There is another case when virtio paravirtualization is not viable: when a VM has been converted from a physical machine and one thus needs to install the drivers before rebooting to rely on virtio.</p></div></div><p>The next subsection focuses on the five disk caching/buffering options available for our Proxmox VE virtual machines.</p></div><div class="section" title="Understanding cache options"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec6"/>Understanding cache options</h3></div></div></div><p>The <span class="strong"><strong>Hard Disk</strong></span> tab of the <span class="strong"><strong>Create: Virtual Machine</strong></span> dialog includes a field labeled <span class="strong"><strong>Cache</strong></span> that accepts five distinct values:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Default (No cache)</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Direct sync</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Write through</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Write back</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Write back (unsafe)</strong></span></li></ul></div><p>Essentially, the chosen setting determines how the abstraction of a HDD's buffer should be handled. With Proxmox VE, the choice of cache has been demonstrated to significantly affect I/O performance.</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_005.png" alt="Understanding cache options"/><div class="caption"><p>Proxmox VE cache options</p></div></div><p>
</p><p>After working to somewhat articulate the function of a disk buffer, this section will briefly explore recommendations to optimize the performance of a VM.</p><p>On a physical hard disk, the disk buffer is a kind of memory on the controller board mounted outside the vacuum-sealed disk housing. Contemporary hard-disk drives have between 16 and 128 MB of disk buffer. (To take drives that are currently on the market as an example, Western Digital's Black line of HDDs has either 32 or 64 MB of buffer, depending on the model.)</p><p>The function of this cache is primarily to sequence disk writes for optimum performance and manage and execute read requests from a client (such as the CPU or OS) in a strategic way.</p><p>Put another way, when we keep in mind that the bus attaching the physical HDD to the motherboard is rarely the same speed as the rotation of the hard disk platters and the mechanical motion of the read/write heads, the buffer stores data read from the disk before it's sent to the client; it likewise stores data to be written to the disk until the actual disk write can be executed. It's up to the buffer and the drive's processor to organize the data so it gets to its destination as efficiently as possible.</p><p>However accurate this explanation may be, it lacks concretion. <span class="emphasis"><em>Linux System Administrator's Guide</em></span> offers a more concrete explanation (<a class="ulink" href="http://www.tldp.org/LDP/sag/html/buffer-cache.html">http://www.tldp.org/LDP/sag/html/buffer-cache.html</a>):</p><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>Reading from a disk is very slow compared to accessing (real) memory. In addition, it is common to read the same part of a disk several times during relatively short periods of time. For example, one might first read an e-mail message, then read the letter into an editor when replying to it, then make the mail program read it again when copying it to a folder. Or, consider how often the command ls might be run on a system with many users. By reading the information from disk only once and then keeping it in memory until no longer needed, one can speed up all but the first read. This is called disk buffering, and the memory used for the purpose is called the buffer cache.</em></span>
</p></blockquote></div><p>We must choose from among five cache options to define a Proxmox VE VM: the default is <span class="strong"><strong>No Cache</strong></span>. The following alternatives are available:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Direct sync</li><li class="listitem" style="list-style-type: disc">Write Through</li><li class="listitem" style="list-style-type: disc">Write back</li><li class="listitem" style="list-style-type: disc">Write back (unsafe)</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note57"/>Note</h3><p>There's a very clear, concise, general-purpose differentiation between write-through and write-back caches offered at <a class="ulink" href="https://simple.wikipedia.org/wiki/Cache#Caches_for_writing">https://simple.wikipedia.org/wiki/Cache#Caches_for_writing</a>.</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip58"/>Tip</h3><p>
<span class="strong"><strong>Write back</strong></span> is KVM-QEMU's default; note Proxmox VE's default is instead <span class="strong"><strong>No cache</strong></span>.</p></div></div><p>While the resources are ordered from most to least pertinent, each provides a helpful perspective and is included precisely because, when we rely on all three, we can begin to start conceptualizing use cases for each mode. The following list provides resources that describe each of these modes.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Virtualization_Tuning_and_Optimization_Guide/sect-Virtualization_Tuning_Optimization_Guide-BlockIO-Caching.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Virtualization_Tuning_and_Optimization_Guide/sect-Virtualization_Tuning_Optimization_Guide-BlockIO-Caching.html</a>: From <span class="emphasis"><em>Virtualization Tuning and Optimization Guide</em></span> from Red Hat</li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.suse.com/documentation/sles11/book_kvm/data/sect1_1_chapter_book_kvm.html">https://www.suse.com/documentation/sles11/book_kvm/data/sect1_1_chapter_book_kvm.html</a>: From <span class="emphasis"><em>Virtualization with KVM</em></span> and provided by SUSE</li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www-01.ibm.com/support/knowledgecenter/linuxonibm/liaat/liaatbpkvmguestcache.htm">https://www-01.ibm.com/support/knowledgecenter/linuxonibm/liaat/liaatbpkvmguestcache.htm</a>: From <span class="emphasis"><em>Linux on IBM Systems</em></span></li></ul></div><p>To evaluate performance, we need either consistent anecdotes or structured comparisons of the performances of each combination of cache mode, bus, and virtual disk format.</p><p>The most thorough and visually compelling results available on the Web are published at <a class="ulink" href="http://jrs-s.net/2013/05/17/kvm-io-benchmarking/">http://jrs-s.net/2013/05/17/kvm-io-benchmarking/</a> (by Jim Salter, 2013) and <a class="ulink" href="http://www.ilsistemista.net/index.php/virtualization/11-kvm-io-slowness-on-rhel-6.html">http://www.ilsistemista.net/index.php/virtualization/11-kvm-io-slowness-on-rhel-6.html</a> (by Ginatan Dante, 2011).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip59"/>Tip</h3><p>Additional benchmarks, specific to Proxmox VE, are posted at <a class="ulink" href="http://i51.tinypic.com/158bcl4.gif">http://i51.tinypic.com/158bcl4.gif</a>; however, the results are posted without offering methodology, version, date, or attribution info.</p></div></div><p>The benchmarks provided in these studies from 2011 and 2013 are a helpful starting point, but significantly conflict with the KVM best practice statements.</p><p>These investigations sometimes resonate, but can also conflict with Proxmox VE or the KVMs best I/O performance tips and testimonials available on the web.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The investigations cited previously support virtio paravirtualizaiton as being the bus of choice whenever the choice is possible, which is anytime drivers are available for the guest OS.</li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Generally, caching adds redundant data and bus traffic and ultimately will impact performance negatively. For best results, choose <span class="strong"><strong>No Cache</strong></span> for RAW images and avoid the <span class="strong"><strong>Directsync</strong></span> and <span class="strong"><strong>Write Through</strong></span> cache options with QCOW2 images except when working with the ZSF filesystem and a RAID array as your primary storage.</li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">RAW is broadly acknowledged to provide the best performance among the three formats available in Proxmox VE; however, it's at the cost of the significant bundle of features QCOW2 images offer. The benefits of the feature set should certainly be weighed against RAW's performance, particularly with <span class="strong"><strong>No Cache</strong></span> selected in combination with <span class="strong"><strong>VIRTIO</strong></span>. If you rely on solid state storage, much of the performance difference between RAW and QCOW2 becomes unnoticeable.</li><li class="listitem" style="list-style-type: disc">The Consensus from the Proxmox VE user community is that there is no practical benefit to building a VM in Proxmox VE with a VMDK disk image.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip60"/>Tip</h3><p>Proxmox VE and KVM-QEMU best practice resources</p><p><a class="ulink" href="https://pve.proxmox.com/wiki/Performance_Tweaks">https://pve.proxmox.com/wiki/Performance_Tweaks</a></p><p>
<a class="ulink" href="http://www.ilsistemista.net/index.php/virtualization/23-kvm-storage-performance-and-cache-settings-on-red-hat-enterprise-linux-62.html?start=2">http://www.ilsistemista.net/index.php/virtualization/23-kvm-storage-performance-and-cache-settings-on-red-hat-enterprise-linux-62.html?start=2</a></p><p>
<a class="ulink" href="http://www.linux-kvm.org/page/Tuning_KVM">http://www.linux-kvm.org/page/Tuning_KVM</a>
</p></div></div><p>With the conclusion of this section, we have accomplished several objectives towards the goal articulated at the beginning of the chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have the resources to make informed choices about appropriate bus/interfaces for our Proxmox VE VM guests</li><li class="listitem" style="list-style-type: disc">We can now deliberately choose from among virtual disk image formats available for use through the Proxmox VE interface and can pursue further support as needed</li></ul></div></div></div></div></div>
<div class="section" title="Learning more"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Learning more</h1></div></div></div><p>If this chapter sends any single message clearly, it should be this: optimizing the I/O performance of a VM in Proxmox VE involves carefully considering and combining three components, each with a very rich set of options—tuning a VM to perform optimally and with the features you want is a complex balancing act.</p><p>Consequently, a list of helpful resources is provided here so each of us can pursue more information based on our specific needs. The first two resources have rich chapters on virtual disks. The third is a work in progress that's thoroughly committed to virtual disk documentation:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Hands-On Virtual Computing</em></span>, Ted Simpson, Cengage</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Virtualization from the Desktop to the Enterprise</em></span>, Chris Wolf, Apress</li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>The Linux Sysadmins Guide to Virtual Disks: From the Basics to the Advanced</em></span>, Tim Bielawa, <a class="ulink" href="http://lnx.cx/docs/vdg/output/Virtual-Disk-Operations.pdf">http://lnx.cx/docs/vdg/output/Virtual-Disk-Operations.pdf</a>.</li></ul></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Summary</h1></div></div></div><p>Our purpose in this chapter has been to understand virtual disks in the context of Proxmox VE virtual machine guests (the chapter does not address anything that concerns container guests).</p><p>We've accomplished a difficult task by focusing on how Proxmox VE, and its underlying virtualization technology, handle the abstractions of the components of physical hard disk drives: storage media, the bus interface, and the disk buffer.</p><p>As we proceeded, we worked hard to understand and be able to articulate how the choices we make when determining disk format, bus, and disk buffer preferences can significantly affect both features and I/O performance.</p><p>At the most fundamental level, we recognized that our VMs will not have better I/O proficiency than our physical host's hardware allows. However, you also learned that, by relying on paravirtualization drivers, you can minimize the overhead cost of virtualization on I/O performance.</p><p>To summarize, you learned that our decision making regarding virtual-disk configuration depends on how we answer some fundamental questions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What physical hardware do you already have that affects I/O performance?</li><li class="listitem" style="list-style-type: disc">What are the performance needs of the application and/or database that the VM is dedicated to serving?</li><li class="listitem" style="list-style-type: disc">What features of a virtual disk format should you take advantage of? Which redundant features can be provided by other technologies in your datacenter?</li><li class="listitem" style="list-style-type: disc">What are the capacity needs of the OS and application and/or database?</li><li class="listitem" style="list-style-type: disc">What are the administrators already familiar with and willing to support in terms of file systems, virtualization generally, and virtual disks in particular?</li><li class="listitem" style="list-style-type: disc">What are the OS concerns and requirements in regards to file systems and bus drivers?</li></ul></div><p>In the next chapter, we continue to build on what you learned in <a class="link" href="ch04.html" title="Chapter 4. Creating Virtual Machines">Chapter 4</a>, <span class="emphasis"><em>Creating a Virtual Machine</em></span>, about the creation of virtual machines by focusing on the Proxmox VE network model. To do this, we'll rely on our prior knowledge of TCP/IP LAN networks, switching, and subnets.</p><p>It'll be a great transitional chapter as we move toward understanding Proxmox VE and virtualization security.</p><p>Let's connect nodes and build bridges!</p></div></body></html>