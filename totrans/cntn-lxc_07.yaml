- en: Chapter 7. Monitoring and Backups in a Containerized World
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。容器化世界中的监控与备份
- en: In the previous chapter, we looked at a few examples of how to scale applications
    running inside LXC containers, by creating multiple instances behind a proxy service
    such as HAProxy. This ensures the application has enough resources and can withstand
    failures, thus achieving a level of high availability.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看了一些如何通过创建多个实例并放置在代理服务（如 HAProxy）后面，来扩展在 LXC 容器中运行的应用程序的示例。这确保了应用程序有足够的资源并能承受故障，从而实现一定程度的高可用性。
- en: For applications running in a single LXC instance, it's often desirable to perform
    periodic backups of the container, which includes the root filesystem and the
    container's configuration file. Depending on the backing store, there are a few
    available options that we are going to explore in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于运行在单个 LXC 实例中的应用程序，通常希望定期备份容器，包括根文件系统和容器的配置文件。根据后台存储的不同，有一些可用的选项，我们将在本章中探讨。
- en: Using a highly available or shared backend store can help with quickly recovering
    from failed hosts, or when one needs to migrate LXC containers between servers.
    We are going to look at how to create containers on an **Internet Small Computer
    Systems Interface** (**iSCSI**) target and migrate LXC between servers. We are
    also going to look at an example of how to use GlusterFS as a shared filesystem
    to host the root filesystem of containers, thus creating active-passive LXC deployments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高度可用或共享的后台存储可以帮助快速从失败的主机恢复，或者在需要在服务器之间迁移 LXC 容器时提供帮助。我们将看看如何在**互联网小型计算机系统接口**（**iSCSI**）目标上创建容器，并在服务器之间迁移
    LXC。我们还将给出一个示例，展示如何使用 GlusterFS 作为共享文件系统来托管容器的根文件系统，从而创建活动-被动 LXC 部署。
- en: We are also going to go over how to monitor LXC containers and the services
    running inside them, with various tools such as Monit and Sensu; we will end the
    chapter with an example of creating a simple autoscaling solution based on monitoring
    triggers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍如何使用各种工具（如 Monit 和 Sensu）监控 LXC 容器及其内部运行的服务；我们将在本章结束时给出一个基于监控触发器创建简单自动扩展解决方案的示例。
- en: 'In this chapter, we are going to explore the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: Backing up LXC containers using `tar` and `rsync`
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `tar` 和 `rsync` 备份 LXC 容器
- en: Backing up LXC containers with the `lxc-copy` utility
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `lxc-copy` 工具备份 LXC 容器
- en: Creating LXC containers using the iSCSI target as a backend store and demonstrating
    how to migrate containers if the primary server goes down
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 iSCSI 目标作为后台存储创建 LXC 容器，并演示当主服务器故障时如何迁移容器
- en: Demonstrating how to use GlusterFS as a shared filesystem for the root filesystem
    of containers and deploy active-passive LXC nodes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示如何使用 GlusterFS 作为容器根文件系统的共享文件系统，并部署活动-被动 LXC 节点
- en: Looking into what metrics LXC exposes, and how to monitor, alert, and act on
    them
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 LXC 暴露了哪些度量指标，以及如何监控、警报和处理这些指标
- en: Backing up and migrating LXC
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 备份和迁移 LXC
- en: Creating backups of LXC instances ensures that we can recover from events such
    as server crashes or corrupt backend stores. Backups also provide for a quick
    way of migrating instances between hosts or starting multiple similar containers,
    by just changing their config files.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 LXC 实例的备份可以确保我们在服务器崩溃或后台存储损坏等事件发生时能够恢复数据。备份还为在主机之间迁移实例或通过更改配置文件快速启动多个相似容器提供了一种便捷的方式。
- en: Creating LXC backup using tar and rsync
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 tar 和 rsync 创建 LXC 备份
- en: 'In most use cases, we build containers from templates, or with tools such as
    debootstrap, which result in an entire root filesystem, for the instance. Creating
    backups in such cases is a matter of stopping the container, archiving its configuration
    file along with the actual root filesystem, then storing them on a remote server.
    Let''s demonstrate this concept with a simple example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数使用场景中，我们从模板构建容器，或使用如 debootstrap 等工具，这些工具会为实例创建完整的根文件系统。在这种情况下，创建备份的方法是停止容器，归档其配置文件以及实际的根文件系统，然后将它们存储在远程服务器上。让我们通过一个简单的示例来演示这个概念：
- en: 'Start by updating your Ubuntu distribution and installing LXC. For this example,
    we''ll be using Ubuntu 16.04, but the instructions apply to any other Linux distribution:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先更新你的 Ubuntu 发行版并安装 LXC。在本示例中，我们将使用 Ubuntu 16.04，但该说明适用于任何其他 Linux 发行版：
- en: '[PRE0]'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, proceed by creating a container using the default directory backend store,
    and start it:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用默认目录后台存储创建一个容器，并启动它：
- en: '[PRE1]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Install an application inside LXC, in this case, Nginx, and create a custom
    index file:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 LXC 内安装一个应用程序，本例中是 Nginx，并创建一个自定义的索引文件：
- en: '[PRE2]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the container running, ensure we can reach the HTTP service:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在容器运行时，确保我们能够访问 HTTP 服务：
- en: '[PRE3]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Notice how the container''s filesystem and its configuration file are self-contained
    in the `/var/lib/lxc` directory:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，容器的文件系统及其配置文件都自包含在 `/var/lib/lxc` 目录中：
- en: '[PRE4]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is quite convenient for creating a backup using standard Linux utilities
    such as `tar` and `rsync`. Stop the container before we back it up:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准 Linux 工具（如 `tar` 和 `rsync`）创建备份非常方便。在备份之前，请先停止容器：
- en: '[PRE5]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create a `bzip` archive of the root filesystem and the configuration file,
    making sure to preserve the numeric owner IDs, so that we can later start it on
    a different server without creating user ID collisions:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建根文件系统和配置文件的 `bzip` 归档文件，确保保留数字所有者 ID，这样我们就可以在不同的服务器上启动它而不会发生用户 ID 冲突：
- en: '[PRE6]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, copy the archive to a different server, in this case, `ubuntu-backup`,
    and delete the archive from the original server:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将归档文件复制到另一台服务器，在本例中是 `ubuntu-backup`，并从原始服务器删除该归档文件：
- en: '[PRE7]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the archive on the destination server, we are now ready to restore it when
    needed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标服务器上有了归档文件后，我们现在可以在需要时恢复它。
- en: Restoring from the archived backup
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从归档备份恢复
- en: 'To restore the container from the `bz2` backup, on the destination server,
    extract the archive in `/var/lib/lxc`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 `bz2` 备份中恢复容器，在目标服务器上，提取 `/var/lib/lxc` 目录中的归档文件：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Notice how, after extracting the root filesystem and the configuration file,
    listing all containers will show the container we just restored, albeit in a stopped
    state:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在提取根文件系统和配置文件后，列出所有容器将显示我们刚刚恢复的容器，尽管它处于停止状态：
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s start it and hit the HTTP endpoint, ensuring we get the same result
    back as with the original LXC instance:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始并访问 HTTP 端点，确保我们得到与原始 LXC 实例相同的结果：
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To clean up, run the following commands:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要清理，请运行以下命令：
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating container backup using lxc-copy
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 lxc-copy 创建容器备份
- en: 'Regardless of the backend store of the container, we can use the `lxc-copy`
    utility to create a full copy of the LXC instance. Follow these steps for creating
    container backup:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 无论容器的后端存储是什么，我们都可以使用 `lxc-copy` 工具来创建 LXC 实例的完整副本。按照以下步骤创建容器备份：
- en: 'We start by specifying the name of the container on the original host we want
    to back up, and a name for the copy:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先指定要备份的原始主机上的容器名称，以及副本的名称：
- en: '[PRE12]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This command created a new root filesystem and configuration file on the host:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该命令在主机上创建了一个新的根文件系统和配置文件：
- en: '[PRE13]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Creating a full copy will update the configuration file of the new container
    with the newly specified name and location of the `rootfs`:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建完整副本时，会更新新容器的配置文件，指定新的容器名称和 `rootfs` 的位置：
- en: '[PRE14]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice the container's name and directory have changed from the original instance.
    Now we can archive and store remotely, as shown in the previous section, with
    `tar` and `rsync`. Using this method is convenient, if we need to ensure the name
    of the container and the location of its `rootfs` differ from the original, in
    the case where we would like to keep the backup on the same server, or on hosts
    with the same LXC name.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，容器的名称和目录已从原始实例更改。现在，我们可以像上一节中展示的那样使用 `tar` 和 `rsync` 归档并远程存储。如果我们需要确保容器的名称和
    `rootfs` 位置不同于原始实例，这种方法非常方便，尤其是在我们希望将备份保存在同一台服务器上，或在具有相同 LXC 名称的主机上时。
- en: 'Finally, to clean up, execute the following command:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，要清理，请执行以下命令：
- en: '[PRE15]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Migrating LXC containers on an iSCSI target
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 iSCSI 目标上迁移 LXC 容器
- en: Moving containers from one host to another can be quite useful when performing
    server maintenance, or when redistributing server load. Cloud platforms such as
    OpenStack leverage schedulers that choose where an LXC instance should be created
    and tools to migrate them, based on certain criteria such as instance density,
    resource utilization, and others, as we'll see in the next chapter. Nevertheless,
    it helps to know how to manually migrate instances between hosts should the need
    arise.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将容器从一台主机迁移到另一台主机在进行服务器维护或重新分配服务器负载时非常有用。像 OpenStack 这样的云平台利用调度程序来选择 LXC 实例创建的位置，并基于某些标准（如实例密度、资源利用率等）迁移它们，正如我们在下一章中将看到的那样。不过，了解如何在需要时手动迁移主机间的实例是很有帮助的。
- en: Creating a backup or a copy of the container is easier when we use the default
    `dir` backing store with LXC; however, it is not as trivial when leveraging other
    types such as LVM, Btrfs, or ZFS, unless using a shared storage, such as GlusterFS,
    or iSCSI block devices, which we are going to explore next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用默认的 `dir` 存储后端与 LXC 时，创建容器的备份或副本更加容易；然而，当使用其他类型的存储（如 LVM、Btrfs 或 ZFS）时，除非使用共享存储（如
    GlusterFS 或 iSCSI 块设备），否则这并不那么简单，我们将接下来探讨这一点。
- en: The iSCSI protocol has been around for a while, and lots of **Storage Area Networks**
    (**SAN**) solutions exist around it. It's a great way of providing access to block
    devices over a TCP/IP network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI 协议已经存在了一段时间，围绕它有许多**存储区域网络**（**SAN**）解决方案。它是通过 TCP/IP 网络提供块设备访问的绝佳方式。
- en: Setting up the iSCSI target
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 iSCSI 目标
- en: The endpoint that exposes the block device is called the **target**. Creating
    an iSCSI target on Linux is quite trivial. All we need is a block device. In the
    following example, we are going to use two servers; one will be the iSCSI target
    that exposes a block device, and the other will be the initiator server, which
    will connect to the target and use the presented block device as the location
    for the LXC containers' root filesystem and configuration files.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 暴露块设备的端点被称为**目标**。在 Linux 上创建 iSCSI 目标是相当简单的。我们所需要的仅仅是一个块设备。在下面的示例中，我们将使用两台服务器；一台将作为
    iSCSI 目标，暴露一个块设备，另一台将是发起者服务器，它将连接到目标并将提供的块设备作为 LXC 容器的根文件系统和配置文件的存储位置。
- en: 'Follow these steps for setting up the iSCSI target:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 iSCSI 目标的步骤如下：
- en: 'Let''s start by installing the necessary package on Ubuntu, and then demonstrate
    the same on CentOS:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先在 Ubuntu 上安装必要的软件包，然后在 CentOS 上演示相同的操作：
- en: '[PRE16]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With the package installed, enable the target functionality:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装软件包后，启用目标功能：
- en: '[PRE17]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, let''s create the target configuration file. The file is well-documented
    with comments; we are going to use a small subset of the available configuration
    options. We start by specifying an arbitrary identifier, `iqn.2001-04.com.example:lxc`
    in this example, a username and password, and, most importantly, the block device
    we are going to expose over iSCSI -`/dev/xvdb`. The iSCSI identifier is in the
    following form:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们创建目标配置文件。该文件有良好的注释说明；我们将使用一些可用配置选项中的一个小子集。我们从指定一个任意标识符开始，在此示例中为 `iqn.2001-04.com.example:lxc`，用户名和密码，最重要的是，我们将暴露的块设备
    `/dev/xvdb`。iSCSI 标识符的格式如下：
- en: '[PRE18]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The description of this identifier is as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该标识符的描述如下：
- en: '`yyyy-mm` : This is the year and month when the naming authority was established'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yyyy-mm` : 这是命名机构成立的年份和月份'
- en: '`naming-authority`:  This is usually reverse syntax of the Internet domain
    name of the naming authority, or the domain name of the server'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`naming-authority`：通常是命名机构的互联网域名的反向语法，或者是服务器的域名'
- en: '`unique name`: This is any name you would like to use'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unique name`：这是你希望使用的任何名称'
- en: 'With these in mind, the minimal working target configuration file is as follows:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到这些，最小的工作目标配置文件如下：
- en: '[PRE19]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, specify which hosts or initiators are allowed to connect to the iSCSI
    target; replace the IP with that of the host that is going to connect to the target
    server:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，指定哪些主机或发起者可以连接到 iSCSI 目标；将 IP 地址替换为将连接到目标服务器的主机的 IP 地址：
- en: '[PRE20]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, start the iSCSI target service:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，启动 iSCSI 目标服务：
- en: '[PRE21]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'On CentOS, the process and configuration files are slightly different. To install
    the package, execute the following command:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 CentOS 上，过程和配置文件稍有不同。要安装软件包，请执行以下命令：
- en: '[PRE22]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The configuration file has the following format:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置文件的格式如下：
- en: '[PRE23]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To start the service, run the following command:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动服务，请运行以下命令：
- en: '[PRE24]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s list the exposed target by running the following command:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过运行以下命令列出暴露的目标：
- en: '[PRE25]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Setting up the iSCSI initiator
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 iSCSI 发起者
- en: The initiator is the server that will connect to the target and access the exposed
    block device over the iSCSI protocol.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 发起者是将连接到目标并通过 iSCSI 协议访问暴露的块设备的服务器。
- en: 'To install the initiator tools on Ubuntu, run the following command:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在 Ubuntu 上安装发起者工具，请运行以下命令：
- en: '[PRE26]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'On CentOS, the package name is different:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 CentOS 上，软件包名称有所不同：
- en: '[PRE27]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, enable the service and start the iSCSI daemon:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，启用服务并启动 iSCSI 守护进程：
- en: '[PRE28]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `iscsiadm` command is the userspace tool that we can use from the initiating
    server to ask the target for available devices. From the initiator host, let''s
    ask the target server we configured earlier what block devices are available:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`iscsiadm` 命令是我们可以从发起服务器使用的用户空间工具，用来请求目标提供可用的设备。从启动主机上，我们向之前配置的目标服务器询问可用的块设备：'
- en: '[PRE29]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: From the preceding output, we can see that the target server presented one target
    identified by the **iSCSI Qualified Name** (**IQN**) of `iqn.2001-04.com.example:lxc`.
    In this case, the target server has three IP addresses, thus the three lines from
    the output.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从上面的输出中，我们可以看到目标服务器提供了一个目标，其 **iSCSI 合格名称**（**IQN**）为 `iqn.2001-04.com.example:lxc`。在这种情况下，目标服务器有三个
    IP 地址，因此输出中有三行。
- en: 'Earlier, we configured the target to use a username and a password. We need
    to configure the initiator host to present the same credentials to the target
    host in order to access the resources:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前，我们配置了目标使用用户名和密码。我们需要配置启动主机，以便向目标主机提供相同的凭证来访问资源：
- en: '[PRE30]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After updating the initiator''s configuration, we can check that the credentials
    have been applied by examining the configuration file that was automatically created:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新启动器配置后，我们可以通过检查自动创建的配置文件来确认凭证是否已应用：
- en: '[PRE31]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Logging in to the iSCSI target using the presented block device as rootfs for
    LXC
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用提供的块设备登录到 iSCSI 目标，并将其作为 LXC 的 rootfs
- en: 'With the initiator host configured, we are now ready to log in to the iSCSI
    target and use the presented block device:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好启动主机后，我们现在可以登录到 iSCSI 目标并使用提供的块设备：
- en: 'To log in, from the initiator host run the following command:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要登录，从启动主机运行以下命令：
- en: '[PRE32]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s verify that the initiator host now contains an iSCSI session:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们验证启动主机现在是否包含 iSCSI 会话：
- en: '[PRE33]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The initiator host should now have a new block device available for use:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动主机现在应该有一个新的块设备可以使用：
- en: '[PRE34]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This new block device can be used as a regular storage device. Let''s create
    a filesystem on it:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个新的块设备可以作为常规存储设备使用。我们来在上面创建一个文件系统：
- en: '[PRE35]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'With the filesystem present, let''s use the block device as the default location
    for the LXC filesystem, by mounting it at `/var/lib/lxc`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件系统存在的情况下，我们将块设备作为默认位置，挂载到 `/var/lib/lxc`，以便用于 LXC 文件系统：
- en: '[PRE36]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Since we mounted the iSCSI device at the default location for the root filesystem
    of the LXC containers, next time we build one, its root filesystem will be on
    the iSCSI device. As we'll see shortly, this is useful if the initiator host needs
    to undergo maintenance, or becomes unavailable for whatever reason, because we
    can mount the same iSCSI target to a new host and just start the same containers
    there with no configuration changes on the LXC side.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将 iSCSI 设备挂载到 LXC 容器根文件系统的默认位置，下次我们创建容器时，其根文件系统将位于 iSCSI 设备上。正如我们稍后将看到的那样，这在启动主机需要维护或由于某种原因变得不可用时非常有用，因为我们可以将相同的
    iSCSI 目标挂载到新主机上，并且只需在 LXC 端没有配置更改的情况下启动相同的容器。
- en: Building the iSCSI container
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建 iSCSI 容器
- en: 'Create a new LXC container as usual, start it, and ensure it''s running:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例创建一个新的 LXC 容器，启动它，并确保它正在运行：
- en: '[PRE37]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Even though the root filesystem is now on the new iSCSI block device, from
    the perspective of the host OS, nothing is different:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 即使根文件系统现在位于新的 iSCSI 块设备上，从主机操作系统的角度来看，也没有任何变化：
- en: '[PRE38]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Since the `rootfs` of the container now resides on a block device that is remote,
    depending on the network connection between the LXC initiator host and the ISCSI
    target host, some latency might be present. In production deployments, an isolated
    low-latency network is preferred between the iSCSI target and the initiator hosts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于容器的 `rootfs` 现在位于远程块设备上，取决于 LXC 启动主机与 iSCSI 目标主机之间的网络连接，可能会有一些延迟。在生产部署中，建议
    iSCSI 目标和启动主机之间使用隔离的低延迟网络。
- en: 'To migrate the container to a different host, we need to stop it, unmount the
    disk, then log out the block device from the iSCSI target:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要将容器迁移到另一个主机，我们需要停止容器，卸载磁盘，然后从 iSCSI 目标注销块设备：
- en: '[PRE39]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Restoring the iSCSI container
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恢复 iSCSI 容器
- en: 'To restore the iSCI container, follow these steps:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要恢复 iSCSI 容器，请按以下步骤操作：
- en: 'On a new host that is configured as an initiator, we can log in the same target
    as we saw earlier:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在配置为启动主机的新主机上，我们可以登录到与之前相同的目标：
- en: '[PRE40]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Ensure that a new block device has been presented after logging in to the iSCSI
    target:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保在登录到 iSCSI 目标后已提供新的块设备：
- en: '[PRE41]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When logging in the iSCSI target on a new host, keep in mind that the name of
    the presented block device might be different from what it was on the original
    server.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当在新主机上登录 iSCSI 目标时，请记住，呈现的块设备的名称可能与原始服务器上的不同。
- en: 'Next, mount the block device to the default location of LXC root filesystem:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将块设备挂载到 LXC 根文件系统的默认位置：
- en: '[PRE42]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If we list all available containers now, we''ll see that the container we created
    on the previous host is now present on the new server, just by mounting the iSCSI
    target:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们现在列出所有可用的容器，我们会看到之前在主机上创建的容器现在已经出现在新服务器上，仅通过挂载 iSCSI 目标即可实现：
- en: '[PRE43]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we can start the container as usual:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以像往常一样启动容器：
- en: '[PRE44]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If the container is configured with a static IP address, the same IP will be
    present on the new host; however, if the container obtains its network configuration
    dynamically, its IP might change. Keep this in mind if there is an associated
    A record in DNS for the container you are migrating.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器配置了静态 IP 地址，则新主机上也会有相同的 IP；然而，如果容器通过动态方式获取网络配置，则其 IP 地址可能会发生变化。如果你的容器迁移时在
    DNS 中有关联的 A 记录，请记住这一点。
- en: LXC active backup with replicated GlusterFS storage
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用复制 GlusterFS 存储的 LXC 活跃备份
- en: In the previous section, we saw how to use a remote block device as a local
    storage for LXC by means of exporting it over iSCSI. We formatted the block device
    with a filesystem that does not allow shared access from multiple servers. If
    you log in the target device to more than one server and they try to write to
    it at the same time, corruption will occur. Using iSCSI devices on a single node
    to host the LXC containers provides for an excellent way of achieving a level
    of redundancy should the LXC server go down; we just log in the same block device
    on a new initiator server and start the containers. We can consider this as a
    form of cold backup, since there will be downtime while moving the iSCSI block
    device to the new host, logging in to it, and starting the container.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们展示了如何通过 iSCSI 导出远程块设备，将其用作 LXC 的本地存储。我们将块设备格式化为不允许多个服务器共享访问的文件系统。如果你在多个服务器上登录目标设备，并且它们同时尝试写入，数据将会损坏。使用单节点上的
    iSCSI 设备托管 LXC 容器提供了一种出色的冗余解决方案，当 LXC 服务器宕机时，我们只需在新发起服务器上登录相同的块设备并启动容器。我们可以将这种方式视为冷备份，因为在将
    iSCSI 块设备迁移到新主机、登录并启动容器时会有停机时间。
- en: There's an alternative way, where we can use a shared filesystem that can be
    attached and accessed from multiple servers at the same time, with the same LXC
    containers running on multiple hosts, with different IP addresses. Let's explore
    this scenario using the scalable network filesystem GlusterFS as the remotely
    shared filesystem of choice.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种方式，我们可以使用一个可以同时被多个服务器附加和访问的共享文件系统，在多个主机上运行相同的 LXC 容器，并且这些容器有不同的 IP 地址。让我们探索这种使用可扩展网络文件系统
    GlusterFS 作为远程共享文件系统的场景。
- en: Creating the shared storage
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建共享存储
- en: GlusterFS has two main components - a server component that runs the GlusterFS
    daemon, which exports local block devices called **bricks**, and a client component
    that connects to the servers with a custom protocol over TCP/IP network and creates
    aggregate virtual volumes that can then be mounted and used as a regular filesystem.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS 具有两个主要组件——一个服务器组件，运行 GlusterFS 守护进程，导出称为 **砖块**（bricks）的本地块设备；一个客户端组件，通过
    TCP/IP 网络与服务器连接，使用自定义协议，创建聚合的虚拟卷，然后可以将其挂载并用作常规文件系统。
- en: 'There are three types of volumes:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种类型的卷：
- en: '**Distributed**: These are volumes that distribute files throughout the cluster'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：这些是将文件分布到集群中的卷'
- en: '**Replicated**: These are volumes that replicate data across two or more nodes
    in the storage cluster'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制**：这些是将数据复制到存储集群中两个或更多节点的卷'
- en: '**Striped**: These stripe files across multiple storage nodes'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条带化**：这些条带文件分布在多个存储节点上'
- en: To achieve high availability of the LXC containers running on such shared volumes,
    we are going to use the replicated volumes on two GlusterFS servers. For this
    example, we are going to use a block device on each GlusterFS server with LVM.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现运行在这些共享卷上的 LXC 容器的高可用性，我们将使用两个 GlusterFS 服务器上的复制卷。在本例中，我们将在每个 GlusterFS
    服务器上使用 LVM 上的块设备。
- en: 'On the first storage node, let''s create the PV, the VG, and the LV on the
    `/dev/xvdb` block device:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个存储节点上，我们将在 `/dev/xvdb` 块设备上创建 PV、VG 和 LV：
- en: '[PRE45]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, let''s create a filesystem on the LVM device and mount it. XFS performs
    quite well with GlusterFS:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们在LVM设备上创建文件系统并挂载它。XFS在GlusterFS中表现非常好：
- en: '[PRE46]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, let''s install the GlusterFS service:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们安装GlusterFS服务：
- en: '[PRE47]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Repeat the preceding steps on the second GlusterFS node, replacing `node1` with
    `node2` as necessary.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第二个GlusterFS节点上重复上述步骤，必要时将`node1`替换为`node2`。
- en: 'Once we have the GlusterFS daemon running on both nodes, it''s time to probe
    from `node1` to see if we can see any peers:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦在两个节点上运行了GlusterFS守护进程，就可以从`node1`进行探测，看看是否能看到任何对等节点：
- en: '[PRE48]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: By probing from `node1`, we can now see that both `node1` and `node2` are part
    of a cluster.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过从`node1`进行探测，我们现在可以看到`node1`和`node2`都属于同一个集群。
- en: 'The next step is to create the replicated volume by specifying the mount path
    of the LV we created and mounted earlier. Since we are using two nodes in the
    storage cluster, the replication factor is going to be `2`. The following commands
    create the replicated volume, which will contain block devices from both `node1`
    and `node2` and list the created volume:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建复制卷，指定之前创建和挂载的LV的挂载路径。由于我们在存储集群中使用了两个节点，复制因子将为`2`。以下命令将创建复制卷，该卷将包含来自`node1`和`node2`的块设备，并列出创建的卷：
- en: '[PRE49]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'To start the newly created replicated volume, run the following commands:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动新创建的复制卷，运行以下命令：
- en: '[PRE50]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: To create and start the volume, we only need to run the preceding commands from
    one of the storage nodes.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要创建并启动卷，我们只需要从其中一个存储节点运行上述命令。
- en: 'To obtain information about the new volume, run this command on any of the
    nodes:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取新卷的信息，请在任何一个节点上运行此命令：
- en: '[PRE51]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'From the preceding output, notice both bricks from `node1` and `node2`. We
    can see them on both nodes as well:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的输出中，注意到来自`node1`和`node2`的两个砖块。我们也可以在两个节点上看到它们：
- en: '[PRE52]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We see the following on `node2`:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在`node2`上看到以下内容：
- en: '[PRE53]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We should see the same files because we are using the replicated volume. Creating
    files on one of the mounted volumes should appear on the other, although, as with
    the iSCSI setup earlier, the network latency is important, since the data needs
    to be transferred over the network.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到相同的文件，因为我们使用的是复制卷。在一个挂载的卷上创建的文件应会出现在另一个上，尽管与之前的iSCSI设置一样，网络延迟很重要，因为数据需要通过网络传输。
- en: Building the GlusterFS LXC container
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建GlusterFS LXC容器
- en: 'With the GlusterFS cluster ready, let''s use a third server to mount the replicated
    volume from the storage cluster and use it as the LXC root filesystem and configuration
    location:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在GlusterFS集群准备好之后，让我们使用第三台服务器从存储集群挂载复制卷，并将其用作LXC根文件系统和配置位置：
- en: 'First, let''s install the GlusterFS client:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装GlusterFS客户端：
- en: '[PRE54]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, mount the replicated volume from the storage cluster to the LXC host:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将存储集群中的复制卷挂载到LXC主机上：
- en: '[PRE55]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In the preceding command, we are using `node2` as the target from which we are
    mounting; however, we can use `node1` in exactly the same way. The name of the
    target device we specify in the `lxc_glusterfs` mount command is what we specified
    as the name of the replicated volume earlier.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的命令中，我们使用`node2`作为目标进行挂载；不过，我们也可以以完全相同的方式使用`node1`。在`lxc_glusterfs`挂载命令中指定的目标设备名称就是我们之前指定的复制卷名称。
- en: 'Now that the replicated GlusterFS volume has been mounted to the default LXC
    location, let''s create and start a container:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，复制的GlusterFS卷已挂载到默认的LXC位置，让我们创建并启动一个容器：
- en: '[PRE56]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Attach to the container and install Nginx so we can later test connectivity
    from multiple servers:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 附加到容器并安装Nginx，以便稍后我们可以从多个服务器测试连接性：
- en: '[PRE57]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Obtain the IP address of the container and ensure we can connect to Nginx from
    the host OS:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取容器的IP地址，并确保我们能从主机操作系统连接到Nginx：
- en: '[PRE58]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'With the container created, the root filesystem and configuration file will
    be visible on both storage nodes:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器创建后，根文件系统和配置文件将在两个存储节点上可见：
- en: '[PRE59]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The following will be visible on `node2`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下内容将在`node2`上可见：
- en: '[PRE60]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Depending on your network bandwidth and latency, replicating the data between
    both storage nodes might take some time. This will also affect how long it takes
    for the LXC container to build, start, and stop.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的网络带宽和延迟，复制数据到两个存储节点之间可能需要一些时间。这也会影响LXC容器的构建、启动和停止所需的时间。
- en: Restoring the GlusterFS container
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恢复GlusterFS容器
- en: 'Let''s create a second LXC host, install the GlusterFS client, and mount the
    replicated volume in the same way we did earlier:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建第二个LXC主机，安装GlusterFS客户端，并以与之前相同的方式挂载复制卷：
- en: '[PRE61]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Notice how, just by mounting the GlusterFS volume, the host now sees the container
    in a stopped state. This is exactly the same container as the running one on `node1` -
    same config and root filesystem, that is. Since we are using a shared filesystem,
    we can start the container on multiple hosts without worrying about data corruption,
    unlike the case with iSCSI:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，仅通过挂载 GlusterFS 卷，主机现在看到容器处于停止状态。这与 `node1` 上运行的容器完全相同——即相同的配置和根文件系统。由于我们使用的是共享文件系统，因此可以在多个主机上启动容器，而不必担心数据损坏，这与使用
    iSCSI 的情况不同：
- en: '[PRE62]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Since we are using DHCP to dynamically assign IP addresses to the containers,
    the same container on the new hosts gets a new IP. Notice how connecting to Nginx
    running in the container gives us the same result, since the container shares
    its filesystem and configuration file across multiple LXC nodes:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用 DHCP 动态分配 IP 地址给容器，新的主机上的相同容器会获得一个新的 IP 地址。请注意，连接到容器中运行的 Nginx 仍然会返回相同的结果，因为容器在多个
    LXC 节点之间共享其文件系统和配置文件：
- en: '[PRE63]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This setup in a sense implements a hot standby backup, where we can use both
    containers behind a proxy service such as HAProxy, with the second node only being
    used when the first node goes down, ensuring that any configuration changes are
    immediately available. As an alternative, both LXC containers can be used at the
    same time, but keep in mind that they'll write to the same filesystem, so the
    Nginx logs in this case will be written from both LXC containers on the `lxc1`
    and `lxc2` nodes.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置在某种程度上实现了热备份，在这种情况下，我们可以使用两个容器通过像 HAProxy 这样的代理服务，第二个节点仅在第一个节点宕机时才会使用，确保任何配置更改立即生效。作为替代方案，可以同时使用两个
    LXC 容器，但请注意，它们会写入相同的文件系统，因此在这种情况下，Nginx 日志将由 `lxc1` 和 `lxc2` 节点上的两个 LXC 容器写入。
- en: Monitoring and alerting on LXC metrics
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和告警 LXC 指标
- en: Monitoring LXC containers is not much different than monitoring a VM or a server -
    we can run a monitoring client inside the container, or on the actual host that
    runs LXC. Since the root filesystem of the containers is exposed on the host and
    LXC uses cgroups and namespaces, we can collect various information directly from
    the host OS, if we would rather not run a monitoring agent in the container. Before
    we look at two examples of LXC monitoring, let's first see how we can gather various
    metrics that we can monitor and alert on.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 监控 LXC 容器与监控虚拟机或服务器没有太大区别——我们可以在容器内部运行监控客户端，或者在运行 LXC 的实际主机上运行。由于容器的根文件系统在主机上可见，且
    LXC 使用 cgroups 和命名空间，我们可以直接从主机操作系统收集各种信息，如果我们不想在容器中运行监控代理的话。在我们查看两个 LXC 监控示例之前，先来看看如何收集我们可以监控和告警的各种指标。
- en: Gathering container metrics
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集容器指标
- en: LXC provides a few simple tools that can be used to monitor the state of the
    container and its resource utilization. The information they provide, as you are
    going to see next, is not that verbose; however, we can utilize the cgroup filesystem
    and collect even more information from it. Let's explore each of these options.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: LXC 提供了一些简单的工具，用于监控容器的状态和资源利用率。正如你接下来将看到的那样，这些工具提供的信息并不冗长；但是，我们可以利用 cgroup 文件系统从中收集更多的信息。让我们来探索一下这些选项。
- en: Using lxc-monitor to track container state
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 lxc-monitor 跟踪容器状态
- en: The `lxc-monitor` tool can be used to track containers, state changes - when
    they start or stop.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxc-monitor` 工具可用于跟踪容器的状态变化——例如它们何时启动或停止。'
- en: 'To demonstrate this, open two terminals; in one, create a new container and
    in the other, run the `lxc-monitor` command. Start the container and observe the
    output in the second terminal:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这一点，打开两个终端；在一个终端创建一个新容器，在另一个终端运行 `lxc-monitor` 命令。启动容器并观察第二个终端中的输出：
- en: '[PRE64]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Stop the container and notice the state change:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 停止容器并注意状态变化：
- en: '[PRE65]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Using lxc-top to obtain CPU and memory utilization
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 lxc-top 获取 CPU 和内存利用率
- en: 'The `lxc-top` tool is similar to the standard `top` Linux command; it shows
    CPU, memory, and I/O utilization. To start it, execute the following command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxc-top` 工具类似于标准的 `top` Linux 命令，它显示 CPU、内存和 I/O 的使用情况。要启动它，请执行以下命令：'
- en: '[PRE66]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Using lxc-info to gather container information
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 lxc-info 收集容器信息
- en: 'We can use the `lxc-info` tool to periodically poll for information such as
    CPU, memory, I/O, and network utilization:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `lxc-info` 工具定期轮询信息，例如 CPU、内存、I/O 和网络利用率：
- en: '[PRE67]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Leveraging cgroups to collect memory metrics
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用 cgroups 收集内存指标
- en: 'In *[Chapter 1](ch01.html "Chapter 1. Introduction to Linux Containers"),**Introduction
    to Linux Containers* we explored cgroups in great details and saw how LXC creates
    a directory hierarchy in the cgroup virtual filesystem for each container it starts.
    To find where the cgroup hierarchy for the container we built earlier is, run
    the following command:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *[第 1 章](ch01.html "第 1 章。Linux 容器介绍")，**Linux 容器介绍* 中，我们详细探讨了 cgroups，并了解了
    LXC 如何在 cgroup 虚拟文件系统中为每个启动的容器创建目录层级。要查找我们之前构建的容器的 cgroup 层级，请运行以下命令：
- en: '[PRE68]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We can use the files in the memory group to set and obtain metrics that we
    can monitor and alert on. For example, to set the memory of the container to 512
    MB, run the following command:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用内存组中的文件来设置和获取可以监控并告警的指标。例如，要将容器的内存设置为 512 MB，运行以下命令：
- en: '[PRE69]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'To read the current memory utilization for the container, execute the following
    command:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取容器当前的内存利用率，请执行以下命令：
- en: '[PRE70]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'To collect more information about the container''s memory, read from the following
    file:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要收集有关容器内存的更多信息，请从以下文件读取：
- en: '[PRE71]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Using cgroups to collect CPU statistics
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 cgroups 收集 CPU 统计数据
- en: 'To collect CPU usage, we can read from the `cpuacct` cgroup subsystem:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要收集 CPU 使用情况，我们可以从 `cpuacct` cgroup 子系统中读取：
- en: '[PRE72]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Collecting network metrics
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集网络指标
- en: 'Each container creates a virtual interface on the host; the output from the
    `lxc-info` command earlier displayed it as `veth8OX0PW`. We can collect information
    about the packets sent and received, error rates, and so on, by running the following
    command:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 每个容器在主机上创建一个虚拟接口；之前从 `lxc-info` 命令显示的接口是 `veth8OX0PW`。我们可以通过运行以下命令来收集发送和接收的包、错误率等信息：
- en: '[PRE73]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Alternatively, we can connect to the container''s network namespace and obtain
    the information that way. The following three commands demonstrate how to execute
    commands in the container''s network namespace. Note the PID of `19967`; it can
    be obtained from the `lxc-info` command:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以连接到容器的网络命名空间并通过这种方式获取信息。以下三个命令演示了如何在容器的网络命名空间中执行命令。注意 `19967` 的 PID；它可以从
    `lxc-info` 命令中获取：
- en: '[PRE74]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Notice how we can see the network interfaces that are inside the LXC container,
    even though we ran the commands on the host.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以看到 LXC 容器内部的网络接口，尽管我们是在主机上运行这些命令。
- en: Simple container monitoring and alerting with Monit
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Monit 进行简单的容器监控和告警
- en: Now that we've demonstrated how to gather various monitoring data points, let's
    actually set up a system to monitor and alert on them. In this section, we are
    going to install a simple monitoring solution utilizing the `monit` daemon. Monit
    is an easy-to-configure service that uses scripts that can be automatically executed,
    based on certain monitoring events and thresholds.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经演示了如何收集各种监控数据，接下来我们将实际设置一个监控系统并对其进行告警。在本节中，我们将安装一个简单的监控解决方案，利用 `monit`
    守护进程。Monit 是一个易于配置的服务，使用可以基于特定监控事件和阈值自动执行的脚本。
- en: 'Let''s look at a few examples next:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一些例子：
- en: 'To install `monit`, run the following command:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装 `monit`，运行以下命令：
- en: '[PRE75]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Next, create a minimal configuration file. The config that is packaged with
    the installation is documented quite well:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个最小配置文件。随安装包提供的配置文档写得相当清楚：
- en: '[PRE76]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The preceding config starts a web interface that can be reached on port `2812`
    with the specified credentials. It also defines two more directories where config
    files can be read.
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的配置启动了一个可以通过指定凭证在端口 `2812` 访问的 Web 界面。它还定义了两个可以读取配置文件的目录。
- en: 'Next, let''s create a monitoring configuration that checks whether a container
    is running. Executing a script, which we''ll write next, performs the actual check:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个监控配置，检查容器是否在运行。执行一个脚本，我们将写在下面，来执行实际检查：
- en: '[PRE77]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The preceding configuration tells `monit` to run the `container_running.sh`
    script periodically and, if the exit status is `1`, to execute a second script
    called `alert.sh` that will alert us. Simple enough. The `container_running.sh`
    script follows:'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的配置指示 `monit` 定期运行 `container_running.sh` 脚本，如果退出状态为 `1`，则执行一个名为 `alert.sh`
    的第二个脚本来发出警报。很简单。`container_running.sh` 脚本如下：
- en: '[PRE78]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We can see that we are utilizing the `lxc-info` command to check the status
    of the container. The `alert.sh` script is even simpler:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到我们正在使用 `lxc-info` 命令来检查容器的状态。`alert.sh` 脚本更加简单：
- en: '[PRE79]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Reload the `monit` service and check the status of the new monitoring service
    that we named `container_status` earlier in the configuration file:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新加载`monit`服务并检查我们在配置文件中之前命名的新的监控服务`container_status`的状态：
- en: '[PRE80]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We can also connect to the web interface on port `2812` and see the newly defined
    monitoring target:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还可以通过端口 `2812` 连接到网页界面，并查看新定义的监控目标：
- en: '![Simple container monitoring and alerting with Monit](img/image_07_001.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 Monit 进行简单容器监控和警报](img/image_07_001.jpg)'
- en: 'Let''s stop the container and check the status of `monit`:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们停止容器并检查`monit`的状态：
- en: '[PRE81]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '![Simple container monitoring and alerting with Monit](img/image_07_002.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用 Monit 进行简单容器监控和警报](img/image_07_002.jpg)'
- en: 'Notice from the output of the command and the web interface that the status
    of the `container_status` service is now `failed`. Since we set up `monit` to
    send an e-mail when the service we are monitoring is failing, check the mail log.
    You should have received an e-mail from `monit`, which will most likely end up
    in your `spam` folder:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令输出和网页界面中注意到，`container_status`服务的状态现在是`failed`。由于我们设置了`monit`，当我们监控的服务失败时会发送电子邮件，检查邮件日志。你应该已经收到了来自`monit`的电子邮件，这封邮件很可能会被归入你的`垃圾邮件`文件夹：
- en: '[PRE82]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: For more information on Monit, refer to [https://mmonit.com/monit/documentation/monit.html](https://mmonit.com/monit/documentation/monit.html).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多关于 Monit 的信息，请参见[https://mmonit.com/monit/documentation/monit.html](https://mmonit.com/monit/documentation/monit.html)。
- en: Monit is a quick and easy way to set up monitoring for LXC containers per server.
    It is agentless, and thanks to the exposed metrics from the cgroup hierarchies,
    it is easy to alert on various data points, without the need to attach or run
    anything extra in the containers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Monit 是为每个服务器设置 LXC 容器监控的快速简便方法。它是无代理的，且由于 cgroup 层次结构中暴露的指标，可以轻松对各种数据点设置警报，而无需在容器中附加或运行任何额外的组件。
- en: Container monitoring and alert triggers with Sensu
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Sensu 进行容器监控和警报触发
- en: Monit is a great tool for monitoring and alerting in a decentralized setup.
    However, for a more robust and feature-rich way of deploying centralized-based
    monitoring, other monitoring tools such as Sensu can be leveraged. There are two
    main ways to implement monitoring with Sensu - with an agent in each container,
    or on the LXC host with standalone checks collecting data from sources such as
    cgroups, in a way similar to Monit.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Monit 是一个非常适合在分散式环境中进行监控和警报的工具。然而，为了实现更强大且功能丰富的集中式监控部署，可以使用其他监控工具，如 Sensu。使用
    Sensu 实现监控有两种主要方式——在每个容器中部署代理，或者在 LXC 主机上使用独立检查，类似于 Monit，从 cgroups 等数据源收集数据。
- en: Sensu uses the client-server architecture, in the sense that the server publishes
    checks in a messaging queue that is provided by RabbitMQ, and the clients subscribe
    to the topic in that queue and execute checks and alerts based on set thresholds.
    State and historical data is stored in a Redis server.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Sensu 使用客户端-服务器架构，服务器通过 RabbitMQ 提供的消息队列发布检查，客户端订阅该队列中的主题，并根据设定的阈值执行检查和警报。状态和历史数据存储在
    Redis 服务器中。
- en: Let's demonstrate a Sensu deployment with an agent inside the LXC container
    first, then move on to an agentless monitoring.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先展示在 LXC 容器内部的 Sensu 部署，然后再进行无代理监控。
- en: Monitoring LXC containers with Sensu agent and server
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Sensu 代理和服务器监控 LXC 容器
- en: 'We need to install the required services that Sensu will use, Redis and RabbitMQ:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装 Sensu 使用的必要服务，包括 Redis 和 RabbitMQ：
- en: 'Let''s start with the Redis server, and, once it''s installed, ensure it''s
    running:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先安装 Redis 服务器，安装完成后，确保它正在运行：
- en: '[PRE83]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Installing RabbitMQ is just as easy:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 RabbitMQ 也同样简单：
- en: '[PRE84]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Once installed, we need to create the virtual host that the agents will be
    subscribing to and reading messages from:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，我们需要创建代理将订阅并读取消息的虚拟主机：
- en: '[PRE85]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Next, create a username and a password to connect to that topic:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个用户名和密码来连接到该主题：
- en: '[PRE86]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Time to install the Sensu server and client:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是安装 Sensu 服务器和客户端的时候了：
- en: '[PRE87]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'At a minimum, we need five configuration files, one for the Sensu API endpoint,
    two that specify what transport we are using - RabbitMQ in this case - the Redis
    config file for Sensu, and a client config file, for the Sensu client running
    on the same server. The following config files are pretty much self-explanatory -
    we specify the IP addresses and ports of the RabbitMQ and Redis servers, along
    with the API service:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，我们需要五个配置文件，一个是Sensu API端点的配置文件，两个指定我们使用的传输方式——在本例中是RabbitMQ——还有Sensu的Redis配置文件，以及在同一服务器上运行的Sensu客户端的客户端配置文件。以下配置文件基本上不言自明——我们指定RabbitMQ和Redis服务器的IP地址和端口，以及API服务：
- en: '[PRE88]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: For more information on Sensu, refer to [https://sensuapp.org/docs/](https://sensuapp.org/docs/).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Sensu的更多信息，请参阅[https://sensuapp.org/docs/](https://sensuapp.org/docs/)。
- en: 'Before we start the Sensu server, we can install a web-based frontend called
    Uchiwa:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动Sensu服务器之前，我们可以安装一个名为Uchiwa的基于Web的前端：
- en: '[PRE89]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'In the configuration file, we specify the address and port of the Sensu API
    service - localhost and port `4567` - and the port Uchiwa will be listening on - `3000` -
    in this case:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件中，我们指定Sensu API服务的地址和端口——本地主机和端口`4567`——以及Uchiwa监听的端口——`3000`——在本例中：
- en: '[PRE90]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'With all of the configuration in place, let''s start the Sensu services:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 配置完成后，我们来启动Sensu服务：
- en: '[PRE91]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'With the Sensu server now fully configured, we need to attach to a container
    on the host and install the Sensu agent:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Sensu服务器已经完全配置好，我们需要连接到主机上的一个容器并安装Sensu代理：
- en: '[PRE92]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'To configure the Sensu agent, we need to edit the client config, where we specify
    the IP address and the name of the container:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置Sensu代理，我们需要编辑客户端配置文件，在其中指定IP地址和容器的名称：
- en: '[PRE93]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We tell the agent how to connect to the RabbitMQ server on the host, by providing
    its IP address, port, and the credentials we created earlier:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提供之前创建的IP地址、端口和凭据，告诉代理如何连接到主机上的RabbitMQ服务器：
- en: '[PRE94]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Then, specify the transport mechanism:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，指定传输机制：
- en: '[PRE95]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'With the preceding three files in place, let''s start the agent:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面提到的三个文件配置好之后，我们来启动代理：
- en: '[PRE96]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '![Monitoring LXC containers with Sensu agent and server](img/image_07_003.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![使用Sensu代理和服务器监控LXC容器](img/image_07_003.jpg)'
- en: To verify that all services are running normally and that the Sensu agent can
    connect to the server, we can connect to the Uchiwa interface on port 3000 using
    the host IP, as shown earlier.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证所有服务是否正常运行，并且Sensu代理能够连接到服务器，我们可以使用主机IP连接到Uchiwa界面，端口为3000，如前所述。
- en: 'While still attached to the LXC container, let''s install a Sensu check. Sensu
    checks are available as gems, or can be written manually. Let''s search the `gem`
    repository for any memory checks instead of writing our own:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然连接到LXC容器时，让我们安装一个Sensu检查。Sensu检查可以作为gem使用，或者可以手动编写。我们来搜索`gem`库，看看是否有现成的内存检查，而不是自己编写：
- en: '[PRE97]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Install the memory check and restart the agent:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 安装内存检查并重启代理：
- en: '[PRE98]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'On the Sensu server host (and not on the container), we need to define the
    new memory check so that the Sensu server can tell the agent to execute it. We
    do that by creating a new checks file:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在Sensu服务器主机上（而不是容器内），我们需要定义新的内存检查，以便Sensu服务器可以告诉代理执行它。我们通过创建一个新的检查文件来完成这一步：
- en: '[PRE99]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'We specify the path and the name of the check that needs to be run from the
    Sensu agent, in this case, the Ruby script that we installed in the container
    from the `gem`. Restart the Sensu services for the changes to take effect:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定需要从Sensu代理运行的检查的路径和名称，在本例中就是我们从`gem`安装到容器中的Ruby脚本。重启Sensu服务以使更改生效：
- en: '[PRE100]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Checking the Uchiwa interface, we can see that the memory check is now active:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Uchiwa界面，我们可以看到内存检查现在已经激活：
- en: '![Monitoring LXC containers with Sensu agent and server](img/image_07_004.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![使用Sensu代理和服务器监控LXC容器](img/image_07_004.jpg)'
- en: We can install multiple Sensu check scripts from gems inside the LXC container;
    define them on the Sensu server just like we would on a normal server or a virtual
    machine, and end up with a full-fledged monitoring solution.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在LXC容器内从gem安装多个Sensu检查脚本；像在普通服务器或虚拟机上一样，在Sensu服务器上定义它们，最终获得一个完整的监控解决方案。
- en: 'There are a few caveats when running an agent inside the container:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器内运行代理时有一些注意事项：
- en: The agent consumes resources; if we are trying to run lightweight containers
    with a minimum amount of memory, this might not be the best solution.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理会消耗资源；如果我们尝试运行内存占用较小的容器，这可能不是最好的解决方案。
- en: When measuring CPU load, the load inside the container will reflect the general
    load on the host itself, since the containers are not isolated from the host by
    a hypervisor. The best way to measure CPU utilization is by obtaining the data
    from the cgroups, or with the `lxc-info` command on the host.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在衡量CPU负载时，容器内的负载将反映宿主机本身的总体负载，因为容器并未通过虚拟化程序与宿主机隔离。衡量CPU利用率的最佳方法是从cgroups获取数据，或者使用宿主机上的`lxc-info`命令。
- en: If using a shared root filesystem such as the one we saw in the previous chapter,
    monitoring the disk space inside the container might reflect the total space on
    the server.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用共享根文件系统（如我们在上一章中看到的那样），监控容器内的磁盘空间可能会反映服务器上的总空间。
- en: If running a Sensu agent inside the LXC container is not desired, we can perform
    standalone checks from the Sensu server on the same host instead. Let's explore
    this setup next.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不希望在LXC容器内运行Sensu代理，我们可以改为在同一主机上的Sensu服务器执行独立检查。接下来，我们来探索这种设置。
- en: Monitoring LXC containers using standalone Sensu checks
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用独立Sensu检查监控LXC容器
- en: On servers that run LXC containers, we can install the Sensu agent directly
    on the host OS, instead of inside each container, just like we did earlier in
    this chapter. We can also leverage Sensu's standalone checks, which provide a
    decentralized way of monitoring, meaning that the Sensu agent defines and schedules
    the checks instead of the Sensu server. This provides us with the benefit of not
    having to install agents and monitoring scripts inside the containers, and having
    the Sensu agents run the checks on each server.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行LXC容器的服务器上，我们可以像本章前面所做的那样，直接在宿主操作系统上安装Sensu代理，而不是在每个容器内安装。我们还可以利用Sensu的独立检查，它提供了一种去中心化的监控方式，这意味着Sensu代理定义并调度检查，而不是由Sensu服务器来完成。这为我们提供了一个好处，即无需在容器内安装代理和监控脚本，Sensu代理会在每个服务器上运行这些检查。
- en: 'Let''s demonstrate how to create a standalone check on the LXC host we''ve
    been using so far:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示如何在我们一直使用的LXC主机上创建一个独立的检查：
- en: '[PRE101]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The main difference when defining a standalone check is the presence of the
    `"standalone": true` stanza, as shown from the preceding output. In the command
    section of the check configuration, we specify what script to execute to perform
    the actual check and the thresholds it should alert on. The script can be anything,
    as long as it exits with error code `2` for Critical alerts, error code `1` for
    Warning, and `0` if all is OK.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '定义独立检查的主要区别在于存在`"standalone": true`的字段，正如前面的输出所示。在检查配置的命令部分，我们指定执行什么脚本来执行实际检查，以及它应当在什么阈值时发出警报。脚本可以是任何东西，只要它在出现关键警报时以错误代码`2`退出，警告时以错误代码`1`退出，所有正常时以`0`退出。'
- en: 'This is a very simple bash script that uses the `memory.usage_in_bytes` cgroup
    file to collect metrics on the memory usage and alert on it if the specified threshold
    is reached:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的bash脚本，它使用`memory.usage_in_bytes` cgroup文件来收集内存使用指标，并在达到指定阈值时发出警报：
- en: '[PRE102]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: The preceding script, albeit simple, should be a good starting point for the
    reader to start writing more useful Sensu checks that operate on various LXC metrics.
    Looking at the script, we can see that it has three basic functions. First, it
    checks for the provided container name, and warning and error thresholds in the
    `sanitiy_check()` function. Then it gets the memory usage in bites from the cgroup
    file in the `get_memory_usage()` function, and finally, reports the results in
    the `report_result()` function, by returning the appropriate error code, as described
    earlier.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本虽然简单，但应该是读者开始编写更多有用的Sensu检查的良好起点，这些检查能够操作各种LXC指标。通过查看脚本，我们可以看到它有三个基本功能。首先，它在`sanitiy_check()`函数中检查提供的容器名称，以及警告和错误阈值。然后，它在`get_memory_usage()`函数中从cgroup文件获取内存使用情况（以字节为单位），最后，它在`report_result()`函数中报告结果，通过返回适当的错误代码，正如之前所描述的那样。
- en: 'Change the permissions and the execution flag of the script, reload Sensu services,
    and make sure the check is showing in Uchiwa:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 更改脚本的权限和执行标志，重新加载Sensu服务，并确保检查在Uchiwa中显示：
- en: '[PRE103]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '![Monitoring LXC containers using standalone Sensu checks](img/image_07_005.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![使用独立Sensu检查监控LXC容器](img/image_07_005.jpg)'
- en: Just like Monit, Sensu provides handlers that get triggered when an alert is
    fired. This can be a custom script that sends an e-mail, makes a call to an external
    service such as PagerDuty, and so on. All of this provides the capability for
    an automated and proactive way of handling LXC alerts.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Monit 类似，Sensu 提供了处理程序，当触发警报时将被触发。这可以是发送电子邮件的自定义脚本，向 PagerDuty 等外部服务打电话等。所有这些都提供了处理
    LXC 警报的自动化和主动的方式。
- en: For more information on Sensu handlers, refer to the documentation at [https://sensuapp.org/docs/latest/reference/handlers.html](https://sensuapp.org/docs/latest/reference/handlers.html).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Sensu 处理程序的更多信息，请参阅 [https://sensuapp.org/docs/latest/reference/handlers.html](https://sensuapp.org/docs/latest/reference/handlers.html)
    的文档。
- en: Simple autoscaling pattern with LXC, Jenkins, and Sensu
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LXC、Jenkins 和 Sensu 的简单自动扩展模式
- en: 'In [Chapter 6](ch06.html "Chapter 6. Clustering and Horizontal Scaling with
    LXC")*, Clustering and Horizontal Scaling with LXC*, we looked at how to horizontally
    scale services with LXC and HAProxy, by provisioning more containers on multiple
    hosts. In this chapter, we explored different ways of monitoring the resource
    utilization of LXC containers and triggering actions based on the alerts. With
    all of this knowledge in place, we can now implement a commonly used autoscaling
    pattern, as shown in the following diagram:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 6 章](ch06.html "第 6 章. 使用 LXC 进行集群和水平扩展") 中，我们探讨了如何通过在多个主机上为 LXC 和 HAProxy
    配置更多容器来实现水平扩展服务。在本章中，我们探讨了监控 LXC 容器资源利用率并根据警报触发操作的不同方法。有了这些知识基础，我们现在可以实现一种常用的自动扩展模式，如下图所示：
- en: '![Simple autoscaling pattern with LXC, Jenkins, and Sensu](img/image_07_006.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![LXC、Jenkins 和 Sensu 的简单自动扩展模式](img/image_07_006.jpg)'
- en: The pattern uses Jenkins as a build system, controlled by the Sensu alert handlers.
    When a Sensu agent running inside an LXC container receives a scheduled check
    from the Sensu server, for example, a memory check, it executes the script and
    returns either `OK`, `Warning`, or a `Critical` status, depending on the configured
    alert thresholds. If the `Critical` status is returned, then a configured Sensu
    handler, which can be as simple as a `curl` command, makes an API call to the
    Jenkins server, which in turn executes a preconfigured job. The Jenkins job can
    be a script that selects an LXC host from a list of hosts, based on a set of criteria
    that either builds a new container, or increases the available memory on the alerting
    LXC container, if possible. This is one of the simplest autoscaling design patterns,
    utilizing a monitoring system and a RESTful build service such as Jenkins.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 此模式使用 Jenkins 作为构建系统，由 Sensu 警报处理程序控制。例如，当运行在 LXC 容器内的 Sensu 代理接收来自 Sensu 服务器的预定检查（例如内存检查）时，它执行脚本并根据配置的警报阈值返回
    `OK`、`Warning` 或 `Critical` 状态。如果返回 `Critical` 状态，则配置的 Sensu 处理程序（可以简单到一个 `curl`
    命令）将 API 调用发送到 Jenkins 服务器，后者依次执行预配置的作业。Jenkins 作业可以是选择从主机列表中的主机的脚本，根据一组标准来构建新容器，或者尝试增加提醒的
    LXC 容器的可用内存。这是一种最简单的自动扩展设计模式之一，利用监控系统和 Jenkins 等 RESTful 构建服务。
- en: In the next chapter, we are going to explore a full OpenStack deployment that
    utilizes a smart scheduler to select compute hosts on which to provision new LXC
    containers, based on available memory or just the number of already running containers.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨一个完整的 OpenStack 部署，该部署利用智能调度程序在可用内存或已运行的容器数量基础上选择计算主机以部署新的 LXC
    容器。
- en: We have already looked at some examples of how to implement most of the components
    and interactions in the preceding diagram. Let's quickly touch on Jenkins and
    set up a simple job that creates new LXC containers when called through the REST
    API remotely. The rest will be left to the reader to experiment with.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过如何在前面的图表中实现大多数组件和交互的示例。让我们快速触及 Jenkins 并设置一个简单的作业，通过远程 REST API 调用时创建新的
    LXC 容器。其余部分留给读者自行尝试。
- en: 'To install Jenkins on Ubuntu, run the following commands:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Ubuntu 上安装 Jenkins，请运行以下命令：
- en: '[PRE104]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Jenkins listens on port `8080`. The following two iptables rules forward port
    `80` to port `8080`, making it easier to connect:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins 监听端口 `8080`。以下两条 iptables 规则将端口 `80` 转发到端口 `8080`，使连接更加便捷：
- en: '[PRE105]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Make sure to add the `jenkins` user to the `sudoers` file:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将 `jenkins` 用户添加到 `sudoers` 文件中：
- en: '[PRE106]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Once installed, Jenkins will start and be accessible over HTTP. Open the Jenkins
    web page and paste the content of the following file as requested:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，Jenkins 将启动并可通过 HTTP 访问。打开 Jenkins 网页并粘贴如下文件的内容：
- en: '[PRE107]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'After installing the recommended plugins, do the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 安装推荐的插件后，执行以下操作：
- en: Create a new job named `LXC Provision`.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`LXC Provision`的新作业。
- en: In **Build Triggers**, select **Trigger builds remotely (e.g., from scripts)**
    and type a random string, for example, `somesupersecrettoken`.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**构建触发器**中，选择**远程触发构建（例如，从脚本中）**，并输入一个随机字符串，例如`somesupersecrettoken`。
- en: In the **Build** section, click on **Add build step** and select **Execute shell**.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**构建**部分，点击**添加构建步骤**并选择**执行shell**。
- en: 'In the **Command** window, add the following simple bash script:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**命令**窗口中，添加以下简单的bash脚本：
- en: '[PRE108]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Finally, click on **Save**.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，点击**保存**。
- en: 'The Jenkins job should look similar to the following screenshot:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins作业应该类似于以下屏幕截图：
- en: '![Simple autoscaling pattern with LXC, Jenkins, and Sensu](img/image_07_007.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![带有LXC、Jenkins和Sensu的简单自动扩展模式](img/image_07_007.jpg)'
- en: 'To trigger the job remotely, run the following, replacing the username, password,
    and IP address with those configured on your host:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 要远程触发该作业，运行以下命令，替换用户名、密码和IP地址为你主机上配置的：
- en: '[PRE109]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'This will trigger the Jenkins job, which in turn will create a new LXC container
    with a random name:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发Jenkins作业，进而创建一个随机名称的新LXC容器：
- en: '[PRE110]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Now, all that is left is to create a Sensu handler (or a `monit` trigger script)
    for a check, or set of checks, which in turn can execute a similar `curl` command
    as the preceding one. Of course, this deployment is only meant to scratch the
    surface of what is possible by combining Sensu, Jenkins, and LXC for autoscaling
    services running inside containers.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只需为检查或一组检查创建一个Sensu处理程序（或`monit`触发脚本），然后可以执行类似前面`curl`命令的操作。 当然，这个部署只是为了让我们略微了解通过结合Sensu、Jenkins和LXC实现容器内自动扩展服务的可能性。
- en: Summary
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at how to back up LXC containers using Linux native
    tools such as `tar` and `rsync`, and LXC utilities such as `lxc-copy`. We looked
    at examples of how to create cold and hot standby LXC container backups using
    the iSCSI target as the LXC root filesystem and configuration files store. We
    also looked at how to deploy a shared network filesystem using GlusterFS, and
    the benefits of running multiple containers on the same filesystem, but on different
    hosts.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用Linux原生工具如`tar`和`rsync`，以及LXC工具如`lxc-copy`来备份LXC容器。我们查看了如何使用iSCSI目标作为LXC根文件系统和配置文件存储，创建冷备份和热备份的示例。我们还学习了如何使用GlusterFS部署共享网络文件系统，以及在同一文件系统上运行多个容器，但在不同主机上的好处。
- en: We also touched on how to monitor the state, health, and resource utilization
    of LXC containers using tools such as Monit and Sensu, and how to trigger actions,
    such as running a script to act on those alerts.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还简要讨论了如何使用Monit和Sensu等工具监控LXC容器的状态、健康状况和资源利用率，并如何触发操作，如运行脚本来响应这些警报。
- en: Finally, we reviewed one of the common autoscaling patterns, combining several
    tools to automatically create new containers based on alert events.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回顾了一种常见的自动扩展模式，通过结合多个工具，根据警报事件自动创建新容器。
- en: In the next chapter, we are going to look at a complete OpenStack deployment,
    which will allow us to create LXC containers utilizing smart schedulers.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将查看一个完整的OpenStack部署，这将使我们能够利用智能调度器创建LXC容器。
