- en: Chapter 7. Monitoring and Backups in a Containerized World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at a few examples of how to scale applications
    running inside LXC containers, by creating multiple instances behind a proxy service
    such as HAProxy. This ensures the application has enough resources and can withstand
    failures, thus achieving a level of high availability.
  prefs: []
  type: TYPE_NORMAL
- en: For applications running in a single LXC instance, it's often desirable to perform
    periodic backups of the container, which includes the root filesystem and the
    container's configuration file. Depending on the backing store, there are a few
    available options that we are going to explore in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using a highly available or shared backend store can help with quickly recovering
    from failed hosts, or when one needs to migrate LXC containers between servers.
    We are going to look at how to create containers on an **Internet Small Computer
    Systems Interface** (**iSCSI**) target and migrate LXC between servers. We are
    also going to look at an example of how to use GlusterFS as a shared filesystem
    to host the root filesystem of containers, thus creating active-passive LXC deployments.
  prefs: []
  type: TYPE_NORMAL
- en: We are also going to go over how to monitor LXC containers and the services
    running inside them, with various tools such as Monit and Sensu; we will end the
    chapter with an example of creating a simple autoscaling solution based on monitoring
    triggers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Backing up LXC containers using `tar` and `rsync`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing up LXC containers with the `lxc-copy` utility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating LXC containers using the iSCSI target as a backend store and demonstrating
    how to migrate containers if the primary server goes down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrating how to use GlusterFS as a shared filesystem for the root filesystem
    of containers and deploy active-passive LXC nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking into what metrics LXC exposes, and how to monitor, alert, and act on
    them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backing up and migrating LXC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating backups of LXC instances ensures that we can recover from events such
    as server crashes or corrupt backend stores. Backups also provide for a quick
    way of migrating instances between hosts or starting multiple similar containers,
    by just changing their config files.
  prefs: []
  type: TYPE_NORMAL
- en: Creating LXC backup using tar and rsync
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In most use cases, we build containers from templates, or with tools such as
    debootstrap, which result in an entire root filesystem, for the instance. Creating
    backups in such cases is a matter of stopping the container, archiving its configuration
    file along with the actual root filesystem, then storing them on a remote server.
    Let''s demonstrate this concept with a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by updating your Ubuntu distribution and installing LXC. For this example,
    we''ll be using Ubuntu 16.04, but the instructions apply to any other Linux distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, proceed by creating a container using the default directory backend store,
    and start it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install an application inside LXC, in this case, Nginx, and create a custom
    index file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the container running, ensure we can reach the HTTP service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice how the container''s filesystem and its configuration file are self-contained
    in the `/var/lib/lxc` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is quite convenient for creating a backup using standard Linux utilities
    such as `tar` and `rsync`. Stop the container before we back it up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `bzip` archive of the root filesystem and the configuration file,
    making sure to preserve the numeric owner IDs, so that we can later start it on
    a different server without creating user ID collisions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, copy the archive to a different server, in this case, `ubuntu-backup`,
    and delete the archive from the original server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the archive on the destination server, we are now ready to restore it when
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring from the archived backup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To restore the container from the `bz2` backup, on the destination server,
    extract the archive in `/var/lib/lxc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how, after extracting the root filesystem and the configuration file,
    listing all containers will show the container we just restored, albeit in a stopped
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start it and hit the HTTP endpoint, ensuring we get the same result
    back as with the original LXC instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To clean up, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Creating container backup using lxc-copy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Regardless of the backend store of the container, we can use the `lxc-copy`
    utility to create a full copy of the LXC instance. Follow these steps for creating
    container backup:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by specifying the name of the container on the original host we want
    to back up, and a name for the copy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command created a new root filesystem and configuration file on the host:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Creating a full copy will update the configuration file of the new container
    with the newly specified name and location of the `rootfs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice the container's name and directory have changed from the original instance.
    Now we can archive and store remotely, as shown in the previous section, with
    `tar` and `rsync`. Using this method is convenient, if we need to ensure the name
    of the container and the location of its `rootfs` differ from the original, in
    the case where we would like to keep the backup on the same server, or on hosts
    with the same LXC name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, to clean up, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Migrating LXC containers on an iSCSI target
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Moving containers from one host to another can be quite useful when performing
    server maintenance, or when redistributing server load. Cloud platforms such as
    OpenStack leverage schedulers that choose where an LXC instance should be created
    and tools to migrate them, based on certain criteria such as instance density,
    resource utilization, and others, as we'll see in the next chapter. Nevertheless,
    it helps to know how to manually migrate instances between hosts should the need
    arise.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a backup or a copy of the container is easier when we use the default
    `dir` backing store with LXC; however, it is not as trivial when leveraging other
    types such as LVM, Btrfs, or ZFS, unless using a shared storage, such as GlusterFS,
    or iSCSI block devices, which we are going to explore next.
  prefs: []
  type: TYPE_NORMAL
- en: The iSCSI protocol has been around for a while, and lots of **Storage Area Networks**
    (**SAN**) solutions exist around it. It's a great way of providing access to block
    devices over a TCP/IP network.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the iSCSI target
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The endpoint that exposes the block device is called the **target**. Creating
    an iSCSI target on Linux is quite trivial. All we need is a block device. In the
    following example, we are going to use two servers; one will be the iSCSI target
    that exposes a block device, and the other will be the initiator server, which
    will connect to the target and use the presented block device as the location
    for the LXC containers' root filesystem and configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps for setting up the iSCSI target:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by installing the necessary package on Ubuntu, and then demonstrate
    the same on CentOS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the package installed, enable the target functionality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s create the target configuration file. The file is well-documented
    with comments; we are going to use a small subset of the available configuration
    options. We start by specifying an arbitrary identifier, `iqn.2001-04.com.example:lxc`
    in this example, a username and password, and, most importantly, the block device
    we are going to expose over iSCSI -`/dev/xvdb`. The iSCSI identifier is in the
    following form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The description of this identifier is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`yyyy-mm` : This is the year and month when the naming authority was established'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`naming-authority`:  This is usually reverse syntax of the Internet domain
    name of the naming authority, or the domain name of the server'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unique name`: This is any name you would like to use'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these in mind, the minimal working target configuration file is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, specify which hosts or initiators are allowed to connect to the iSCSI
    target; replace the IP with that of the host that is going to connect to the target
    server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, start the iSCSI target service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On CentOS, the process and configuration files are slightly different. To install
    the package, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The configuration file has the following format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start the service, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s list the exposed target by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Setting up the iSCSI initiator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The initiator is the server that will connect to the target and access the exposed
    block device over the iSCSI protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the initiator tools on Ubuntu, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On CentOS, the package name is different:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enable the service and start the iSCSI daemon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `iscsiadm` command is the userspace tool that we can use from the initiating
    server to ask the target for available devices. From the initiator host, let''s
    ask the target server we configured earlier what block devices are available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the preceding output, we can see that the target server presented one target
    identified by the **iSCSI Qualified Name** (**IQN**) of `iqn.2001-04.com.example:lxc`.
    In this case, the target server has three IP addresses, thus the three lines from
    the output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Earlier, we configured the target to use a username and a password. We need
    to configure the initiator host to present the same credentials to the target
    host in order to access the resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After updating the initiator''s configuration, we can check that the credentials
    have been applied by examining the configuration file that was automatically created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Logging in to the iSCSI target using the presented block device as rootfs for
    LXC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the initiator host configured, we are now ready to log in to the iSCSI
    target and use the presented block device:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To log in, from the initiator host run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s verify that the initiator host now contains an iSCSI session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The initiator host should now have a new block device available for use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This new block device can be used as a regular storage device. Let''s create
    a filesystem on it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the filesystem present, let''s use the block device as the default location
    for the LXC filesystem, by mounting it at `/var/lib/lxc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we mounted the iSCSI device at the default location for the root filesystem
    of the LXC containers, next time we build one, its root filesystem will be on
    the iSCSI device. As we'll see shortly, this is useful if the initiator host needs
    to undergo maintenance, or becomes unavailable for whatever reason, because we
    can mount the same iSCSI target to a new host and just start the same containers
    there with no configuration changes on the LXC side.
  prefs: []
  type: TYPE_NORMAL
- en: Building the iSCSI container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Create a new LXC container as usual, start it, and ensure it''s running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though the root filesystem is now on the new iSCSI block device, from
    the perspective of the host OS, nothing is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Since the `rootfs` of the container now resides on a block device that is remote,
    depending on the network connection between the LXC initiator host and the ISCSI
    target host, some latency might be present. In production deployments, an isolated
    low-latency network is preferred between the iSCSI target and the initiator hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To migrate the container to a different host, we need to stop it, unmount the
    disk, then log out the block device from the iSCSI target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Restoring the iSCSI container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To restore the iSCI container, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On a new host that is configured as an initiator, we can log in the same target
    as we saw earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Ensure that a new block device has been presented after logging in to the iSCSI
    target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When logging in the iSCSI target on a new host, keep in mind that the name of
    the presented block device might be different from what it was on the original
    server.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, mount the block device to the default location of LXC root filesystem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we list all available containers now, we''ll see that the container we created
    on the previous host is now present on the new server, just by mounting the iSCSI
    target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can start the container as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the container is configured with a static IP address, the same IP will be
    present on the new host; however, if the container obtains its network configuration
    dynamically, its IP might change. Keep this in mind if there is an associated
    A record in DNS for the container you are migrating.
  prefs: []
  type: TYPE_NORMAL
- en: LXC active backup with replicated GlusterFS storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we saw how to use a remote block device as a local
    storage for LXC by means of exporting it over iSCSI. We formatted the block device
    with a filesystem that does not allow shared access from multiple servers. If
    you log in the target device to more than one server and they try to write to
    it at the same time, corruption will occur. Using iSCSI devices on a single node
    to host the LXC containers provides for an excellent way of achieving a level
    of redundancy should the LXC server go down; we just log in the same block device
    on a new initiator server and start the containers. We can consider this as a
    form of cold backup, since there will be downtime while moving the iSCSI block
    device to the new host, logging in to it, and starting the container.
  prefs: []
  type: TYPE_NORMAL
- en: There's an alternative way, where we can use a shared filesystem that can be
    attached and accessed from multiple servers at the same time, with the same LXC
    containers running on multiple hosts, with different IP addresses. Let's explore
    this scenario using the scalable network filesystem GlusterFS as the remotely
    shared filesystem of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the shared storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GlusterFS has two main components - a server component that runs the GlusterFS
    daemon, which exports local block devices called **bricks**, and a client component
    that connects to the servers with a custom protocol over TCP/IP network and creates
    aggregate virtual volumes that can then be mounted and used as a regular filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed**: These are volumes that distribute files throughout the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replicated**: These are volumes that replicate data across two or more nodes
    in the storage cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Striped**: These stripe files across multiple storage nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve high availability of the LXC containers running on such shared volumes,
    we are going to use the replicated volumes on two GlusterFS servers. For this
    example, we are going to use a block device on each GlusterFS server with LVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the first storage node, let''s create the PV, the VG, and the LV on the
    `/dev/xvdb` block device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s create a filesystem on the LVM device and mount it. XFS performs
    quite well with GlusterFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s install the GlusterFS service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Repeat the preceding steps on the second GlusterFS node, replacing `node1` with
    `node2` as necessary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once we have the GlusterFS daemon running on both nodes, it''s time to probe
    from `node1` to see if we can see any peers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By probing from `node1`, we can now see that both `node1` and `node2` are part
    of a cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to create the replicated volume by specifying the mount path
    of the LV we created and mounted earlier. Since we are using two nodes in the
    storage cluster, the replication factor is going to be `2`. The following commands
    create the replicated volume, which will contain block devices from both `node1`
    and `node2` and list the created volume:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start the newly created replicated volume, run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To create and start the volume, we only need to run the preceding commands from
    one of the storage nodes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To obtain information about the new volume, run this command on any of the
    nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the preceding output, notice both bricks from `node1` and `node2`. We
    can see them on both nodes as well:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see the following on `node2`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We should see the same files because we are using the replicated volume. Creating
    files on one of the mounted volumes should appear on the other, although, as with
    the iSCSI setup earlier, the network latency is important, since the data needs
    to be transferred over the network.
  prefs: []
  type: TYPE_NORMAL
- en: Building the GlusterFS LXC container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the GlusterFS cluster ready, let''s use a third server to mount the replicated
    volume from the storage cluster and use it as the LXC root filesystem and configuration
    location:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s install the GlusterFS client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, mount the replicated volume from the storage cluster to the LXC host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding command, we are using `node2` as the target from which we are
    mounting; however, we can use `node1` in exactly the same way. The name of the
    target device we specify in the `lxc_glusterfs` mount command is what we specified
    as the name of the replicated volume earlier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that the replicated GlusterFS volume has been mounted to the default LXC
    location, let''s create and start a container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Attach to the container and install Nginx so we can later test connectivity
    from multiple servers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the IP address of the container and ensure we can connect to Nginx from
    the host OS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the container created, the root filesystem and configuration file will
    be visible on both storage nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following will be visible on `node2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Depending on your network bandwidth and latency, replicating the data between
    both storage nodes might take some time. This will also affect how long it takes
    for the LXC container to build, start, and stop.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring the GlusterFS container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s create a second LXC host, install the GlusterFS client, and mount the
    replicated volume in the same way we did earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how, just by mounting the GlusterFS volume, the host now sees the container
    in a stopped state. This is exactly the same container as the running one on `node1` -
    same config and root filesystem, that is. Since we are using a shared filesystem,
    we can start the container on multiple hosts without worrying about data corruption,
    unlike the case with iSCSI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are using DHCP to dynamically assign IP addresses to the containers,
    the same container on the new hosts gets a new IP. Notice how connecting to Nginx
    running in the container gives us the same result, since the container shares
    its filesystem and configuration file across multiple LXC nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This setup in a sense implements a hot standby backup, where we can use both
    containers behind a proxy service such as HAProxy, with the second node only being
    used when the first node goes down, ensuring that any configuration changes are
    immediately available. As an alternative, both LXC containers can be used at the
    same time, but keep in mind that they'll write to the same filesystem, so the
    Nginx logs in this case will be written from both LXC containers on the `lxc1`
    and `lxc2` nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and alerting on LXC metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring LXC containers is not much different than monitoring a VM or a server -
    we can run a monitoring client inside the container, or on the actual host that
    runs LXC. Since the root filesystem of the containers is exposed on the host and
    LXC uses cgroups and namespaces, we can collect various information directly from
    the host OS, if we would rather not run a monitoring agent in the container. Before
    we look at two examples of LXC monitoring, let's first see how we can gather various
    metrics that we can monitor and alert on.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering container metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LXC provides a few simple tools that can be used to monitor the state of the
    container and its resource utilization. The information they provide, as you are
    going to see next, is not that verbose; however, we can utilize the cgroup filesystem
    and collect even more information from it. Let's explore each of these options.
  prefs: []
  type: TYPE_NORMAL
- en: Using lxc-monitor to track container state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `lxc-monitor` tool can be used to track containers, state changes - when
    they start or stop.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this, open two terminals; in one, create a new container and
    in the other, run the `lxc-monitor` command. Start the container and observe the
    output in the second terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Stop the container and notice the state change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Using lxc-top to obtain CPU and memory utilization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `lxc-top` tool is similar to the standard `top` Linux command; it shows
    CPU, memory, and I/O utilization. To start it, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Using lxc-info to gather container information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the `lxc-info` tool to periodically poll for information such as
    CPU, memory, I/O, and network utilization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Leveraging cgroups to collect memory metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In *[Chapter 1](ch01.html "Chapter 1. Introduction to Linux Containers"),**Introduction
    to Linux Containers* we explored cgroups in great details and saw how LXC creates
    a directory hierarchy in the cgroup virtual filesystem for each container it starts.
    To find where the cgroup hierarchy for the container we built earlier is, run
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the files in the memory group to set and obtain metrics that we
    can monitor and alert on. For example, to set the memory of the container to 512
    MB, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'To read the current memory utilization for the container, execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'To collect more information about the container''s memory, read from the following
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Using cgroups to collect CPU statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To collect CPU usage, we can read from the `cpuacct` cgroup subsystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Collecting network metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each container creates a virtual interface on the host; the output from the
    `lxc-info` command earlier displayed it as `veth8OX0PW`. We can collect information
    about the packets sent and received, error rates, and so on, by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can connect to the container''s network namespace and obtain
    the information that way. The following three commands demonstrate how to execute
    commands in the container''s network namespace. Note the PID of `19967`; it can
    be obtained from the `lxc-info` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we can see the network interfaces that are inside the LXC container,
    even though we ran the commands on the host.
  prefs: []
  type: TYPE_NORMAL
- en: Simple container monitoring and alerting with Monit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've demonstrated how to gather various monitoring data points, let's
    actually set up a system to monitor and alert on them. In this section, we are
    going to install a simple monitoring solution utilizing the `monit` daemon. Monit
    is an easy-to-configure service that uses scripts that can be automatically executed,
    based on certain monitoring events and thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a few examples next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `monit`, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a minimal configuration file. The config that is packaged with
    the installation is documented quite well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding config starts a web interface that can be reached on port `2812`
    with the specified credentials. It also defines two more directories where config
    files can be read.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let''s create a monitoring configuration that checks whether a container
    is running. Executing a script, which we''ll write next, performs the actual check:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding configuration tells `monit` to run the `container_running.sh`
    script periodically and, if the exit status is `1`, to execute a second script
    called `alert.sh` that will alert us. Simple enough. The `container_running.sh`
    script follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that we are utilizing the `lxc-info` command to check the status
    of the container. The `alert.sh` script is even simpler:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reload the `monit` service and check the status of the new monitoring service
    that we named `container_status` earlier in the configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also connect to the web interface on port `2812` and see the newly defined
    monitoring target:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Simple container monitoring and alerting with Monit](img/image_07_001.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Let''s stop the container and check the status of `monit`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Simple container monitoring and alerting with Monit](img/image_07_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Notice from the output of the command and the web interface that the status
    of the `container_status` service is now `failed`. Since we set up `monit` to
    send an e-mail when the service we are monitoring is failing, check the mail log.
    You should have received an e-mail from `monit`, which will most likely end up
    in your `spam` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: For more information on Monit, refer to [https://mmonit.com/monit/documentation/monit.html](https://mmonit.com/monit/documentation/monit.html).
  prefs: []
  type: TYPE_NORMAL
- en: Monit is a quick and easy way to set up monitoring for LXC containers per server.
    It is agentless, and thanks to the exposed metrics from the cgroup hierarchies,
    it is easy to alert on various data points, without the need to attach or run
    anything extra in the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Container monitoring and alert triggers with Sensu
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monit is a great tool for monitoring and alerting in a decentralized setup.
    However, for a more robust and feature-rich way of deploying centralized-based
    monitoring, other monitoring tools such as Sensu can be leveraged. There are two
    main ways to implement monitoring with Sensu - with an agent in each container,
    or on the LXC host with standalone checks collecting data from sources such as
    cgroups, in a way similar to Monit.
  prefs: []
  type: TYPE_NORMAL
- en: Sensu uses the client-server architecture, in the sense that the server publishes
    checks in a messaging queue that is provided by RabbitMQ, and the clients subscribe
    to the topic in that queue and execute checks and alerts based on set thresholds.
    State and historical data is stored in a Redis server.
  prefs: []
  type: TYPE_NORMAL
- en: Let's demonstrate a Sensu deployment with an agent inside the LXC container
    first, then move on to an agentless monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring LXC containers with Sensu agent and server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to install the required services that Sensu will use, Redis and RabbitMQ:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the Redis server, and, once it''s installed, ensure it''s
    running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Installing RabbitMQ is just as easy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once installed, we need to create the virtual host that the agents will be
    subscribing to and reading messages from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a username and a password to connect to that topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Time to install the Sensu server and client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At a minimum, we need five configuration files, one for the Sensu API endpoint,
    two that specify what transport we are using - RabbitMQ in this case - the Redis
    config file for Sensu, and a client config file, for the Sensu client running
    on the same server. The following config files are pretty much self-explanatory -
    we specify the IP addresses and ports of the RabbitMQ and Redis servers, along
    with the API service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: For more information on Sensu, refer to [https://sensuapp.org/docs/](https://sensuapp.org/docs/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start the Sensu server, we can install a web-based frontend called
    Uchiwa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'In the configuration file, we specify the address and port of the Sensu API
    service - localhost and port `4567` - and the port Uchiwa will be listening on - `3000` -
    in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of the configuration in place, let''s start the Sensu services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'With the Sensu server now fully configured, we need to attach to a container
    on the host and install the Sensu agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'To configure the Sensu agent, we need to edit the client config, where we specify
    the IP address and the name of the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We tell the agent how to connect to the RabbitMQ server on the host, by providing
    its IP address, port, and the credentials we created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, specify the transport mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding three files in place, let''s start the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '![Monitoring LXC containers with Sensu agent and server](img/image_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To verify that all services are running normally and that the Sensu agent can
    connect to the server, we can connect to the Uchiwa interface on port 3000 using
    the host IP, as shown earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'While still attached to the LXC container, let''s install a Sensu check. Sensu
    checks are available as gems, or can be written manually. Let''s search the `gem`
    repository for any memory checks instead of writing our own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the memory check and restart the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'On the Sensu server host (and not on the container), we need to define the
    new memory check so that the Sensu server can tell the agent to execute it. We
    do that by creating a new checks file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the path and the name of the check that needs to be run from the
    Sensu agent, in this case, the Ruby script that we installed in the container
    from the `gem`. Restart the Sensu services for the changes to take effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the Uchiwa interface, we can see that the memory check is now active:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring LXC containers with Sensu agent and server](img/image_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can install multiple Sensu check scripts from gems inside the LXC container;
    define them on the Sensu server just like we would on a normal server or a virtual
    machine, and end up with a full-fledged monitoring solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few caveats when running an agent inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent consumes resources; if we are trying to run lightweight containers
    with a minimum amount of memory, this might not be the best solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When measuring CPU load, the load inside the container will reflect the general
    load on the host itself, since the containers are not isolated from the host by
    a hypervisor. The best way to measure CPU utilization is by obtaining the data
    from the cgroups, or with the `lxc-info` command on the host.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If using a shared root filesystem such as the one we saw in the previous chapter,
    monitoring the disk space inside the container might reflect the total space on
    the server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If running a Sensu agent inside the LXC container is not desired, we can perform
    standalone checks from the Sensu server on the same host instead. Let's explore
    this setup next.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring LXC containers using standalone Sensu checks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On servers that run LXC containers, we can install the Sensu agent directly
    on the host OS, instead of inside each container, just like we did earlier in
    this chapter. We can also leverage Sensu's standalone checks, which provide a
    decentralized way of monitoring, meaning that the Sensu agent defines and schedules
    the checks instead of the Sensu server. This provides us with the benefit of not
    having to install agents and monitoring scripts inside the containers, and having
    the Sensu agents run the checks on each server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s demonstrate how to create a standalone check on the LXC host we''ve
    been using so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The main difference when defining a standalone check is the presence of the
    `"standalone": true` stanza, as shown from the preceding output. In the command
    section of the check configuration, we specify what script to execute to perform
    the actual check and the thresholds it should alert on. The script can be anything,
    as long as it exits with error code `2` for Critical alerts, error code `1` for
    Warning, and `0` if all is OK.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a very simple bash script that uses the `memory.usage_in_bytes` cgroup
    file to collect metrics on the memory usage and alert on it if the specified threshold
    is reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script, albeit simple, should be a good starting point for the
    reader to start writing more useful Sensu checks that operate on various LXC metrics.
    Looking at the script, we can see that it has three basic functions. First, it
    checks for the provided container name, and warning and error thresholds in the
    `sanitiy_check()` function. Then it gets the memory usage in bites from the cgroup
    file in the `get_memory_usage()` function, and finally, reports the results in
    the `report_result()` function, by returning the appropriate error code, as described
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the permissions and the execution flag of the script, reload Sensu services,
    and make sure the check is showing in Uchiwa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '![Monitoring LXC containers using standalone Sensu checks](img/image_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Just like Monit, Sensu provides handlers that get triggered when an alert is
    fired. This can be a custom script that sends an e-mail, makes a call to an external
    service such as PagerDuty, and so on. All of this provides the capability for
    an automated and proactive way of handling LXC alerts.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on Sensu handlers, refer to the documentation at [https://sensuapp.org/docs/latest/reference/handlers.html](https://sensuapp.org/docs/latest/reference/handlers.html).
  prefs: []
  type: TYPE_NORMAL
- en: Simple autoscaling pattern with LXC, Jenkins, and Sensu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 6](ch06.html "Chapter 6. Clustering and Horizontal Scaling with
    LXC")*, Clustering and Horizontal Scaling with LXC*, we looked at how to horizontally
    scale services with LXC and HAProxy, by provisioning more containers on multiple
    hosts. In this chapter, we explored different ways of monitoring the resource
    utilization of LXC containers and triggering actions based on the alerts. With
    all of this knowledge in place, we can now implement a commonly used autoscaling
    pattern, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple autoscaling pattern with LXC, Jenkins, and Sensu](img/image_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The pattern uses Jenkins as a build system, controlled by the Sensu alert handlers.
    When a Sensu agent running inside an LXC container receives a scheduled check
    from the Sensu server, for example, a memory check, it executes the script and
    returns either `OK`, `Warning`, or a `Critical` status, depending on the configured
    alert thresholds. If the `Critical` status is returned, then a configured Sensu
    handler, which can be as simple as a `curl` command, makes an API call to the
    Jenkins server, which in turn executes a preconfigured job. The Jenkins job can
    be a script that selects an LXC host from a list of hosts, based on a set of criteria
    that either builds a new container, or increases the available memory on the alerting
    LXC container, if possible. This is one of the simplest autoscaling design patterns,
    utilizing a monitoring system and a RESTful build service such as Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to explore a full OpenStack deployment that
    utilizes a smart scheduler to select compute hosts on which to provision new LXC
    containers, based on available memory or just the number of already running containers.
  prefs: []
  type: TYPE_NORMAL
- en: We have already looked at some examples of how to implement most of the components
    and interactions in the preceding diagram. Let's quickly touch on Jenkins and
    set up a simple job that creates new LXC containers when called through the REST
    API remotely. The rest will be left to the reader to experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Jenkins on Ubuntu, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Jenkins listens on port `8080`. The following two iptables rules forward port
    `80` to port `8080`, making it easier to connect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure to add the `jenkins` user to the `sudoers` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, Jenkins will start and be accessible over HTTP. Open the Jenkins
    web page and paste the content of the following file as requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing the recommended plugins, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new job named `LXC Provision`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Build Triggers**, select **Trigger builds remotely (e.g., from scripts)**
    and type a random string, for example, `somesupersecrettoken`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Build** section, click on **Add build step** and select **Execute shell**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Command** window, add the following simple bash script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, click on **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Jenkins job should look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple autoscaling pattern with LXC, Jenkins, and Sensu](img/image_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To trigger the job remotely, run the following, replacing the username, password,
    and IP address with those configured on your host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'This will trigger the Jenkins job, which in turn will create a new LXC container
    with a random name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: Now, all that is left is to create a Sensu handler (or a `monit` trigger script)
    for a check, or set of checks, which in turn can execute a similar `curl` command
    as the preceding one. Of course, this deployment is only meant to scratch the
    surface of what is possible by combining Sensu, Jenkins, and LXC for autoscaling
    services running inside containers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to back up LXC containers using Linux native
    tools such as `tar` and `rsync`, and LXC utilities such as `lxc-copy`. We looked
    at examples of how to create cold and hot standby LXC container backups using
    the iSCSI target as the LXC root filesystem and configuration files store. We
    also looked at how to deploy a shared network filesystem using GlusterFS, and
    the benefits of running multiple containers on the same filesystem, but on different
    hosts.
  prefs: []
  type: TYPE_NORMAL
- en: We also touched on how to monitor the state, health, and resource utilization
    of LXC containers using tools such as Monit and Sensu, and how to trigger actions,
    such as running a script to act on those alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we reviewed one of the common autoscaling patterns, combining several
    tools to automatically create new containers based on alert events.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to look at a complete OpenStack deployment,
    which will allow us to create LXC containers utilizing smart schedulers.
  prefs: []
  type: TYPE_NORMAL
