- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containerize on Azure – Building Solutions with Azure Kubernetes Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we built and automated our solution on Azure utilizing
    Azure VMs. We built VM images with Packer and provisioned our VMs using Terraform.
    In this chapter, we’ll follow a similar path, but instead of working with VMs,
    we’ll look at hosting our application in containers within a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we’ll need to alter our approach by ditching Packer and replacing
    it with Docker to create a deployable artifact for our application. Once again,
    we’ll be using the `azurerm` provider for Terraform and revisiting the `kubernetes`
    provider for Terraform that we saw when we took the same step while on our journey
    with AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Since an overwhelming majority of this remains the same when we move to Azure,
    we won’t revisit these topics at the same length in this chapter. However, I would
    encourage you to put a bookmark in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402)
    and reference it frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our story continues through the lens of Söze Enterprises, founded by the enigmatic
    Turkish billionaire Keyser Söze. Our team has been hard at work building the next-generation
    autonomous vehicle orchestration platform. Previously, we had hoped to leapfrog
    the competition by leveraging Azure’s rock-solid platform, leveraging our team’s
    existing skills, and focusing on feature development. The team was just getting
    into their groove when a curveball came out of nowhere.
  prefs: []
  type: TYPE_NORMAL
- en: Over the weekend, our elusive executive was influenced by a rendezvous with
    Scott Guthrie, the President of Microsoft’s Cloud + AI Division, in Abu Dhabi.
    The Yas Marina Circuit was buzzing with energy. The sun was setting, casting a
    golden glow over the track as fans and celebrities gathered for the season-ending
    Abu Dhabi Grand Prix. While in the exclusive Paddock Club, Keyser spotted Scott
    “Gu” in his iconic red polo near the hors d’oeuvres. Scott excitedly shared news
    about some recent improvements to **Azure Kubernetes Service** (**AKS**). Keyser
    was enchanted by the prospect of more efficient resource utilization, leading
    to improved cost optimization and faster deployment and rollback times, and he
    was hooked. His new autonomous vehicle platform needed to harness the power of
    the cloud, and container-based architecture was the way to do it. So, he decided
    to accelerate his plans to adopt cloud-native architecture!
  prefs: []
  type: TYPE_NORMAL
- en: The news of transitioning to a container-based architecture means reevaluating
    their approach, diving into new technologies, and possibly even reshuffling team
    dynamics. For the team, containers were always the long-term plan, but now, things
    need to be sped up, which will require a significant investment in time, resources,
    and training.
  prefs: []
  type: TYPE_NORMAL
- en: As the team scrambles to adjust their plans, they can’t help but feel a mix
    of excitement and apprehension. They know that they are part of something groundbreaking
    under Keyser’s leadership. His vision for the future of autonomous vehicles is
    bold and transformative. And while his methods may be unconventional, they have
    learned that his instincts are often right. In this chapter, we’ll explore this
    transformation from VMs to containers using Microsoft Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in the previous chapter, where we built our solution using VMs on
    Azure, we had full control over the operating system configuration through the
    VM images we provisioned with Packer. Just as we did when we went through the
    same process on our journey with AWS in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402),
    we’ll need to introduce a new tool to replace VM images with container images
    – Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Logical architecture for the autonomous vehicle platform](img/B21183_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Logical architecture for the autonomous vehicle platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application architecture, comprising a frontend, a backend, and a database,
    will remain the same but we will need to provision different resources with Terraform
    and harness new tools from Docker and Kubernetes to automate the deployment of
    our solution to this new infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Source control structure of our repository](img/B21183_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Source control structure of our repository
  prefs: []
  type: TYPE_NORMAL
- en: In this solution, we’ll have seven parts. We still have the application code
    and Dockerfiles (replacing the Packer-based VM images) for both the frontend and
    backend. We still have GitHub Actions to implement our CI/CD process, but now
    we have two Terraform code bases – one for provisioning the underlying infrastructure
    to Azure and another for provisioning our application to the Kubernetes cluster
    hosted on AKS. Then, we have the two code bases for our application’s frontend
    and backend.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There will be many similarities between the work that we did in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402)
    when we performed a similar transition from VMs to containers using AWS. We’ll
    try and focus only on the key differences and avoid retreading the same ground.
    To obtain a complete and multi-cloud perspective, I’d encourage you to read [*Chapter
    8*](B21183_08.xhtml#_idTextAnchor402) (in case you skipped it) as well as the
    upcoming chapter, where we’ll tackle the same problem on **Google Cloud** **Platform**
    (**GCP**).
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous chapter, we set up a virtual network for two distinct groups
    of VMs, and then we connected our application to a database-managed service. When
    setting up a virtual network for a Kubernetes cluster, we’ll use a similar approach.
    However, the considerations are slightly different. We no longer have distinct
    and loose VMs where we host different components of our application. However,
    depending on the configuration of our Kubernetes cluster, we may need to consider
    the placement of the different node pools that we configure and other services
    that we want to provision within that network to allow the workloads we host on
    Kubernetes to access them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – With AKS, virtual network subnets are organized along infrastructure
    boundaries rather than application boundaries](img/B21183_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – With AKS, virtual network subnets are organized along infrastructure
    boundaries rather than application boundaries
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest form, a single subnet can be designated for all the node pools
    within an AKS cluster, but this can be very limiting as your workload needs to
    scale up over time. For more advanced scenarios, you should carefully consider
    the segmentation of your subnets based on your node pool design and scale considerations
    for each of your workloads. In doing so, you can provide better network isolation
    for the various workloads you host on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw when working with Amazon’s Kubernetes offering in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402),
    Azure’s Kubernetes offering also supports two networking modes: **Kubenet** and
    **CNI**. For this book, we’ll be focusing on Kubenet as it’s the most commonly
    used option.'
  prefs: []
  type: TYPE_NORMAL
- en: Container registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as we saw with AWS, Azure has a robust container registry service known
    as **Azure Container Registry** (**ACR**). It acts as a private registry for hosting
    and managing your container images and Helm charts. As we did in the expedition
    along the Amazon, we’ll be using Docker to publish our container images to this
    repository so that we can reference them later from the Terraform code that provisions
    resources to our AKS cluster. We’ll need to grant our cluster access using Azure
    managed identities and Azure **Role-Based Access Control** (**RBAC**), which is
    similar to how we granted access to Amazon EKS using AWS’s IAM service policies.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the biggest advantages of hosting your container-based workloads using
    a Kubernetes-managed service is that much of the underlying infrastructure is
    automatically configured and maintained on your behalf. The service interprets
    your Kubernetes resource configuration and provisions the necessary resources
    within the cluster to properly configure Azure to support your workloads. Sometimes,
    this is handled transparently, and other times, there are special hooks that allow
    you more control over the configuration of the underlying resources on Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this manner, under the hood, AKS streamlines load balancing using either
    a basic Azure load balancer or a more feature-rich Azure application gateway.
    AKS manages the creation and configuration of these load balancers when services
    of the `LoadBalancer` type are created within the Kubernetes cluster. For more
    control, users can also utilize Ingress controllers such as NGINX or the Azure
    **Application Gateway Ingress Controller** (**AGIC**) for advanced routing, SSL
    termination, and other capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Network traffic flow of an AKS cluster](img/B21183_11_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Network traffic flow of an AKS cluster
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) when working with
    AWS, we will be using the NGINX ingress controller but this time, we’ll be provisioning
    an Azure Application Gateway service to route traffic to NGINX. This works a bit
    differently than on AWS, where the NGINX ingress controller automatically configures
    the ALB through Kubernetes annotation. With Azure, we need to set up the NGINX
    ingress controller and then provision Azure Application Gateway and configure
    it to forward traffic to NGINX.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In AKS, network security is managed in a manner akin to the practices described
    in [*Chapter 10*](B21183_10.xhtml#_idTextAnchor474) for VMs as they are deployed
    within Azure virtual networks, thus allowing them to integrate seamlessly with
    existing Azure networking features. However, because Kubernetes has an overlay
    network called Kubenet, which is the network on which our workloads (or pods)
    live, we need to use Kubernetes Network policies to control network traffic between
    our workloads based on Kubernetes tags or namespaces. There are more advanced
    networking security capabilities when you are working with Azure CNI and other
    open source solutions such as Calico, but these are beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as we saw on our tour down the Amazon, Azure’s Kubernetes offering also
    integrates with other Azure services, such as Azure’s secret management service,
    **Azure Key Vault**. This integration is done through a combination of an AKS
    extension being enabled on the cluster itself and Kubernetes resources that are
    provisioned within the cluster, creating Kubernetes resources that our pods can
    use as a conduit to the secrets hosted on Azure Key Vault. Again, nothing is stopping
    us from using native Kubernetes secrets, but Azure Key Vault provides a much more
    streamlined and secure mechanism for granting Azure secrets. It allows us to keep
    secrets up-to-date to avoid outages when secret rotations occur, and it allows
    us to use managed identities to access the secrets rather than storing them on
    the cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we saw in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) when building
    our solution with AWS EKS, we need to facilitate a bridge between Kubernetes and
    the cloud platform’s identity management system. On AWS, that was IAM; on Azure,
    that’s Entra ID. The process is largely the same but the terminology is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – AKS with Workload Identity](img/B21183_11_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – AKS with Workload Identity
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a managed identity that will represent the workload.
    This is an Azure resource that represents an Entra ID identity that is managed
    by the Azure platform. As we did with EKS, we need to federate between the Kubernetes
    cluster and Entra ID. On Azure, we do that by creating a federated identity credential
    that links the managed identity, the AKS cluster’s internal Open ID Connect provider,
    and Entra ID. Like on AWS, we plant a seed for this managed identity so that it
    can linked to a Kubernetes service account resource that will be provisioned later
    within Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – AKS Secrets Manager integration](img/B21183_11_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – AKS Secrets Manager integration
  prefs: []
  type: TYPE_NORMAL
- en: After Workload Identity has been established, we can grant access to Azure resources
    such as Key Vault and databases such as Azure Cosmos DB or Azure SQL Database.
    Just as we did in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) with EKS, we’ll
    use the secrets store CSI driver and the Azure provider to integrate our Kubernetes
    deployments with Azure Key Vault.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, creating a Kubernetes cluster using AKS involves a few critical components.
    As we’ve established, we need a virtual network, managed identities, and sufficient
    RBAC to access the resources our cluster needs, such as container registries and
    Azure Key Vault secrets. However, the main components of our Kubernetes cluster
    are the node pools, which provide compute resources to host our pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Anatomy of an AKS cluster](img/B21183_11_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Anatomy of an AKS cluster
  prefs: []
  type: TYPE_NORMAL
- en: By default, every AKS cluster comes with a default node pool, which is where
    Kubernetes’ system services are hosted. However, we can add additional node pools
    either to isolate our application workloads or to grant access to different types
    of computing resources, such as different hardware profiles to meet the specific
    needs of different workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw with the cloud architecture, there were many similarities between
    the work that we did in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) with AWS.
    The deployment architecture will mirror what we saw in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402)
    as well. We looked at the differences in the Terraform provider in the previous
    chapter when we configured the `azurerm` provider to provision our solution to
    Azure VMs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, using container-based architecture, the only real differences from the
    way we deployed in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) with AWS will
    be the way we authenticate with the container registry and the Kubernetes cluster.
    I encourage you to review the deployment architectural approach outlined in the
    corresponding section of [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402). In the
    next section, we’ll go into the details of building the same solution on Azure,
    but again, we’ll take care not to re-tread the same ground.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we reviewed the key changes in our architecture as we transitioned
    from VM-based architecture to container-based architecture. We were careful not
    to retread the ground we covered in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402),
    where we went through this transformation first on AWS. In the next section, we’ll
    get tactical in building the solution, but again, we’ll be careful to build on
    the foundations we built in the previous chapter when we first set up our solution
    on Microsoft Azure using VMs.
  prefs: []
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll be taking our theoretical knowledge and applying it to
    a tangible, functioning solution while harnessing the power of Docker, Terraform,
    and Kubernetes on the Microsoft Azure platform. Some parts of this process will
    require significant change, such as when we provision our Azure infrastructure
    using Terraform; other parts will have minor changes, such as the Kubernetes configuration
    that we use to deploy our application to our Kubernetes cluster; and some will
    have almost no change whatsoever, such as when we build and push our Docker images
    to our container registry.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll learn how to implement our Dockerfile, which installs
    our .NET application code and runs the service in a container. If you skipped
    *Chapters 7* through *9* due to a lack of interest in AWS, I can’t hold that against
    you – particularly if your primary interest in reading this book is working on
    the Microsoft Azure cloud platform. However, I encourage you to review the corresponding
    section within [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) to see how we use
    Docker to configure a container with our .NET application code.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in the previous section, much of the infrastructure is unchanged
    when using container-based architecture. Therefore, in this section, we’ll be
    focusing on what’s different when we use Azure’s Kubernetes managed service.
  prefs: []
  type: TYPE_NORMAL
- en: Container registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first component we need to provision is our **container registry**. The
    container registry is often provisioned as part of a separate deployment that’s
    reserved for shared infrastructure that is reused across multiple applications.
    This can help when you have a common set of custom-built images that multiple
    teams or projects need to use in their applications or services. However, you
    should keep in mind that the container registry does act as an important security
    boundary, so if you want to ensure that application teams can only access images
    built for their applications, you should provision an isolated container registry
    for each project team:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code provisions the Azure container registry. It’s important
    to note that this resource has very specific requirements for the name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates a role assignment that will allow different users
    to push container images to this container registry. This is a critical requirement
    that allows our GitHub Action to publish the Docker image we build to our Azure
    container registry. Here, `principal_id` must be set to the identity of the service
    account that our GitHub Action impersonates. In this case, I passed in a collection
    of these and iterated over that collection using the `count` meta-argument. In
    the case of role assignments, because these resources are so lightweight, it doesn’t
    matter much if we use `for_each` or `count` because the drop-create that will
    occur more frequently when using `count` has little impact on the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step is to provision a Kubernetes cluster using the `azurerm_kubernetes_cluster`
    resource. This resource will be the central figure in our AKS infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code configures some important top-level attributes that influence
    pricing, networking, and internally managed resource placement. AKS will provision
    resources to two resource groups. One is where the AKS resource exists, and the
    other is where AKS provisions the internal Azure resources that make up the internals
    of the cluster. This secondary resource group’s name is controlled by the `node_resource_group`
    attribute. I would always recommend setting the `node_resource_group` name to
    something cohesive with the naming convention of the AKS cluster resource itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we learned in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), Kubernetes
    has several system services that need to be deployed and in good health for the
    cluster to function correctly. Our AKS cluster needs to have one or more node
    pools to host system and user workloads. The default node pool is a great place
    to host these system services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Additional node pools, such as the following one, can be created to allow us
    to isolate our custom deployments on dedicated computing resources so that they
    don’t impact the day-to-day operations of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By setting a taint on the nodes within this node pool, we can ensure that only
    Kubernetes deployments that are explicitly targeted to this node pool will be
    scheduled here. By employing taints on your additional node pools, you can isolate
    Kubernetes system services from the default node pool and keep your workloads
    in their own space. This does have additional costs, but it will greatly improve
    the health and performance of the cluster. It is something you should do if you’re
    planning on deploying production workloads to your cluster – but if you’re just
    kicking the tires, feel free to skip it!
  prefs: []
  type: TYPE_NORMAL
- en: Identity and access management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Managed identities play an integral role in the configuration of AKS in several
    different ways. The first and most important is the managed identity that AKS
    will use to provision the internal resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This identity needs to be assigned the `Managed Identity Operator` role to
    perform this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates this role assignment using a **user-assigned managed
    identity**. We explored this topic in the previous chapter, so we know that this
    is a special type of managed identity that we explicitly provision and assign
    role assignments to. This is in contrast to the system-assigned identity, which
    is a managed identity that is automatically provisioned and managed by the platform
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another important identity that needs to be set on the AKS cluster:
    the managed identity used by the kubelet system service that’s deployed to each
    node within the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code configures the cluster’s kubelet identity. This is a little
    inconsistent than how managed identities are typically attached within the `azurerm`
    provider, so it’s important to get the correct outputs from the user-assigned
    identity to the right attributes of the `kubelet_identity` block.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we learned in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), the kubelet
    system service processes orders from the scheduler. To do this, kubelet will need
    access to pull container images from our ACR. This will require the `AcrPull`
    role assignment to be added to the preceding managed identity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To integrate with Azure’s secret management service, Key Vault, we need to
    take a couple of steps. The first is to simply enable the subsystem on the cluster
    itself. AKS has an extensible model for such features – including but not limited
    to enabling integrations with other Azure services and Kubernetes features such
    as **Kubernetes Event Driven Architecture** (**KEDA**), **Azure Monitor**, and
    **Open** **Service Mesh**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code enables and configures secret rotation. This is just the
    first step in enabling AKS integration with Azure Key Vault; we also have to set
    up the CSI provider for pods to pull secrets from Key Vault. We’ll look at that
    in the next section when we start provisioning things to the Kubernetes control
    plane.
  prefs: []
  type: TYPE_NORMAL
- en: Workload identity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To allow our pods to access other resources that are deployed to Azure, we
    need to allow them to impersonate a managed identity. Like the integration with
    Key Vault, we first need to enable this extension on the AKS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code activates an internal **OpenID Connect** (**OIDC**) endpoint
    that’s used to sign and issue **JSON Web Tokens** (**JWTs**) for the service accounts
    within the cluster. After this is enabled, we’ll also need Azure federated identity
    credential, which, once linked to the AKS cluster’s OIDC issuer endpoint and the
    managed identity to be used by the workloads, creates federation between the cluster
    and Microsoft Entra ID. This allows the pods using the corresponding Kubernetes
    service account to interact with Azure services using the privileges of the managed
    identity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Just as we did in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) when working
    with AWS, we’ll link this to a Kubernetes service account in the next section
    when we provision resources to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), we built out the
    Kubernetes deployments using the Terraform provider for Kubernetes. Like Packer
    and Docker, Kubernetes, in its own way, provides a control plane that operates
    consistently across cloud platforms. As a result, much of the Kubernetes deployment
    process is reusable, regardless of what cloud platform you choose. This is also
    one of the appeals of Kubernetes as a way to implement cloud agnostic or cloud
    portable workloads yet leverage the efficiency and elasticity that Kubernetes
    managed service offerings provide.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we won’t retread the same topics. If you happened to skip *Chapters
    7* through *9* due to a lack of interest in AWS, I highly recommend going back
    and reviewing the corresponding section in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402)
    for more details about the implementation of the Kubernetes deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Provider setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we saw in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), when executing
    Terraform using the Kubernetes provider to provision resources to the Kubernetes
    control plane, we don’t have to make many changes. We still authenticate against
    our target cloud platform, we still follow Terraform’s core workflow, and we still
    pass in additional input parameters for platform-specific resources that we need
    to reference. Most notably, information about the cluster, other Azure services,
    such as ACR, Key Vault, and managed identities, and other details might need to
    be put into Kubernetes ConfigMaps that can be used by the pods to point themselves
    at the endpoint of their database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’re using a layered approach to provision the infrastructure first and
    then provision to Kubernetes. As a result, we can reference the Kubernetes cluster
    using the data source for a resource that was provisioned by the Terraform workspace
    that’s responsible for the Azure infrastructure. This allows users to access important
    connectivity details without exporting them outside of Terraform and passing them
    around during the deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code is a reference to the AKS cluster that was provisioned in
    the previous deployment stage. Using this reference, we can initialize the `kubernetes`
    provider by using several pieces of data to authenticate with the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The client key is the private key that’s used for authentication, the client
    certificate is the certificate that’s paired with the private key to perform authentication,
    and the cluster’s CA certificate is the certificate of the certificate authority
    that’s used to verify the Kubernetes API server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the `helm` provider can be configured using the same parameters.
    This can help provide pre-packaged templates of Kubernetes resources via Helm
    charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Secrets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we enabled the Key Vault extension on the cluster
    itself. Now, we need to provide a way for the pods to connect to Azure Key Vault.
    This requires us to use the Kubernetes secrets store **Container Storage Interface**
    (**CSI**) driver. This configuration acts as a conduit, granting Workload Identity
    the necessary permissions to read specific secrets from the designated Key Vault:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we need to provision this Kubernetes resource into the
    namespace we plan on deploying our pods and specify the Key Vault, the managed
    identity that we configured with the Azure federated identity credential, and
    the Kubernetes service account.
  prefs: []
  type: TYPE_NORMAL
- en: Workload identity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ensure that our pods use the managed identity, we need to take a few actions
    that use both Azure-specific schema and standard Kubernetes schema by provisioning
    resources within Kubernetes and configurations within the deployment specifications
    of our pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is create a Kubernetes service account. This
    is a standard resource within Kubernetes but we use Azure-specific schema to associate
    it with the Azure federated identity credential:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Using Terraform allows us to substitute dynamic values that are created during
    the earlier stage of the provisioning process. Kubernetes has its own way of doing
    things but it involves using Helm and has additional implementation overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the service account exists in Kubernetes and is linked to the appropriate
    Azure managed identity credential, the next step is to enable Azure Workload Identity
    within the deployment. To do this, we need to specify a special label, `azure.workload.identity/use`,
    and set its value to `true`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This will inform AKS to connect the pods within this deployment to the managed
    identity linked through the Azure federated identity credential.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to specify the corresponding Kubernetes service account that
    we already linked to the Azure federated identity credential in the previous section.
    This service account is set on the pod’s specification within the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have built out the three components of our architecture, in the
    next section, we’ll learn how to automate the deployment using Docker so that
    we can build and publish the container images. Then, we’ll use Terraform to provision
    our infrastructure and deploy our solution to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll learn how to automate the deployment process for container-based
    architectures. We’ll be employing similar techniques that we saw in [*Chapter
    8*](B21183_08.xhtml#_idTextAnchor402) when we took this same journey down the
    Amazon. As a result, we’ll focus on what changes we need to make when we want
    to deploy to Microsoft Azure and AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), we covered each step of
    the GitHub Actions workflow that executes Docker to build, tag, and push our Docker
    container images. Thanks to the nature of Docker’s cloud-agnostic architecture,
    this overwhelmingly stays the same. The only thing that changes is the way we
    must configure Docker so that it targets our Azure container registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), we need to connect
    to the container registry that we provisioned with Terraform. On Azure, that means
    we’ll need the Entra ID service principal’s client ID and client secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This service principal is the same identity that we configured as inputs in
    Terraform that provision the infrastructure. As part of that process, the `AcrPush`
    role assignment was associated with this identity. This grants it permission to
    publish images to ACR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses `docker\build-push-action` to push the container image
    that we built in this GitHub Action to our Azure container registry. As we did
    in AWS, we reference the outputs from the Terraform infrastructure stage to obtain
    the ACR endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B21183_10.xhtml#_idTextAnchor474), we comprehensively covered
    the process of creating a Terraform GitHub Action that authenticates with Azure
    using a Microsoft Entra ID service principal. Therefore, we won’t be delving into
    it any further. I encourage you to refer back to [*Chapter 10*](B21183_10.xhtml#_idTextAnchor474)
    to review the process.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we automate Kubernetes with Terraform, we are just running `terraform apply`
    again with a different root module. This time, the root module will configure
    the `kubernetes` and `helm` providers in addition to the `azurerm` provider. However,
    we won’t create new resources with the `azurerm` provider; we will only obtain
    data sources to existing resources we provisioned in the previous `terraform apply`
    command that provisioned the infrastructure to Azure.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the GitHub Action that executes this process will look strikingly
    similar to how we executed Terraform with Azure. Some of the variables might change
    to include things such as the container image details and cluster information.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we designed, built, and automated the deployment of a complete
    and end-to-end solution using container-based architecture. We built onto the
    foundations from [*Chapter 10*](B21183_10.xhtml#_idTextAnchor474), where we worked
    with the foundational infrastructure of Azure virtual networks but layered on
    AKS to host our application in containers. In the next and final step in our Azure
    journey, we’ll be looking at serverless architecture, moving beyond the underlying
    infrastructure, and letting the platform itself take our solution to new heights.
  prefs: []
  type: TYPE_NORMAL
