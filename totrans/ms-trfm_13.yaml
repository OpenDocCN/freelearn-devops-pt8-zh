- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Getting Started on Google Cloud – Building Solutions with GCE
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Google Cloud上入门——使用GCE构建解决方案
- en: You’ve made it. After the previous six chapters, where we used two different
    cloud platforms and three different cloud computing paradigms to build six distinct
    solutions, we are finally ready to take our final journey by adapting our solution
    to **Google Cloud** **Platform** (**GCP**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你成功了。在前六章中，我们使用了两个不同的云平台和三种不同的云计算范式，构建了六个不同的解决方案，现在我们终于准备好通过将我们的解决方案适应**Google
    Cloud** **Platform**（**GCP**）来踏上最后的旅程。
- en: Like the last two adventures, in this alternate universe, we will be starting
    our journey by building our solution with a **virtual machine** (**VM**) architecture
    on Google Cloud. As we saw when we transitioned between AWS and Azure when we
    compared how the same solution architecture was built on the two different cloud
    platforms, some things changed a lot, while many things changed only a tiny bit
    – or not at all. We observed that our Terraform code changed pretty consistently
    across all chapters. However, other things, such as Packer, Docker, and GitHub
    Actions workflows, only changed slightly. Our .NET-based application code didn’t
    change at all, whether being hosted in VMs or containers, but when we got to serverless,
    the application code went through radical refactoring.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前的两次冒险一样，在这个平行宇宙中，我们将通过在Google Cloud上使用**虚拟机**（**VM**）架构来开始我们的旅程。正如我们在AWS和Azure之间迁移时所看到的，当我们比较在两个不同的云平台上如何构建相同的解决方案架构时，有些东西变化很大，而有些东西变化很小——或者根本没有变化。我们注意到，在所有章节中，我们的Terraform代码几乎保持一致。然而，其他的一些内容，比如Packer、Docker和GitHub
    Actions工作流，只是稍微发生了变化。我们的基于.NET的应用程序代码无论是在虚拟机还是容器中托管，都没有变化，但当我们进入无服务器时，应用程序代码经历了彻底的重构。
- en: The same is true as we move the solution to GCP. As a result, we won’t be revisiting
    these topics at the same length in this chapter. However, I would encourage you
    to bookmark *Chapters 7* and *8* and refer to them frequently. This chapter will
    only focus on the changes we must make to deploy our solution on GCP.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的情况也适用于我们将解决方案移到GCP时。因此，我们在本章中不会再详细讨论这些主题。不过，我鼓励你收藏*第7章*和*第8章*，并经常参考它们。本章将重点介绍我们在GCP上部署解决方案时必须做出的更改。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: Laying the foundation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奠定基础
- en: Designing the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Building the solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: Automating the deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化部署
- en: Laying the foundation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奠定基础
- en: 'Our team at Söze Enterprises applauds their achievement of responding to the
    whimsical technical course correction of their fearless leader, Keyser Söze, and
    marvels at their success and fortune in launching their product successfully on
    Microsoft Azure. They utilized VMs, Kubernetes, and, finally, serverless technology.
    The comforting deep blue of the Azure portal begins to fade away when suddenly,
    the air fills with an eerie yet familiar sound: doodle-oo doodle-oo doodle-oo.
    The familiar duo appears – sitting in a cozy wood-paneled basement, a look typical
    of suburban basements from the late ''80s and early ''90s. The walls are adorned
    with posters and memorabilia, including a prominent Chicago Bears pennant, highlighting
    Wayne’s love for rock music and sports. They start the familiar chant: doodle-oo
    doodle-oo doodle-oo. Suddenly, we’re transported to another world – another universe,
    perhaps, where Google Cloud’s sleek multicolored logo replaces Azure’s deep blue.
    Söze Enterprises has now partnered with Google Cloud for their next-generation
    autonomous vehicle platform.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Söze企业的团队对他们响应他们无畏的领导者Keyser Söze的技术调整，取得的成就表示赞赏，并且对他们在微软Azure上成功推出产品的成功和运气感到惊讶。他们使用了虚拟机（VM）、Kubernetes，最终还使用了无服务器技术。Azure门户的深蓝色渐渐褪去，突然，空气中充满了一种神秘而又熟悉的声音：doodle-oo
    doodle-oo doodle-oo。熟悉的二重奏出现在我们面前——坐在一个温馨的木质面板地下室里，仿佛是90年代末和2000年代初郊区地下室的典型装饰。墙上挂着海报和纪念品，其中一个显眼的芝加哥熊队旗帜突出展示了Wayne对摇滚音乐和体育的热爱。他们开始熟悉的咒语：doodle-oo
    doodle-oo doodle-oo。突然间，我们被传送到了另一个世界——也许是另一个宇宙，在这个宇宙里，Google Cloud的光滑多彩标志取代了Azure的深蓝色。Söze企业现在已经与Google
    Cloud合作，打造下一代自动驾驶平台。
- en: 'As before, we inherited a team from one of Söze Enterprises’ other divisions
    with a strong core team of C# .NET developers, so we’ll build version 1.0 of the
    platform using .NET technologies. The elusive CEO, Keyser, was rumored to have
    joined Google co-founder Sergey Brin aboard his super yacht, the Dragonfly, off
    the Amalfi Coast, and word has come down from corporate that we will be using
    Google Cloud to host the platform. Since the team doesn’t have much experience
    with containers and timelines are tight, we’ve decided to build a simple three-tier
    architecture and host on Azure VMs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所说，我们从 Söze Enterprises 的其他部门继承了一个拥有强大核心团队的 C# .NET 开发人员团队，因此我们将使用 .NET
    技术构建平台的 1.0 版本。据信，神秘的 CEO Keyser 曾和 Google 联合创始人 Sergey Brin 一起在阿马尔菲海岸附近的超级游艇
    Dragonfly 上，公司的消息传下来，我们将使用 Google Cloud 来托管平台。由于团队对容器技术不太熟悉，而且时间紧迫，我们决定构建一个简单的三层架构并托管在
    Azure 虚拟机上：
- en: '![Figure 13.1 – Logical architecture for the autonomous vehicle platform](img/B21183_13_1.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1 – 自动驾驶平台的逻辑架构](img/B21183_13_1.jpg)'
- en: Figure 13.1 – Logical architecture for the autonomous vehicle platform
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – 自动驾驶平台的逻辑架构
- en: The platform will need a frontend, which will be a web UI built using ASP.NET
    Core Blazor. The frontend will be powered by a REST API backend, which will be
    built using ASP.NET Core Web API. Having our core functionality encapsulated into
    a REST API will allow autonomous vehicles to communicate directly with the platform
    and allow us to expand by adding client interfaces with additional frontend technologies
    such as native mobile apps and virtual or mixed reality in the future. The backend
    will use a PostgreSQL database for persistent storage since it’s lightweight,
    industry-standard, and relatively inexpensive.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 平台需要一个前端，它将是一个使用 ASP.NET Core Blazor 构建的 Web UI。前端将由一个 REST API 后端提供支持，后端将使用
    ASP.NET Core Web API 构建。将我们的核心功能封装到 REST API 中将允许自动驾驶车辆直接与平台进行通信，并使我们能够通过添加客户端接口（例如原生移动应用程序和未来的虚拟或混合现实技术）进行扩展。后端将使用
    PostgreSQL 数据库进行持久存储，因为它轻量、行业标准且相对廉价。
- en: Designing the solution
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Due to the tight timelines the team is facing, we want to keep the cloud architecture
    simple. Therefore, we’ll be keeping it simple and using Google Cloud services
    that will allow us to provision using familiar VM technologies as opposed to trying
    to learn something new. The first decision we have to make is what Google Cloud
    service each component of our logical architecture will be hosted on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于团队面临紧迫的时间表，我们希望保持云架构的简单性。因此，我们将保持简单，使用 Google Cloud 服务，这将允许我们利用熟悉的虚拟机技术进行资源配置，而不是尝试学习新技术。我们需要做出的第一个决策是，确定我们的逻辑架构中每个组件将托管在
    Google Cloud 的哪个服务上。
- en: 'Our application architecture consists of three components: a frontend, a backend,
    and a database. The frontend and backend are application components and need to
    be hosted on a cloud service that provides general computing, while the database
    needs to be hosted on a cloud database service. There are many options for both
    types of services:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用架构由三个组件组成：前端、后端和数据库。前端和后端是应用程序组件，需要托管在提供通用计算的云服务上，而数据库则需要托管在云数据库服务上。对于这两种类型的服务，有很多选择：
- en: '![Figure 13.2 – Logical architecture for the autonomous vehicle platform and
    the hosts](img/B21183_13_2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 – 自动驾驶平台及其主机的逻辑架构](img/B21183_13_2.jpg)'
- en: Figure 13.2 – Logical architecture for the autonomous vehicle platform and the
    hosts
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 自动驾驶平台及其主机的逻辑架构
- en: 'Since we have decided we’re going to use VMs to host our application, we have
    narrowed down the different services that we can use to host our application.
    We have decided **Google Compute Engine** (**GCE**) is the ideal choice for our
    current situation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经决定使用虚拟机来托管我们的应用程序，我们已缩小了可以用来托管应用程序的不同服务范围。我们已决定 **Google Compute Engine**
    (**GCE**) 是当前情况下的理想选择：
- en: '![Figure 13.3 – Source control structure of our repository](img/B21183_13_3..jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.3 – 我们代码库的源控制结构](img/B21183_13_3..jpg)'
- en: Figure 13.3 – Source control structure of our repository
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 我们代码库的源控制结构
- en: This solution will have six parts. We still have the application code and Packer
    templates for both the frontend and backend. Then, we have GitHub Actions to implement
    our CI/CD process and Terraform to provision our Google Cloud infrastructure and
    reference the Packer-built VM images for our GCE instances.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该解决方案将包括六个部分。我们仍然需要前端和后端的应用代码以及 Packer 模板。然后，我们使用 GitHub Actions 来实现我们的 CI/CD
    流程，并使用 Terraform 来配置我们的 Google Cloud 基础设施，并引用 Packer 构建的 VM 镜像用于我们的 GCE 实例。
- en: Cloud architecture
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云架构
- en: 'The first part of our design is adapting our solution’s architecture to the
    target cloud platform: Google Cloud. This involves mapping application architecture
    components to GCP services and thinking through the configuration of those services
    so that they meet the requirements of our solution.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计的第一部分是将解决方案架构适配到目标云平台：Google Cloud。这涉及到将应用架构组件映射到 GCP 服务，并深入思考这些服务的配置，以确保它们符合我们解决方案的要求。
- en: Projects and API access
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 项目和 API 访问
- en: Before we get started, we need a project within the organization where a service
    account can be created for Terraform to use. This service account needs to be
    granted access to the `roles/resourcemanager.projectCreator` organizational role.
    This will allow you to create projects with Terraform, which will allow you to
    keep a complete solution together and avoid additional boilerplate prerequisites
    that are executed outside of Terraform using the command-line interface.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要在组织内创建一个项目，用于为 Terraform 创建服务账户。该服务账户需要被授予 `roles/resourcemanager.projectCreator`
    组织角色的访问权限。这样，你就可以使用 Terraform 创建项目，保持整个解决方案的完整性，避免使用命令行界面在 Terraform 外部执行额外的样板代码。
- en: Once this has been done, you need to enable the **Cloud Resource Manager API**
    within the project where the Terraform service account resides. This API is required
    within the context of the Google Cloud project because of the way Google Cloud
    grants access to different features of the platform at the project level. It creates
    another gate for the Google Cloud identity to be able to access resources on GCP.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些设置后，你需要在 Terraform 服务账户所在的项目中启用 **Cloud Resource Manager API**。由于 Google
    Cloud 在项目级别通过特定方式授予对平台不同功能的访问权限，因此该 API 在 Google Cloud 项目上下文中是必需的。它为 Google Cloud
    身份访问 GCP 上的资源创建了另一道关卡。
- en: Your Terraform service account will also need access to Cloud Storage, which
    you plan on using to store Terraform state. When using the AWS and Azure providers,
    you can use different credentials to access the Terraform backend than you use
    to provision your environment. On Google Cloud, this can be accomplished by setting
    `GOOGLE_BACKEND_CREDENTIALS` with credentials for the identity you wish to use
    to communicate with the Google Cloud Storage bucket and `GOOGLE_APPLICATION_CREDENTIALS`
    with credentials for the identity you wish to use to communicate with Google Cloud
    to provision your environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 Terraform 服务账户还需要访问 Cloud Storage，你计划用它来存储 Terraform 状态。在使用 AWS 和 Azure 提供程序时，你可以使用与配置环境时不同的凭证来访问
    Terraform 后端。在 Google Cloud 上，可以通过设置 `GOOGLE_BACKEND_CREDENTIALS`，指定与 Google Cloud
    Storage 存储桶进行通信所用的身份凭证，和设置 `GOOGLE_APPLICATION_CREDENTIALS`，指定与 Google Cloud 进行通信以配置环境时所用的身份凭证来实现。
- en: Virtual network
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: VMs must be deployed within a virtual network. As you may recall from [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), when we provisioned this solution on AWS,
    we needed to set up multiple Subnets for our solution to span Availability Zones.
    In [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), when deploying the solution
    to Azure, we only needed two subnets – one for the frontend and one for the backend.
    That’s because Azure’s virtual network architecture is structured differently
    than AWS’s and subnets on Azure span multiple Availability Zones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机必须部署在虚拟网络中。你可能还记得在 [*第7章*](B21183_07.xhtml#_idTextAnchor365) 中，当我们在 AWS 上配置这个解决方案时，我们需要为解决方案设置多个子网，以跨越多个可用区。在
    [*第8章*](B21183_08.xhtml#_idTextAnchor402) 中，当我们将解决方案部署到 Azure 时，我们只需要两个子网——一个用于前端，一个用于后端。这是因为
    Azure 的虚拟网络架构与 AWS 不同，Azure 上的子网跨越多个可用区。
- en: Google Cloud’s virtual network service is also structured differently. Unlike
    both AWS and Azure, which have virtual networks scoped to a particular region,
    virtual networks on GCP span multiple regions by default. Subnets are scoped to
    the region, which means, like Azure, a subnet on GCP can host VMs from multiple
    Availability Zones.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 的虚拟网络服务结构也不同。与 AWS 和 Azure 都只将虚拟网络范围限定于特定区域不同，GCP 上的虚拟网络默认跨多个区域。子网范围限定于区域，这意味着，像
    Azure 一样，GCP 上的子网可以承载来自多个可用区的虚拟机。
- en: 'The following diagram shows that the Google compute network is not tied to
    the region like it is on AWS and Azure:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示，Google 计算网络不像在 AWS 和 Azure 中那样与区域绑定：
- en: '![Figure 13.4 – Google Cloud network architecture](img/B21183_13_4..jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.4 – Google Cloud 网络架构](img/B21183_13_4..jpg)'
- en: Figure 13.4 – Google Cloud network architecture
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – Google Cloud 网络架构
- en: 'Although this seems like a significant difference at the root of the deployment
    hierarchy, it doesn’t materially impact the design as the subnets (or *subnetworks*)
    are still tied to a region:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来是部署层次结构根本上的一个显著差异，但它并不会实质性地影响设计，因为子网（或*子网络*）仍然与一个区域相关联：
- en: '![Figure 13.5 – Isolated subnets for the frontend and backend application components](img/B21183_13_5..jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5 – 前端和后端应用组件的隔离子网](img/B21183_13_5..jpg)'
- en: Figure 13.5 – Isolated subnets for the frontend and backend application components
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 前端和后端应用组件的隔离子网
- en: When building a single-region solution, the multi-region capability of Google
    Cloud might seem like overkill. However, automatic spanning does simplify infrastructure
    management as businesses don’t have to manually set up and maintain inter-regional
    connections. This not only reduces administrative overhead but also allows for
    more agile and scalable deployments in response to changing demands by making
    active-active multi-region deployments easier to build and maintain.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建单区域解决方案时，Google Cloud 的多区域功能可能显得有些过剩。然而，自动跨越多个区域的功能简化了基础设施管理，因为企业不必手动设置和维护跨区域连接。这不仅减少了管理开销，还通过使得活跃-活跃的多区域部署更易于构建和维护，支持了企业对变化需求的更敏捷和可扩展的响应。
- en: Network routing
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络路由
- en: Inside Google Cloud networks, the default setup is designed to provide straightforward
    and secure connectivity. As we know, by default, Google Cloud networks are global
    resources, meaning all the subnets (or *subnetworks*) within a single network
    can communicate with each other, regardless of their regional location, without
    the need for explicit routes or VPNs. This inter-subnet communication uses the
    system-generated routes in the network.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud 网络内，默认设置旨在提供简便且安全的连接。如我们所知，默认情况下，Google Cloud 网络是全球资源，这意味着同一网络中的所有子网（或*子网络*）都可以相互通信，无论它们位于哪个区域，而无需显式路由或
    VPN。这种子网间通信使用网络中的系统生成路由。
- en: For routing configurations, Google Cloud has **routes**, which perform a role
    similar to AWS’s route tables, directing traffic based on IP ranges. For situations
    where instances need to initiate outbound connections to the internet without
    revealing their IP, Google Cloud provides Cloud NAT, which is analogous to AWS’s
    **NAT gateways**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于路由配置，Google Cloud 有 **routes**，其功能类似于 AWS 的路由表，根据 IP 范围来引导流量。对于需要发起出站连接到互联网但又不希望暴露其
    IP 的实例，Google Cloud 提供了 Cloud NAT，这类似于 AWS 的 **NAT gateways**。
- en: Like Azure, Google Cloud does not have a direct equivalent named **internet
    gateway**. Instead, internet connectivity in GCP is managed using a combination
    of system-generated routes and firewall rules.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Azure 一样，Google Cloud 没有一个直接等效的名为 **internet gateway** 的组件。相反，GCP 中的互联网连接是通过系统生成的路由和防火墙规则的组合进行管理的。
- en: Load balancing
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'Google Cloud has two options when it comes to load balancers: global and regional.
    Global load balancers distribute traffic across multiple regions, ensuring users
    are served from the nearest or most suitable region, while regional load balancers
    distribute traffic within a single region. The choice between them typically depends
    on the application’s user distribution and the need for low-latency access. However,
    sometimes, other limitations force your hand:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 有两种负载均衡器选项：全球和区域。全球负载均衡器将流量分配到多个区域，确保用户从最靠近或最适合的区域获得服务，而区域负载均衡器则在单个区域内分配流量。选择使用哪种通常取决于应用程序的用户分布以及低延迟访问的需求。然而，有时候，其他的限制条件也会迫使你做出选择：
- en: '![Figure 13.6 – Google Cloud regional load balancer](img/B21183_13_6..jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.6 – Google Cloud 区域负载均衡器](img/B21183_13_6..jpg)'
- en: Figure 13.6 – Google Cloud regional load balancer
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 – Google Cloud区域负载均衡器
- en: Unfortunately, the regional load balancer’s target pool does not allow you to
    specify a different port for the backend instances. This means the target pool
    will forward traffic to the same port where it received traffic. For instance,
    if the forwarding rule is listening on port `80`, the target pool will send traffic
    to port `80` of the backend instances.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，区域负载均衡器的目标池不允许您为后端实例指定不同的端口。这意味着目标池将把流量转发到接收流量时使用的相同端口。例如，如果转发规则在端口`80`上监听，目标池将把流量发送到后端实例的端口`80`。
- en: 'To achieve your goal of forwarding from port `80` to port `5000`, you would
    need to use the global load balancer instead of the regional load balancer:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现从端口`80`转发到端口`5000`的目标，您需要使用全球负载均衡器，而不是区域负载均衡器：
- en: '![Figure 13.7 – Google Cloud global load balancer](img/B21183_13_7..jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图13.7 – Google Cloud全球负载均衡器](img/B21183_13_7..jpg)'
- en: Figure 13.7 – Google Cloud global load balancer
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7 – Google Cloud全球负载均衡器
- en: The global load balancer requires that you set up instance groups to organize
    the VMs that the load will be distributed across. Google Cloud instance groups
    are similar to AWS Auto Scaling groups and Azure’s **Virtual Machine Scale Sets**
    (**VMSS**), but they have a bit more flexibility in that you can either provide
    a VM template and allow GCP to *manage* the instances or you can provision the
    instances explicitly and add them later to the instance group. This dual-mode
    capability is similar to Azure’s VMSS rather than AWS’s Auto Scaling group, which
    can only operate in a *managed* mode.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 全球负载均衡器要求您设置实例组，以组织负载将要分配到的虚拟机（VM）。Google Cloud实例组类似于AWS的自动扩展组和Azure的**虚拟机规模集**（**VMSS**），但它们更具灵活性，您可以提供虚拟机模板并允许GCP来*管理*实例，或者您可以显式地配置实例并稍后将它们添加到实例组中。这种双模式能力类似于Azure的VMSS，而不是AWS的自动扩展组，后者只能在*管理*模式下运行。
- en: 'As we saw when comparing AWS and Azure, all the anatomical parts of a load
    balancer are present and accounted for – they just might go by different names
    and connect in slightly different ways. The following table extends the mapping
    that we did between AWS and Azure and includes the GCP equivalents:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在比较AWS和Azure时所看到的，负载均衡器的所有基本组成部分都存在并得到体现——它们可能只是使用不同的名称，并以略微不同的方式连接。下表扩展了我们在AWS和Azure之间的映射，并包括了GCP的对应项：
- en: '| **AWS** | **Azure** | **GCP** | **Description** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **AWS** | **Azure** | **GCP** | **描述** |'
- en: '| **Application Load** **Balancer** (**ALB**) | Azure Load Balancer | URL map
    | Load balancer |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **应用负载均衡器**（**ALB**） | Azure负载均衡器 | URL映射 | 负载均衡器 |'
- en: '| Listener | Frontend IP configuration | Global forwarding rule | The singular
    endpoint that accepts incoming traffic on a load balancer |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 监听器 | 前端IP配置 | 全球转发规则 | 接受负载均衡器传入流量的单一端点 |'
- en: '| Target Group | Backend Address pool | Backend Service | A collection of VMs
    that incoming traffic is forwarded to |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 目标组 | 后端地址池 | 后端服务 | 传入流量被转发到的虚拟机集合 |'
- en: '| Health check | Health probe | Health check | An endpoint published by each
    of the backend VMs that indicates it is healthy and ready to handle traffic |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 健康检查 | 健康探针 | 健康检查 | 每个后端虚拟机发布的端点，指示其健康且准备好处理流量 |'
- en: Table 13.1 – Mapping of synonymous load balancer components between AWS, Azure,
    and GCP
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.1 – AWS、Azure与GCP之间同义负载均衡器组件的映射
- en: The URL map and the Target HTTP proxy compose the global load balancer, which
    attaches to the forwarding rule, which acts as the singular endpoint, and the
    backend service, which represents the collection of VMs to distribute load across.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: URL映射和目标HTTP代理共同构成了全球负载均衡器，它连接到转发规则，后者充当单一端点，以及后端服务，后端服务代表着用于分配负载的虚拟机集合。
- en: Network security
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: To control network traffic, Google Cloud offers firewall rules that allow users
    to specify which packets are allowed into and out of instances. While Google Cloud’s
    firewall rules share some similarities with AWS’s **network access control lists**
    (**NACLs**), it’s crucial to note that GCP firewall rules are stateful, while
    AWS NACLs are stateless.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制网络流量，Google Cloud提供了防火墙规则，允许用户指定哪些数据包可以进出实例。虽然Google Cloud的防火墙规则与AWS的**网络访问控制列表**（**NACLs**）有一些相似之处，但需要注意的是，GCP防火墙规则是有状态的，而AWS
    NACLs是无状态的。
- en: Secrets management
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 秘密管理
- en: Secrets such as database credentials or service access keys need to be stored
    securely. Each cloud platform has a service that provides this functionality.
    On GCP, this service is called Google Cloud Secret Manager.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 像数据库凭证或服务访问密钥这样的秘密需要安全存储。每个云平台都有提供此功能的服务。在 GCP 中，这项服务称为 Google Cloud Secret
    Manager。
- en: 'Again, we will see slight naming convention differences but all the anatomical
    parts are there. The following table extends the mapping that we did between AWS
    and Azure and includes the GCP equivalents:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们会看到命名约定的细微差别，但所有的结构部分都在其中。以下表格扩展了我们在 AWS 和 Azure 之间所做的映射，并包括了 GCP 的等效项：
- en: '| **AWS** | **Azure** | **GCP** | **Description** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **AWS** | **Azure** | **GCP** | **描述** |'
- en: '| IAM | Microsoft Entra | Cloud Identity | Identity provider |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| IAM | Microsoft Entra | Cloud Identity | 身份提供者 |'
- en: '| Secrets Manager | Key Vault | Secret Manager | Secure secret storage |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Secret Manager | Key Vault | Secret Manager | 安全的秘密存储 |'
- en: '| IAM role | User-assigned managed identity | Service account | Identity for
    machine-to-machine interaction |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| IAM 角色 | 用户分配的托管身份 | 服务账户 | 机器间交互的身份 |'
- en: '| IAM policy | **Role-based access** **control** (**RBAC**) | IAM member |
    Permission to perform specific operations on specific services or resources |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| IAM 策略 | **基于角色的访问控制** (**RBAC**) | IAM 成员 | 执行特定操作所需的权限 |'
- en: '| IAM role policy | Role assignment | IAM member | Associates specific permissions
    to specific identities |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| IAM 角色策略 | 角色分配 | IAM 成员 | 将特定权限关联到特定身份 |'
- en: Table 13.2 – Mapping of synonymous IAM components between AWS, Azure, and GCP
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.2 – AWS、Azure 和 GCP 之间同义 IAM 组件的映射
- en: 'Secrets stored in Google Cloud Secret Manager can be accessed by VMs once they
    have the necessary access granted. In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365),
    we used an AWS IAM role assignment to allow a VM to do this, and with Azure, we
    used user-assigned managed identities and ole assignments. On GCP, we need to
    use a service account and grant it permissions to the specific secrets:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在 Google Cloud Secret Manager 中的秘密可以通过虚拟机（VM）访问，前提是它们已获得必要的访问权限。在 [*第7章*](B21183_07.xhtml#_idTextAnchor365)
    中，我们使用了 AWS IAM 角色分配来允许 VM 执行此操作，而在 Azure 中，我们使用了用户分配的托管身份和角色分配。在 GCP 中，我们需要使用服务账户并授予它对特定秘密的权限：
- en: '![Figure 13.8 – Key Vault architecture](img/B21183_13_8..jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.8 – Key Vault 架构](img/B21183_13_8..jpg)'
- en: Figure 13.8 – Key Vault architecture
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – Key Vault 架构
- en: Granting the managed identity that is attached to the VMs access to the **Key
    Vault Secrets User** role will allow the VMs to read the secret values from Key
    Vault. This does not put the secrets on the machine. The VM will need to use the
    Azure CLI to access the Key Vault secrets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 授予附加到 VM 的托管身份对 **Key Vault Secrets User** 角色的访问权限将允许 VM 从 Key Vault 中读取密钥值。这不会将秘密存储到机器上。VM
    需要使用 Azure CLI 来访问 Key Vault 中的秘密。
- en: VMs
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机（VM）
- en: 'Now that we have everything we need for our solution, we can finish by talking
    about where our application components will run: VMs provisioned on Google Cloud’s
    Compute Engine service. When provisioning VMs on GCP, you have two options. First,
    you can provide static VMs. In this approach, you need to specify key characteristics
    for every VM. You can organize these VMs into an instance group to better manage
    the health and life cycle of the VMs. The second option is to provision an instance
    group manager. This will allow you to dynamically scale up and down based on demand,
    as well as auto-heal VMs that fail:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了解决方案所需的所有内容，我们可以最后讨论应用程序组件将运行的位置：在 Google Cloud 的 Compute Engine 服务上配置的虚拟机（VM）。在
    GCP 上配置 VM 时，有两种选择。首先，可以提供静态 VM。在这种方法中，需要为每个 VM 指定关键特征。可以将这些 VM 组织到实例组中，以更好地管理其健康状况和生命周期。第二种选择是配置实例组管理器。这将允许根据需求动态地进行扩展和缩减，并且可以自动修复失败的
    VM：
- en: '![Figure 13.9 – Google Cloud Compute Engine instance architecture](img/B21183_13_9..jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.9 – Google Cloud Compute Engine 实例架构](img/B21183_13_9..jpg)'
- en: Figure 13.9 – Google Cloud Compute Engine instance architecture
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 – Google Cloud Compute Engine 实例架构
- en: 'Similar to Azure, Google Cloud separates the concept of grouping VMs that are
    tied together through the application life cycle from the management of their
    health and dynamic provisioning. In Azure, an availability set is a logical group
    that can be used to place individual VMs into a relationship so that their relationship
    is taken into consideration by the underlying platform:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Azure，Google Cloud 将通过应用生命周期将 VM 组织在一起的概念与其健康管理和动态配置区分开来。在 Azure 中，**可用性集**是一个逻辑组，可以将单独的
    VM 放入其中，使其关系被底层平台考虑：
- en: '![Figure 13.10 – Instance group manager architecture](img/B21183_13_10..jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.10 – 实例组管理器架构](img/B21183_13_10..jpg)'
- en: Figure 13.10 – Instance group manager architecture
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 – 实例组管理器架构
- en: On Google Cloud, that is an instance group. Both allow you to easily attach
    a pool of VMs to other services relevant to multiple VMs working a problem together,
    such as load balancers and health monitoring. To add dynamic provisioning and
    management, on Azure, you would need a VMSS. On Google Cloud, this is called an
    instance group manager.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上，这就是一个实例组。两者都允许你轻松地将虚拟机池附加到其他相关服务，这些服务有助于多个虚拟机共同解决问题，例如负载均衡器和健康监测。为了添加动态配置和管理，在
    Azure 上，你需要一个 VMSS。在 Google Cloud 上，这被称为实例组管理器。
- en: 'Again, just as we saw previously, the names have been changed to protect the
    innocent, but make no mistake, they work the same way. The following table extends
    the mapping that we did between AWS and Azure and includes the GCP equivalents:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，正如我们之前看到的那样，名称已经更改以保护无辜者，但请不要误解，它们的工作方式是相同的。下表扩展了我们在 AWS 和 Azure 之间所做的映射，并包括了
    GCP 的等效项：
- en: '| **AWS** | **Azure** | **GCP** | **Description** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **AWS** | **Azure** | **GCP** | **描述** |'
- en: '| EC2 | VMs | Compute instance | VM service |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| EC2 | 虚拟机 | 计算实例 | 虚拟机服务 |'
- en: '| AMI | VM image | Google compute image | VM image either from the marketplace
    or a custom build (for example, using tools such s Packer) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| AMI | 虚拟机映像 | Google 计算映像 | 来自市场或自定义构建的虚拟机映像（例如，使用 Packer 等工具） |'
- en: '| IAM role | User-assigned managed identity | Service account | Identity for
    machine-to-machine interaction |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| IAM 角色 | 用户分配的托管身份 | 服务账户 | 用于机器到机器交互的身份 |'
- en: '| Auto Scaling group | VMSS | Instance group manager | Set of dynamically provisioned
    VMs that can be scaled up/down using a VM configuration template |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 自动伸缩组 | VMSS | 实例组管理器 | 一组动态配置的虚拟机，可以使用虚拟机配置模板进行扩展/缩减 |'
- en: '| Launch template | VM profile | Instance template | Configuration template
    used to create new VMs |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 启动模板 | 虚拟机配置 | 实例模板 | 用于创建新虚拟机的配置模板 |'
- en: Table 13.3 – Mapping of synonymous VM service components between AWS, Azure,
    and GCP
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.3 – AWS、Azure 和 GCP 之间的虚拟机服务组件同义词映射
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we provisioned our solution
    using AWS **Elastic Cloud Compute** (**EC2**) service and in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402),
    we did the same but with the Azure VM service. Like both of these platforms, on
    GCP, VMs are connected to virtual networks using network interfaces. Unlike AWS
    and Azure, these network interfaces cannot be provisioned independently of the
    VM and are then attached later.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 7 章*](B21183_07.xhtml#_idTextAnchor365) 中，我们使用 AWS **弹性云计算**（**EC2**）服务配置了我们的解决方案，而在
    [*第 8 章*](B21183_08.xhtml#_idTextAnchor402) 中，我们做了同样的事情，但使用的是 Azure 虚拟机服务。像这两个平台一样，在
    GCP 上，虚拟机通过网络接口连接到虚拟网络。与 AWS 和 Azure 不同的是，这些网络接口不能独立于虚拟机进行配置，而是在后期附加。
- en: We also discussed the subtle differences between how Azure and AWS handle network
    security, with AWS having low-level network security handled by NACLs that attach
    at the subnet and more logical security groups that attach at the instance and
    process network traffic in a stateful manner. Azure has similar constructs with
    network security groups, which focus more on network traffic between physical
    endpoints (IP address ranges and network gateways), and application security groups,
    which focus on network traffic between logical application endpoints. Google Cloud
    combines the two into Google Compute firewall resources that can control network
    traffic using physical network characteristics such as IP address ranges and logical
    constructs such as service accounts and tags.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了 Azure 和 AWS 在处理网络安全方面的细微差别，其中 AWS 通过 NACL 处理低级别的网络安全，这些 NACL 附加在子网中，并通过更具逻辑性的安全组附加在实例上，按状态方式处理网络流量。Azure
    有类似的构造，网络安全组更多关注物理端点之间的网络流量（IP 地址范围和网络网关），而应用程序安全组则关注逻辑应用程序端点之间的网络流量。Google Cloud
    将两者结合起来，使用 Google Compute 防火墙资源，能够利用物理网络特性（如 IP 地址范围）和逻辑构造（如服务账户和标签）控制网络流量。
- en: This pattern of using tags to attach behavior or grant permissions is a common
    pattern on GCP and you should make note of it as other platforms do not regard
    tags as a method for establishing security boundaries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标签来附加行为或授予权限的模式是 GCP 中常见的模式，你应该注意这一点，因为其他平台并不将标签视为建立安全边界的方法。
- en: Deployment architecture
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署架构
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on Google Cloud, we need to come up with a plan on how to
    provision our environments and deploy our code.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个大致的 Google Cloud 云架构方案，我们需要制定一个计划，来配置我们的环境并部署我们的代码。
- en: VM configuration
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VM 配置
- en: 'In our solution, we have two VM roles: the frontend role, which is responsible
    for handling web page requests from the end user’s web browser, and the backend
    role, which is responsible for handling REST API requests from the web application.
    Each of these roles has a different code and different configuration that needs
    to be set. Each will require its own Packer template to build a VM image that
    we can use to launch a VM on Google Cloud:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们有两个 VM 角色：前端角色，负责处理来自终端用户网页浏览器的网页请求，以及后端角色，负责处理来自 Web 应用程序的 REST
    API 请求。这两个角色有不同的代码和配置，需要进行设置。每个角色都需要自己的 Packer 模板来构建一个 VM 镜像，我们可以用这个镜像在 Google
    Cloud 上启动 VM：
- en: '![Figure 13.11 – Packer pipeline to build a VM image for the frontend](img/B21183_13_11..jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.11 – 使用 Packer 管道为前端构建 VM 镜像](img/B21183_13_11..jpg)'
- en: Figure 13.11 – Packer pipeline to build a VM image for the frontend
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 – 使用 Packer 管道为前端构建 VM 镜像
- en: A GitHub Actions workflow that triggers off changes to the frontend application
    code and the frontend Packer template will execute `packer build` and create a
    new VM image for the solution’s frontend.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 GitHub Actions 工作流，触发前端应用程序代码和前端 Packer 模板的更改，执行 `packer build` 并为解决方案的前端创建一个新的
    VM 镜像。
- en: 'Both the frontend and the backend will have identical GitHub workflows that
    execute `packer build`. The key difference between the workflows is the code bases
    that they execute against. Both the frontend and the backend might have slightly
    different operating system configurations, and both require different deployment
    packages for their respective application components:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 前端和后端将有相同的 GitHub 工作流，执行 `packer build`。这两个工作流的主要区别在于它们执行的代码库。前端和后端可能有稍微不同的操作系统配置，并且它们分别需要不同的部署包来部署各自的应用组件：
- en: '![Figure 13.12 – Packer pipeline to build a VM image for the backend](img/B21183_13_12..jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.12 – 使用 Packer 管道为后端构建 VM 镜像](img/B21183_13_12..jpg)'
- en: Figure 13.12 – Packer pipeline to build a VM image for the backend
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 – 使用 Packer 管道为后端构建 VM 镜像
- en: It’s important to note that the application code will be baked into the VM image
    rather than copied to an already running VM. This means that to update the software
    running on the VMs, each VM will need to be restarted so that it can be restarted
    with a new VM image containing the latest copy of the code.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，应用程序代码将被“烘焙”进 VM 镜像，而不是复制到已经运行的 VM 上。这意味着，要更新运行在 VM 上的软件，每个 VM 都需要重新启动，以便用包含最新代码副本的新
    VM 镜像重新启动。
- en: This approach makes the VM image an immutable deployment artifact that is versioned
    and updated each time there is a release of the application code that needs to
    be deployed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使得 VM 镜像成为一个不可变的部署工件，每次发布需要部署的应用程序代码时，镜像都会进行版本更新和更新。
- en: Cloud environment configuration
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云环境配置
- en: 'Once the VM images have been built for both the frontend and the backend, we
    can execute the final workflow that will both provision and deploy our solution
    to Google Cloud:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为前端和后端构建了 VM 镜像，我们就可以执行最终的工作流，该工作流将同时配置并将我们的解决方案部署到 Google Cloud：
- en: '![Figure 13.13 – VM images as inputs to the Terraform code, which provisions
    the environment on Google Cloud](img/B21183_13_13..jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.13 – VM 镜像作为 Terraform 代码的输入，用于在 Google Cloud 上配置环境](img/B21183_13_13..jpg)'
- en: Figure 13.13 – VM images as inputs to the Terraform code, which provisions the
    environment on Google Cloud
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 – VM 镜像作为 Terraform 代码的输入，用于在 Google Cloud 上配置环境
- en: The Terraform code base will have two input variables for the version of the
    VM image for both the frontend and the backend. When new versions of the application
    software need to be deployed, the input parameters for these versions will be
    incremented to reflect the target version for deployment. When the workflow is
    executed, `terraform apply` will simply replace the existing VMs with VMs using
    the new VM image.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 代码库将有两个输入变量，分别表示前端和后端的 VM 镜像版本。当需要部署新版本的应用软件时，这些版本的输入参数将增加，以反映目标版本。当工作流执行时，`terraform
    apply` 将简单地用新的 VM 镜像替换现有的 VM。
- en: Now that we have a solid plan for how we will implement both the cloud architecture
    using Google Cloud and the deployment architecture using GitHub Actions, let’s
    start building! In the next section, we’ll break down the **HashiCorp Configuration
    Language** (**HCL**) code that we’ll use to implement the Terraform and Packer
    solutions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个明确的计划，如何使用Google Cloud来实现云架构，以及如何使用GitHub Actions来实现部署架构，让我们开始构建吧！在下一节中，我们将解析我们将用于实现Terraform和Packer解决方案的**HashiCorp配置语言**（**HCL**）代码。
- en: Building the solution
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: Now that we have a solid design for our solution, we can begin building it.
    As discussed in the previous section, we’ll be using VMs powered by Google Cloud
    Compute Engine. As we did with AWS and Azure in *Chapters 7* and *10*, respectively,
    we’ll need to package our application into VM images using Packer and then provision
    an environment that provisions an environment using these VM images.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个坚实的解决方案设计，我们可以开始构建它了。正如在前一节中讨论的那样，我们将使用由Google Cloud Compute Engine提供支持的虚拟机。正如我们在*第7章*和*第10章*中分别做的那样，我们需要使用Packer将应用程序打包成虚拟机镜像，然后再配置一个使用这些虚拟机镜像的环境。
- en: Packer
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Packer
- en: In this section, we’ll cover how to implement our Packer templates provisioners
    so that we can install our .NET application code on a Linux VM. If you skipped
    *Chapters 7* through *9* due to a lack of interest in (AWS, I can’t hold that
    against you – particularly if your primary interest in reading this book is working
    on GCP. However, I would encourage you to review the corresponding section within
    [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) to see how we use Packer’s provisioners
    to configure a Debian-based Linux VM with our .NET application code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何实现Packer模板配置器，以便我们能够在Linux虚拟机上安装我们的.NET应用程序代码。如果你由于对AWS不感兴趣而跳过了*第7章*到*第9章*，我不怪你——特别是如果你阅读本书的主要兴趣是在GCP上工作。然而，我建议你查看[*第7章*](B21183_07.xhtml#_idTextAnchor365)中的相关部分，看看我们如何使用Packer的配置器在基于Debian的Linux虚拟机上配置.NET应用程序代码。
- en: Google Cloud plugin
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud插件
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), Packer
    – like Terraform – is an extensible command-line executable. Each cloud platform
    provides a plugin for Packer that encapsulates the integration with its services:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第4章*](B21183_04.xhtml#_idTextAnchor239)中讨论的，Packer – 与Terraform类似 – 是一个可扩展的命令行可执行文件。每个云平台都为Packer提供了一个插件，封装了与其服务的集成：
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In *Chapters 7* and *10*, we saw how to declare the Packer plugin for AWS and
    Azure (respectively) as a required plugin. The preceding code demonstrates how
    to declare Google Cloud’s plugin – at the time of writing, the latest version
    is 1.1.2.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*和*第10章*中，我们展示了如何声明AWS和Azure的Packer插件（分别）作为必需插件。前面的代码演示了如何声明Google Cloud的插件——截至写作时，最新版本是1.1.2。
- en: The Google Cloud plugin for Packer provides a `googlecompute` builder that will
    generate Google Cloud compute images by creating a new VM from a base image, executing
    the provisioners, taking a snapshot of the Google Cloud instance’s boot disk,
    and creating a Google Cloud compute image from it. Like the AWS and Azure plugins,
    this behavior is encapsulated within Google Cloud’s builder.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Packer的Google Cloud插件提供了一个`googlecompute`构建器，该构建器将通过从基础镜像创建新虚拟机、执行配置器、拍摄Google
    Cloud实例启动磁盘的快照，并从中创建Google Cloud计算镜像来生成Google Cloud计算镜像。像AWS和Azure插件一样，这一行为被封装在Google
    Cloud的构建器内部。
- en: Just as the other plugins encapsulated the logic to build VMs on their respective
    platforms, its configuration was oriented using terminology specific to each platform.
    Packer does not try to create a standard builder interface across cloud platforms
    – rather, it isolates the cloud-specific configuration within the builders. This
    keeps things simple for users who know the target platform well and allows the
    builder to take advantage of any platform-specific features without additional
    layers of complexity by trying to rationalize the syntax across every platform.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他插件封装了在各自平台上构建虚拟机的逻辑一样，其配置使用了特定平台的术语。Packer并不试图在各云平台之间创建一个标准的构建器接口 – 而是将特定平台的配置封装在构建器内部。这使得对于熟悉目标平台的用户来说，操作更为简单，同时允许构建器利用平台特有的功能，而不需要通过在每个平台之间统一语法来增加额外的复杂性。
- en: 'As a result, the structure of the AWS, Azure, and Google Cloud builders is
    radically different in almost every way – from how they authenticate to how they
    look up marketplace images. There are some common fields and similarities, but
    they are very different animals:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，AWS、Azure和Google Cloud的构建器在几乎每个方面都存在根本差异——从它们的身份验证方式到如何查找市场镜像。虽然它们有一些共同的字段和相似性，但它们实质上是完全不同的：
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code shows how we reference the Google Cloud marketplace version
    of the Ubuntu 22.04 VM. Notice how, unlike the other providers, which have rather
    complex lookup mechanisms, Google Cloud simply has a single string to represent
    the desired image. Each approach produces the same outcome: we select a marketplace
    image hosted by the cloud platform to use as our boot disk, but we see different
    organizational philosophies manifesting in the three different clouds.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了我们如何引用Google Cloud市场版的Ubuntu 22.04虚拟机。请注意，与其他提供者不同，后者有相对复杂的查找机制，Google
    Cloud只需一个字符串来表示所需的镜像。每种方式都能产生相同的结果：我们选择一个由云平台托管的市场镜像作为我们的启动磁盘，但我们能看到三种不同的云平台展现出不同的组织理念。
- en: Operating system configuration
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作系统配置
- en: We must configure the operating system so that it installs software dependencies
    (such as .NET 6.0), copies and deploys our application code’s deployment package
    to the correct location in the local filesystem, configures a Linux service that
    runs on boot, and sets a local user and group with necessary access for the service
    to run.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须配置操作系统，以便它安装软件依赖项（例如 .NET 6.0），将我们的应用程序代码的部署包复制并部署到本地文件系统中的正确位置，配置一个在启动时运行的Linux服务，并设置一个具有必要访问权限的本地用户和组，以便该服务能够运行。
- en: I expanded on these steps in detail in the corresponding section in [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), so I encourage you to review this section
    if you want to refresh your memory.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[*第7章*](B21183_07.xhtml#_idTextAnchor365)的相应部分详细扩展了这些步骤，因此如果你想刷新记忆，建议查看该部分。
- en: Terraform
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Terraform
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base of application code that needs
    to be deployed. Since this is the first time we will be using the `google` provider,
    we’ll look at basic provider setup and how to configure the backend before we
    consider the nuts and bolts of each component of our architecture.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中讨论的那样，我们的解决方案由两个应用组件组成：前端和后端。每个组件都有需要部署的应用代码库。由于这是我们第一次使用`google`提供者，我们将首先了解基本的提供者设置以及如何配置后端，然后再深入讨论我们架构中每个组件的细节。
- en: Provider setup
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提供者设置
- en: 'First, we need to specify all the providers that we intend to use in this solution
    within the `required_providers` block:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在`required_providers`块中指定所有我们打算在此解决方案中使用的提供者：
- en: '[PRE2]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We’ll also configure the Google Cloud provider. The Google Cloud provider,
    like Azure but unlike AWS, is not scoped to a particular region. The Google Cloud
    provider doesn’t even need to be scoped to a project. In this way, it is extremely
    flexible and can be used to provision cross-project and multi-region resources
    with the same provider declaration:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将配置Google Cloud提供者。与AWS不同，Google Cloud提供者像Azure一样，并不限于某个特定区域。Google Cloud提供者甚至不需要限定为某个项目。通过这种方式，它非常灵活，可以用于在不同项目和多个区域之间使用相同的提供者声明来配置资源：
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One major difference between the Google provider and the AWS and Azure providers
    is how you authenticate. While Azure and AWS have environment variables that specify
    the identity, the Google Cloud provider relies on an authentication file, so this
    will alter how our pipeline tools integrate with Terraform to ensure a Google
    Cloud solution has the right identity. The `GOOGLE_APPLICATION_CREDENTIALS` environment
    variable specifies the path to this file. It is important to note that this file
    is a JSON file, but it contains secret information; therefore, it should be treated
    as a credential and protected as such.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Google提供者与AWS和Azure提供者之间的一个主要区别是身份验证的方式。虽然Azure和AWS有环境变量指定身份，但Google Cloud提供者依赖于认证文件，因此这将改变我们的管道工具与Terraform集成的方式，以确保Google
    Cloud解决方案具有正确的身份。`GOOGLE_APPLICATION_CREDENTIALS`环境变量指定该文件的路径。需要注意的是，这个文件是一个JSON文件，但它包含机密信息，因此应当将其视为凭证并加以保护。
- en: Backend
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后端
- en: Because we will be using a CI/CD pipeline to provision and maintain our environment
    in the long term, we need to set up a remote backend for our Terraform state.
    Because our solution will be hosted on Google Cloud, we’ll use the Google Cloud
    Storage backend to store our Terraform state.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将使用 CI/CD 流水线来长期提供和维护我们的环境，所以我们需要为我们的 Terraform 状态设置一个远程后端。由于我们的解决方案将在 Google
    Cloud 上托管，我们将使用 Google Cloud Storage 后端来存储 Terraform 状态。
- en: 'Just like the Google Cloud provider, we don’t want to hard code the backend
    configuration in our code, so we’ll simply set up a placeholder for the backend:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Google Cloud 提供商一样，我们不希望在代码中硬编码后端配置，因此我们将简单地为后端设置一个占位符：
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’ll configure the backend’s parameters using the `-backend-config` parameters
    when we run `terraform init` in our CI/CD pipeline.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 CI/CD 流水线中运行 `terraform init` 时，使用 `-backend-config` 参数来配置后端的参数。
- en: Input variables
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入变量
- en: It’s good practice to pass in short names that identify the application’s name
    and the application’s environment. This allows you to embed consistent naming
    conventions across the resources that make up your solution, which makes it easier
    to identify and track resources from the Google Cloud Console.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 传递短名称来标识应用程序的名称和环境是良好的实践。这使得你可以在构成解决方案的资源中嵌入一致的命名约定，从而更容易在 Google Cloud 控制台中识别和跟踪资源。
- en: The `primary_region`, `network_cidr_block`, and `az_count` input variables drive
    key architectural characteristics of the deployment. They mustn’t be hard-coded
    as this will limit the reusability of the Terraform code base.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`primary_region`、`network_cidr_block` 和 `az_count` 输入变量驱动部署的关键架构特征。它们不应硬编码，因为这样会限制
    Terraform 代码库的可重用性。'
- en: The `network_cidr_block` input variable establishes the virtual network address
    space, which is often tightly regulated by an enterprise governance body. There
    is usually a process to ensure that teams across an organization do not use IP
    address ranges that conflict, thus making it impossible to allow those two applications
    to integrate in the future or integrate with shared network resources within the
    enterprise.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`network_cidr_block` 输入变量建立了虚拟网络地址空间，通常会受到企业治理机构的严格控制。通常会有一个流程来确保组织内的各个团队不会使用冲突的
    IP 地址范围，从而避免将来无法让这两个应用程序集成，或无法与企业内部共享的网络资源集成。'
- en: The `az_count` input variable allows you to configure how much redundancy you
    want within our solution. This will affect the high availability of the solution
    but also the cost of the deployment. As you can imagine, cost is also a tightly
    regulated characteristic of cloud infrastructure deployments.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`az_count` 输入变量允许你配置在解决方案中所需的冗余度。这将影响解决方案的高可用性，但也会影响部署的成本。正如你可以想象的那样，成本也是云基础设施部署的一个严格受控的特性。'
- en: Consistent naming and tagging
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一致的命名和标签
- en: 'Unlike the AWS console, and very similar to Azure, Google Cloud is designed
    in such a way that it is extremely easy to get an application-centric view of
    your deployment through projects. Therefore, it’s not as important as an organizational
    strategy for your application to specify tags. By default, you will have a project-centric
    view of all the resources on Google Cloud:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS 控制台不同，并且与 Azure 非常相似，Google Cloud 的设计方式使得你通过项目非常容易获得应用程序中心的部署视图。因此，为应用程序指定标签并不是像组织策略那样重要。默认情况下，你将获得一个项目中心的视图，查看
    Google Cloud 上的所有资源：
- en: '[PRE5]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It’s still important to tag the resources that you deploy that indicate what
    application and what environment they belong to. This helps with other reporting
    needs, such as budgets and compliance. Almost all resources within the Google
    Cloud provider have a `map` attribute called `tags`. Like Azure, each resource
    usually has `name` as a required attribute.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 标记你部署的资源仍然很重要，这些资源可以指示它们属于哪个应用程序和哪个环境。这有助于其他报告需求，如预算和合规性。几乎所有 Google Cloud 提供商中的资源都有一个名为
    `tags` 的 `map` 属性。像 Azure 一样，每个资源通常都有 `name` 作为必填属性。
- en: Virtual network
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: 'Just as we did in *Chapters 7* and *8*, we need to construct a virtual network
    and keep its address space as tight as possible to avoid gobbling up unnecessary
    address space for the broader organization in the future:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *第7章* 和 *第8章* 中所做的那样，我们需要构建一个虚拟网络，并尽可能将其地址空间保持紧凑，以避免在将来为更广泛的组织吞噬不必要的地址空间：
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The network creation in Google Cloud is simpler than what we did with AWS because
    we don’t have to segment our subnets based on Availability Zone. This approach
    resembles how Azure structures subnets to span Availability Zones:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud 中创建网络比我们在 AWS 中所做的更简单，因为我们无需根据可用区来划分子网。这种方式类似于 Azure 如何结构化子网以跨可用区展开：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Load balancing
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: As we discussed in the design, the Google Cloud Load Balancing service is structured
    quite a bit differently than AWS and Azure’s equivalent offerings.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中讨论的，Google Cloud 的负载均衡服务与 AWS 和 Azure 的类似服务结构差异较大。
- en: 'The global forwarding rule acts as the main entry point for the global load
    balancer:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 全局转发规则充当全局负载均衡器的主要入口点：
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It then references a target HTTP proxy:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它会引用一个目标 HTTP 代理：
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Subsequently, this references a URL map:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，这将引用一个 URL 映射：
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The URL map points to a backend service, which ultimately defines which Google
    Cloud services will be handling the requests:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: URL 映射指向一个后端服务，最终定义哪些 Google Cloud 服务将处理请求：
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding code, you can see that we are connecting the backend to both
    a health check and the instance group that contains the VMs that will ultimately
    be handling the incoming requests:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你可以看到我们正在将后端连接到健康检查和包含虚拟机的实例组，最终这些虚拟机会处理传入的请求：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The health check provides the configuration for the platform to determine if
    the backend service is healthy or not, with requests being sent to the health
    check endpoint on the corresponding backend service to determine if it is healthy
    enough to receive incoming traffic.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 健康检查为平台提供配置，以判断后端服务是否健康，请求会发送到相应后端服务的健康检查端点，以确定该服务是否足够健康以接收传入流量。
- en: Network security
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: 'First, we need to set up the logical firewall for each application architectural
    component. We’ll have one for the frontend and one for the backend:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为每个应用程序架构组件设置逻辑防火墙。我们将为前端和后端分别设置一个：
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Google Cloud often has specific well-known IP addresses that need to be included
    in your firewall rules for them to grant the necessary permissions to communicate
    between services.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 通常具有一些特定的知名 IP 地址，必须将这些地址包括在你的防火墙规则中，以便它们授予服务之间必要的通信权限。
- en: Secrets management
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密钥管理
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we set up secrets using
    AWS Secrets Manager and in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), we
    did the same with Key Vault on Microsoft Azure. As you might remember from [*Chapter
    8*](B21183_08.xhtml#_idTextAnchor402), Azure Key Vault is provisioned within a
    region. It’s within this context that secrets can be created. Google Cloud’s Secret
    Manager service works similarly to AWS in that there is no logical endpoint that
    needs to be provisioned where secrets are scoped within. The following code shows
    how to provision a secret within Google Cloud Secret Manager:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 7 章*](B21183_07.xhtml#_idTextAnchor365)中，我们使用 AWS Secrets Manager 设置了密钥，在[*第
    8 章*](B21183_08.xhtml#_idTextAnchor402)中，我们在 Microsoft Azure 的 Key Vault 中也做了类似的设置。正如你可能还记得的，在[*第
    8 章*](B21183_08.xhtml#_idTextAnchor402)中，Azure Key Vault 是在一个区域内配置的。密钥的创建是在这个上下文中进行的。Google
    Cloud 的 Secret Manager 服务与 AWS 相似，因为不需要配置逻辑端点来划定密钥的范围。以下代码展示了如何在 Google Cloud
    Secret Manager 中配置密钥：
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is a logical container for a secret that may have many different values
    over its life cycle as a result of regular secret rotation. The following code
    shows how we can define a specific version of the secret:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于存储密钥的逻辑容器，由于定期的密钥轮换，它的生命周期内可能有许多不同的值。以下代码展示了如何定义密钥的特定版本：
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This might be a value that we pull in from other Google Cloud resources that
    we provision. The following code grants a service account access to our secrets
    within Google Cloud Secret Manager:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个我们从其他 Google Cloud 资源中提取的值。以下代码授予服务帐户访问我们在 Google Cloud Secret Manager
    中的密钥：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: VMs
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机
- en: When provisioning static VMs, we have much more control over the configuration
    of each machine. Some VMs have specific network and storage configurations to
    meet workload demands.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置静态虚拟机时，我们可以更好地控制每台机器的配置。一些虚拟机具有特定的网络和存储配置，以满足工作负载的需求。
- en: 'First, we’ll obtain the VM image from our input variables. This is the VM image
    that we built with Packer and provisioned into a different Google Cloud project:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从输入变量中获取虚拟机镜像。这是我们使用 Packer 构建并部署到另一个 Google Cloud 项目的虚拟机镜像：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we’ll create a VM using the Google Cloud instance. This resource will
    contain the network interface, disks, and service account configuration to set
    up our VM and connect it to the right subnetwork in our virtual network:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在Google Cloud实例中创建虚拟机。该资源将包含网络接口、磁盘和服务帐户配置，以设置我们的虚拟机并将其连接到虚拟网络中的正确子网：
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we’ll create the network interface for each VM by iterating over the
    `var.az_count` input variable:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过遍历`var.az_count`输入变量来为每个虚拟机创建网络接口：
- en: '[PRE19]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'At this point, we can set up instance groups for each zone:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以为每个区域设置实例组：
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Finally, we’ll set up the VM with all the necessary attributes before linking
    it to the network interface, the VM image, and the managed identity.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将设置虚拟机的所有必要属性，然后将其与网络接口、虚拟机镜像和托管身份关联。
- en: With that, we have implemented the Packer and Terraform solutions and have a
    working code base that will build VM images for both our frontend and backend
    application components and provision our cloud environment into Google Cloud.
    In the next section, we’ll dive into the YAML and Bash and implement the necessary
    GitHub Actions workflows.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些步骤，我们已经实现了Packer和Terraform解决方案，并且拥有一个可以为前端和后端应用程序组件构建虚拟机镜像的工作代码库，同时将我们的云环境部署到Google
    Cloud中。在下一节中，我们将深入探讨YAML和Bash，并实现所需的GitHub Actions工作流。
- en: Automating the deployment
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化部署
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base of application code and operating
    system configuration encapsulated within a Packer template. These two application
    components are then deployed into a cloud environment on Azure that is defined
    within our Terraform code base.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中所讨论的，我们的解决方案由两个应用程序组件组成：前端和后端。每个组件都有一个封装在Packer模板中的应用程序代码和操作系统配置。这两个应用程序组件随后被部署到我们Terraform代码库中定义的Azure云环境中。
- en: 'Just as we did in *Chapters 7* and *8* with the AWS and Azure solutions, there
    is an additional code base that we have to discuss: our automation pipelines on
    GitHub Actions.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在*第7章*和*第8章*中讨论的AWS和Azure解决方案一样，还有一个额外的代码库需要讨论：我们在GitHub Actions上的自动化流水线。
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went over the folder
    structure for our code base and where our GitHub Actions fit in so that we know
    that our automation pipelines are called workflows, and they’re stored in `/.github/workflows`.
    Each of our code bases is stored in its respective folder. Our solutions source
    code repository’s folder structure will look like this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，我们讨论了代码库的文件夹结构以及我们的GitHub Actions如何适应其中，从而了解到我们的自动化流水线被称为工作流，它们存储在`/.github/workflows`中。我们的每个代码库都存储在各自的文件夹中。我们解决方案的源代码仓库的文件夹结构如下所示：
- en: '`.``github`'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.``github`'
- en: '`workflows`'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workflows`'
- en: '`dotnet`'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dotnet`'
- en: '`backend`'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend`'
- en: '`frontend`'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frontend`'
- en: '`packer`'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packer`'
- en: '`backend`'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`backend`'
- en: '`frontend`'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frontend`'
- en: '`terraform`'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terraform`'
- en: 'As per our design, we will have GitHub Actions workflows that will execute
    Packer and build VM images for both the frontend (for example, `packer-frontend.yaml`)
    and the backend (for example, `packer-backend.yaml`). We’ll also have workflows
    that will run `terraform plan` and `terraform apply`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，我们将拥有GitHub Actions工作流，执行Packer并为前端（例如，`packer-frontend.yaml`）和后端（例如，`packer-backend.yaml`）构建虚拟机镜像。我们还将拥有执行`terraform
    plan`和`terraform apply`的工作流：
- en: '`.``github`'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.``github`'
- en: '`workflows`'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workflows`'
- en: '`packer-backend.yaml`'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packer-backend.yaml`'
- en: '`packer-frontend.yaml`'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`packer-frontend.yaml`'
- en: '`terraform-apply.yaml`'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terraform-apply.yaml`'
- en: '`terraform-plan.yaml`'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terraform-plan.yaml`'
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we covered the GitFlow process
    and how it interacts with our GitHub Actions workflows in greater detail. So,
    for now, let’s dig into how these pipelines will differ when targeting the Azure
    platform.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，我们更详细地讨论了GitFlow流程及其如何与我们的GitHub
    Actions工作流交互。因此，现在让我们深入了解这些流水线在面向Azure平台时将如何不同。
- en: Packer
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Packer
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we covered each step of
    the GitHub Actions workflow that executes Packer to build VM images. Thanks to
    the nature of Packer’s cloud-agnostic architecture, this overwhelmingly stays
    the same. The only thing that changes is the final step where we execute Packer.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中，我们详细介绍了执行Packer以构建虚拟机镜像的GitHub Actions工作流的每一步。由于Packer具有云平台无关的架构，这部分基本保持不变。唯一的变化是在最后一步，我们执行Packer。
- en: Because Packer needs to be configured to build a VM on Google Cloud, we need
    to pass in different input variables that are Google Cloud-specific. This includes
    the file path to the Google Cloud credential file, a Google Cloud region, and
    a Google Cloud project ID.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Packer 需要配置在 Google Cloud 上构建虚拟机，所以我们需要传入一些 Google Cloud 特有的输入变量。这些变量包括 Google
    Cloud 凭证文件的文件路径、Google Cloud 区域以及 Google Cloud 项目 ID。
- en: Just as we did with the input variables for the Packer template for AWS, we
    must ensure that all Google Cloud input variables are prefixed with `gcp_`. This
    will help if we ever want to introduce multi-targeting as many cloud platforms
    will have similar required inputs, such as target region and VM size. While most
    clouds will have similar required inputs, the input values are not interchangeable.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在为 AWS 配置 Packer 模板时使用输入变量一样，我们必须确保所有 Google Cloud 输入变量都以 `gcp_` 为前缀。如果我们将来想要实现多目标支持，这将非常有帮助，因为许多云平台需要相似的输入，例如目标区域和虚拟机大小。虽然大多数云平台需要类似的输入，但这些输入值是不能互换的。
- en: For example, each cloud platform will require you to specify the region that
    you want Packer to provide the temporary VM into and the resulting VM image to
    be stored. On Google Cloud, the region has a value of `us-west2-a`, as we saw
    with Azure and AWS, and each cloud platform will have infuriatingly similar and
    slightly different region names.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，每个云平台都要求你指定一个区域，以便 Packer 将临时虚拟机提供到该区域，并将生成的虚拟机镜像存储到该区域。在 Google Cloud 上，区域的值为`us-west2-a`，正如我们在
    Azure 和 AWS 中看到的那样，每个云平台都会有令人恼火的相似且略有不同的区域名称。
- en: 'Google Cloud does have a major difference in the way credentials are usually
    specified. Whereas AWS and Azure usually have particular environment variables
    that will house context and credentials, Google Cloud uses a file. As a result,
    before we run Packer, we need to ensure that the Google Cloud secret file has
    been dropped at a well-known location so that our Packer action can pick it up:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 在凭证指定方式上有一个重要的区别。与 AWS 和 Azure 通常使用特定的环境变量来存储上下文和凭证不同，Google Cloud
    使用一个文件。因此，在运行 Packer 之前，我们需要确保 Google Cloud 密钥文件已经被放置在一个已知的位置，以便我们的 Packer 操作可以找到它：
- en: '[PRE21]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The GitHub Actions workflow YAML files are identical for Google Cloud, except
    for the use of a single input variable that is needed to specify the path to the
    credential file – that is, `gcp.json`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Actions 工作流 YAML 文件对于 Google Cloud 来说是相同的，唯一的区别是需要使用一个输入变量来指定凭证文件的路径——即
    `gcp.json`：
- en: '[PRE22]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The preceding code references the secret file we created from the GitHub Actions
    secret. The Google Cloud plugin for Packer will use the `GOOGLE_APPLICATION_CREDENTIALS`
    environment variable to load the secret file so that it can authenticate with
    Google Cloud.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码引用了我们从 GitHub Actions 秘密中创建的密钥文件。Packer 的 Google Cloud 插件将使用 `GOOGLE_APPLICATION_CREDENTIALS`
    环境变量加载密钥文件，以便进行 Google Cloud 认证。
- en: Terraform
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Terraform
- en: With both of our VM images built and their versions input into our `.tfvars`
    file, our Terraform automation pipeline is ready to take the reigns and not only
    provision our environment but deploy our solution as well (although not technically).
    The deployment was technically done within the `packer build` process, with the
    physical deployment packages being copied to the home directory and the Linux
    service setup primed and ready. Terraform is finishing the job by actually launching
    VMs using these images.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的虚拟机镜像已经构建完成，并且它们的版本已经输入到`.tfvars`文件中，Terraform 自动化管道已经准备好接管，不仅可以配置我们的环境，还可以部署我们的解决方案（尽管严格来说，部署是在`packer
    build`过程中完成的）。部署实际上是在`packer build`过程中完成的，物理部署包被复制到主目录，Linux服务已设置并准备就绪。Terraform
    通过实际启动虚拟机并使用这些镜像来完成剩余的工作。
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we covered each step of
    the GitHub Actions workflow that executes Terraform to provision the cloud environment
    and deploy the application code. Thanks to the nature of Terraform’s cloud-agnostic
    architecture, this overwhelmingly stays the same. The only thing that changes
    is the final step where we execute Terraform.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 7 章*](B21183_07.xhtml#_idTextAnchor365)中，我们介绍了执行 Terraform 来配置云环境并部署应用代码的
    GitHub Actions 工作流的每一步。由于 Terraform 的云无关架构的特性，这部分几乎保持不变。唯一不同的是最终一步，我们执行 Terraform。
- en: 'Just like we did in *Chapters 7* and *8* with the AWS and Azure providers,
    we need to set the authentication context using environment variables that are
    specific to the `google` provider. In this case, the single `GOOGLE_APPLICATION_CREDENTIALS`
    attribute is passed to connect the provider with how it should authenticate with
    Terraform to provision the environment:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding code generates the necessary secret file for Terraform to authenticate
    with Google Cloud to provision the environment.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did in *Chapters 7* and *8* with the AWS and Azure providers,
    we need to configure the Google-Cloud-specific backend that stores the Terraform
    state by using the `-backend-config` command-line arguments alongside the `terraform
    init` command. The additional `GOOGLE_BACKEND_CREDENTIALS` argument informs Terraform
    how to authenticate with the Google Cloud Storage backend that we are using to
    store the Terraform state:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code generates the necessary secret file for Terraform to authenticate
    with Google Cloud so that it can store and retrieve the Terraform state for the
    environment.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike with the AWS and Azure providers – and highlighting how significantly
    the Terraform backend implementations can diverge – the backend uses a *prefix*
    and the Terraform workspace name to uniquely identify the location to store state
    files:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Notice how, like with the Azure solution, we don’t need to perform a targeted
    `terraform apply` command. This is because we don’t need to do dynamic calculations
    based on the number of Availability Zones in the region to configure our virtual
    network.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: These subtle architectural differences between the cloud platforms can create
    radical structural changes, even when we’re deploying the same solution using
    the same technologies. It is a sobering reminder that while knowledge of the core
    concepts we looked at in *Chapters 4* through *6* will help us transcend to a
    multi-cloud point of view, to implement practical solutions, we need to understand
    the subtle nuances of each platform.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a multi-tier cloud architecture using VMs powered
    by Google Cloud Compute Engine with a fully operation GitFlow process and an end-to-end
    CI/CD pipeline using GitHub Actions.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, our fearless leader at Söze Enterprises will be throwing
    us into turmoil with some big new ideas, and we’ll have to respond to his call
    to action. It turns out our CEO, Keyser, has been up late watching some YouTube
    videos about the next big thing – containers – and after talking with his pal
    Sundar on his superyacht, he has decided that we need to refactor our whole solution
    so that it can run on Docker and Kubernetes. Luckily, the good people at Google
    have a service that might help us out: **Google Kubernetes** **Engine** (**GKE**).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
