- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started on Google Cloud – Building Solutions with GCE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve made it. After the previous six chapters, where we used two different
    cloud platforms and three different cloud computing paradigms to build six distinct
    solutions, we are finally ready to take our final journey by adapting our solution
    to **Google Cloud** **Platform** (**GCP**).
  prefs: []
  type: TYPE_NORMAL
- en: Like the last two adventures, in this alternate universe, we will be starting
    our journey by building our solution with a **virtual machine** (**VM**) architecture
    on Google Cloud. As we saw when we transitioned between AWS and Azure when we
    compared how the same solution architecture was built on the two different cloud
    platforms, some things changed a lot, while many things changed only a tiny bit
    – or not at all. We observed that our Terraform code changed pretty consistently
    across all chapters. However, other things, such as Packer, Docker, and GitHub
    Actions workflows, only changed slightly. Our .NET-based application code didn’t
    change at all, whether being hosted in VMs or containers, but when we got to serverless,
    the application code went through radical refactoring.
  prefs: []
  type: TYPE_NORMAL
- en: The same is true as we move the solution to GCP. As a result, we won’t be revisiting
    these topics at the same length in this chapter. However, I would encourage you
    to bookmark *Chapters 7* and *8* and refer to them frequently. This chapter will
    only focus on the changes we must make to deploy our solution on GCP.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our team at Söze Enterprises applauds their achievement of responding to the
    whimsical technical course correction of their fearless leader, Keyser Söze, and
    marvels at their success and fortune in launching their product successfully on
    Microsoft Azure. They utilized VMs, Kubernetes, and, finally, serverless technology.
    The comforting deep blue of the Azure portal begins to fade away when suddenly,
    the air fills with an eerie yet familiar sound: doodle-oo doodle-oo doodle-oo.
    The familiar duo appears – sitting in a cozy wood-paneled basement, a look typical
    of suburban basements from the late ''80s and early ''90s. The walls are adorned
    with posters and memorabilia, including a prominent Chicago Bears pennant, highlighting
    Wayne’s love for rock music and sports. They start the familiar chant: doodle-oo
    doodle-oo doodle-oo. Suddenly, we’re transported to another world – another universe,
    perhaps, where Google Cloud’s sleek multicolored logo replaces Azure’s deep blue.
    Söze Enterprises has now partnered with Google Cloud for their next-generation
    autonomous vehicle platform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we inherited a team from one of Söze Enterprises’ other divisions
    with a strong core team of C# .NET developers, so we’ll build version 1.0 of the
    platform using .NET technologies. The elusive CEO, Keyser, was rumored to have
    joined Google co-founder Sergey Brin aboard his super yacht, the Dragonfly, off
    the Amalfi Coast, and word has come down from corporate that we will be using
    Google Cloud to host the platform. Since the team doesn’t have much experience
    with containers and timelines are tight, we’ve decided to build a simple three-tier
    architecture and host on Azure VMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Logical architecture for the autonomous vehicle platform](img/B21183_13_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Logical architecture for the autonomous vehicle platform
  prefs: []
  type: TYPE_NORMAL
- en: The platform will need a frontend, which will be a web UI built using ASP.NET
    Core Blazor. The frontend will be powered by a REST API backend, which will be
    built using ASP.NET Core Web API. Having our core functionality encapsulated into
    a REST API will allow autonomous vehicles to communicate directly with the platform
    and allow us to expand by adding client interfaces with additional frontend technologies
    such as native mobile apps and virtual or mixed reality in the future. The backend
    will use a PostgreSQL database for persistent storage since it’s lightweight,
    industry-standard, and relatively inexpensive.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the tight timelines the team is facing, we want to keep the cloud architecture
    simple. Therefore, we’ll be keeping it simple and using Google Cloud services
    that will allow us to provision using familiar VM technologies as opposed to trying
    to learn something new. The first decision we have to make is what Google Cloud
    service each component of our logical architecture will be hosted on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application architecture consists of three components: a frontend, a backend,
    and a database. The frontend and backend are application components and need to
    be hosted on a cloud service that provides general computing, while the database
    needs to be hosted on a cloud database service. There are many options for both
    types of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Logical architecture for the autonomous vehicle platform and
    the hosts](img/B21183_13_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Logical architecture for the autonomous vehicle platform and the
    hosts
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have decided we’re going to use VMs to host our application, we have
    narrowed down the different services that we can use to host our application.
    We have decided **Google Compute Engine** (**GCE**) is the ideal choice for our
    current situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Source control structure of our repository](img/B21183_13_3..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Source control structure of our repository
  prefs: []
  type: TYPE_NORMAL
- en: This solution will have six parts. We still have the application code and Packer
    templates for both the frontend and backend. Then, we have GitHub Actions to implement
    our CI/CD process and Terraform to provision our Google Cloud infrastructure and
    reference the Packer-built VM images for our GCE instances.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first part of our design is adapting our solution’s architecture to the
    target cloud platform: Google Cloud. This involves mapping application architecture
    components to GCP services and thinking through the configuration of those services
    so that they meet the requirements of our solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Projects and API access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we get started, we need a project within the organization where a service
    account can be created for Terraform to use. This service account needs to be
    granted access to the `roles/resourcemanager.projectCreator` organizational role.
    This will allow you to create projects with Terraform, which will allow you to
    keep a complete solution together and avoid additional boilerplate prerequisites
    that are executed outside of Terraform using the command-line interface.
  prefs: []
  type: TYPE_NORMAL
- en: Once this has been done, you need to enable the **Cloud Resource Manager API**
    within the project where the Terraform service account resides. This API is required
    within the context of the Google Cloud project because of the way Google Cloud
    grants access to different features of the platform at the project level. It creates
    another gate for the Google Cloud identity to be able to access resources on GCP.
  prefs: []
  type: TYPE_NORMAL
- en: Your Terraform service account will also need access to Cloud Storage, which
    you plan on using to store Terraform state. When using the AWS and Azure providers,
    you can use different credentials to access the Terraform backend than you use
    to provision your environment. On Google Cloud, this can be accomplished by setting
    `GOOGLE_BACKEND_CREDENTIALS` with credentials for the identity you wish to use
    to communicate with the Google Cloud Storage bucket and `GOOGLE_APPLICATION_CREDENTIALS`
    with credentials for the identity you wish to use to communicate with Google Cloud
    to provision your environment.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VMs must be deployed within a virtual network. As you may recall from [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), when we provisioned this solution on AWS,
    we needed to set up multiple Subnets for our solution to span Availability Zones.
    In [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), when deploying the solution
    to Azure, we only needed two subnets – one for the frontend and one for the backend.
    That’s because Azure’s virtual network architecture is structured differently
    than AWS’s and subnets on Azure span multiple Availability Zones.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud’s virtual network service is also structured differently. Unlike
    both AWS and Azure, which have virtual networks scoped to a particular region,
    virtual networks on GCP span multiple regions by default. Subnets are scoped to
    the region, which means, like Azure, a subnet on GCP can host VMs from multiple
    Availability Zones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows that the Google compute network is not tied to
    the region like it is on AWS and Azure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Google Cloud network architecture](img/B21183_13_4..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – Google Cloud network architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this seems like a significant difference at the root of the deployment
    hierarchy, it doesn’t materially impact the design as the subnets (or *subnetworks*)
    are still tied to a region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Isolated subnets for the frontend and backend application components](img/B21183_13_5..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – Isolated subnets for the frontend and backend application components
  prefs: []
  type: TYPE_NORMAL
- en: When building a single-region solution, the multi-region capability of Google
    Cloud might seem like overkill. However, automatic spanning does simplify infrastructure
    management as businesses don’t have to manually set up and maintain inter-regional
    connections. This not only reduces administrative overhead but also allows for
    more agile and scalable deployments in response to changing demands by making
    active-active multi-region deployments easier to build and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Network routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inside Google Cloud networks, the default setup is designed to provide straightforward
    and secure connectivity. As we know, by default, Google Cloud networks are global
    resources, meaning all the subnets (or *subnetworks*) within a single network
    can communicate with each other, regardless of their regional location, without
    the need for explicit routes or VPNs. This inter-subnet communication uses the
    system-generated routes in the network.
  prefs: []
  type: TYPE_NORMAL
- en: For routing configurations, Google Cloud has **routes**, which perform a role
    similar to AWS’s route tables, directing traffic based on IP ranges. For situations
    where instances need to initiate outbound connections to the internet without
    revealing their IP, Google Cloud provides Cloud NAT, which is analogous to AWS’s
    **NAT gateways**.
  prefs: []
  type: TYPE_NORMAL
- en: Like Azure, Google Cloud does not have a direct equivalent named **internet
    gateway**. Instead, internet connectivity in GCP is managed using a combination
    of system-generated routes and firewall rules.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Google Cloud has two options when it comes to load balancers: global and regional.
    Global load balancers distribute traffic across multiple regions, ensuring users
    are served from the nearest or most suitable region, while regional load balancers
    distribute traffic within a single region. The choice between them typically depends
    on the application’s user distribution and the need for low-latency access. However,
    sometimes, other limitations force your hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Google Cloud regional load balancer](img/B21183_13_6..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – Google Cloud regional load balancer
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the regional load balancer’s target pool does not allow you to
    specify a different port for the backend instances. This means the target pool
    will forward traffic to the same port where it received traffic. For instance,
    if the forwarding rule is listening on port `80`, the target pool will send traffic
    to port `80` of the backend instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve your goal of forwarding from port `80` to port `5000`, you would
    need to use the global load balancer instead of the regional load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Google Cloud global load balancer](img/B21183_13_7..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – Google Cloud global load balancer
  prefs: []
  type: TYPE_NORMAL
- en: The global load balancer requires that you set up instance groups to organize
    the VMs that the load will be distributed across. Google Cloud instance groups
    are similar to AWS Auto Scaling groups and Azure’s **Virtual Machine Scale Sets**
    (**VMSS**), but they have a bit more flexibility in that you can either provide
    a VM template and allow GCP to *manage* the instances or you can provision the
    instances explicitly and add them later to the instance group. This dual-mode
    capability is similar to Azure’s VMSS rather than AWS’s Auto Scaling group, which
    can only operate in a *managed* mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw when comparing AWS and Azure, all the anatomical parts of a load
    balancer are present and accounted for – they just might go by different names
    and connect in slightly different ways. The following table extends the mapping
    that we did between AWS and Azure and includes the GCP equivalents:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS** | **Azure** | **GCP** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| **Application Load** **Balancer** (**ALB**) | Azure Load Balancer | URL map
    | Load balancer |'
  prefs: []
  type: TYPE_TB
- en: '| Listener | Frontend IP configuration | Global forwarding rule | The singular
    endpoint that accepts incoming traffic on a load balancer |'
  prefs: []
  type: TYPE_TB
- en: '| Target Group | Backend Address pool | Backend Service | A collection of VMs
    that incoming traffic is forwarded to |'
  prefs: []
  type: TYPE_TB
- en: '| Health check | Health probe | Health check | An endpoint published by each
    of the backend VMs that indicates it is healthy and ready to handle traffic |'
  prefs: []
  type: TYPE_TB
- en: Table 13.1 – Mapping of synonymous load balancer components between AWS, Azure,
    and GCP
  prefs: []
  type: TYPE_NORMAL
- en: The URL map and the Target HTTP proxy compose the global load balancer, which
    attaches to the forwarding rule, which acts as the singular endpoint, and the
    backend service, which represents the collection of VMs to distribute load across.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To control network traffic, Google Cloud offers firewall rules that allow users
    to specify which packets are allowed into and out of instances. While Google Cloud’s
    firewall rules share some similarities with AWS’s **network access control lists**
    (**NACLs**), it’s crucial to note that GCP firewall rules are stateful, while
    AWS NACLs are stateless.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Secrets such as database credentials or service access keys need to be stored
    securely. Each cloud platform has a service that provides this functionality.
    On GCP, this service is called Google Cloud Secret Manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we will see slight naming convention differences but all the anatomical
    parts are there. The following table extends the mapping that we did between AWS
    and Azure and includes the GCP equivalents:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS** | **Azure** | **GCP** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| IAM | Microsoft Entra | Cloud Identity | Identity provider |'
  prefs: []
  type: TYPE_TB
- en: '| Secrets Manager | Key Vault | Secret Manager | Secure secret storage |'
  prefs: []
  type: TYPE_TB
- en: '| IAM role | User-assigned managed identity | Service account | Identity for
    machine-to-machine interaction |'
  prefs: []
  type: TYPE_TB
- en: '| IAM policy | **Role-based access** **control** (**RBAC**) | IAM member |
    Permission to perform specific operations on specific services or resources |'
  prefs: []
  type: TYPE_TB
- en: '| IAM role policy | Role assignment | IAM member | Associates specific permissions
    to specific identities |'
  prefs: []
  type: TYPE_TB
- en: Table 13.2 – Mapping of synonymous IAM components between AWS, Azure, and GCP
  prefs: []
  type: TYPE_NORMAL
- en: 'Secrets stored in Google Cloud Secret Manager can be accessed by VMs once they
    have the necessary access granted. In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365),
    we used an AWS IAM role assignment to allow a VM to do this, and with Azure, we
    used user-assigned managed identities and ole assignments. On GCP, we need to
    use a service account and grant it permissions to the specific secrets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Key Vault architecture](img/B21183_13_8..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Key Vault architecture
  prefs: []
  type: TYPE_NORMAL
- en: Granting the managed identity that is attached to the VMs access to the **Key
    Vault Secrets User** role will allow the VMs to read the secret values from Key
    Vault. This does not put the secrets on the machine. The VM will need to use the
    Azure CLI to access the Key Vault secrets.
  prefs: []
  type: TYPE_NORMAL
- en: VMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have everything we need for our solution, we can finish by talking
    about where our application components will run: VMs provisioned on Google Cloud’s
    Compute Engine service. When provisioning VMs on GCP, you have two options. First,
    you can provide static VMs. In this approach, you need to specify key characteristics
    for every VM. You can organize these VMs into an instance group to better manage
    the health and life cycle of the VMs. The second option is to provision an instance
    group manager. This will allow you to dynamically scale up and down based on demand,
    as well as auto-heal VMs that fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Google Cloud Compute Engine instance architecture](img/B21183_13_9..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – Google Cloud Compute Engine instance architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Azure, Google Cloud separates the concept of grouping VMs that are
    tied together through the application life cycle from the management of their
    health and dynamic provisioning. In Azure, an availability set is a logical group
    that can be used to place individual VMs into a relationship so that their relationship
    is taken into consideration by the underlying platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Instance group manager architecture](img/B21183_13_10..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – Instance group manager architecture
  prefs: []
  type: TYPE_NORMAL
- en: On Google Cloud, that is an instance group. Both allow you to easily attach
    a pool of VMs to other services relevant to multiple VMs working a problem together,
    such as load balancers and health monitoring. To add dynamic provisioning and
    management, on Azure, you would need a VMSS. On Google Cloud, this is called an
    instance group manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, just as we saw previously, the names have been changed to protect the
    innocent, but make no mistake, they work the same way. The following table extends
    the mapping that we did between AWS and Azure and includes the GCP equivalents:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS** | **Azure** | **GCP** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| EC2 | VMs | Compute instance | VM service |'
  prefs: []
  type: TYPE_TB
- en: '| AMI | VM image | Google compute image | VM image either from the marketplace
    or a custom build (for example, using tools such s Packer) |'
  prefs: []
  type: TYPE_TB
- en: '| IAM role | User-assigned managed identity | Service account | Identity for
    machine-to-machine interaction |'
  prefs: []
  type: TYPE_TB
- en: '| Auto Scaling group | VMSS | Instance group manager | Set of dynamically provisioned
    VMs that can be scaled up/down using a VM configuration template |'
  prefs: []
  type: TYPE_TB
- en: '| Launch template | VM profile | Instance template | Configuration template
    used to create new VMs |'
  prefs: []
  type: TYPE_TB
- en: Table 13.3 – Mapping of synonymous VM service components between AWS, Azure,
    and GCP
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we provisioned our solution
    using AWS **Elastic Cloud Compute** (**EC2**) service and in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402),
    we did the same but with the Azure VM service. Like both of these platforms, on
    GCP, VMs are connected to virtual networks using network interfaces. Unlike AWS
    and Azure, these network interfaces cannot be provisioned independently of the
    VM and are then attached later.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed the subtle differences between how Azure and AWS handle network
    security, with AWS having low-level network security handled by NACLs that attach
    at the subnet and more logical security groups that attach at the instance and
    process network traffic in a stateful manner. Azure has similar constructs with
    network security groups, which focus more on network traffic between physical
    endpoints (IP address ranges and network gateways), and application security groups,
    which focus on network traffic between logical application endpoints. Google Cloud
    combines the two into Google Compute firewall resources that can control network
    traffic using physical network characteristics such as IP address ranges and logical
    constructs such as service accounts and tags.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern of using tags to attach behavior or grant permissions is a common
    pattern on GCP and you should make note of it as other platforms do not regard
    tags as a method for establishing security boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on Google Cloud, we need to come up with a plan on how to
    provision our environments and deploy our code.
  prefs: []
  type: TYPE_NORMAL
- en: VM configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our solution, we have two VM roles: the frontend role, which is responsible
    for handling web page requests from the end user’s web browser, and the backend
    role, which is responsible for handling REST API requests from the web application.
    Each of these roles has a different code and different configuration that needs
    to be set. Each will require its own Packer template to build a VM image that
    we can use to launch a VM on Google Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Packer pipeline to build a VM image for the frontend](img/B21183_13_11..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Packer pipeline to build a VM image for the frontend
  prefs: []
  type: TYPE_NORMAL
- en: A GitHub Actions workflow that triggers off changes to the frontend application
    code and the frontend Packer template will execute `packer build` and create a
    new VM image for the solution’s frontend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the frontend and the backend will have identical GitHub workflows that
    execute `packer build`. The key difference between the workflows is the code bases
    that they execute against. Both the frontend and the backend might have slightly
    different operating system configurations, and both require different deployment
    packages for their respective application components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Packer pipeline to build a VM image for the backend](img/B21183_13_12..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – Packer pipeline to build a VM image for the backend
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the application code will be baked into the VM image
    rather than copied to an already running VM. This means that to update the software
    running on the VMs, each VM will need to be restarted so that it can be restarted
    with a new VM image containing the latest copy of the code.
  prefs: []
  type: TYPE_NORMAL
- en: This approach makes the VM image an immutable deployment artifact that is versioned
    and updated each time there is a release of the application code that needs to
    be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud environment configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the VM images have been built for both the frontend and the backend, we
    can execute the final workflow that will both provision and deploy our solution
    to Google Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.13 – VM images as inputs to the Terraform code, which provisions
    the environment on Google Cloud](img/B21183_13_13..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – VM images as inputs to the Terraform code, which provisions the
    environment on Google Cloud
  prefs: []
  type: TYPE_NORMAL
- en: The Terraform code base will have two input variables for the version of the
    VM image for both the frontend and the backend. When new versions of the application
    software need to be deployed, the input parameters for these versions will be
    incremented to reflect the target version for deployment. When the workflow is
    executed, `terraform apply` will simply replace the existing VMs with VMs using
    the new VM image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a solid plan for how we will implement both the cloud architecture
    using Google Cloud and the deployment architecture using GitHub Actions, let’s
    start building! In the next section, we’ll break down the **HashiCorp Configuration
    Language** (**HCL**) code that we’ll use to implement the Terraform and Packer
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a solid design for our solution, we can begin building it.
    As discussed in the previous section, we’ll be using VMs powered by Google Cloud
    Compute Engine. As we did with AWS and Azure in *Chapters 7* and *10*, respectively,
    we’ll need to package our application into VM images using Packer and then provision
    an environment that provisions an environment using these VM images.
  prefs: []
  type: TYPE_NORMAL
- en: Packer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll cover how to implement our Packer templates provisioners
    so that we can install our .NET application code on a Linux VM. If you skipped
    *Chapters 7* through *9* due to a lack of interest in (AWS, I can’t hold that
    against you – particularly if your primary interest in reading this book is working
    on GCP. However, I would encourage you to review the corresponding section within
    [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) to see how we use Packer’s provisioners
    to configure a Debian-based Linux VM with our .NET application code.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), Packer
    – like Terraform – is an extensible command-line executable. Each cloud platform
    provides a plugin for Packer that encapsulates the integration with its services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In *Chapters 7* and *10*, we saw how to declare the Packer plugin for AWS and
    Azure (respectively) as a required plugin. The preceding code demonstrates how
    to declare Google Cloud’s plugin – at the time of writing, the latest version
    is 1.1.2.
  prefs: []
  type: TYPE_NORMAL
- en: The Google Cloud plugin for Packer provides a `googlecompute` builder that will
    generate Google Cloud compute images by creating a new VM from a base image, executing
    the provisioners, taking a snapshot of the Google Cloud instance’s boot disk,
    and creating a Google Cloud compute image from it. Like the AWS and Azure plugins,
    this behavior is encapsulated within Google Cloud’s builder.
  prefs: []
  type: TYPE_NORMAL
- en: Just as the other plugins encapsulated the logic to build VMs on their respective
    platforms, its configuration was oriented using terminology specific to each platform.
    Packer does not try to create a standard builder interface across cloud platforms
    – rather, it isolates the cloud-specific configuration within the builders. This
    keeps things simple for users who know the target platform well and allows the
    builder to take advantage of any platform-specific features without additional
    layers of complexity by trying to rationalize the syntax across every platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, the structure of the AWS, Azure, and Google Cloud builders is
    radically different in almost every way – from how they authenticate to how they
    look up marketplace images. There are some common fields and similarities, but
    they are very different animals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code shows how we reference the Google Cloud marketplace version
    of the Ubuntu 22.04 VM. Notice how, unlike the other providers, which have rather
    complex lookup mechanisms, Google Cloud simply has a single string to represent
    the desired image. Each approach produces the same outcome: we select a marketplace
    image hosted by the cloud platform to use as our boot disk, but we see different
    organizational philosophies manifesting in the three different clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: Operating system configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We must configure the operating system so that it installs software dependencies
    (such as .NET 6.0), copies and deploys our application code’s deployment package
    to the correct location in the local filesystem, configures a Linux service that
    runs on boot, and sets a local user and group with necessary access for the service
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: I expanded on these steps in detail in the corresponding section in [*Chapter
    7*](B21183_07.xhtml#_idTextAnchor365), so I encourage you to review this section
    if you want to refresh your memory.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base of application code that needs
    to be deployed. Since this is the first time we will be using the `google` provider,
    we’ll look at basic provider setup and how to configure the backend before we
    consider the nuts and bolts of each component of our architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Provider setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we need to specify all the providers that we intend to use in this solution
    within the `required_providers` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll also configure the Google Cloud provider. The Google Cloud provider,
    like Azure but unlike AWS, is not scoped to a particular region. The Google Cloud
    provider doesn’t even need to be scoped to a project. In this way, it is extremely
    flexible and can be used to provision cross-project and multi-region resources
    with the same provider declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: One major difference between the Google provider and the AWS and Azure providers
    is how you authenticate. While Azure and AWS have environment variables that specify
    the identity, the Google Cloud provider relies on an authentication file, so this
    will alter how our pipeline tools integrate with Terraform to ensure a Google
    Cloud solution has the right identity. The `GOOGLE_APPLICATION_CREDENTIALS` environment
    variable specifies the path to this file. It is important to note that this file
    is a JSON file, but it contains secret information; therefore, it should be treated
    as a credential and protected as such.
  prefs: []
  type: TYPE_NORMAL
- en: Backend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because we will be using a CI/CD pipeline to provision and maintain our environment
    in the long term, we need to set up a remote backend for our Terraform state.
    Because our solution will be hosted on Google Cloud, we’ll use the Google Cloud
    Storage backend to store our Terraform state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the Google Cloud provider, we don’t want to hard code the backend
    configuration in our code, so we’ll simply set up a placeholder for the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We’ll configure the backend’s parameters using the `-backend-config` parameters
    when we run `terraform init` in our CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Input variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s good practice to pass in short names that identify the application’s name
    and the application’s environment. This allows you to embed consistent naming
    conventions across the resources that make up your solution, which makes it easier
    to identify and track resources from the Google Cloud Console.
  prefs: []
  type: TYPE_NORMAL
- en: The `primary_region`, `network_cidr_block`, and `az_count` input variables drive
    key architectural characteristics of the deployment. They mustn’t be hard-coded
    as this will limit the reusability of the Terraform code base.
  prefs: []
  type: TYPE_NORMAL
- en: The `network_cidr_block` input variable establishes the virtual network address
    space, which is often tightly regulated by an enterprise governance body. There
    is usually a process to ensure that teams across an organization do not use IP
    address ranges that conflict, thus making it impossible to allow those two applications
    to integrate in the future or integrate with shared network resources within the
    enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: The `az_count` input variable allows you to configure how much redundancy you
    want within our solution. This will affect the high availability of the solution
    but also the cost of the deployment. As you can imagine, cost is also a tightly
    regulated characteristic of cloud infrastructure deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Consistent naming and tagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike the AWS console, and very similar to Azure, Google Cloud is designed
    in such a way that it is extremely easy to get an application-centric view of
    your deployment through projects. Therefore, it’s not as important as an organizational
    strategy for your application to specify tags. By default, you will have a project-centric
    view of all the resources on Google Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It’s still important to tag the resources that you deploy that indicate what
    application and what environment they belong to. This helps with other reporting
    needs, such as budgets and compliance. Almost all resources within the Google
    Cloud provider have a `map` attribute called `tags`. Like Azure, each resource
    usually has `name` as a required attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just as we did in *Chapters 7* and *8*, we need to construct a virtual network
    and keep its address space as tight as possible to avoid gobbling up unnecessary
    address space for the broader organization in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The network creation in Google Cloud is simpler than what we did with AWS because
    we don’t have to segment our subnets based on Availability Zone. This approach
    resembles how Azure structures subnets to span Availability Zones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in the design, the Google Cloud Load Balancing service is structured
    quite a bit differently than AWS and Azure’s equivalent offerings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The global forwarding rule acts as the main entry point for the global load
    balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It then references a target HTTP proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Subsequently, this references a URL map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The URL map points to a backend service, which ultimately defines which Google
    Cloud services will be handling the requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, you can see that we are connecting the backend to both
    a health check and the instance group that contains the VMs that will ultimately
    be handling the incoming requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The health check provides the configuration for the platform to determine if
    the backend service is healthy or not, with requests being sent to the health
    check endpoint on the corresponding backend service to determine if it is healthy
    enough to receive incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we need to set up the logical firewall for each application architectural
    component. We’ll have one for the frontend and one for the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Google Cloud often has specific well-known IP addresses that need to be included
    in your firewall rules for them to grant the necessary permissions to communicate
    between services.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we set up secrets using
    AWS Secrets Manager and in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), we
    did the same with Key Vault on Microsoft Azure. As you might remember from [*Chapter
    8*](B21183_08.xhtml#_idTextAnchor402), Azure Key Vault is provisioned within a
    region. It’s within this context that secrets can be created. Google Cloud’s Secret
    Manager service works similarly to AWS in that there is no logical endpoint that
    needs to be provisioned where secrets are scoped within. The following code shows
    how to provision a secret within Google Cloud Secret Manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a logical container for a secret that may have many different values
    over its life cycle as a result of regular secret rotation. The following code
    shows how we can define a specific version of the secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This might be a value that we pull in from other Google Cloud resources that
    we provision. The following code grants a service account access to our secrets
    within Google Cloud Secret Manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: VMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When provisioning static VMs, we have much more control over the configuration
    of each machine. Some VMs have specific network and storage configurations to
    meet workload demands.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll obtain the VM image from our input variables. This is the VM image
    that we built with Packer and provisioned into a different Google Cloud project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll create a VM using the Google Cloud instance. This resource will
    contain the network interface, disks, and service account configuration to set
    up our VM and connect it to the right subnetwork in our virtual network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we’ll create the network interface for each VM by iterating over the
    `var.az_count` input variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can set up instance groups for each zone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we’ll set up the VM with all the necessary attributes before linking
    it to the network interface, the VM image, and the managed identity.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have implemented the Packer and Terraform solutions and have a
    working code base that will build VM images for both our frontend and backend
    application components and provision our cloud environment into Google Cloud.
    In the next section, we’ll dive into the YAML and Bash and implement the necessary
    GitHub Actions workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base of application code and operating
    system configuration encapsulated within a Packer template. These two application
    components are then deployed into a cloud environment on Azure that is defined
    within our Terraform code base.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we did in *Chapters 7* and *8* with the AWS and Azure solutions, there
    is an additional code base that we have to discuss: our automation pipelines on
    GitHub Actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we went over the folder
    structure for our code base and where our GitHub Actions fit in so that we know
    that our automation pipelines are called workflows, and they’re stored in `/.github/workflows`.
    Each of our code bases is stored in its respective folder. Our solutions source
    code repository’s folder structure will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.``github`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workflows`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dotnet`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frontend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packer`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frontend`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terraform`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As per our design, we will have GitHub Actions workflows that will execute
    Packer and build VM images for both the frontend (for example, `packer-frontend.yaml`)
    and the backend (for example, `packer-backend.yaml`). We’ll also have workflows
    that will run `terraform plan` and `terraform apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.``github`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workflows`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packer-backend.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`packer-frontend.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terraform-apply.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terraform-plan.yaml`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we covered the GitFlow process
    and how it interacts with our GitHub Actions workflows in greater detail. So,
    for now, let’s dig into how these pipelines will differ when targeting the Azure
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Packer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we covered each step of
    the GitHub Actions workflow that executes Packer to build VM images. Thanks to
    the nature of Packer’s cloud-agnostic architecture, this overwhelmingly stays
    the same. The only thing that changes is the final step where we execute Packer.
  prefs: []
  type: TYPE_NORMAL
- en: Because Packer needs to be configured to build a VM on Google Cloud, we need
    to pass in different input variables that are Google Cloud-specific. This includes
    the file path to the Google Cloud credential file, a Google Cloud region, and
    a Google Cloud project ID.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we did with the input variables for the Packer template for AWS, we
    must ensure that all Google Cloud input variables are prefixed with `gcp_`. This
    will help if we ever want to introduce multi-targeting as many cloud platforms
    will have similar required inputs, such as target region and VM size. While most
    clouds will have similar required inputs, the input values are not interchangeable.
  prefs: []
  type: TYPE_NORMAL
- en: For example, each cloud platform will require you to specify the region that
    you want Packer to provide the temporary VM into and the resulting VM image to
    be stored. On Google Cloud, the region has a value of `us-west2-a`, as we saw
    with Azure and AWS, and each cloud platform will have infuriatingly similar and
    slightly different region names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Cloud does have a major difference in the way credentials are usually
    specified. Whereas AWS and Azure usually have particular environment variables
    that will house context and credentials, Google Cloud uses a file. As a result,
    before we run Packer, we need to ensure that the Google Cloud secret file has
    been dropped at a well-known location so that our Packer action can pick it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The GitHub Actions workflow YAML files are identical for Google Cloud, except
    for the use of a single input variable that is needed to specify the path to the
    credential file – that is, `gcp.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code references the secret file we created from the GitHub Actions
    secret. The Google Cloud plugin for Packer will use the `GOOGLE_APPLICATION_CREDENTIALS`
    environment variable to load the secret file so that it can authenticate with
    Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With both of our VM images built and their versions input into our `.tfvars`
    file, our Terraform automation pipeline is ready to take the reigns and not only
    provision our environment but deploy our solution as well (although not technically).
    The deployment was technically done within the `packer build` process, with the
    physical deployment packages being copied to the home directory and the Linux
    service setup primed and ready. Terraform is finishing the job by actually launching
    VMs using these images.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we covered each step of
    the GitHub Actions workflow that executes Terraform to provision the cloud environment
    and deploy the application code. Thanks to the nature of Terraform’s cloud-agnostic
    architecture, this overwhelmingly stays the same. The only thing that changes
    is the final step where we execute Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did in *Chapters 7* and *8* with the AWS and Azure providers,
    we need to set the authentication context using environment variables that are
    specific to the `google` provider. In this case, the single `GOOGLE_APPLICATION_CREDENTIALS`
    attribute is passed to connect the provider with how it should authenticate with
    Terraform to provision the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code generates the necessary secret file for Terraform to authenticate
    with Google Cloud to provision the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did in *Chapters 7* and *8* with the AWS and Azure providers,
    we need to configure the Google-Cloud-specific backend that stores the Terraform
    state by using the `-backend-config` command-line arguments alongside the `terraform
    init` command. The additional `GOOGLE_BACKEND_CREDENTIALS` argument informs Terraform
    how to authenticate with the Google Cloud Storage backend that we are using to
    store the Terraform state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code generates the necessary secret file for Terraform to authenticate
    with Google Cloud so that it can store and retrieve the Terraform state for the
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike with the AWS and Azure providers – and highlighting how significantly
    the Terraform backend implementations can diverge – the backend uses a *prefix*
    and the Terraform workspace name to uniquely identify the location to store state
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Notice how, like with the Azure solution, we don’t need to perform a targeted
    `terraform apply` command. This is because we don’t need to do dynamic calculations
    based on the number of Availability Zones in the region to configure our virtual
    network.
  prefs: []
  type: TYPE_NORMAL
- en: These subtle architectural differences between the cloud platforms can create
    radical structural changes, even when we’re deploying the same solution using
    the same technologies. It is a sobering reminder that while knowledge of the core
    concepts we looked at in *Chapters 4* through *6* will help us transcend to a
    multi-cloud point of view, to implement practical solutions, we need to understand
    the subtle nuances of each platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a multi-tier cloud architecture using VMs powered
    by Google Cloud Compute Engine with a fully operation GitFlow process and an end-to-end
    CI/CD pipeline using GitHub Actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, our fearless leader at Söze Enterprises will be throwing
    us into turmoil with some big new ideas, and we’ll have to respond to his call
    to action. It turns out our CEO, Keyser, has been up late watching some YouTube
    videos about the next big thing – containers – and after talking with his pal
    Sundar on his superyacht, he has decided that we need to refactor our whole solution
    so that it can run on Docker and Kubernetes. Luckily, the good people at Google
    have a service that might help us out: **Google Kubernetes** **Engine** (**GKE**).'
  prefs: []
  type: TYPE_NORMAL
