<html><head></head><body>
<div id="_idContainer176">
<h1 class="chapter-number" id="_idParaDest-248"><a id="_idTextAnchor605"/><span class="koboSpan" id="kobo.1.1">14</span></h1>
<h1 id="_idParaDest-249"><a id="_idTextAnchor606"/><span class="koboSpan" id="kobo.2.1">Containerize on Google Cloud – Building Solutions with GKE</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we built and automated our solution on Google Cloud by utilizing </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Google Compute Engine</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">GCE</span></strong><span class="koboSpan" id="kobo.7.1">). </span><span class="koboSpan" id="kobo.7.2">We built </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">virtual machine</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">VM</span></strong><span class="koboSpan" id="kobo.11.1">) images with Packer and provisioned our VM using Terraform. </span><span class="koboSpan" id="kobo.11.2">In this chapter, we’ll follow a similar path, but instead of working with VMs, we’ll look at hosting our application in containers within a </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">Kubernetes cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">To achieve this, we’ll</span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.14.1"> need to alter our approach by ditching Packer and replacing it with Docker to create a deployable artifact for our application. </span><span class="koboSpan" id="kobo.14.2">Once again, we’ll be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.15.1">google</span></strong><span class="koboSpan" id="kobo.16.1"> provider for Terraform and revisiting the </span><strong class="source-inline"><span class="koboSpan" id="kobo.17.1">kubernetes</span></strong><span class="koboSpan" id="kobo.18.1"> provider for Terraform that we looked at when we took the same step while on our journey with AWS </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">and Azure.</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">Since the overwhelming majority of this remains the same when we move to Google Cloud, we won’t revisit these topics at the same length in this chapter. </span><span class="koboSpan" id="kobo.20.2">However, I would encourage you to bookmark </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.21.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.22.1"> and refer to </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">it frequently.</span></span></p>
<p><span class="koboSpan" id="kobo.24.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.26.1">Laying </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">the foundation</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Designing </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.30.1">Building </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.32.1">Automating </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">the deployment</span></span><a id="_idTextAnchor607"/></li>
</ul>
<h1 id="_idParaDest-250"><a id="_idTextAnchor608"/><span class="koboSpan" id="kobo.34.1">Laying the foundation</span></h1>
<p><span class="koboSpan" id="kobo.35.1">Our</span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.36.1"> story continues through the lens of Söze Enterprises, founded by the enigmatic Turkish billionaire Keyser Söze. </span><span class="koboSpan" id="kobo.36.2">Our team has been hard at work building the next-generation autonomous vehicle orchestration platform. </span><span class="koboSpan" id="kobo.36.3">Previously, we had hoped to leapfrog the competition by leveraging Google Cloud’s rock-solid platform, leveraging our team’s existing skills, and focusing on feature development. </span><span class="koboSpan" id="kobo.36.4">The team was just getting into their groove when a curveball came out </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">of nowhere.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">Over the</span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.39.1"> weekend, our elusive executive was influenced by a rendezvous with Sundar Pichai, the CEO of Alphabet and Google’s parent company, in Singapore. </span><span class="koboSpan" id="kobo.39.2">Keyser was seen gobbling down street food with Sundar on Satay Street. </span><span class="koboSpan" id="kobo.39.3">During this brief but enjoyable encounter, Sundar extolled the virtues and prowess of Kubernetes and Google’s unique position as the original developers of the open source technology. </span><span class="koboSpan" id="kobo.39.4">Keyser was enchanted by the prospect of more efficient resource utilization, leading to improved cost optimization and faster deployment and rollback times, and he was hooked. </span><span class="koboSpan" id="kobo.39.5">His new autonomous vehicle platform needed to harness the power of the cloud, and container-based architecture was the way to do it. </span><span class="koboSpan" id="kobo.39.6">So, he decided to accelerate his plans to adopt </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">cloud-native architecture!</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">The news of transitioning to a container-based architecture means reevaluating their approach, diving into new technologies, and possibly even reshuffling team dynamics. </span><span class="koboSpan" id="kobo.41.2">For the team, containers were always the long-term plan, but now, things need to be sped up, which will require a significant investment in time, resources, </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">and training.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">As the team scrambles to adjust their plans, they can’t help but feel a mix of excitement and apprehension. </span><span class="koboSpan" id="kobo.43.2">They know that they are part of something groundbreaking under Keyser’s leadership. </span><span class="koboSpan" id="kobo.43.3">His vision for the future of autonomous vehicles is bold and transformative. </span><span class="koboSpan" id="kobo.43.4">And while his methods may be unconventional, they have learned that his instincts are often correct. </span><span class="koboSpan" id="kobo.43.5">In this chapter, we’ll explore this transformation from VMs to containers using </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">Google Clou</span><a id="_idTextAnchor609"/><span class="koboSpan" id="kobo.45.1">d.</span></span></p>
<h1 id="_idParaDest-251"><a id="_idTextAnchor610"/><span class="koboSpan" id="kobo.46.1">Designing the solution</span></h1>
<p><span class="koboSpan" id="kobo.47.1">As we saw in </span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.48.1">the previous chapter, where we built our solution using VMs on Google Cloud, we had full control over the operating system configuration through the VM images we provisioned with Packer. </span><span class="koboSpan" id="kobo.48.2">Just as we did when we went through the same process on our journey with AWS and Azure in </span><em class="italic"><span class="koboSpan" id="kobo.49.1">Chapters 8</span></em><span class="koboSpan" id="kobo.50.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.51.1">11</span></em><span class="koboSpan" id="kobo.52.1">, we’ll need to introduce a new tool to replace VM images with container images – </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.53.1">Docker</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<span class="koboSpan" id="kobo.55.1"><img alt="Figure 14.1 – Logical architecture for the autonomous vehicle platform" src="image/B21183_14_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.56.1">Figure 14.1 – Logical architecture for the autonomous vehicle platform</span></p>
<p><span class="koboSpan" id="kobo.57.1">Our application</span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.58.1"> architecture, comprising a frontend, a backend, and a database, will remain the same, but we will need to provision different resources with Terraform and harness new tools from Docker and Kubernetes to automate the deployment of our solution to this </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">new infrastructure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer172">
<span class="koboSpan" id="kobo.60.1"><img alt="Figure 14.2 – Source control structure of our repository" src="image/B21183_14_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.61.1">Figure 14.2 – Source control structure of our repository</span></p>
<p><span class="koboSpan" id="kobo.62.1">This solution will have seven parts. </span><span class="koboSpan" id="kobo.62.2">We still have the application code and Dockerfiles (replacing the Packer-based VM images) for both the frontend and backend. </span><span class="koboSpan" id="kobo.62.3">We also still have GitHub Actions to implement our CI/CD process, but we now have two Terraform code bases – one for provisioning the underlying infrastructure to Google Cloud and another for provisioning our application to the Kubernetes cluster hosted on GKE. </span><span class="koboSpan" id="kobo.62.4">Then, we have the two code bases for our application’s frontend </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">and backen</span><a id="_idTextAnchor611"/><span class="koboSpan" id="kobo.64.1">d.</span></span></p>
<h2 id="_idParaDest-252"><a id="_idTextAnchor612"/><span class="koboSpan" id="kobo.65.1">Cloud architecture</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.66.1">Google Kubernetes Engine</span></strong><span class="koboSpan" id="kobo.67.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.68.1">GKE</span></strong><span class="koboSpan" id="kobo.69.1">) is a </span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.70.1">sophisticated offering that allows you to provision a </span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.71.1">managed Kubernetes cluster in a multitude </span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.72.1">of ways, depending on your objectives, whether that is to maximize simplicity of operations or highly </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">customized configurat</span><a id="_idTextAnchor613"/><span class="koboSpan" id="kobo.74.1">ions.</span></span></p>
<h3><span class="koboSpan" id="kobo.75.1">Autopilot</span></h3>
<p><span class="koboSpan" id="kobo.76.1">One of the</span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.77.1"> simplest ways of operating a Kubernetes cluster on Google Cloud is using the Autopilot feature of GKE. </span><span class="koboSpan" id="kobo.77.2">Turning on the Autopilot feature abstracts much of the complexity of operating a Kubernetes cluster. </span><span class="koboSpan" id="kobo.77.3">This option changes the operating model radically, so much so that it is probably more akin to some of the container-based serverless options on other clouds than it does the managed Kubernetes offerings that we’ve delved into in previous chapters. </span><span class="koboSpan" id="kobo.77.4">As a result, it is outside the scope of this book. </span><span class="koboSpan" id="kobo.77.5">However, if this approach appeals to you, I suggest that you investigate further in Google’s documentation (</span><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview"><span class="koboSpan" id="kobo.78.1">https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</span></a><span class="koboSpan" id="kobo.79.1">). </span><span class="koboSpan" id="kobo.79.2">I’m pointing this out because, unlike AWS and Azure, which have separately branded services that abstract away container orchestration, </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">Google Cloud Platform</span></strong><span class="koboSpan" id="kobo.81.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.82.1">GCP</span></strong><span class="koboSpan" id="kobo.83.1">) has</span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.84.1"> this capability coupled with its managed </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">Kubernetes of</span><a id="_idTextAnchor614"/><span class="koboSpan" id="kobo.86.1">fering.</span></span></p>
<h3><span class="koboSpan" id="kobo.87.1">Regional versus zonal</span></h3>
<p><span class="koboSpan" id="kobo.88.1">GKE supports</span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.89.1"> two primary cluster types: regional and zonal. </span><span class="koboSpan" id="kobo.89.2">The cluster type affects how the cluster’s underlying physical infrastructure is provisioned across GCP, which subsequently affects the resiliency of the </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">Kubernetes cluster:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer173">
<span class="koboSpan" id="kobo.91.1"><img alt="Figure 14.3 – The GKE zonal cluster hosts the control plane and all nodes within a single Availability Zone" src="image/B21183_14_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.92.1">Figure 14.3 – The GKE zonal cluster hosts the control plane and all nodes within a single Availability Zone</span></p>
<p><span class="koboSpan" id="kobo.93.1">A zonal cluster is </span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.94.1">deployed within a single Availability Zone within a given region. </span><span class="koboSpan" id="kobo.94.2">As we know, each region has a name, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.95.1">us-west1</span></strong><span class="koboSpan" id="kobo.96.1">. </span><span class="koboSpan" id="kobo.96.2">To reference a specific zone, we append the zone number to the end of the region name. </span><span class="koboSpan" id="kobo.96.3">For example, to reference Availability Zone A in the West US 1 region, we can refer to it by its name – that </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">is, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.98.1">us-west1-a</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<span class="koboSpan" id="kobo.100.1"><img alt="Figure 14.4 – The GKE regional cluster replicates the control plane and nodes across all zones within the region" src="image/B21183_14_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.101.1">Figure 14.4 – The GKE regional cluster replicates the control plane and nodes across all zones within the region</span></p>
<p><span class="koboSpan" id="kobo.102.1">A regional cluster is </span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.103.1">deployed across Availability Zones within a given region. </span><span class="koboSpan" id="kobo.103.2">When you deploy a regional cluster, by default, your cluster is deployed across three Availability Zones within that region. </span><span class="koboSpan" id="kobo.103.3">This approach results in higher availability and resiliency in case of an Availability </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">Zone</span><a id="_idTextAnchor615"/><span class="koboSpan" id="kobo.105.1"> outage.</span></span></p>
<h3><span class="koboSpan" id="kobo.106.1">Virtual network</span></h3>
<p><span class="koboSpan" id="kobo.107.1">As we </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.108.1">discussed in the previous chapter, when we set up our VM-based solution on Google Cloud, we will need a virtual network to host our GKE cluster. </span><span class="koboSpan" id="kobo.108.2">This will allow us to configure a private GKE cluster so that the Kubernetes control plane and the node have private IP addresses and are not directly accessible from </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">the internet.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">In the previous chapter, where we set up our VM-based solution, we set up two subnets: one for the frontend and one for the backend. </span><span class="koboSpan" id="kobo.110.2">However, when using a Kubernetes cluster to host our solution, both the frontend and backend will be hosted on the same </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">Kubernetes nodes.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">This </span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.113.1">straightforward approach, where multiple node pools share one subnet, can suffice for less complex configurations. </span><span class="koboSpan" id="kobo.113.2">However, while this setup simplifies network management, it can potentially limit the scalability of individual node pools due to shared network resources and address </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">space constraints.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">For more scalable and flexible architectures, especially in larger or more dynamic environments, it’s often advantageous to allocate separate subnets for different node pools. </span><span class="koboSpan" id="kobo.115.2">This method allows each node pool to scale independently and optimizes network organization, providing better resource allocation and isolation. </span><span class="koboSpan" id="kobo.115.3">This kind of structured subnetting becomes increasingly important as the complexity and scale of the Kubernetes deployments grow, making it a key consideration in GKE network planning </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">and conf</span><a id="_idTextAnchor616"/><span class="koboSpan" id="kobo.117.1">iguration.</span></span></p>
<h3><span class="koboSpan" id="kobo.118.1">Container registry</span></h3>
<p><span class="koboSpan" id="kobo.119.1">Like the </span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.120.1">other cloud platforms we’ve been delving into in this book, Google Cloud also offers a robust container registry service</span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.121.1"> known as </span><strong class="bold"><span class="koboSpan" id="kobo.122.1">Google Artifact Registry</span></strong><span class="koboSpan" id="kobo.123.1">, which is a private registry for hosting and managing container images and Helm charts. </span><span class="koboSpan" id="kobo.123.2">Artifact Registry supports many other formats besides container images but we’ll only be using it in </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">this capacity.</span></span></p>
<p><span class="koboSpan" id="kobo.125.1">Google Artifact Registry is set up pretty similarly to other cloud providers. </span><span class="koboSpan" id="kobo.125.2">It resembles </span><strong class="bold"><span class="koboSpan" id="kobo.126.1">Azure Container Registry</span></strong><span class="koboSpan" id="kobo.127.1"> a bit </span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.128.1">more though because it can host multiple repositories, allowing you to host multiple container images in the same </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">Artif</span><a id="_idTextAnchor617"/><span class="koboSpan" id="kobo.130.1">act Registry.</span></span></p>
<h3><span class="koboSpan" id="kobo.131.1">Load balancing</span></h3>
<p><span class="koboSpan" id="kobo.132.1">GKE has a </span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.133.1">very similar experience to other managed Kubernetes offerings that we have looked at in this book. </span><span class="koboSpan" id="kobo.133.2">By default, when a Kubernetes service is provisioned to a private cluster, GKE will automatically provision an internal load balancer for this service. </span><span class="koboSpan" id="kobo.133.3">This will make the Kubernetes service available within the virtual network but not to the </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">outside world.</span></span></p>
<p><span class="koboSpan" id="kobo.135.1">This works well for our backend REST API but doesn’t work for our public web application, which is intended to be accessible from the public Internet. </span><span class="koboSpan" id="kobo.135.2">Like on AWS and Azure, to make the frontend service accessible to the internet, we need to configure an ingress</span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.136.1"> controller on the cluster and a public load balancer that has a public IP address and will route traffic to the ingress controller on the </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">GKE cluster:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<span class="koboSpan" id="kobo.138.1"><img alt="Figure 14.5 – The GKE cluster with an NGINX ingress controller automating a Google Cloud load balancer" src="image/B21183_14_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.139.1">Figure 14.5 – The GKE cluster with an NGINX ingress controller automating a Google Cloud load balancer</span></p>
<p><span class="koboSpan" id="kobo.140.1">As we did in previous chapters, we’ll set up an NGINX ingress controller and configure it to automatically provision the necessary external </span><a id="_idTextAnchor618"/><span class="No-Break"><span class="koboSpan" id="kobo.141.1">load balancer.</span></span></p>
<h3><span class="koboSpan" id="kobo.142.1">Network security</span></h3>
<p><span class="koboSpan" id="kobo.143.1">When working with GKE, network security</span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.144.1"> is managed in a manner akin to the practices described in </span><a href="B21183_13.xhtml#_idTextAnchor569"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.145.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.146.1"> for VMs, leveraging similar concepts and tools within the Google Cloud ecosystem. </span><span class="koboSpan" id="kobo.146.2">GKE clusters are typically deployed within a virtual network, allowing them to seamlessly integrate with other Google </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">Cloud services.</span></span></p>
<p><span class="koboSpan" id="kobo.148.1">Similar to the other managed Kubernetes offerings, the virtual network acts as the primary boundary for network security, within which GKE has its internal network where pods and services communicate. </span><span class="koboSpan" id="kobo.148.2">Google Cloud firewalls are used to define security rules at the subnet level, controlling inbound and outbound traffic similar to how they are employed </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">with VMs.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">Additionally, GKE takes </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.151.1">advantage of native Kubernetes network policies for finer-grained control within the cluster, allowing administrators to define how Pods communicate with each other and with other resources in the virtual network. </span><span class="koboSpan" id="kobo.151.2">This dual-layered approach, combining the external security controls of the virtual network with the internal mechanisms of GKE, creates a comprehensive and robust network security environment for </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">Kuberne</span><a id="_idTextAnchor619"/><span class="koboSpan" id="kobo.153.1">tes deployments.</span></span></p>
<h3><span class="koboSpan" id="kobo.154.1">Workload identity</span></h3>
<p><span class="koboSpan" id="kobo.155.1">As we did </span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.156.1">with AWS and Azure in previous chapters, we’ll be setting up a workload identity to allow our application’s pods to authenticate with other Google Cloud services using a Google Cloud identity provider. </span><span class="koboSpan" id="kobo.156.2">This will allow us to use the built-in role-based access control to grant access for Kubernetes service accounts to other Google</span><a id="_idTextAnchor620"/> <span class="No-Break"><span class="koboSpan" id="kobo.157.1">Cloud resources.</span></span></p>
<h3><span class="koboSpan" id="kobo.158.1">Secrets management</span></h3>
<p><span class="koboSpan" id="kobo.159.1">GKE does </span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.160.1">not have direct integration with Google Secrets Manager like other cloud platforms. </span><span class="koboSpan" id="kobo.160.2">Instead, the options available to you are to leverage native Kubernetes secrets or to access Google Secrets Manager from your application code itself. </span><span class="koboSpan" id="kobo.160.3">This approach does have some security advantages but it is less ideal as it tightly couples your applic</span><a id="_idTextAnchor621"/><span class="koboSpan" id="kobo.161.1">ation to </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">GCP SDKs.</span></span></p>
<h3><span class="koboSpan" id="kobo.163.1">Kubernetes cluster</span></h3>
<p><span class="koboSpan" id="kobo.164.1">Building a</span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.165.1"> Kubernetes cluster using GKE involves a few key decisions that determine the modality of your cluster. </span><span class="koboSpan" id="kobo.165.2">As we’ve discussed in this book, we will omit the use of Autopilot to maintain congruency with the other managed Kubernetes offerings from the other cloud platforms we’ve looked at in this book. </span><span class="koboSpan" id="kobo.165.3">So, we will focus on building a private Kubernetes cluster with its own </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">virtual network.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">Like other managed Kubernetes offerings, GKE provides flexibility to configure node pools based on workload types, but unlike those offerings, you don’t need to set up node pools </span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.168.1">for running core Kubernetes services. </span><span class="koboSpan" id="kobo.168.2">GKE handles all that on your behalf! </span><span class="koboSpan" id="kobo.168.3">This abstraction greatly simplifies cluster design. </span><span class="koboSpan" id="kobo.168.4">Overall, GKE’s simplicity and robust feature set allow us to build highly scalable Kubernetes clusters </span><a id="_idTextAnchor622"/><span class="koboSpan" id="kobo.169.1">with </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">minimal effort.</span></span></p>
<h2 id="_idParaDest-253"><a id="_idTextAnchor623"/><span class="koboSpan" id="kobo.171.1">Deployment architecture</span></h2>
<p><span class="koboSpan" id="kobo.172.1">As we </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.173.1">saw with the cloud architecture, there were many similarities between our work in </span><em class="italic"><span class="koboSpan" id="kobo.174.1">Chapters 8</span></em><span class="koboSpan" id="kobo.175.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.176.1">11</span></em><span class="koboSpan" id="kobo.177.1"> with AWS and Microsoft Azure. </span><span class="koboSpan" id="kobo.177.2">The deployment architecture will mirror what we saw in those chapters as well. </span><span class="koboSpan" id="kobo.177.3">In the previous chapter, we saw the differences in the Terraform provider when we configured the </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">google</span></strong><span class="koboSpan" id="kobo.179.1"> provider to provision our solution to VMs </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">using GCE.</span></span></p>
<p><span class="koboSpan" id="kobo.181.1">In the context of container-based architecture, the only significant difference from our deployment in the previous chapters with AWS and Azure will be the way we authenticate with the container registry and the Kubernetes cluster. </span><span class="koboSpan" id="kobo.181.2">It’s important to recall the deployment architectural approach outlined in the corresponding section of </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.182.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.183.1">. </span><span class="koboSpan" id="kobo.183.2">In the next section, we’ll build the same solution on GCP, ensuring we don’t repeat </span><a id="_idTextAnchor624"/><span class="koboSpan" id="kobo.184.1">the </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">same information.</span></span></p>
<p><span class="koboSpan" id="kobo.186.1">In this section, we reviewed the key changes in our architecture as we transitioned from VM-based architecture to container-based architecture. </span><span class="koboSpan" id="kobo.186.2">We were careful not to retread the ground we covered in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.187.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.188.1">, where we first went through this transformation on the AWS platform. </span><span class="koboSpan" id="kobo.188.2">In the next section, we’ll get tactical in building the solution, but again, we’ll be careful to build on the foundations we built in the previous chapter when we first set up our solution on GCP </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">using VMs.</span></span></p>
<h1 id="_idParaDest-254"><a id="_idTextAnchor625"/><span class="koboSpan" id="kobo.190.1">Building the solution</span></h1>
<p><span class="koboSpan" id="kobo.191.1">In this section, we’ll </span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.192.1">be taking our theoretical knowledge and applying it to a tangible, functioning solution while harnessing the power of Docker, Terraform, and Kubernetes on GCP. </span><span class="koboSpan" id="kobo.192.2">Some parts of this process will require significant change, such as when we provision our Google Cloud infrastructure using Terraform; other parts will have minor changes, such as the Kubernetes configuration that we use to deploy our application to our Kubernetes cluster, and some will have almost no change whatsoever, such as when we build a push our Docker images to o</span><a id="_idTextAnchor626"/><span class="koboSpan" id="kobo.193.1">ur </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">container registry.</span></span></p>
<h2 id="_idParaDest-255"><a id="_idTextAnchor627"/><span class="koboSpan" id="kobo.195.1">Docker</span></h2>
<p><span class="koboSpan" id="kobo.196.1">In this </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.197.1">section, we’ll go into great detail on how we can implement our Dockerfile, which installs our .NET application code and runs the service in a container. </span><span class="koboSpan" id="kobo.197.2">If you skipped </span><em class="italic"><span class="koboSpan" id="kobo.198.1">Chapters 7</span></em><span class="koboSpan" id="kobo.199.1"> through </span><em class="italic"><span class="koboSpan" id="kobo.200.1">9</span></em><span class="koboSpan" id="kobo.201.1"> due to a lack of interest in AWS, I can’t hold that against you – particularly if your primary interest in reading this book is working on GCP. </span><span class="koboSpan" id="kobo.201.2">However, I would encourage you to review the corresponding section within </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.202.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.203.1"> to see how we can use Docker to configure a container with our</span><a id="_idTextAnchor628"/><span class="koboSpan" id="kobo.204.1"> .NET </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">application code.</span></span></p>
<h2 id="_idParaDest-256"><a id="_idTextAnchor629"/><span class="koboSpan" id="kobo.206.1">Infrastructure</span></h2>
<p><span class="koboSpan" id="kobo.207.1">As we </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.208.1">know, Terraform is not a write-once, run-anywhere solution. </span><span class="koboSpan" id="kobo.208.2">It is a highly extensible </span><strong class="bold"><span class="koboSpan" id="kobo.209.1">Infrastructure as Code</span></strong><span class="koboSpan" id="kobo.210.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.211.1">IaC</span></strong><span class="koboSpan" id="kobo.212.1">) tool that </span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.213.1">uses a well-defined strategy pattern to facilitate the management of multiple cloud platforms. </span><span class="koboSpan" id="kobo.213.2">This yields very similar conceptually structured solutions but with significant variations embedded within the differing implementation details and nomenclature of each corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">cloud platform.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">As we discussed in the previous section, the virtual network configuration will largely be identical and the load balancer will be automatically provisioned by GKE via the NGINX ingress controller. </span><span class="koboSpan" id="kobo.215.2">Therefore, in this section, we will only focus on the new resources that we need to replace our VMs with a </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">Kubernetes cluster.</span></span></p>
<h3><span class="koboSpan" id="kobo.217.1">Container registry</span></h3>
<p><span class="koboSpan" id="kobo.218.1">The </span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.219.1">first thing we need is a Google Cloud Artifact Registry that we can push Docker images to. </span><span class="koboSpan" id="kobo.219.2">We’ll use this as part of our Docker build process later when we build and push Docker images to be used by our </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">GKE cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.221.1">
resource "google_artifact_registry_repository" "main" {
  project       = google_project.main.project_id
  location      = var.primary_region
  repository_id = "${var.application_name}-${var.environment_name}"
  </span><a id="_idTextAnchor630"/><span class="koboSpan" id="kobo.222.1">format        = "DOCKER"
}</span></pre> <h3><span class="koboSpan" id="kobo.223.1">Service account</span></h3>
<p><span class="koboSpan" id="kobo.224.1">To </span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.225.1">grant our applications and services the ability to implicitly authenticate with Google Cloud and access other services and resources hosted therein, we need to set up a service account that we can associate with the workloads running on our cluster. </span><span class="koboSpan" id="kobo.225.2">This is similar to the IAM role and managed identity we specified on AWS and </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">Azure, respectively:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.227.1">
resource "google_service_account" "cluster" {
  project      = google_project.main.project_id
  account_id   = "sa-gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}"
  display_name = "sa-gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}"
}</span></pre> <h3><span class="koboSpan" id="kobo.228.1">Kubernetes cluster</span></h3>
<p><span class="koboSpan" id="kobo.229.1">This</span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.230.1"> Terraform code creates a GKE cluster with a customized name – that is, Google Cloud Region. </span><span class="koboSpan" id="kobo.230.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">location</span></strong><span class="koboSpan" id="kobo.232.1"> attribute is extremely critical as its value can determine if the cluster is regional or zonal. </span><span class="koboSpan" id="kobo.232.2">Simply making a tiny change from </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">us-west1</span></strong><span class="koboSpan" id="kobo.234.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">us-west1-a</span></strong><span class="koboSpan" id="kobo.236.1"> has </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">this effect:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.238.1">
resource "google_container_cluster" "main" {
  project  = google_project.main.project_id
  name     = "gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}"
  location = var.primary_region
  remove_default_node_pool = true
  initial_node_count       = 1
}</span></pre> <p><span class="koboSpan" id="kobo.239.1">By default, GKE</span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.240.1"> will automatically provision a default node pool. </span><span class="koboSpan" id="kobo.240.2">This is a common practice that, unfortunately, prioritizes the graphical user experience via the Google Cloud console over the IaC experience. </span><span class="koboSpan" id="kobo.240.3">This problem is not unique to Google Cloud; both AWS and Azure have similar areas of friction where automation is an afterthought. </span><span class="koboSpan" id="kobo.240.4">As a result, we are at least left with attributes that allow us to circumvent this behavior. </span><span class="koboSpan" id="kobo.240.5">By setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.241.1">remove_default_node_pool</span></strong><span class="koboSpan" id="kobo.242.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">true</span></strong><span class="koboSpan" id="kobo.244.1">, we can ensure that this default behavior is eliminated. </span><span class="koboSpan" id="kobo.244.2">Furthermore, setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">initial_node_count</span></strong><span class="koboSpan" id="kobo.246.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">1</span></strong><span class="koboSpan" id="kobo.248.1"> can further speed up </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">this process.</span></span></p>
<p><span class="koboSpan" id="kobo.250.1">As we discussed previously, GKE abstracts the Kubernetes master services from us so that we don’t need to worry about deploying a node pool for these Kubernetes system components. </span><span class="koboSpan" id="kobo.250.2">Therefore, we are left with defining our node pools for our applications and services to </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">run on:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.252.1">
resource "google_container_node_pool" "primary" {
  project    = google_project.main.project_id
  name       = "gke-${var.application_name}-${var.environment_name}-${random_string.project_id.result}-primary"
  location   = var.primary_region
  cluster    = google_container_cluster.main.name
  node_count = var.node_count
  node_config {
    ...
</span><span class="koboSpan" id="kobo.252.2">  }
}</span></pre> <p><span class="koboSpan" id="kobo.253.1">The basic configuration of a node pool resource connects it to the corresponding cluster and specifies a </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">node_count</span></strong><span class="koboSpan" id="kobo.255.1"> value. </span><span class="koboSpan" id="kobo.255.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">node_config</span></strong><span class="koboSpan" id="kobo.257.1"> block is where we configure more details for the nodes within the pool. </span><span class="koboSpan" id="kobo.257.2">The node pool configuration should look similar </span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.258.1">to what we saw in </span><em class="italic"><span class="koboSpan" id="kobo.259.1">Chapters 8</span></em><span class="koboSpan" id="kobo.260.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.261.1">11</span></em><span class="koboSpan" id="kobo.262.1"> when we configured the managed Kubernetes offerings of AWS and Azure. </span><span class="koboSpan" id="kobo.262.2">Node pools have a count that controls how many VMs we can spin up and a VM size that specifies how many CPU cores and memory each node gets. </span><span class="koboSpan" id="kobo.262.3">We also need to specify the service account under which the node pool </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">will operate:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.264.1">
node_config {
  machine_type = var.node_size
  preemptible  = false
  spot         = false
  service_account = google_service_account.cluster.email
  oauth_scopes = [
    "https://www.googleapis.com/auth/cloud-platform",
    "https://www.googleapis.com/auth/logging.write",
    "https://www.googleapis.com/auth/monitoring"
  ]
}</span></pre> <p><span class="koboSpan" id="kobo.265.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">oauth_scopes</span></strong><span class="koboSpan" id="kobo.267.1"> is used to specify what permissions the nodes should have access to. </span><span class="koboSpan" id="kobo.267.2">To</span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.268.1"> enable Google Cloud logging and monitoring, we need to add scopes to allow the nodes to tap into these existing Google </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">Cloud services.</span></span></p>
<h3><span class="koboSpan" id="kobo.270.1">Workload identity</span></h3>
<p><span class="koboSpan" id="kobo.271.1">To </span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.272.1">enable a workload identity, we need to modify both our cluster and node pool configuration. </span><span class="koboSpan" id="kobo.272.2">The cluster needs to have the </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">workload_identity_config</span></strong><span class="koboSpan" id="kobo.274.1"> block defined with </span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">workload_pool</span></strong><span class="koboSpan" id="kobo.276.1"> set with a specific magic string that will provision the GKE metadata service within </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">the cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.278.1">
resource "google_container_cluster" "main" {
  ...
</span><span class="koboSpan" id="kobo.278.2">  workload_identity_config {
    workload_pool = "${google_project.main.project_id}.svc.id.goog"
  }
}</span></pre> <p><span class="koboSpan" id="kobo.279.1">Once the GKE metadata service is made available within the cluster, we need to configure our node pools so that they integrate with it using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">workload_metadata_config</span></strong><span class="koboSpan" id="kobo.281.1"> block. </span><span class="koboSpan" id="kobo.281.2">We can do this by specifying </span><strong class="source-inline"><span class="koboSpan" id="kobo.282.1">GKE_METADATA</span></strong><span class="koboSpan" id="kobo.283.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">the mode:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.285.1">
node_config {
  ...
</span><span class="koboSpan" id="kobo.285.2">  workload_metadata_config {
</span><a id="_idTextAnchor631"/><span class="koboSpan" id="kobo.286.1">    mode = "GKE_METADATA"
  }
}</span></pre> <h2 id="_idParaDest-257"><a id="_idTextAnchor632"/><span class="koboSpan" id="kobo.287.1">Kubernetes</span></h2>
<p><span class="koboSpan" id="kobo.288.1">In </span><em class="italic"><span class="koboSpan" id="kobo.289.1">Chapters 8</span></em><span class="koboSpan" id="kobo.290.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.291.1">11</span></em><span class="koboSpan" id="kobo.292.1">, we built </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.293.1">out the Kubernetes deployments using the Terraform provider for Kubernetes on AWS and Azure, respectively. </span><span class="koboSpan" id="kobo.293.2">We’ll follow the same approach here, building on the infrastructure we pro</span><a id="_idTextAnchor633"/><span class="koboSpan" id="kobo.294.1">visioned in the </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">previous section.</span></span></p>
<h3><span class="koboSpan" id="kobo.296.1">Provider setup</span></h3>
<p><span class="koboSpan" id="kobo.297.1">As we </span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.298.1">saw in </span><a href="B21183_11.xhtml#_idTextAnchor509"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.299.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.300.1">, there is not much that changes when executing Terraform using the Kubernetes provider to provision resources to the Kubernetes control plane. </span><span class="koboSpan" id="kobo.300.2">We still authenticate against our target cloud platform, follow Terraform’s core workflow, and pass in additional input parameters for platform-specific resources that we need to reference. </span><span class="koboSpan" id="kobo.300.3">Most notably, information about the cluster and other GCP services such as Secrets Manager and other details that might need to be put into Kubernetes ConfigMaps can be used by the pods to point them at the endpoint of </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">their database.</span></span></p>
<p><span class="koboSpan" id="kobo.302.1">As we saw in </span><em class="italic"><span class="koboSpan" id="kobo.303.1">Chapters 8</span></em><span class="koboSpan" id="kobo.304.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.305.1">11</span></em><span class="koboSpan" id="kobo.306.1">, when we accomplished the same task on AWS and Azure, I am using a layered approach to provision the infrastructure first and then provision to Kubernetes. </span><span class="koboSpan" id="kobo.306.2">As a result, we can reference the Kubernetes cluster using the data source from the Terraform workspace that provisions the Google Cloud infrastructure. </span><span class="koboSpan" id="kobo.306.3">This allows us to access important connectivity details without exporting them outside of Terraform and passing them around </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">during deployment:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.308.1">
data "google_container_cluster" "main" {
  name     = var.cluster_name
  location = var.primary_region
}</span></pre> <p><span class="koboSpan" id="kobo.309.1">As </span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.310.1">you can see, in the preceding code, when using the data source, we only need to specify the cluster name and its target region. </span><span class="koboSpan" id="kobo.310.2">Using this data source, we can then initialize the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">kubernetes</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.312.1"> provider:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.313.1">
provider "kubernetes" {
  token                  = data.google_client_config.current.access_token
  host                   = data.google_container_cluster.main.endpoint
  client_certificate     = base64decode(data.google_container_cluster.main.master_auth.0.client_certificate)
  client_key             = base64decode(data.google_container_cluster.main.master_auth.0.client_key)
  cluster_ca_certificate = base64decode(data.google_container_cluster.main.master_auth.0.cluster_ca_certificate)
}</span></pre> <p><span class="koboSpan" id="kobo.314.1">This configuration varies slightly from the provider initialization techniques we used with AWS and Azure in previous chapters with the addition of </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">token</span></strong><span class="koboSpan" id="kobo.316.1">. </span><span class="koboSpan" id="kobo.316.2">Similar to how we initialized the </span><strong class="source-inline"><span class="koboSpan" id="kobo.317.1">helm</span></strong><span class="koboSpan" id="kobo.318.1"> provider on other cloud platforms, we can pass the same inputs to set up the </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">Helm provider.</span></span></p>
<h3><span class="koboSpan" id="kobo.320.1">Workload identity</span></h3>
<p><span class="koboSpan" id="kobo.321.1">As we discussed in </span><em class="italic"><span class="koboSpan" id="kobo.322.1">Chapters 8</span></em><span class="koboSpan" id="kobo.323.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.324.1">11</span></em><span class="koboSpan" id="kobo.325.1">, where we implemented a workload identity on both AWS and Azure, we need a way for our Kubernetes workloads to be able to implicitly authenticate</span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.326.1"> with Google Cloud services and resources. </span><span class="koboSpan" id="kobo.326.2">To do so, we need an identity provisioned within Google Cloud, which we saw in the previous section of this chapter, but we also need something provisioned within Kubernetes that will connect our pod specifications to the Google Cloud </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">service account:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.328.1">
resource "kubernetes_service_account" "main" {
  metadata {
    namespace = var.namespace
    name      = var.service_account_name
    annotations = {
      "iam.gke.io/gcp-service-account" = var.service_account_email
    }
  }
  automount_service_account_token = var.service_account_token
}</span></pre> <p><span class="koboSpan" id="kobo.329.1">The preceding code will provision the Kubernetes service account that will complete the linkage with the Google Cloud configuration that we provisioned in the </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">previous section.</span></span></p>
<p><span class="koboSpan" id="kobo.331.1">Now that we’ve built out the three components of our architecture, in the next section, we’ll move on to how we can automate the deployment using Docker so that we can build and publish the container images. </span><span class="koboSpan" id="kobo.331.2">We’ll also look at doing this using Terraform so that we can provision our infrastructure and deploy our solution </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">to Kubernetes.</span></span></p>
<h1 id="_idParaDest-258"><a id="_idTextAnchor634"/><span class="koboSpan" id="kobo.333.1">Automating the deployment</span></h1>
<p><span class="koboSpan" id="kobo.334.1">In this section, we’ll look </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.335.1">at how we can automate the deployment process for container-based architectures. </span><span class="koboSpan" id="kobo.335.2">We’ll employ similar techniques we saw in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.336.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.337.1"> when we took this same journey down the Amazon. </span><span class="koboSpan" id="kobo.337.2">As a result, we’ll focus on what changes we need to make when we want to deploy to Microsoft Azure and the Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">Kubernetes Service.</span></span></p>
<h2 id="_idParaDest-259"><a id="_idTextAnchor635"/><span class="koboSpan" id="kobo.339.1">Docker</span></h2>
<p><span class="koboSpan" id="kobo.340.1">In </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.341.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.342.1">, we </span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.343.1">covered each step of the GitHub Actions workflow that causes Docker to build, tag, and push our Docker container images. </span><span class="koboSpan" id="kobo.343.2">Thanks to the nature of Docker’s cloud-agnostic architecture, this overwhelmingly stays </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">the same.</span></span></p>
<p><span class="koboSpan" id="kobo.345.1">The only thing that changes is that Google Cloud encapsulates a service account’s credentials into a JSON file that is downloaded from the Google Cloud console rather than a secret string like on AWS or Azure. </span><span class="koboSpan" id="kobo.345.2">As a result, much of the Google Cloud tooling is set up to look for this file at a specific </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">path location.</span></span></p>
<p><span class="koboSpan" id="kobo.347.1">Therefore, we need to use a special username </span><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">_json_key</span></strong><span class="koboSpan" id="kobo.349.1"> and reference the value of the JSON file stored in a GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">Actions secret:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.351.1">
    - name: Login to Google Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ needs.terraform-apply.outputs.container_registry_endpoint }}
        username: _json_key
        password: ${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}</span></pre> <p><span class="koboSpan" id="kobo.352.1">The only thing that changes is the way we must configure Docker so tha</span><a id="_idTextAnchor636"/><span class="koboSpan" id="kobo.353.1">t it targets Google </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">Artifact Registry.</span></span></p>
<h2 id="_idParaDest-260"><a id="_idTextAnchor637"/><span class="koboSpan" id="kobo.355.1">Terraform</span></h2>
<p><span class="koboSpan" id="kobo.356.1">In </span><a href="B21183_13.xhtml#_idTextAnchor569"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.357.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.358.1">, we </span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.359.1">comprehensively covered the process of creating a Terraform GitHub Action that authenticates with GCP using a service account. </span><span class="koboSpan" id="kobo.359.2">Therefore, we won’t be delving into it any further. </span><span class="koboSpan" id="kobo.359.3">I encourage you to refer back to </span><a href="B21183_10.xhtml#_idTextAnchor474"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.360.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.361.1"> to review </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">the process.</span></span></p>
<h2 id="_idParaDest-261"><a id="_idTextAnchor638"/><span class="koboSpan" id="kobo.363.1">Kubernetes</span></h2>
<p><span class="koboSpan" id="kobo.364.1">When we </span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.365.1">automate Kubernetes with Terraform, we are just running </span><strong class="source-inline"><span class="koboSpan" id="kobo.366.1">terraform apply</span></strong><span class="koboSpan" id="kobo.367.1"> again with a different root module. </span><span class="koboSpan" id="kobo.367.2">This time, the root module will configure the </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">kubernetes</span></strong><span class="koboSpan" id="kobo.369.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">helm</span></strong><span class="koboSpan" id="kobo.371.1"> providers in addition to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">google</span></strong><span class="koboSpan" id="kobo.373.1"> provider. </span><span class="koboSpan" id="kobo.373.2">However, we won’t ever create new resources with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">google</span></strong><span class="koboSpan" id="kobo.375.1"> provider; we will only obtain data sources to existing resources we provisioned in the previous </span><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">terraform apply</span></strong><span class="koboSpan" id="kobo.377.1"> command that provisioned the infrastructure to </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">Google Cloud.</span></span></p>
<p><span class="koboSpan" id="kobo.379.1">As a result, the GitHub Action that executes this process will look strikingly similar to how we executed Terraform with Google Cloud. </span><span class="koboSpan" id="kobo.379.2">Some of the variables might change to include things such as the containe</span><a id="_idTextAnchor639"/><span class="koboSpan" id="kobo.380.1">r image details and </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">cluster information.</span></span></p>
<h1 id="_idParaDest-262"><a id="_idTextAnchor640"/><span class="koboSpan" id="kobo.382.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.383.1">In this chapter, we designed, built, and automated the deployment of a complete and end-to-end solution using container-based architecture. </span><span class="koboSpan" id="kobo.383.2">We built onto the foundations from </span><a href="B21183_13.xhtml#_idTextAnchor569"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.384.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.385.1">, where we worked with the foundational infrastructure of Google Cloud networking but layered on GKE to host our application in containers. </span><span class="koboSpan" id="kobo.385.2">In the next and final step in our GCP journey, we’ll be looking at serverless architecture, thus moving beyond the underlying infrastructure and letting the platform itself take our solution to </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">new heights.</span></span></p>
</div>
</body></html>