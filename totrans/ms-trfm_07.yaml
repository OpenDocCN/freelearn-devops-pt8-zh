- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Getting Started on AWS – Building Solutions with AWS EC2
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS上入门 – 使用AWS EC2构建解决方案
- en: 'Now that we have a good foundation of the concepts needed to build real-world
    cloud solutions and automate them using **Infrastructure as Code** (**IaC**) with
    Terraform, we will start our journey with arguably the most popular cloud platform:
    **Amazon Web** **Services** (**AWS**).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了构建现实世界云解决方案并使用Terraform进行基础设施即代码（IaC）自动化所需的概念基础，我们将从目前最受欢迎的云平台之一 –
    Amazon Web Services（AWS）开始我们的旅程。
- en: In this chapter, we’ll take a step-by-step approach to designing, building,
    and automating a solution using AWS’s **virtual machine** (**VM**) service – **Elastic
    Cloud Compute** or **EC2** for short. We’ll also explore several other AWS services
    that are crucial for ensuring our solution’s robustness and production readiness,
    such as secrets management, logging, and network security.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将采取逐步的方法来设计、构建和自动化使用AWS的虚拟机服务 – Elastic Cloud Compute（EC2）或简称EC2的解决方案。我们还将探讨几个其他对确保我们解决方案稳健性和生产就绪性至关重要的AWS服务，如密钥管理、日志记录和网络安全。
- en: We have much to accomplish, but this is where the rubber hits the road. We’ll
    begin to really apply the concepts we’ve been discussing and put them into practice
    on AWS.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有很多工作要完成，但这正是实践真正开始的地方。我们将开始真正应用我们讨论过的概念，并在AWS上实际应用它们。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Laying the foundation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奠定基础
- en: Designing the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Building the solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: Automating the deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化部署
- en: Laying the foundation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奠定基础
- en: Cloud infrastructure is only as good as the applications and services deployed
    to it, so for this book, we will be building our sample architectures around a
    function use case for a fictional company called Söze Enterprises. Söze Enterprises
    was founded by a mysterious Turkish billionaire, Keyser Söze, who wants to take
    autonomous vehicles to the next level by building a platform that will allow both
    land and air vehicles – from any manufacturer – to coordinate their actions to
    improve safety and efficiency. Somehow, Keyser has already got Elon onboard, so
    it’s only a matter of time before the other EV vendors follow suit.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 云基础设施的好坏取决于部署到其上的应用程序和服务，因此在本书中，我们将围绕一个名为Söze Enterprises的虚构公司的功能用例构建我们的样例架构。Söze
    Enterprises是由一个神秘的土耳其亿万富翁Keyser Söze创立的，他希望通过建立一个平台将自动驾驶车辆推向新的高度，使地面和空中的车辆（来自任何制造商）协调行动，以提高安全性和效率。不知何故，Keyser已经获得了Elon的支持，所以其他电动车供应商很快也会效仿。
- en: 'We have inherited a team from one of Söze Enterprises’ other divisions that
    has a strong core team of C# .NET developers, so we’ll be building version 1.0
    of the platform using .NET technologies. The elusive CEO, Keyser, was seen hobnobbing
    with Jeff Bezos in Monaco over the weekend, and word has come down from corporate
    that we will be using AWS to host the platform. Since the team doesn’t have a
    ton of experience with containers and timelines are tight, we’ve decided to build
    a simple three-tier architecture and host on VMs using AWS’s EC2 service. We’ve
    decided to use a Linux operating system to make it easier to convert containers
    in the future:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继承了Söze Enterprises的另一个部门的团队，这个团队有一支强大的C# .NET开发人员核心团队，所以我们将使用.NET技术构建平台的1.0版本。这位神秘的CEO
    Keyser上周末在摩纳哥与Jeff Bezos交际，公司的消息称我们将使用AWS来托管这个平台。因为团队对容器的经验不多，时间又很紧，我们决定建立一个简单的三层架构，并在AWS的EC2服务上托管。我们决定使用Linux操作系统，以便将来更容易转换容器：
- en: '![Figure 7.1 – Logical architecture for the autonomous vehicle platform](img/B21183_07_1.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 自动驾驶车辆平台的逻辑架构](img/B21183_07_1.jpg)'
- en: Figure 7.1 – Logical architecture for the autonomous vehicle platform
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 自动驾驶车辆平台的逻辑架构
- en: The platform will need a frontend, which will be a web UI built using ASP.NET
    Core Blazor. The frontend will be powered by a REST API backend, which will be
    built using ASP.NET Core Web API. Having our core functionality encapsulated into
    a REST API will allow autonomous vehicles to communicate directly with the platform
    and allow us to expand by adding client interfaces with additional frontend technologies
    such as native mobile apps and virtual or mixed reality in the future. The backend
    will use a PostgreSQL database for persistent storage since it’s lightweight,
    industry-standard, and relatively inexpensive.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 平台将需要一个前端，这将是一个使用 ASP.NET Core Blazor 构建的 Web 用户界面。前端将由一个 REST API 后端提供支持，该后端将使用
    ASP.NET Core Web API 构建。将核心功能封装到 REST API 中将允许自动驾驶车辆直接与平台通信，并使我们能够通过将客户端接口与其他前端技术（如原生移动应用和未来的虚拟或混合现实）结合，进行扩展。后端将使用
    PostgreSQL 数据库进行持久存储，因为它轻量、符合行业标准且相对便宜。
- en: Designing the solution
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Due to the tight timelines the team is facing, we want to keep the cloud architecture
    simple. Therefore, we’ll be keeping it simple and using tried and tested services
    from AWS to implement the platform as opposed to trying to learn something new.
    The first decision we have to make is what AWS service each component of our logical
    architecture will be hosted on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于团队面临紧迫的时间表，我们希望保持云架构的简单性。因此，我们将保持简单，并使用 AWS 的经过验证的服务来实现平台，而不是尝试学习新东西。我们需要做出的第一个决定是每个逻辑架构组件将托管在哪个
    AWS 服务上。
- en: 'Our application architecture consists of three components: a frontend, a backend,
    and a database. The frontend and backend are application components and need to
    be hosted on a cloud service that provides general computing, while the database
    needs to be hosted on a cloud database service. There are many options for both
    types of services:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用架构由三个组件组成：前端、后端和数据库。前端和后端是应用组件，需要托管在提供通用计算的云服务上，而数据库则需要托管在云数据库服务上。两种类型的服务都有很多选择：
- en: '![Figure 7.2 – Logical architecture for the autonomous vehicle platform](img/B21183_07_2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 自动驾驶车辆平台的逻辑架构](img/B21183_07_2.jpg)'
- en: Figure 7.2 – Logical architecture for the autonomous vehicle platform
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 自动驾驶车辆平台的逻辑架构
- en: 'Since we’ve decided that we’re going to use VMs to host our application, we
    have narrowed down the different services that we can use to host our application,
    and we have decided that AWS EC2 is the ideal choice for our current situation.
    There are other options, such as **Elastic Beanstalk**, that also use VMs, but
    we want to have total control over the solution and maintain as many cross-platform
    capabilities as possible in case we ever have to migrate to a different cloud
    platform:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经决定使用虚拟机（VM）来托管我们的应用，因此我们已缩小了可以用来托管应用的不同服务的选择范围，并决定 AWS EC2 是我们当前情况的理想选择。还有其他选项，例如**Elastic
    Beanstalk**，它们也使用虚拟机，但我们希望对解决方案有完全的控制权，并尽可能保持跨平台的能力，以防我们未来需要迁移到不同的云平台：
- en: '![Figure 7.3 – Source control structure of our repository](img/B21183_07_3..jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 我们仓库的源代码控制结构](img/B21183_07_3..jpg)'
- en: Figure 7.3 – Source control structure of our repository
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 我们仓库的源代码控制结构
- en: This solution will consist of six parts. We still have the application code
    and Packer templates for both the frontend and backend. Then, we have GitHub Actions
    to implement our CI/CD process and Terraform to provision our AWS infrastructure
    and reference the Packer-built VM images for our EC2 instances.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案将由六个部分组成。我们仍然有前端和后端的应用代码以及 Packer 模板。接着，我们有 GitHub Actions 来实现我们的 CI/CD
    流程，以及使用 Terraform 来配置我们的 AWS 基础设施，并引用 Packer 构建的 VM 镜像来部署我们的 EC2 实例。
- en: Cloud architecture
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云架构
- en: 'The first part of our design is adapting our solution’s architecture to the
    target cloud platform: AWS. This involves mapping application architecture components
    to AWS services and thinking through the configuration of those services so that
    they meet the requirements of our solution.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计的第一部分是将我们解决方案的架构适配到目标云平台：AWS。这涉及到将应用架构组件映射到 AWS 服务，并深入思考这些服务的配置，以确保它们符合我们解决方案的要求。
- en: Virtual network
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: 'VMs must be deployed within a virtual network. On AWS, we use the AWS EC2 service
    to provide our VMs, and we use AWS **Virtual Private Cloud** (**VPC**) to provide
    our virtual network. When working on AWS, the term *EC2 instance* is used interchangeably
    with the term *virtual machine*. Likewise, the term *VPC* is used interchangeably
    with the term *virtual network*. In this book, I will try to use industry-standard
    terminology wherever possible. You should get in the habit of thinking this way
    as this will allow your knowledge and skills to better transition between the
    different cloud platforms:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机必须部署在虚拟网络内。在 AWS 上，我们使用 AWS EC2 服务提供虚拟机，并使用 AWS **虚拟私有云**（**VPC**）提供虚拟网络。在
    AWS 上工作时，术语*EC2 实例*与*虚拟机*可以互换使用。同样，术语*VPC*也可以与*虚拟网络*互换使用。在本书中，我将尽量使用行业标准术语。你应该养成这种思维习惯，因为这将使你在不同的云平台之间更好地过渡：
- en: '![Figure 7.4 – AWS virtual network architecture](img/B21183_07_4..jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – AWS 虚拟网络架构](img/B21183_07_4..jpg)'
- en: Figure 7.4 – AWS virtual network architecture
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – AWS 虚拟网络架构
- en: As we’ve discussed previously, a virtual network is divided into a set of subnets.
    On AWS, a virtual network is scoped to a specific region, and a subnet is scoped
    to an Availability Zone within that region. Therefore, to build highly available
    systems on AWS, we must distribute our workloads across multiple Availability
    Zones. Therefore, if one Availability Zone experiences an outage, our workload,
    when deployed into the other Availability Zone, will prevent disruption to the
    end users.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所讨论的，虚拟网络被划分为一组子网。在 AWS 上，虚拟网络的范围限定在特定的区域内，而子网的范围限定在该区域内的可用区。因此，为了在 AWS
    上构建高可用的系统，我们必须将工作负载分布在多个可用区。如果一个可用区发生故障，我们将工作负载部署到另一个可用区，以避免对最终用户的干扰。
- en: 'Our application’s VMs need to be provisioned into subnets within a virtual
    network. The frontend of our application needs to be accessible over the internet,
    while the backend only needs to be accessible to the frontend. Therefore, we should
    provision separate subnets for the internet-accessible frontend and our private
    backend. This is a common pattern when it comes to creating *public* and *private*
    subnets:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用程序的虚拟机需要在虚拟网络中的子网中进行部署。我们的应用程序前端需要可以通过互联网访问，而后端只需要对前端可访问。因此，我们应该为可通过互联网访问的前端和私有后端分别配置子网。这是创建*公有*和*私有*子网的常见模式：
- en: '![Figure 7.5 – Public and private subnets for the frontend and backend application
    components](img/B21183_07_5..jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 前端和后端应用组件的公有子网与私有子网](img/B21183_07_5..jpg)'
- en: Figure 7.5 – Public and private subnets for the frontend and backend application
    components
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 前端和后端应用组件的公有子网与私有子网
- en: In this pattern, two pairs of public and private subnets are created. Each pair
    is provisioned in the same Availability Zone. The reason why each pair shares
    the same Availability Zone is due to the dependency between the frontend and the
    backend. For example, if there is an outage affecting the Availability Zone of
    the backend, the frontend won’t be able to operate. Likewise, if there is an outage
    affecting the Availability Zone of the frontend, no traffic will be routed to
    the backend. We can create as many pairs of these public/private subnets as there
    are Availability Zones within a region. Most regions have four to five Availability
    Zones, but usually, two to three Availability Zones are sufficient for most workloads.
    After that, you are more likely to benefit from setting up a multi-region deployment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，创建了两对公有子网和私有子网。每一对都部署在相同的可用区内。每对子网共享同一可用区的原因是前端和后端之间存在依赖关系。例如，如果后端所在的可用区发生故障，前端将无法运行。同样，如果前端所在的可用区发生故障，将无法将流量路由到后端。我们可以根据区域内的可用区数量创建任意多对公有/私有子网。大多数区域有四到五个可用区，但通常，两个到三个可用区就足以应对大部分工作负载。之后，您更有可能从设置多区域部署中受益。
- en: Network routing
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络路由
- en: 'There are a few other components that we need to set up within this virtual
    network to enable our VMs to function properly. In AWS, when you provision a VM
    in a virtual network, you won’t have internet access! For most connected applications,
    internet access is required to allow connectivity to third-party services. Without
    this, operators would be inconvenienced as they would be unable to perform operating
    system upgrades and patches using internet-hosted package repositories:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他组件需要在此虚拟网络中进行配置，以确保虚拟机能够正常运行。在 AWS 中，当你在虚拟网络中配置虚拟机时，虚拟机将无法访问互联网！对于大多数连接的应用程序来说，互联网访问是必需的，因为它允许连接到第三方服务。没有这个功能，操作员将会面临不便，因为他们无法使用托管在互联网上的软件包仓库来进行操作系统的升级和修补：
- en: '![Figure 7.6 – Internet and NAT gateways enable internet access for VMs within
    the subnets](img/B21183_07_6..jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 互联网和 NAT 网关为子网内的虚拟机提供互联网访问](img/B21183_07_6..jpg)'
- en: Figure 7.6 – Internet and NAT gateways enable internet access for VMs within
    the subnets
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 互联网和 NAT 网关为子网内的虚拟机提供互联网访问
- en: 'The internet gateway is attached to the virtual network at the region level,
    providing internet access to the entire VPC, while the NAT gateways are deployed
    into each public subnet at the Availability Zone level to allow EC2 instances
    in private subnets to access the internet without being directly accessible from
    the internet. Each NAT gateway also needs its own static public IP address to
    grant access. This can be achieved by using the Elastic IP service on AWS:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网网关连接到区域级别的虚拟网络，为整个 VPC 提供互联网访问，而 NAT 网关则部署在每个公有子网的可用区级别，允许私有子网中的 EC2 实例访问互联网，同时不会直接暴露给互联网。每个
    NAT 网关还需要拥有自己的静态公共 IP 地址才能提供访问权限。这可以通过使用 AWS 的弹性 IP 服务来实现：
- en: '![Figure 7.7 – Route tables associated with the subnets that direct traffic
    to the correct gateway](img/B21183_07_7..jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 路由表与子网关联，将流量引导到正确的网关](img/B21183_07_7..jpg)'
- en: Figure 7.7 – Route tables associated with the subnets that direct traffic to
    the correct gateway
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 路由表与子网关联，将流量引导到正确的网关
- en: The final step in establishing internet access to our VMs in private subnets
    is routing internet-bound traffic to the correct NAT gateway for each subnet;
    VMs in public subnets can directly access the internet. This can be done using
    route tables. In the public subnet, we route internet traffic to the internet
    gateway. In the private subnet, we route internet traffic to the NAT gateway.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在为私有子网中的虚拟机建立互联网访问的最后一步中，我们需要将面向互联网的流量路由到每个子网的正确 NAT 网关；而公有子网中的虚拟机可以直接访问互联网。这可以通过路由表来实现。在公有子网中，我们将互联网流量路由到互联网网关；在私有子网中，我们将互联网流量路由到
    NAT 网关。
- en: Load balancing
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'Now that our subnets have been set up and connected using proper routing tables,
    we can provision our VMs. To achieve high availability, we need at least one VM
    to be provisioned for each subnet for both the frontend and the backend of our
    solution. We can increase the number of VMs in each subnet to achieve even more
    reliability or scale:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的子网已经设置完毕，并通过适当的路由表进行了连接，我们可以开始配置虚拟机（VMs）。为了实现高可用性，我们需要为每个子网配置至少一台虚拟机，前端和后端都需要如此。我们可以通过增加每个子网中的虚拟机数量来实现更高的可靠性或扩展性：
- en: '![Figure 7.8 – VMs provisioned for our virtual network](img/B21183_07_8..jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 为我们的虚拟网络配置的虚拟机](img/B21183_07_8..jpg)'
- en: Figure 7.8 – VMs provisioned for our virtual network
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 为我们的虚拟网络配置的虚拟机
- en: The problem with the current design is that we need a way for our system to
    respond correctly to an outage affecting one of our Availability Zones. This is
    where a load balancer comes in. It allows us to get the double benefit of routing
    traffic to healthy endpoints and distributing the load evenly across our resources.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当前设计的问题在于，我们需要一种方式让系统能够正确响应影响其中一个可用区的故障。这时，负载均衡器发挥了作用。它允许我们获得双重好处：将流量路由到健康的终端，并将负载均匀分配到我们的资源上。
- en: 'In AWS, the **Application Load Balancer** (**ALB**) service performs this function.
    The load balancer’s job is to be the single point of contact for clients to send
    requests to. The load balancer then forwards that traffic to VMs and routes the
    corresponding responses back to the client from where the request originated:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 中，**应用负载均衡器**（**ALB**）服务执行此功能。负载均衡器的任务是作为客户端发送请求的单一接入点。负载均衡器随后将流量转发到虚拟机，并将相应的响应从请求来源返回给客户端：
- en: '![Figure 7.9 – Load balancer forwarding traffic to VMs across Availability
    Zones](img/B21183_07_9..jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 负载均衡器将流量转发到跨可用区的虚拟机](img/B21183_07_9..jpg)'
- en: Figure 7.9 – Load balancer forwarding traffic to VMs across Availability Zones
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 负载均衡器将流量转发到跨可用区的虚拟机
- en: In the AWS ALB, the first thing we need to set up is the listener. On this listener,
    you specify a port, a protocol, and one or more actions you want to perform when
    a request is received. The most basic type of action is to forward the request
    to a target group.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS ALB中，我们首先需要设置监听器。在这个监听器上，你需要指定端口、协议以及在接收到请求时希望执行的一个或多个操作。最基本的操作类型是将请求转发到目标组。
- en: In our solution, the target group will consist of a set of VMs. The target group
    specifies what port and protocol the request should be sent to, as well as a health
    probe with a specific application path. The health probe can optionally be set
    up on a different port and protocol, where it provides several different settings
    to control how frequently it should be probed and how to evaluate whether the
    endpoint is healthy or unhealthy. Healthy is usually indicated by an HTTP status
    code of `200`. Anything else is considered unhealthy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，目标组将由一组虚拟机（VM）组成。目标组指定请求应该发送到哪个端口和协议，以及一个具有特定应用路径的健康探测。健康探测可以选择性地设置在不同的端口和协议上，它提供了几个不同的设置来控制探测频率以及如何评估端点是否健康或不健康。健康通常通过HTTP状态码`200`表示，其他任何情况都被视为不健康。
- en: For both our frontend and backend, we have a simple set of VMs for the target
    group with an endpoint configured for the HTTP protocol on port `5000` (the default
    port for ASP.NET Core).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的前端和后端，我们为目标组设置了一组简单的虚拟机，端点配置为HTTP协议，端口为`5000`（ASP.NET Core的默认端口）。
- en: The frontend is an **ASP.NET Core Blazor** application. As a result, it uses
    **SignalR** (which abstracts WebSocket communication) to provide real-time connectivity
    between the web browser and the server. As a result, we need to enable sticky
    sessions so that this can function properly. Sticky sessions will allow the client
    to continue to use the same VM, thus allowing the WebSocket to stay alive and
    not be disrupted by changing which web server it communicates with.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前端是**ASP.NET Core Blazor**应用程序。因此，它使用**SignalR**（抽象了WebSocket通信）来提供浏览器和服务器之间的实时连接。因此，我们需要启用粘性会话，以便其能够正常工作。粘性会话将允许客户端继续使用相同的虚拟机，从而保持WebSocket连接不受更改与哪个Web服务器通信的影响。
- en: For the health probe, the frontend will use the root path of the web application,
    while the backend will use a special path that routes to a controller that’s been
    configured to respond to the health probe.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于健康探测，前端将使用Web应用程序的根路径，而后端将使用一个特殊路径，该路径会路由到一个已配置响应健康探测的控制器。
- en: Network security
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: Now that our virtual network has been fully configured and our VMs have been
    set up behind load balancers, we need to think about what network traffic we want
    to allow through the system. In AWS, this can be controlled by creating security
    groups, which allow traffic to be sent between components of your architecture
    on specific ports using specific protocols.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的虚拟网络已经完全配置好，虚拟机也已经设置在负载均衡器后面，我们需要思考希望允许哪些网络流量通过系统。在AWS中，可以通过创建安全组来控制这一点，安全组允许在特定端口和协议上，组件之间的流量传输。
- en: 'The first step in this process is to think through the logical stops for our
    network traffic as it makes its way through our solution:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的第一步是思考网络流量在通过解决方案时的逻辑停靠点：
- en: '![Figure 7.10 – Logical components of our architecture](img/B21183_07_10..jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 我们架构的逻辑组件](img/B21183_07_10..jpg)'
- en: Figure 7.10 – Logical components of our architecture
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 我们架构的逻辑组件
- en: The application components, including the frontend and the backend, are on this
    list, followed by the database. However, these aren’t the only places where our
    network traffic flows. Since we introduced load balancers in front of both the
    frontend and the backend, we have two additional stops for network traffic.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 应用组件，包括前端和后端，列在此列表中，后面是数据库。然而，这些并不是我们网络流量流动的唯一地方。由于我们在前端和后端前面引入了负载均衡器，因此我们有两个额外的网络流量停靠点。
- en: 'The next step is to think about how each component communicates with others.
    This includes both the port and protocol but also the direction of the traffic.
    To do this, we need to think about the network traffic from the perspective of
    each component:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是思考每个组件如何与其他组件通信。这不仅包括端口和协议，还包括流量的方向。为此，我们需要从每个组件的角度思考网络流量：
- en: '![Figure 7.11 – Frontend load balancer network traffic flow](img/B21183_07_11..jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 前端负载均衡器网络流量](img/B21183_07_11..jpg)'
- en: Figure 7.11 – Frontend load balancer network traffic flow
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 前端负载均衡器网络流量
- en: 'From the perspective of the frontend load balancer, we’ll be receiving traffic
    from the internet on port `80` using the HTTP protocol. This inbound traffic is
    called `5000` using the HTTP protocol. This outbound traffic is called **egress**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从前端负载均衡器的角度来看，我们将通过端口`80`使用HTTP协议接收来自互联网的流量。这些入站流量称为`5000`，使用HTTP协议。这些出站流量称为**egress**：
- en: '![Figure 7.12 – Frontend network traffic flow](img/B21183_07_12..jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 前端网络流量](img/B21183_07_12..jpg)'
- en: Figure 7.12 – Frontend network traffic flow
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 前端网络流量
- en: 'From the perspective of the frontend, we’ll be receiving traffic from the frontend
    load balancer on port `5000` using the HTTP protocol. The C# application code
    will make requests to the REST web API hosted in the backend, but we’ll be routing
    all our requests to the backend through the backend load balancer on port `80`
    using the HTTP protocol:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从前端的角度来看，我们将通过端口`5000`使用HTTP协议接收来自前端负载均衡器的流量。C#应用程序代码将向后端托管的REST Web API发出请求，但我们将通过后端负载均衡器将所有请求通过端口`80`使用HTTP协议路由到后端：
- en: '![Figure 7.13 – Backend load balancer network traffic flow](img/B21183_07_13..jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 后端负载均衡器网络流量](img/B21183_07_13..jpg)'
- en: Figure 7.13 – Backend load balancer network traffic flow
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 后端负载均衡器网络流量
- en: 'From the perspective of the backend load balancer, we’ll be receiving traffic
    from the frontend on port `80` using the HTTP protocol. Due to the target group
    configuration, we’ll be forwarding those requests to the backend on port `5000`
    using the HTTP protocol:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从后端负载均衡器的角度来看，我们将通过端口`80`使用HTTP协议接收来自前端的流量。由于目标组配置，我们将把这些请求转发到后端的端口`5000`，并使用HTTP协议：
- en: '![Figure 7.14 – Backend network traffic flow](img/B21183_07_14..jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 后端网络流量](img/B21183_07_14..jpg)'
- en: Figure 7.14 – Backend network traffic flow
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 后端网络流量
- en: From the perspective of the backend, we’ll be receiving traffic from the backend
    load balancer on port `5000` using the HTTP protocol. The C# application code
    will be making requests to the PostgreSQL database on port `5432` using the HTTPS
    protocol.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从后端的角度来看，我们将通过端口`5000`使用HTTP协议接收来自后端负载均衡器的流量。C#应用程序代码将使用HTTPS协议向PostgreSQL数据库发出请求，端口为`5432`。
- en: Secrets management
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 秘密管理
- en: 'Secrets such as database credentials or service access keys need to be stored
    securely. Each cloud platform has a service that provides this functionality.
    On AWS, this service is called **AWS** **Secrets Manager**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 像数据库凭证或服务访问密钥这样的秘密需要安全存储。每个云平台都有提供此功能的服务。在AWS上，这项服务称为**AWS** **Secrets Manager**：
- en: '![Figure 7.15 – Secrets stored in AWS Secrets Manager can be accessed by VMs
    once they have the necessary IAM privileges](img/B21183_07_15..jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 存储在AWS Secrets Manager中的秘密可以在虚拟机具备必要的IAM权限后进行访问](img/B21183_07_15..jpg)'
- en: Figure 7.15 – Secrets stored in AWS Secrets Manager can be accessed by VMs once
    they have the necessary IAM privileges
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 存储在AWS Secrets Manager中的秘密可以在虚拟机具备必要的IAM权限后进行访问
- en: 'You simply create secrets on this service using a consistent naming convention,
    then construct an IAM role that has permission to access these secrets. The following
    IAM policy will grant permission to just secrets that start with `fleetportal/`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需在此服务上使用一致的命名约定创建秘密，然后构建一个具有访问这些秘密权限的IAM角色。以下IAM策略将仅授予以`fleetportal/`开头的秘密的权限：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The values for `region` and `account-id` will need to be altered to reflect
    where the secrets were created. It’s important to note that an AWS account is
    typically used as a security boundary for an application and an environment. So,
    we would likely have separate AWS accounts for our solution’s development and
    production environments, as well as any other environments we may need. This will
    isolate our secrets manager secrets within the context of the AWS account and
    the region.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`region`和`account-id`的值需要更改，以反映秘密创建的地点。需要注意的是，AWS账户通常作为应用程序和环境的安全边界。因此，我们可能会为解决方案的开发和生产环境使用不同的AWS账户，此外还有可能需要的其他环境。这将使我们的秘密管理器的秘密在AWS账户和区域的上下文中得到隔离。'
- en: The two main attributes we use to grant permissions are `action` and `resource`.
    When implementing the principle of least privilege, it’s important to be as specific
    as possible about the actions that are required for a particular identity. If
    access is not required, don’t grant it. Likewise, we should ensure the resources
    we grant these permissions to are as narrow as possible. It’s easy to be lazy
    and leave `*` in the resources or the actions. Still, we need to be aware that
    a malicious attacker could use overly generous permissions to move laterally within
    our environments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来授予权限的两个主要属性是`action`和`resource`。在实施最小权限原则时，尽可能具体地定义某个身份所需的操作非常重要。如果不需要访问，就不要授予。类似地，我们还应该确保授予这些权限的资源尽可能狭窄。很容易懒惰，留下`*`作为资源或操作，但我们需要意识到，恶意攻击者可能会利用过于宽泛的权限在我们的环境中横向移动。
- en: VMs
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机
- en: 'Now that we have everything we need for our solution, we can finish by talking
    about where our application components will run: in VMs that have been provisioned
    using AWS EC2.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有解决方案所需的内容，接下来可以讨论我们的应用组件将运行在哪里：在使用 AWS EC2 配置的虚拟机中。
- en: When provisioning VMs on AWS, you have two options. First, you can provide static
    VMs. In this approach, you need to specify key characteristics for every VM. Alternatively,
    you can use an **AWS Auto Scaling group** to dynamically provision and manage
    the VMs. In this approach, you provide the Auto Scaling group with some configuration
    and parameters on when to scale up and when to scale down, at which point the
    Auto Scaling group will take care of everything else.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上配置虚拟机（VM）时，您有两种选择。首先，您可以提供静态虚拟机。在这种方式中，您需要为每个虚拟机指定关键特性。或者，您可以使用**AWS
    自动扩展组**来动态配置和管理虚拟机。在这种方式中，您提供自动扩展组一些配置和参数，说明何时进行扩展，何时进行缩减，然后自动扩展组将处理其余的所有事情。
- en: When provisioning a static VM on AWS, you need to associate it with an **AWS
    key pair** to ensure that you can connect to its operating system. This will allow
    your operators to perform diagnostics and update or patch the software and operating
    system.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上配置静态虚拟机时，您需要将其与**AWS 密钥对**关联，以确保您可以连接到其操作系统。这将允许操作员执行诊断、更新或修补软件和操作系统。
- en: All VMs need to be connected to a virtual network, so when you set up a static
    VM, you need to specify the network configuration. This can be accomplished by
    creating a network interface and associating it with the VM. The network interface
    connects the VM to the appropriate subnet, which is the place where you attach
    one or more security groups.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所有虚拟机都需要连接到虚拟网络，因此，当您设置静态虚拟机时，您需要指定网络配置。这可以通过创建网络接口并将其与虚拟机关联来实现。网络接口将虚拟机连接到适当的子网，子网是您附加一个或多个安全组的地方。
- en: 'The internal configuration of your VM is controlled by two critical attributes:
    the VM image and the user data. As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239),
    the VM image can either be a vanilla installation of an operating system or it
    can be a fully configured version of your application. The decision of **build
    versus bake** is up to you.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您虚拟机的内部配置由两个关键属性控制：虚拟机镜像和用户数据。正如我们在[*第 4 章*](B21183_04.xhtml#_idTextAnchor239)中讨论的那样，虚拟机镜像可以是操作系统的原始安装版本，也可以是您的应用程序的完全配置版本。**构建与烘焙**的决定由您决定。
- en: 'User data allows you to run the *last mile* configuration when the VM starts
    up. This can be done using industry-standard `cloud-init` configuration to perform
    a wide variety of tasks such as setting up users/groups, setting up environment
    variables, or mounting disks:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 用户数据允许您在虚拟机启动时执行*最后一公里*配置。这可以通过使用业界标准的`cloud-init`配置来完成，执行各种任务，例如设置用户/组、设置环境变量或挂载磁盘：
- en: '![Figure 7.16 – Resource VMs created statically](img/B21183_07_16..jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – 静态创建的资源虚拟机](img/B21183_07_16..jpg)'
- en: Figure 7.16 – Resource VMs created statically
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 静态创建的资源虚拟机
- en: 'AWS can dynamically manage your VMs based on the load that they incur. This
    is done using an Auto Scaling group. This Auto Scaling group is responsible for
    provisioning the VMs. Consequently, this means that the Auto Scaling group needs
    to have the key characteristics that define your VM set on its launch template.
    The Auto Scaling group uses this launch template to specify the configuration
    of each VM that it provisions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 可以根据虚拟机负载的变化动态管理你的虚拟机。这是通过 Auto Scaling 组来完成的。Auto Scaling 组负责虚拟机的配置，因此，Auto
    Scaling 组需要在启动模板中定义虚拟机集的关键特性。Auto Scaling 组使用该启动模板来指定每个虚拟机的配置：
- en: '![Figure 7.17 – VMs created and managed dynamically using an Auto-Scaling Group](img/B21183_07_17..jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – 使用 Auto-Scaling 组动态创建和管理的虚拟机](img/B21183_07_17..jpg)'
- en: Figure 7.17 – VMs created and managed dynamically using an Auto-Scaling Group
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 使用 Auto-Scaling 组动态创建和管理的虚拟机
- en: Besides this launch template, the Auto Scaling group simply needs to be told
    what subnets the VMs should be provisioned into and under what circumstances it
    should provision or de-provision VMs from the set that it actively manages.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个启动模板，Auto Scaling 组只需要告知哪些子网应该为虚拟机（VM）提供服务，以及在什么情况下它应该为当前管理的虚拟机集群提供或移除虚拟机。
- en: Monitoring
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控
- en: AWS has a cross-cutting service called **CloudWatch** that can capture logs
    and telemetry from various AWS services you consume within your solutions. We’ll
    be using this as the primary logging mechanism within this book. Many services
    support CloudWatch out of the box with minimal to no configuration to get it working.
    At the same time, other services and scenarios require permissions to be granted
    to allow that service to log in to CloudWatch.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 有一个跨服务的工具叫做 **CloudWatch**，它可以捕获你在解决方案中使用的各种 AWS 服务的日志和遥测数据。在本书中，我们将使用它作为主要的日志记录机制。许多服务开箱即用支持
    CloudWatch，几乎不需要配置就能开始使用。同时，其他服务和场景则需要授权才能让该服务在 CloudWatch 中记录日志。
- en: Deployment architecture
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署架构
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on AWS, we need to come up with a plan for how to provision
    our environments and deploy our code.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对解决方案在 AWS 上的云架构有了清晰的了解，我们需要制定一个计划，来决定如何配置环境和部署我们的代码。
- en: VM configuration
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机配置
- en: 'In our solution, we have two VM roles: the frontend role, which is responsible
    for handling web page requests from the end user’s web browser, and the backend
    role, which is responsible for handling REST API requests from the web application.
    Each of these roles has different code and a different configuration that needs
    to be set. Each will require its own Packer template to build a VM image that
    we can use to launch a VM on AWS:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，有两个虚拟机角色：前端角色负责处理终端用户浏览器的网页请求，后端角色负责处理来自 Web 应用程序的 REST API 请求。这些角色有不同的代码和配置，且每个角色需要单独的
    Packer 模板来构建一个虚拟机镜像，我们可以用它来在 AWS 上启动虚拟机：
- en: '![Figure 7.18 – Packer pipeline to build a VM image for the frontend](img/B21183_07_18..jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.18 – 使用 Packer 管道构建前端的虚拟机镜像](img/B21183_07_18..jpg)'
- en: Figure 7.18 – Packer pipeline to build a VM image for the frontend
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – 使用 Packer 管道构建前端的虚拟机镜像
- en: A GitHub Actions workflow that triggers off changes to the frontend application
    code and the frontend Packer template will execute `packer build` and create a
    new VM image for the solution’s frontend.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 GitHub Actions 工作流会根据前端应用程序代码和前端 Packer 模板的变化触发执行 `packer build`，并为解决方案的前端创建一个新的虚拟机镜像。
- en: 'Both the frontend and the backend will have an identical GitHub workflow that
    executes `packer build`. The key difference between the workflows is the code
    bases that they execute against. Both the frontend and the backend might have
    slightly different operating system configurations, and both will require different
    deployment packages for their respective application components:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前端和后端都会有一个相同的 GitHub 工作流来执行 `packer build`。工作流之间的主要区别在于它们执行的代码库。前端和后端可能有稍微不同的操作系统配置，并且都需要不同的部署包来处理各自的应用程序组件：
- en: '![Figure 7.19 – Packer pipeline to build a VM image for the backend](img/B21183_07_19..jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.19 – 使用 Packer 管道构建后端的 VM 镜像](img/B21183_07_19..jpg)'
- en: Figure 7.19 – Packer pipeline to build a VM image for the backend
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 使用 Packer 管道构建后端的 VM 镜像
- en: It’s important to note that the application code will be baked into the VM image
    rather than copied to an already running VM. This means that to update the software
    running on the VMs, each VM will need to be restarted so that it has a new VM
    image containing the latest copy of the code.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，应用程序代码将被集成到虚拟机（VM）镜像中，而不是复制到已经运行的虚拟机中。这意味着，为了更新运行在虚拟机上的软件，每个虚拟机需要重启，以便使用包含最新代码副本的新虚拟机镜像。
- en: This approach makes the VM image an immutable deployment artifact that is versioned
    and updated each time there is a release of the application code that needs to
    be deployed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使得虚拟机镜像成为一个不可变的部署工件，每次发布需要部署的应用程序代码时，它都会被版本化和更新。
- en: Cloud environment configuration
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云环境配置
- en: 'Once the VM images have been built for both the frontend and the backend, we
    can execute the final workflow, which will both provision and deploy our solution
    to AWS:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为前端和后端构建完成虚拟机镜像，我们就可以执行最终工作流，既配置又部署我们的解决方案到 AWS：
- en: '![Figure 7.20 – VM images are used as inputs to the Terraform code, which provisions
    the environment on AWS](img/B21183_07_20..jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.20 – 虚拟机镜像作为 Terraform 代码的输入，Terraform 在 AWS 上配置环境](img/B21183_07_20..jpg)'
- en: Figure 7.20 – VM images are used as inputs to the Terraform code, which provisions
    the environment on AWS
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – 虚拟机镜像作为 Terraform 代码的输入，Terraform 在 AWS 上配置环境
- en: The Terraform code base will have two input variables for the version of the
    VM image for both the frontend and the backend. When new versions of the application
    software need to be deployed, the input parameters for these versions will be
    incremented to reflect the target version for deployment. When the workflow is
    executed, `terraform apply` will simply replace the existing VMs with VMs using
    the new VM image.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform 代码库将包含两个输入变量，分别对应前端和后端的虚拟机镜像版本。当需要部署应用软件的新版本时，这些版本的输入参数将递增，以反映目标版本。工作流执行时，`terraform
    apply`将用新的虚拟机镜像替换现有虚拟机。
- en: Now that we have a solid plan for how we will implement both the cloud architecture
    using AWS and the deployment architecture using GitHub Actions, let’s start building!
    In the next section, we’ll break down the HashiCorp configuration language code
    that we used to implement the Terraform and Packer solutions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了如何使用 AWS 实现云架构和使用 GitHub Actions 实现部署架构的明确计划，接下来就开始构建吧！在下一节中，我们将详细解析用于实现
    Terraform 和 Packer 解决方案的 HashiCorp 配置语言代码。
- en: Building the solution
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: With our design in place, all we need to do is write the code that implements
    the design.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 设计已定，接下来我们只需编写实现该设计的代码。
- en: Packer
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Packer
- en: Our solution has a frontend and a backend application component. Although the
    application code is radically different, the way we build a VM image is not.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案包含前端和后端应用程序组件。尽管应用程序代码有很大不同，但我们构建虚拟机镜像的方式是相同的。
- en: AWS plugin
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS 插件
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), Packer
    – like Terraform – is an extensible command-line executable. Each cloud platform
    provides a plugin for Packer that encapsulates the integration with its services:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第4章*](B21183_04.xhtml#_idTextAnchor239)中讨论的，Packer——与 Terraform 类似——是一个可扩展的命令行可执行文件。每个云平台为
    Packer 提供一个插件，用于封装与其服务的集成：
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Plugins need to be declared within a Packer solution. At the time of writing,
    the latest version of the AWS Packer plugin is `1.2.6`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 插件需要在 Packer 解决方案中声明。截止目前，AWS Packer 插件的最新版本是`1.2.6`。
- en: 'The AWS plugin for Packer provides an `amazon-ebs` builder that will generate
    an AMI by creating a new VM from a base image, executing the provisioners, taking
    an **Elastic Block Store** (**EBS**) disk image snapshot, and creating an **Amazon
    Machine Image** (**AMI**) from it. This behavior is controlled by the Amazon builder:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Packer 插件提供了一个`amazon-ebs`构建器，它通过从基础镜像创建一个新虚拟机、执行配置程序、拍摄**弹性块存储**（**EBS**）磁盘镜像快照，并从中创建**Amazon
    Machine Image**（**AMI**），来生成一个 AMI。此行为由 Amazon 构建器控制：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The first input to the Amazon `amazon-ebs` builder is the base image to use
    when creating the initial VM against which the Packer template’s provisioners
    will be executed. The preceding code references the latest version of the Ubuntu
    `22.04` VM image within the target AWS region:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon `amazon-ebs` 构建器的第一个输入是创建初始虚拟机时所使用的基础镜像，Packer 模板的配置程序将在该虚拟机上执行。上述代码引用了目标
    AWS 区域内最新版本的 Ubuntu `22.04` 虚拟机镜像：
- en: '[PRE3]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `amazon-ebs` builder references the `amazon-ami` data source to ensure that
    the correct base image is used before the provisioners are executed. Here, `ami_name`
    is probably the most important attribute on this block as it dictates the version
    name that the VM image will be referenced by in `terraform` `apply` operations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`amazon-ebs`构建器引用`amazon-ami`数据源，以确保在执行提供者之前使用正确的基础镜像。在此，`ami_name`可能是该块中最重要的属性，因为它决定了虚拟机镜像在`terraform`
    `apply`操作中引用的版本名称。'
- en: Operating system configuration
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作系统配置
- en: 'To avoid access control issues, it’s a good idea to establish the context for
    the provisioners to be executed within:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免访问控制问题，最好为要执行的提供者（provisioners）建立一个执行环境：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is a standard `execute_command` parameter that can be used to set the context
    for all provisioners. It allows you to eliminate any unnecessary `sudo` commands
    within your installation scripts. The preceding `execution_command` parameter
    will allow your Packer template scripts to execute as a privileged user.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个标准的`execute_command`参数，可以用来设置所有提供者的执行环境。它使得你可以在安装脚本中消除不必要的`sudo`命令。前面的`execution_command`参数将允许Packer模板脚本以特权用户身份执行。
- en: Our solution is built using ASP.NET Core. Therefore, we need to install .NET
    6.0 SDK for our solution to work properly on the VMs. Ubuntu, like other Debian-based
    distributions of Linux, uses the `apt` command-line application to perform package
    management. By default, Ubuntu includes several public repositories that include
    most of the common software packages. However, sometimes, you need to set up additional
    package repositories when the default repositories don’t work. Microsoft hosts
    a package repository for `apt`, that houses the correct software package we need
    to install .NET 6.0 on Ubuntu. Therefore, we need to add that repository before
    we can use `apt` to install .NET 6.0.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案是使用ASP.NET Core构建的。因此，为了确保解决方案在虚拟机上正常运行，我们需要安装 .NET 6.0 SDK。Ubuntu，像其他基于Debian的Linux发行版一样，使用`apt`命令行应用程序来执行软件包管理。默认情况下，Ubuntu包括了几个公共仓库，这些仓库包含了大多数常见的软件包。然而，有时当默认仓库无法使用时，我们需要设置额外的软件包仓库。微软为`apt`提供了一个软件包仓库，里面包含了我们需要安装
    .NET 6.0 的正确软件包。因此，在使用`apt`安装 .NET 6.0之前，我们需要先添加该仓库。
- en: 'Our Packer template includes a file called `dotnet.pref` that has the following
    contents:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Packer模板包括一个名为`dotnet.pref`的文件，内容如下：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We use the Packer `file` provisioner to copy this file to the correct location
    on the VM:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Packer的`file`提供者将此文件复制到虚拟机的正确位置：
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, we execute the `install-dotnet6-prereq.sh` bash script, which downloads
    a `.deb` file and installs it using the `dpkg` tool. This registers the third-party
    repository hosted by Microsoft with the Debian package management tool.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们执行`install-dotnet6-prereq.sh` Bash脚本，它会下载一个`.deb`文件并使用`dpkg`工具进行安装。这个操作将会通过Debian包管理工具注册微软提供的第三方仓库。
- en: 'Now, we can simply run `apt-get update -y` to get the latest version of the
    packages from all repositories, and we are ready to install .NET 6.0:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以简单地运行`apt-get update -y`，从所有仓库中获取软件包的最新版本，之后就可以准备安装 .NET 6.0了：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If we don’t include the `packages.microsoft.com` repository, then this `apt-get
    install` command will fail with an error message saying that the `dotnet-sdk-6.0`
    package could not be found.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有包括`packages.microsoft.com`仓库，那么`apt-get install`命令将会失败，并显示错误信息，提示找不到`dotnet-sdk-6.0`软件包。
- en: Setting up a service in Linux
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Linux中设置服务
- en: 'Most applications run as processes within Linux that run perpetually. This
    is often the case when the application needs to listen for network traffic – such
    as a web server. Another great benefit of setting up a service in Linux is that
    the operating system can auto-start the service every time the VM reboots. To
    do that, you need to set up a service definition file:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数应用程序在Linux中以进程的形式运行，并且通常是持续运行的。这种情况通常发生在应用程序需要监听网络流量时——比如Web服务器。设置Linux服务的另一个好处是，操作系统可以在每次虚拟机重启时自动启动服务。为此，你需要设置一个服务定义文件：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This service file needs to be copied to the `/etc/systemd/system` folder. By
    running the `systemctl` command, it will be enabled so that the operating system
    will automatically start the service when the machine reboots. The `systemctl`
    command is also useful to `start`, `stop`, and check the `status` value of your
    service.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务文件需要被复制到`/etc/systemd/system`文件夹中。通过运行`systemctl`命令，它会被启用，这样操作系统就会在机器重启时自动启动该服务。`systemctl`命令还可以用于`start`、`stop`以及检查服务的`status`状态。
- en: 'It’s best practice to run services using their own identity. This allows you
    to grant the service access to only the resources on the VM that it needs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是使用其自己的身份运行服务。这样可以仅将服务访问需要的VM上的资源：
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding code sets up a local user and group for the service to run under
    and changes the ownership of the application’s folder at `/var/www/fleet-portal`
    so that the service’s user account has sufficient access to the application’s
    executable and supporting files. Both the user and the application’s working directory
    are specified in the service definition file.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码设置了一个本地用户和组，用于服务运行，并更改了应用程序文件夹 `/var/www/fleet-portal` 的所有者，以便服务的用户帐户有足够的访问权限来访问应用程序的可执行文件和支持文件。用户和应用程序的工作目录都在服务定义文件中指定。
- en: 'Once the user is ready, we can install the service definition file and enable
    the service:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦用户准备好，我们可以安装服务定义文件并启用服务：
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This concludes the operating system configuration, which can be baked into the
    VM image. Any additional configuration steps require more information from the
    cloud environment that Terraform provisions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容涉及操作系统配置，可以集成到虚拟机镜像中。任何额外的配置步骤都需要来自Terraform所提供的云环境的更多信息。
- en: Terraform
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Terraform
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has an application code base that needs to
    be deployed. Since this is the first time we will be using the `aws` provider,
    we’ll look at the basic provider setup and how to configure the backend before
    we look at the nuts and bolts of each component of our architecture.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中讨论的那样，我们的解决方案由两个应用程序组件组成：前端和后端。每个组件都有一个需要部署的应用程序代码库。因为这是我们第一次使用 `aws`
    提供程序，我们将首先查看基本的提供程序设置以及在查看架构的每个组件的细节之前如何配置后端。
- en: Provider setup
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提供者设置
- en: 'We need to specify all the providers that we intend to use in this solution
    within the `required_providers` block:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 `required_providers` 块中指定我们打算在此解决方案中使用的所有提供者：
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We also need to configure the AWS provider to ensure that it uses the desired
    target region using the `primary_region` input variable:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要配置AWS提供程序以确保它使用所需的目标区域，使用 `primary_region` 输入变量：
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Sometimes, you may want to add a secondary region in the future, so it’s a good
    idea to establish the primary region when you start the project. Even if you only
    deploy to one region, you still have a *primary region*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您可能希望在将来添加次要区域，因此在启动项目时建立主要区域是一个好主意。即使您只部署到一个区域，您仍然有一个 *主要区域*。
- en: The AWS provider does require some additional parameters to specify the credentials
    to use to connect to AWS, but because these are sensitive values, we don’t want
    to embed them into the code. We’ll pass those values in later when we automate
    the deployment using the standard AWS `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
    environment variables. It’s important to note that there are many different ways
    to configure the AWS provider to authenticate with AWS. I recommend using environment
    variables as it is a consistent approach across cloud platforms and other Terraform
    providers, and it integrates easily with different pipeline tools, such as GitHub
    Actions, which we’ll be using in the next section and future chapters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供程序确实需要一些额外的参数来指定连接到AWS所使用的凭据，但因为这些是敏感值，我们不想将它们嵌入到代码中。稍后在自动化部署时，我们将传递这些值，使用标准的AWS
    `AWS_ACCESS_KEY_ID` 和 `AWS_SECRET_ACCESS_KEY` 环境变量。需要注意的是，有许多不同的方法可以配置AWS提供程序与AWS进行身份验证。我建议使用环境变量，因为这是跨云平台和其他Terraform提供程序的一种一致方法，并且可以轻松集成到不同的流水线工具中，例如我们将在下一节和将来的章节中使用的GitHub
    Actions。
- en: Backend
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后端
- en: Because we will be using a CI/CD pipeline to provision and maintain our environment
    in the long term, we need to set up a remote backend for our Terraform state.
    Because our solution will be hosted on AWS, we’ll use the AWS **Simple Storage
    Service** (**S3**) backend to store our Terraform state.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将使用CI/CD流水线来长期进行环境的配置和维护，所以我们需要为我们的Terraform状态设置一个远程后端。因为我们的解决方案将托管在AWS上，我们将使用AWS
    **简单存储服务** (**S3**) 后端来存储我们的Terraform状态。
- en: 'Just like the AWS provider, we don’t want to hard code the backend configuration
    in our code, so we’ll simply set up a placeholder for the backend:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 就像AWS提供程序一样，我们不想在代码中硬编码后端配置，因此我们将简单地为后端设置一个占位符：
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We’ll configure the backend’s parameters using the `-backend-config` parameters
    when we run `terraform init` in our CI/CD pipeline.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 CI/CD 管道中运行 `terraform init` 时，使用 `-backend-config` 参数来配置后端的参数。
- en: It’s important to ensure that the AWS IAM identity you use to authenticate with
    AWS has access to this S3 bucket. Otherwise, you will get authentication errors.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 确保用于身份验证的 AWS IAM 身份有权访问该 S3 存储桶非常重要。否则，您将遇到身份验证错误。
- en: Input variables
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入变量
- en: It’s good practice to pass in short names that identify the application’s name
    and the application’s environment. This allows you to embed consistent naming
    conventions across the resources that make up your solution, which makes it easier
    to identify and track resources from the AWS Console.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的做法是传入能够标识应用程序名称和环境的短名称。这可以让您在组成解决方案的资源上嵌入一致的命名规范，从而使您更容易在 AWS 控制台中识别和追踪这些资源。
- en: The `primary_region`, `vpc_cidr_block`, and `az_count` input variables drive
    key architectural characteristics of the deployment. They can’t be hard-coded
    as it would limit the reusability of the Terraform code base.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`primary_region`、`vpc_cidr_block` 和 `az_count` 输入变量驱动部署的关键架构特性。它们不能硬编码，因为那样会限制
    Terraform 代码库的可重用性。'
- en: The `vpc_cidr_block` input variable establishes the virtual network address
    space, which is often tightly regulated by an enterprise governance body. There
    is usually a process to ensure that teams across an organization do not use IP
    address ranges that conflict, thus making it impossible to allow those two applications
    to integrate or integrate with shared network resources within the enterprise
    in the future.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`vpc_cidr_block` 输入变量建立了虚拟网络地址空间，这通常是由企业治理机构严格管理的。通常会有一个流程，确保组织内部的各个团队不会使用冲突的
    IP 地址范围，从而避免未来无法使这两个应用程序集成，或者无法与企业内的共享网络资源进行集成。'
- en: The `az_count` input variable allows us to configure how much redundancy we
    want within our solution. This will affect the high availability of the solution
    but also the cost of the deployment. As you can imagine, cost is also a tightly
    regulated characteristic of cloud infrastructure deployments.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`az_count` 输入变量允许我们配置解决方案中的冗余度。这个设置将影响解决方案的高可用性以及部署的成本。正如你可以想象的那样，成本也是云基础设施部署中严格管控的一个特性。'
- en: Consistent naming and tagging
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一致的命名和标记
- en: 'The AWS console is designed in such a way that it’s rather difficult to get
    an application-centric view of your deployment. This is why it’s extremely important
    to leave breadcrumbs within the resources that you deploy that indicate what application
    and environment they belong to. Almost all resources within the AWS provider have
    a `map` attribute called `tags`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 控制台的设计方式使得从应用程序角度查看部署变得相当困难。因此，确保在部署的资源中留下能够指示它们所属应用程序和环境的线索至关重要。几乎所有 AWS
    提供的资源都拥有一个名为 `tags` 的 `map` 属性：
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You should make a habit of setting both the AWS console-recognized `Name` tag
    and a tagging scheme for your devices that establishes application and environment
    ownership of that resource. For our solution, we use two top-level input variables,
    `application_name` and `environment_name`, to set this context, and we’ll embed
    these values on all the resources that we provision.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该养成习惯，为你的设备设置 AWS 控制台识别的 `Name` 标签，并采用一种标记方案，以确定该资源属于哪个应用程序和环境。对于我们的解决方案，我们使用两个顶级输入变量
    `application_name` 和 `environment_name` 来设定这个上下文，并将这些值嵌入到我们所有的资源中。
- en: 'AWS can create an application-centric view within the AWS console using something
    called a resource group. Unlike on other platforms, a resource group on AWS is
    not a strong boundary around a set of resources but a loosely coupled relationship
    between resources derived from a common tagging scheme:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 可以通过称为资源组的功能，在 AWS 控制台中创建一个应用程序中心的视图。与其他平台不同，AWS 中的资源组并不是一组资源的强边界，而是基于公共标记方案所衍生的资源之间的松耦合关系：
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding code creates an AWS resource group that creates a central location
    where you can access all of your related resources from one place. Simply adding
    `application` and `environment` tags to all your resources will include them.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了一个 AWS 资源组，为您提供了一个中央位置，从一个地方访问所有相关资源。只需在所有资源上添加 `application` 和 `environment`
    标签即可包含它们。
- en: Virtual network
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: Because our solution is a standard three-tier architecture, we are configuring
    our virtual network into public and private subnets for the frontend and backend
    application components.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的解决方案是标准的三层架构，我们将虚拟网络配置为公共和私有子网，分别对应前端和后端应用组件。
- en: 'We want to distribute our VMs across the Availability Zones to ensure the high
    availability of our solution. Rather than hard-code the Availability Zones or
    just take the first two, we can randomly select the number of Availability Zones
    we want from the list of available ones for the given region using the `aws_availability_zones`
    data source and a `random_shuffle` resource from the `random` provider:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将虚拟机分布到不同的可用区，以确保解决方案的高可用性。与其硬编码可用区或仅选择前两个可用区，不如使用`aws_availability_zones`数据源和`random`提供商中的`random_shuffle`资源，从给定区域的可用可用区列表中随机选择所需的可用区数量：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If the `az_count` input variable has a value of `2`, then the preceding code
    will randomly select two Availability Zones from the region of the current AWS
    provider. Remember that the AWS provider is scoped to a particular region, and
    when we initialized the provider, we used the `primary_region` input variable
    to set that value.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`az_count`输入变量的值为`2`，则上述代码将随机选择当前AWS提供商区域中的两个可用区。请记住，AWS提供商是针对特定区域的，当我们初始化提供商时，我们使用`primary_region`输入变量来设置该值。
- en: 'Rather than hard-code the address space for our subnets, it would be nice if
    we could calculate our subnets’ address space using HCL’s built-in functions.
    The `cidrsubnet` function allows us to take an address space and split it into
    smaller address spaces:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与其为我们的子网硬编码地址空间，不如利用HCL的内置函数来计算子网的地址空间。`cidrsubnet`函数允许我们将地址空间拆分成更小的地址空间：
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code will generate two maps, one for the public subnets and another
    for the private subnets. It accomplishes this by taking the randomly selected
    Availability Zones and using `cidrsubnet` to grab the next available block of
    `/24` or `256` IP addresses for each (this is more than enough for our application
    to scale to a huge number of VMs in each Availability Zone in both the frontend
    and the backend):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成两个映射，一个用于公共子网，另一个用于私有子网。它通过选择随机的可用区，并使用`cidrsubnet`为每个可用区抓取下一个可用的`/24`或`256`个IP地址块来实现（这足够让我们的应用在每个可用区的前端和后端扩展到大量虚拟机）：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding code is the value that the `public_subnets` and `private_subnets`
    maps will have when evaluated with a `vpc_cidr_block` value of `10.0.0.0/16`,
    a `cidr_split_bits` value of `8` and an `az_count` value of `2`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是当使用`vpc_cidr_block`值为`10.0.0.0/16`、`cidr_split_bits`值为`8`和`az_count`值为`2`时，`public_subnets`和`private_subnets`映射的评估值。
- en: 'By manipulating these input variables, we can reasonably size the virtual network
    and its corresponding subnets so that we don’t monopolize available address spaces
    for other applications that we may want to provision within the broader organization.
    For example, setting `vpc_cidr_block` to `10.0.0.0/22` allocates a total IP address
    count of `1024` to our application. With an `az_count` value of `2` and a `cidr_split_bits`
    value of `2`, we can allocate address space for our four subnets, each with `/24`
    and `256` IP addresses. This gives us sufficient room for our application to scale
    without over-allocating valuable IP address space:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过操作这些输入变量，我们可以合理地确定虚拟网络及其相应子网的大小，以便不会独占可用地址空间，给其他可能在更广泛组织内部署的应用程序留出空间。例如，将`vpc_cidr_block`设置为`10.0.0.0/22`为我们的应用分配了总计`1024`个IP地址。设置`az_count`值为`2`和`cidr_split_bits`值为`2`，我们可以为四个子网分配地址空间，每个子网的地址为`/24`，有`256`个IP地址。这为我们的应用提供了足够的扩展空间，而不会过度分配宝贵的IP地址空间：
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We create each subnet by iterating over the corresponding map of subnet address
    spaces. The preceding code demonstrates how we can use this map to set the correct
    Availability Zone and address space for each subnet.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过遍历相应的子网地址空间映射来创建每个子网。上述代码演示了如何使用此映射为每个子网设置正确的可用区和地址空间。
- en: Network routing
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络路由
- en: 'As per our design, the public subnets route internet traffic to the internet
    gateway:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，公共子网将互联网流量路由到互联网网关：
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We use the `aws_route_table` resource to define the route and then `aws_route_table_association`
    to link the route table to the corresponding subnet.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`aws_route_table`资源定义路由，然后使用`aws_route_table_association`将路由表与相应的子网关联。
- en: 'The private subnets route their internet traffic to a NAT gateway, which is
    provisioned in each private subnet:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 私有子网将它们的互联网流量路由到 NAT 网关，该网关在每个私有子网中进行配置：
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Because each private subnet has its own NAT gateway, we need a route table
    for each subnet to route the traffic to the correct NAT gateway:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个私有子网都有自己的 NAT 网关，我们需要为每个子网配置一张路由表，以将流量路由到正确的 NAT 网关：
- en: '[PRE22]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Notice that, unlike the public subnets, which share the same route table, we
    need to iterate on the `private_subnets` map to create a different route table
    for each private subnet and associate it with the corresponding private subnet
    using the `each` symbol.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与共享相同路由表的公共子网不同，我们需要遍历`private_subnets`映射，为每个私有子网创建一个不同的路由表，并使用`each`符号将其与相应的私有子网关联。
- en: Load balancing
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'As per our design, we need two AWS ALB instances – one for the frontend and
    another for the backend. We’ll use the `aws_lb` resource and related resources
    with the `aws_lb` prefix to provision the target group and listener configuration:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，我们需要两个 AWS ALB 实例——一个用于前端，另一个用于后端。我们将使用`aws_lb`资源以及带有`aws_lb`前缀的相关资源来配置目标组和监听器：
- en: '[PRE23]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Notice that the sticky session configuration needed for the ASP.NET Core Blazor
    Web application’s WebSocket configuration is implemented by a nested `stickiness`
    block. Likewise, the health probe is implemented by a nested `health_check` block.
    This structure will be identical for both the frontend and the backend, but the
    configuration will differ slightly, with the backend not requiring sticky sessions
    and having a different path for the health probe.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，ASP.NET Core Blazor Web 应用程序的 WebSocket 配置所需的粘性会话配置是通过一个嵌套的`stickiness`块实现的。同样，健康检查是通过一个嵌套的`health_check`块实现的。这个结构对于前端和后端都是相同的，但配置会有所不同，后端不需要粘性会话，并且健康检查的路径不同。
- en: 'The VMs are explicitly included in the target group using the `aws_lb_target_group_attachment`
    resource:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机通过`aws_lb_target_group_attachment`资源明确地包含在目标组中：
- en: '[PRE24]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notice that we are iterating over the corresponding `aws_instance` resource
    map and referencing the AWS EC2 instance ID using `each.value.id`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在遍历相应的`aws_instance`资源映射，并使用`each.value.id`引用 AWS EC2 实例 ID。
- en: 'Finally, we must provision the AWS ALB itself:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须配置 AWS ALB 本身：
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Notice that we are dynamically constructing a list of subnets using the corresponding
    `aws_subnet` resource map. When a resource block is provisioned with a `count`
    value, that resource block becomes a list, while when it is provisioned with a
    `for_each` iterator, it becomes a map. This is an important detail to pay attention
    to when you want to reference it from other resources.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在动态构建一个子网列表，通过相应的`aws_subnet`资源映射。当资源块通过`count`值进行配置时，该资源块变成一个列表，而当它通过`for_each`迭代器进行配置时，它变成一个映射。当你想从其他资源引用它时，这个细节非常重要。
- en: 'Lastly, we must connect our AWS ALB to the target group using the listener:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须通过监听器将我们的 AWS ALB 连接到目标组：
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Network security
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: 'As per our design, we have three logical components of our solution architecture
    through which network traffic will pass. Each needs its own security group and
    set of rules to allow ingress and egress traffic:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，我们的解决方案架构有三个逻辑组件，网络流量将通过这些组件传输。每个组件需要自己的安全组和规则集，以允许入站和出站流量：
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: A security group is created using the `aws_security_group` resource and attached
    to a virtual network.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 安全组是通过`aws_security_group`资源创建的，并附加到虚拟网络上。
- en: 'Not all components within the architecture will need both ingress and egress
    rules, but it’s important to think about all the ways network traffic should be
    allowed to flow through the system:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是架构中的所有组件都需要入站和出站规则，但思考网络流量应该如何在系统中流动是非常重要的：
- en: '[PRE28]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code establishes the rules we designed for the frontend load balancer,
    which allows traffic in from the internet (for example, `0.0.0.0/0`) and allows
    traffic out to the frontend VMs (for example, `aws_security_group.frontend.id`).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码建立了我们为前端负载均衡器设计的规则，允许来自互联网的流量（例如，`0.0.0.0/0`）进入，并允许流量流向前端虚拟机（例如，`aws_security_group.frontend.id`）。
- en: Secrets management
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密钥管理
- en: 'To allow our VMs to access our AWS Secrets Manager resources, we need to define
    an IAM role and associate it with our VMs. This will allow our VMs to operate
    under the security context defined by the IAM policies attached to this IAM role:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的虚拟机访问AWS Secrets Manager资源，我们需要定义一个IAM角色，并将其与我们的虚拟机关联。这将使我们的虚拟机在由IAM策略定义的安全上下文中操作：
- en: '[PRE29]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code creates the IAM role for the backend VMs, which need access
    to the PostgreSQL database’s connection string that we will store in AWS Secrets
    Manager. The IAM role itself doesn’t do anything unless there is a policy defined.
    We need to attach a policy definition to the role to grant specific privileges
    to the VMs:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码为后端虚拟机创建了IAM角色，这些虚拟机需要访问我们将存储在AWS Secrets Manager中的PostgreSQL数据库连接字符串。除非定义了策略，否则IAM角色本身不会做任何事情。我们需要将策略定义附加到角色上，以授予虚拟机特定的权限：
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding code grants access to all VMs operating with this IAM role associated
    with accessing AWS Secrets Manager secrets that begin with the `fleet-ops/dev`
    prefix. We must build this prefix using our standard naming convention input variables,
    `application_name` and `environment_name`, which have `fleet-ops` and `dev` as
    values, respectively. When we provision the production version of the `fleet-ops`
    platform, the `environment_name` input variable will be set to `prod`, ensuring
    that the VMs in the `dev` environment don’t have access to the secrets in the
    `prod` environment. Deploying the different environments of our application into
    isolated AWS accounts would also create a more secure security boundary.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码授予所有操作的虚拟机访问权限，这些虚拟机与访问AWS Secrets Manager中以`fleet-ops/dev`前缀开头的机密相关联。我们必须使用我们的标准命名约定输入变量`application_name`和`environment_name`来构建此前缀，分别将`fleet-ops`和`dev`作为值。当我们配置`fleet-ops`平台的生产版本时，`environment_name`输入变量将设置为`prod`，确保`dev`环境中的虚拟机无法访问`prod`环境中的机密。将我们应用程序的不同环境部署到隔离的AWS账户中，也会创建一个更安全的安全边界。
- en: VMs
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟机
- en: 'When provisioning static VMs, we have much more control over the configuration
    of each machine. Some VMs have specific network and storage configurations to
    meet workload demands:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置静态虚拟机时，我们可以更好地控制每台机器的配置。一些虚拟机有特定的网络和存储配置，以满足工作负载需求：
- en: '[PRE31]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding code creates a network interface that we can then attach to a
    VM. Notice that we are iterating over the frontend subnets. This will ensure we
    have exactly one VM in each subnet (and consequently each Availability Zone).
    This network interface is where we attach the security group for VMs in the frontend.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了一个网络接口，我们可以将其附加到虚拟机上。请注意，我们正在遍历前端子网。这将确保每个子网中有正好一个虚拟机（因此每个可用区中都有一个）。这个网络接口是我们附加给前端虚拟机的安全组所在的位置。
- en: 'Finally, we provision the VM using the `aws_instance` resource, taking care
    to use the correct instance type, network interface, and AWS AMI:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`aws_instance`资源来配置虚拟机，并确保使用正确的实例类型、网络接口和AWS AMI：
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: AWS has a cross-cutting service called CloudWatch that collects logs and telemetry
    across the various AWS services. To enable CloudWatch on your EC2 instances, you
    simply need to add the `monitoring` attribute and set it to `true`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: AWS有一个跨服务的服务，叫做CloudWatch，它收集各种AWS服务的日志和遥测数据。要在EC2实例上启用CloudWatch，您只需要添加`monitoring`属性并将其设置为`true`。
- en: Monitoring
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控
- en: Depending on the service and its available configuration options within the
    Terraform resources used to provision it, to activate CloudWatch, you might need
    to go through the process of provisioning additional resources and setting up
    additional IAM permissions to grant the respective resource to write to CloudWatch.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 根据服务及其在用于配置的Terraform资源中的可用配置选项，要启用CloudWatch，您可能需要经过配置额外资源并设置额外IAM权限的过程，以授予相应的资源写入CloudWatch的权限。
- en: 'The first thing we need to set up is an IAM policy that will allow the specific
    service access to assume an IAM role. In this case, we are granting VPC Flow Logs
    access to assume an IAM role:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置的第一件事是一个IAM策略，它将允许特定服务访问并假设一个IAM角色。在这种情况下，我们正在授予VPC流日志访问权限来假设一个IAM角色：
- en: '[PRE33]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We’ll use this policy when we set up the IAM role to grant the VPC Flow Logs
    service access to this particular IAM role. This will be important later when
    we link everything together:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在设置IAM角色时使用此策略，以授予VPC流日志服务访问该特定IAM角色的权限。稍后当我们将所有内容链接在一起时，这将变得非常重要：
- en: '[PRE34]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding code allows VPC Flow Logs to assume this role, eventually granting
    it access to writing logs to CloudWatch.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码允许 VPC Flow Logs 假设该角色，从而最终授予它将日志写入 CloudWatch 的权限。
- en: 'Next, we need to set up another IAM policy that will grant access to write
    to CloudWatch logs. You can further narrow the scope of an access policy by narrowing
    the allowed actions and the allowed resources the policy grants access to:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置另一个 IAM 策略，该策略将授予写入 CloudWatch 日志的权限。你可以通过限制允许的操作和策略授予访问权限的资源，进一步缩小访问策略的范围：
- en: '[PRE35]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the preceding code, we do a good job of being specific about the types of
    operations we want to grant access to by giving specific operations such as `logs:PutLogEvents`.
    However, the resources are set to `*`, a very wide access level. We should consider
    narrowing that down to just the resources that we need.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过指定特定的操作（例如 `logs:PutLogEvents`）来明确我们希望授予访问权限的操作类型。然而，资源设置为 `*`，这是一种非常广泛的访问级别。我们应该考虑将其限制到仅需要的资源。
- en: 'The next step is to attach the policy to the IAM role:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将策略附加到 IAM 角色：
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: At this point, we have an IAM role that is allowed to write to CloudWatch and
    we have allowed VPC Flow Logs to assume this role.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经有一个允许写入 CloudWatch 的 IAM 角色，并且我们已经允许 VPC Flow Logs 假设该角色。
- en: 'Next, we need to create a CloudWatch log group that will store the logs from
    VPC:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个 CloudWatch 日志组，以存储来自 VPC 的日志：
- en: '[PRE37]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Finally, we’ll connect VPC Flow Logs to the log group and assign the IAM role
    it should use to gain access to write to CloudWatch:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把 VPC Flow Logs 连接到日志组，并分配它应该使用的 IAM 角色以获得写入 CloudWatch 的权限：
- en: '[PRE38]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding code also links our VPC to the VPC Flow Logs service, thus completing
    the flow and placing the networking logs in the corresponding CloudWatch log group.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码还将我们的 VPC 与 VPC Flow Logs 服务关联，从而完成流动并将网络日志放入相应的 CloudWatch 日志组中。
- en: With that, we have implemented the Packer and Terraform solutions and have a
    working code base that will build VM images for both our frontend and backend
    application components while also provisioning our cloud environment into AWS.
    In the next section, we’ll dive into YAML and Bash and implement GitHub Actions
    workflows.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已经实现了 Packer 和 Terraform 解决方案，并且有了一个工作代码库，该代码库将为我们的前端和后端应用组件构建 VM 镜像，并将我们的云环境配置到
    AWS。在下一节中，我们将深入探讨 YAML 和 Bash，并实现 GitHub Actions 工作流。
- en: Automating the deployment
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化部署
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base consisting of application code
    and an operating system configuration encapsulated within a Packer template. These
    two application components are then deployed into a cloud environment on AWS,
    which is defined within our Terraform code base.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在设计中讨论的，我们的解决方案由两个应用组件组成：前端和后端。每个组件都有一个代码库，其中包含应用程序代码和一个封装在 Packer 模板中的操作系统配置。然后，这两个应用组件被部署到
    AWS 云环境中，这个环境在我们的 Terraform 代码库中定义。
- en: 'There is an additional code base that we have yet to discuss: our automation
    pipelines. We will be implementing our automation pipelines using GitHub Actions:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个我们尚未讨论的额外代码库：我们的自动化管道。我们将使用 GitHub Actions 来实现我们的自动化管道：
- en: '![Figure 7.21 – Source code structure within our GitHub repository](img/B21183_07_21..jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.21 – 我们 GitHub 仓库中的源代码结构](img/B21183_07_21..jpg)'
- en: Figure 7.21 – Source code structure within our GitHub repository
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21 – 我们 GitHub 仓库中的源代码结构
- en: 'In GitHub Actions, automation pipelines are called workflows and they are stored
    in a particular folder within the source code repository, namely `/.github/workflows`.
    Each of our code bases is stored in a separate folder. Our solutions source code
    repository’s folder structure looks like this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub Actions 中，自动化管道被称为工作流，并且它们存储在源代码库中的特定文件夹中，即 `/.github/workflows`。我们的每个代码库都存储在一个单独的文件夹中。我们的解决方案源代码库的文件夹结构如下所示：
- en: '[PRE39]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As per our design, we will have GitHub Actions workflows that will execute
    Packer and build VM images for both the frontend (for example, `packer-frontend.yaml`)
    and the backend (for example, `packer-backend.yaml`). We’ll also have workflows
    that will run `terraform plan` and `terraform apply`:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，我们将拥有 GitHub Actions 工作流，这些工作流将执行 Packer 并构建前端（例如，`packer-frontend.yaml`）和后端（例如，`packer-backend.yaml`）的
    VM 镜像。我们还将拥有运行 `terraform plan` 和 `terraform apply` 的工作流：
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Each folder path will allow us to control which GitHub Actions workflows should
    trigger so that we aren’t unnecessarily running workflows when no applicable changes
    have been made.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件夹路径将允许我们控制哪些 GitHub Actions 工作流应该触发，这样我们就不会在没有相关变更的情况下不必要地运行工作流。
- en: Because we are following GitFlow, we’ll have a main branch where the production
    version of all of our code will reside. Developers, whether they are working on
    updates to the application code (for example, C#), the operating system configuration
    (for example, the Packer template), or the cloud environment configuration (for
    example, the Terraform template), will create a branch off of `main` with the
    `feature/*` naming convention.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们遵循 GitFlow，所以我们将有一个主分支，所有生产版本的代码都将在该分支上。开发人员，无论是在应用代码（例如 C#）的更新、操作系统配置（例如
    Packer 模板）还是云环境配置（例如 Terraform 模板）上进行工作，都将从 `main` 分支创建一个以 `feature/*` 命名约定的分支。
- en: Once they’ve done this, they can submit a pull request. This indicates that
    the developer believes their code changes are ready to be merged back into the
    `main` branch – in other words, their code changes are ready for production!
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这些操作，开发人员可以提交拉取请求。这表明开发人员认为他们的代码更改已经准备好合并回 `main` 分支——换句话说，他们的代码更改已经准备好投入生产！
- en: '![Figure 7.22 – GitFlow’s pull request process](img/B21183_07_22..jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.22 – GitFlow 的拉取请求流程](img/B21183_07_22..jpg)'
- en: Figure 7.22 – GitFlow’s pull request process
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.22 – GitFlow 的拉取请求流程
- en: The pull request is a great time to perform some checks on our solution’s code.
    For the application code, this could take the form of a build, static code analysis,
    and unit or integration tests. Each of these actions tests a different aspect
    of the application code. The build (that is, compiling the C# code base) is one
    of the most basic tests that we can perform. It simply tests whether the application
    code is valid C# and is devoid of inherent language syntax errors. Static code
    analysis can cover a wide range of code quality checks, including readability
    and maintainability or security and vulnerability assessments. The unit and integration
    tests check the functionality of the software components working individually
    and together to accomplish the underlying business purpose of the software. Executing
    these tests regularly is known as **continuous integration** (**CI**) and is half
    of the famous and often elusive **CI/CD pipeline**, where **CD** stands for **continuous
    delivery**.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取请求是检查我们解决方案代码的好时机。对于应用代码，这可以表现为构建、静态代码分析以及单元测试或集成测试。这些操作分别测试了应用代码的不同方面。构建（即编译
    C# 代码库）是我们可以执行的最基本的测试之一。它只是测试应用代码是否是有效的 C# 代码，并且没有固有的语言语法错误。静态代码分析可以涵盖广泛的代码质量检查，包括可读性、可维护性，或安全性和漏洞评估。单元和集成测试检查软件组件的功能，确保它们单独工作和共同工作来实现软件的基本业务目标。定期执行这些测试被称为**持续集成**(**CI**)，它是著名且经常令人困惑的**CI/CD管道**的一部分，其中**CD**代表**持续交付**。
- en: The CI pipeline cuts down on routine work surrounding the built-in quality of
    the application code. Without it, these checks would need to be performed by humans
    through exhaustive code reviews and manual testing. We still need to do code reviews
    and manual testing, but a good CI pipeline will reduce the effort that humans
    need to perform.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: CI 管道减少了与应用代码内建质量相关的例行工作。如果没有它，这些检查需要通过人工代码审查和手动测试来执行。我们仍然需要进行代码审查和手动测试，但一个好的
    CI 管道将减少人力所需的工作量。
- en: Now that we’ve covered what built-in quality controls we can put on application
    code, what can we do with our operating system and our cloud environment configuration?
    Is there a way to test IaC without provisioning the infrastructure? There is,
    but there are limitations.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了可以在应用代码上实现的内建质量控制，我们可以对操作系统和云环境配置做些什么呢？是否有办法在不部署基础设施的情况下测试基础设施即代码（IaC）？有的，但存在一些限制。
- en: Packer
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Packer
- en: 'Because the VM image acts as an immutable artifact that contains a versioned
    copy of the application code and operating system configuration, we need to update
    this artifact any time something changes in either the application code or the
    operating system configuration:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 因为虚拟机镜像充当着不可变的工件，包含了应用代码和操作系统配置的版本化副本，所以每当应用代码或操作系统配置发生变化时，我们都需要更新这个工件：
- en: '[PRE41]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This means that we need a trigger on both code bases that affect the final artifact
    for Packer, which includes the application code and the operating system configuration
    within the Packer template itself. With GitHub Actions, we can add a list of `paths`
    that will trigger our workflow.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要在两个代码库上设置触发器，以影响 Packer 的最终制品，包括应用程序代码和 Packer 模板本身中的操作系统配置。在 GitHub
    Actions 中，我们可以添加一个 `paths` 列表，触发我们的工作流。
- en: 'We should build a new VM image every time there is a pull request and every
    time there is a push onto `main`. When Packer is executed, it is essentially doing
    a pretty rigorous integration test. Therefore, it’s useful to have it performed
    as part of our CI process. That means we need to have a VM image that is tested
    and verified to be production-ready before we push the code into the `main` branch:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 每次有拉取请求或推送到 `main` 时，我们应该构建一个新的虚拟机镜像。当 Packer 执行时，本质上是在进行一个相当严格的集成测试。因此，将其作为我们的
    CI 过程的一部分进行执行是非常有用的。这意味着我们需要有一个经过测试并验证为生产就绪的虚拟机镜像，然后才能将代码推送到 `main` 分支：
- en: '![Figure 7.23 – VM image versioning](img/B21183_07_23..jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.23 – 虚拟机镜像版本控制](img/B21183_07_23..jpg)'
- en: Figure 7.23 – VM image versioning
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.23 – 虚拟机镜像版本控制
- en: Our Packer workflow will generate a unique name and version for each VM image
    it produces. We can build tests into our Packer template to verify that the web
    server is running and listening on port `5000`. Using this version of the image,
    we can also launch a new VM and inspect the operating system’s configuration ourselves
    to make sure everything is in order.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Packer 工作流将为它生成的每个虚拟机镜像创建一个唯一的名称和版本。我们可以在 Packer 模板中构建测试，验证 Web 服务器是否正在运行，并监听端口
    `5000`。使用该版本的镜像，我们还可以启动一个新的虚拟机，并亲自检查操作系统的配置，以确保一切正常。
- en: When we are confident that the code changes to either the application code or
    the operating system configuration are fully functional, we can approve the pull
    request and merge it into the `main` branch. This will trigger a new version of
    the VM image from the production-ready code in the `main` branch. We can use the
    new version of this production-ready VM image to update our cloud environment
    configuration when we are ready to deploy these changes to our environments.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们确信应用程序代码或操作系统配置的代码更改完全正常时，我们可以批准拉取请求，并将其合并到 `main` 分支。这将触发一个新的虚拟机镜像版本，从 `main`
    分支中的生产就绪代码生成。我们可以使用该生产就绪虚拟机镜像的新版本来更新我们的云环境配置，当我们准备好将这些更改部署到环境时。
- en: The GitHub Actions workflow needs some ground rules to be established that control
    the specific versions of software and key locations within the code base. It’s
    important to always be specific. This means using specific versions of software
    instead of relying on the internet Gods to decide which version you’ll use. This
    might work well when you are running things locally on your machine and are there
    to solve the inevitable problems and conflicts that arise, but for an automation
    pipeline, there is no human there to correct things as they are happening; there
    are only assumptions – assumptions about what version of the software you’re using.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Actions 工作流需要建立一些基本规则，以控制软件的特定版本和代码库中的关键位置。始终明确是非常重要的。这意味着要使用特定版本的软件，而不是依赖互联网的神明来决定你将使用哪个版本。当你在本地计算机上运行代码并解决不可避免的问题和冲突时，这可能会很好用，但对于自动化流水线来说，那里没有人类来在问题发生时进行修正；只有假设——关于你正在使用的软件版本的假设。
- en: 'We’ll use two pieces of software: the .NET SDK and Packer. Likewise, we have
    two code bases: the C# .NET code base for the application and the HCL code base
    for Packer. As such, we must establish where these code bases are very clearly
    and upfront. Setting pipeline variables for them is a very useful way of accomplishing
    this as it ensures they are featured prominently in the YAML file and are stored
    in a reusable variable in case they will be repeated multiple times:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两款软件：.NET SDK 和 Packer。同样，我们有两个代码库：用于应用程序的 C# .NET 代码库和用于 Packer 的 HCL
    代码库。因此，我们必须非常明确和提前地确定这些代码库的位置。为它们设置流水线变量是实现这一目标的一个非常有用的方法，因为它确保它们在 YAML 文件中突出显示，并且存储在可重复使用的变量中，以防需要多次重复使用：
- en: '[PRE42]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now that we have the triggers and some variables set for our workflow, we need
    to structure the jobs. For each Packer template, we will have two jobs: one that
    builds the C# .NET application code and produces a deployment package and another
    that runs `packer build` to produce the VM image:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为工作流设置了触发器和一些变量，接下来需要构建作业结构。对于每个Packer模板，我们将有两个作业：一个是构建C# .NET应用程序代码并生成部署包，另一个是运行`packer
    build`以生成虚拟机镜像：
- en: '[PRE43]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `build` job performs a pretty standard .NET build process, which includes
    restoring package dependencies from NuGet (the .NET package manager), building
    the code, running unit and integration tests, publishing a deployable artifact,
    and storing that artifact so that it can be used by future jobs within the pipeline:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`build`作业执行一个相当标准的.NET构建过程，包括从NuGet（.NET包管理器）恢复包依赖、构建代码、运行单元测试和集成测试、发布可部署的工件，并存储该工件，以便将来管道中的其他作业使用：'
- en: '![Figure 7.24 – Packer workflow](img/B21183_07_24..jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.24 – Packer工作流](img/B21183_07_24..jpg)'
- en: Figure 7.24 – Packer workflow
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24 – Packer工作流
- en: 'The `packer` job immediately downloads the `.zip` file containing the deployment
    artifact and puts it into a location where the Packer template’s `file` provisioner
    expects it. Then, it generates a unique version of the name for the VM image that
    will be produced if successful:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`packer`作业会立即下载包含部署工件的`.zip`文件，并将其放入Packer模板的`file`提供程序期望的位置。然后，它会生成一个唯一版本的虚拟机镜像名称，成功的话将被生产出来：'
- en: '[PRE44]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: It does this by using Bash to generate the current year and month and appends
    `github.run_number` to ensure uniqueness if we happen to be running this pipeline
    more than once per day.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过使用Bash生成当前的年份和月份，并附加`github.run_number`来确保唯一性，以防我们一天内多次运行该管道。
- en: 'Next, it obtains the public IP address for the VM on which the GitHub Actions
    workflow is running:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它获取运行GitHub Actions工作流的虚拟机的公共IP地址：
- en: '[PRE45]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: It does this so that when it runs `packer build`, it can configure Packer’s
    plugin for AWS to poke a hole in the firewall to allow SSH traffic from the GitHub
    Actions machine to the temporary VM running on AWS where the Packer provisioners
    are executed.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 它这么做是为了在运行`packer build`时，配置Packer的AWS插件，打开防火墙以允许来自GitHub Actions机器到运行在AWS上的临时虚拟机的SSH流量，Packer提供程序将在该虚拟机上执行。
- en: 'Next, it installs a specific version of Packer:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它安装特定版本的Packer：
- en: '[PRE46]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, it executes `packer build`, making sure to specify the `AWS_ACCESS_KEY_ID`
    and `AWS_SECRET_ACCESS_KEY` environment variables that the AWS plugin relies upon
    to authenticate to AWS’s REST APIs:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它执行`packer build`，确保指定`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`环境变量，这些是AWS插件依赖于它们来认证AWS的REST
    API：
- en: '[PRE47]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: It also specifies two input variables to the Packer template using the `PKR_VAR_`
    prefixed environment variable technique so that it includes the image version
    and the build agent IP address, both of which were dynamically generated within
    the GitHub Actions workflow.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 它还通过使用以`PKR_VAR_`为前缀的环境变量技术，指定了两个输入变量给Packer模板，这样就包括了镜像版本和构建代理IP地址，这两个值在GitHub
    Actions工作流中动态生成。
- en: Terraform
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Terraform
- en: 'With both of our VM images built and their versions input into our `tfvars`
    file, our Terraform automation pipeline is ready to take the reins and not only
    provision our environment but deploy our solution (although not technically).
    The deployment was technically done within the `packer build` process, with the
    physical deployment packages being copied to the home directory and the Linux
    service setup primed and ready. Terraform finishes the job by launching VMs using
    these images:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 随着两个虚拟机镜像构建完成并将其版本输入到我们的`tfvars`文件中，Terraform自动化管道已经准备好接管，不仅能够配置我们的环境，还能部署我们的解决方案（虽然技术上说不完全是）。部署实际上是在`packer
    build`过程中完成的，物理部署包被复制到主目录，并且Linux服务设置已经准备就绪。Terraform通过使用这些镜像启动虚拟机来完成任务：
- en: '[PRE48]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This means that we only need to trigger the Terraform automation pipeline when
    the Terraform code base changes. This could include configuration changes to the
    resources simply be an updated VM image version within the `tfvars` file:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们只需要在Terraform代码库发生变化时触发Terraform自动化管道。这可能包括对资源的配置更改，简单来说，就是`tfvars`文件中更新了虚拟机镜像版本：
- en: '![Figure 7.25 – The terraform apply workflow](img/B21183_07_25..jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.25 – Terraform apply工作流](img/B21183_07_25..jpg)'
- en: Figure 7.25 – The terraform apply workflow
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25 – Terraform apply工作流
- en: As a result, the Terraform pipeline is quite simple. We simply need to execute
    either `terraform plan` or `terraform apply`, depending on whether we want to
    evaluate or execute the changes for our cloud environment.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，Terraform 流水线非常简单。我们只需要执行 `terraform plan` 或 `terraform apply`，具体取决于我们是想评估还是执行针对云环境的更改。
- en: 'In keeping with the *always be specific* mantra, we must dutifully designate
    the version of Terraform that we want to use and specify the location for the
    Terraform code base using pipeline variables:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 继续秉承 *始终具体* 的原则，我们必须恭敬地指定想要使用的 Terraform 版本，并使用流水线变量指定 Terraform 代码库的位置：
- en: '[PRE49]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we must install the particular version of Terraform using the `setup-terraform`
    GitHub Action published by HashiCorp, which will handle the details of its installation
    for us:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须使用 HashiCorp 发布的 `setup-terraform` GitHub Action 安装特定版本的 Terraform，它将为我们处理安装的细节：
- en: '[PRE50]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, it executes `terraform apply` again, making sure to include the AWS
    credentials and the target backend location for the Terraform state:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它再次执行 `terraform apply`，确保包括 AWS 凭证和 Terraform 状态的目标后端位置：
- en: '[PRE51]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The backend configuration is set using the `-backend-config` command-line argument,
    which frees us from having to hardcode these settings in our source code.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 后端配置是通过 `-backend-config` 命令行参数设置的，这样我们就不必在源代码中硬编码这些设置。
- en: Notice that we execute `terraform apply` twice. First, we perform a targeted
    apply on the `random_shuffle.az` resource, after which we perform a general apply.
    The targeted apply ensures that the Availability Zones we are targeting have been
    selected before we calculate the IP address space for our networks. The need for
    this is driven by the dynamic nature of calculating the address space using the
    `cidrsubnet` function. If we wanted to avoid this targeted apply approach, we
    could opt for a more hard-coded approach of the Availability Zones and the corresponding
    address spaces.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们执行了两次 `terraform apply`。第一次，我们对 `random_shuffle.az` 资源进行有针对性的应用，之后再执行一般性的应用。有针对性的应用确保我们在计算网络的
    IP 地址空间之前，已经选择了目标的可用区。这是由于使用 `cidrsubnet` 函数动态计算地址空间的需要。如果我们想避免这种有针对性的应用方式，可以选择一种更硬编码的方式，指定可用区和相应的地址空间。
- en: That’s it! With the completion of our Terraform GitHub Actions workflow, we
    have put the finishing touches on our end-to-end CI/CD pipeline. Our AWS-based
    solution will be up and running our VM cloud architecture in no time.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！随着我们完成 Terraform GitHub Actions 工作流的配置，我们为端到端 CI/CD 流水线画上了完美的句号。我们基于 AWS
    的解决方案将迅速启动并运行我们的虚拟机云架构。
- en: Summary
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we built a multi-tier cloud architecture using AWS and VMs,
    a fully operational GitFlow process, and an end-to-end CI/CD pipeline using GitHub
    Actions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用 AWS 和虚拟机构建了一个多层云架构，建立了一个完全可操作的 GitFlow 流程，并使用 GitHub Actions 创建了端到端的
    CI/CD 流水线。
- en: 'In the next chapter, our fearless leader at Söze Enterprises will be throwing
    us into turmoil with some big new ideas, and we’ll have to respond to his call
    to action. It turns out our CEO, Keyser, has been up late watching some YouTube
    videos about the next big thing – containers – and after talking with his pal
    Jeff on his superyacht, he has decided that we need to refactor our whole solution
    so that it can run on Docker and Kubernetes. Luckily, the good people at Amazon
    have a service that might help us out: AWS **Elastic Kubernetes** **Service**
    (**EKS**).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，Söze Enterprises 的无畏领导者将把我们推入混乱，带来一些新的重大创意，我们将不得不响应他的行动号召。原来，我们的 CEO Keyser
    最近熬夜看了一些关于下一个大趋势——容器——的 YouTube 视频，经过与他的好友 Jeff 在超级游艇上的交谈，他决定我们需要重构整个解决方案，使其能够在
    Docker 和 Kubernetes 上运行。幸运的是，亚马逊的好心人提供了一项可能帮助我们的服务：AWS **弹性 Kubernetes** **服务**（**EKS**）。
