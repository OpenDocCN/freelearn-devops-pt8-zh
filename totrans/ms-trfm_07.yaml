- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started on AWS – Building Solutions with AWS EC2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a good foundation of the concepts needed to build real-world
    cloud solutions and automate them using **Infrastructure as Code** (**IaC**) with
    Terraform, we will start our journey with arguably the most popular cloud platform:
    **Amazon Web** **Services** (**AWS**).'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll take a step-by-step approach to designing, building,
    and automating a solution using AWS’s **virtual machine** (**VM**) service – **Elastic
    Cloud Compute** or **EC2** for short. We’ll also explore several other AWS services
    that are crucial for ensuring our solution’s robustness and production readiness,
    such as secrets management, logging, and network security.
  prefs: []
  type: TYPE_NORMAL
- en: We have much to accomplish, but this is where the rubber hits the road. We’ll
    begin to really apply the concepts we’ve been discussing and put them into practice
    on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud infrastructure is only as good as the applications and services deployed
    to it, so for this book, we will be building our sample architectures around a
    function use case for a fictional company called Söze Enterprises. Söze Enterprises
    was founded by a mysterious Turkish billionaire, Keyser Söze, who wants to take
    autonomous vehicles to the next level by building a platform that will allow both
    land and air vehicles – from any manufacturer – to coordinate their actions to
    improve safety and efficiency. Somehow, Keyser has already got Elon onboard, so
    it’s only a matter of time before the other EV vendors follow suit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have inherited a team from one of Söze Enterprises’ other divisions that
    has a strong core team of C# .NET developers, so we’ll be building version 1.0
    of the platform using .NET technologies. The elusive CEO, Keyser, was seen hobnobbing
    with Jeff Bezos in Monaco over the weekend, and word has come down from corporate
    that we will be using AWS to host the platform. Since the team doesn’t have a
    ton of experience with containers and timelines are tight, we’ve decided to build
    a simple three-tier architecture and host on VMs using AWS’s EC2 service. We’ve
    decided to use a Linux operating system to make it easier to convert containers
    in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Logical architecture for the autonomous vehicle platform](img/B21183_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Logical architecture for the autonomous vehicle platform
  prefs: []
  type: TYPE_NORMAL
- en: The platform will need a frontend, which will be a web UI built using ASP.NET
    Core Blazor. The frontend will be powered by a REST API backend, which will be
    built using ASP.NET Core Web API. Having our core functionality encapsulated into
    a REST API will allow autonomous vehicles to communicate directly with the platform
    and allow us to expand by adding client interfaces with additional frontend technologies
    such as native mobile apps and virtual or mixed reality in the future. The backend
    will use a PostgreSQL database for persistent storage since it’s lightweight,
    industry-standard, and relatively inexpensive.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the tight timelines the team is facing, we want to keep the cloud architecture
    simple. Therefore, we’ll be keeping it simple and using tried and tested services
    from AWS to implement the platform as opposed to trying to learn something new.
    The first decision we have to make is what AWS service each component of our logical
    architecture will be hosted on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application architecture consists of three components: a frontend, a backend,
    and a database. The frontend and backend are application components and need to
    be hosted on a cloud service that provides general computing, while the database
    needs to be hosted on a cloud database service. There are many options for both
    types of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Logical architecture for the autonomous vehicle platform](img/B21183_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Logical architecture for the autonomous vehicle platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we’ve decided that we’re going to use VMs to host our application, we
    have narrowed down the different services that we can use to host our application,
    and we have decided that AWS EC2 is the ideal choice for our current situation.
    There are other options, such as **Elastic Beanstalk**, that also use VMs, but
    we want to have total control over the solution and maintain as many cross-platform
    capabilities as possible in case we ever have to migrate to a different cloud
    platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Source control structure of our repository](img/B21183_07_3..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Source control structure of our repository
  prefs: []
  type: TYPE_NORMAL
- en: This solution will consist of six parts. We still have the application code
    and Packer templates for both the frontend and backend. Then, we have GitHub Actions
    to implement our CI/CD process and Terraform to provision our AWS infrastructure
    and reference the Packer-built VM images for our EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first part of our design is adapting our solution’s architecture to the
    target cloud platform: AWS. This involves mapping application architecture components
    to AWS services and thinking through the configuration of those services so that
    they meet the requirements of our solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'VMs must be deployed within a virtual network. On AWS, we use the AWS EC2 service
    to provide our VMs, and we use AWS **Virtual Private Cloud** (**VPC**) to provide
    our virtual network. When working on AWS, the term *EC2 instance* is used interchangeably
    with the term *virtual machine*. Likewise, the term *VPC* is used interchangeably
    with the term *virtual network*. In this book, I will try to use industry-standard
    terminology wherever possible. You should get in the habit of thinking this way
    as this will allow your knowledge and skills to better transition between the
    different cloud platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – AWS virtual network architecture](img/B21183_07_4..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – AWS virtual network architecture
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve discussed previously, a virtual network is divided into a set of subnets.
    On AWS, a virtual network is scoped to a specific region, and a subnet is scoped
    to an Availability Zone within that region. Therefore, to build highly available
    systems on AWS, we must distribute our workloads across multiple Availability
    Zones. Therefore, if one Availability Zone experiences an outage, our workload,
    when deployed into the other Availability Zone, will prevent disruption to the
    end users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application’s VMs need to be provisioned into subnets within a virtual
    network. The frontend of our application needs to be accessible over the internet,
    while the backend only needs to be accessible to the frontend. Therefore, we should
    provision separate subnets for the internet-accessible frontend and our private
    backend. This is a common pattern when it comes to creating *public* and *private*
    subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Public and private subnets for the frontend and backend application
    components](img/B21183_07_5..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Public and private subnets for the frontend and backend application
    components
  prefs: []
  type: TYPE_NORMAL
- en: In this pattern, two pairs of public and private subnets are created. Each pair
    is provisioned in the same Availability Zone. The reason why each pair shares
    the same Availability Zone is due to the dependency between the frontend and the
    backend. For example, if there is an outage affecting the Availability Zone of
    the backend, the frontend won’t be able to operate. Likewise, if there is an outage
    affecting the Availability Zone of the frontend, no traffic will be routed to
    the backend. We can create as many pairs of these public/private subnets as there
    are Availability Zones within a region. Most regions have four to five Availability
    Zones, but usually, two to three Availability Zones are sufficient for most workloads.
    After that, you are more likely to benefit from setting up a multi-region deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Network routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few other components that we need to set up within this virtual
    network to enable our VMs to function properly. In AWS, when you provision a VM
    in a virtual network, you won’t have internet access! For most connected applications,
    internet access is required to allow connectivity to third-party services. Without
    this, operators would be inconvenienced as they would be unable to perform operating
    system upgrades and patches using internet-hosted package repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Internet and NAT gateways enable internet access for VMs within
    the subnets](img/B21183_07_6..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Internet and NAT gateways enable internet access for VMs within
    the subnets
  prefs: []
  type: TYPE_NORMAL
- en: 'The internet gateway is attached to the virtual network at the region level,
    providing internet access to the entire VPC, while the NAT gateways are deployed
    into each public subnet at the Availability Zone level to allow EC2 instances
    in private subnets to access the internet without being directly accessible from
    the internet. Each NAT gateway also needs its own static public IP address to
    grant access. This can be achieved by using the Elastic IP service on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Route tables associated with the subnets that direct traffic
    to the correct gateway](img/B21183_07_7..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Route tables associated with the subnets that direct traffic to
    the correct gateway
  prefs: []
  type: TYPE_NORMAL
- en: The final step in establishing internet access to our VMs in private subnets
    is routing internet-bound traffic to the correct NAT gateway for each subnet;
    VMs in public subnets can directly access the internet. This can be done using
    route tables. In the public subnet, we route internet traffic to the internet
    gateway. In the private subnet, we route internet traffic to the NAT gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that our subnets have been set up and connected using proper routing tables,
    we can provision our VMs. To achieve high availability, we need at least one VM
    to be provisioned for each subnet for both the frontend and the backend of our
    solution. We can increase the number of VMs in each subnet to achieve even more
    reliability or scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – VMs provisioned for our virtual network](img/B21183_07_8..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – VMs provisioned for our virtual network
  prefs: []
  type: TYPE_NORMAL
- en: The problem with the current design is that we need a way for our system to
    respond correctly to an outage affecting one of our Availability Zones. This is
    where a load balancer comes in. It allows us to get the double benefit of routing
    traffic to healthy endpoints and distributing the load evenly across our resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In AWS, the **Application Load Balancer** (**ALB**) service performs this function.
    The load balancer’s job is to be the single point of contact for clients to send
    requests to. The load balancer then forwards that traffic to VMs and routes the
    corresponding responses back to the client from where the request originated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Load balancer forwarding traffic to VMs across Availability
    Zones](img/B21183_07_9..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Load balancer forwarding traffic to VMs across Availability Zones
  prefs: []
  type: TYPE_NORMAL
- en: In the AWS ALB, the first thing we need to set up is the listener. On this listener,
    you specify a port, a protocol, and one or more actions you want to perform when
    a request is received. The most basic type of action is to forward the request
    to a target group.
  prefs: []
  type: TYPE_NORMAL
- en: In our solution, the target group will consist of a set of VMs. The target group
    specifies what port and protocol the request should be sent to, as well as a health
    probe with a specific application path. The health probe can optionally be set
    up on a different port and protocol, where it provides several different settings
    to control how frequently it should be probed and how to evaluate whether the
    endpoint is healthy or unhealthy. Healthy is usually indicated by an HTTP status
    code of `200`. Anything else is considered unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: For both our frontend and backend, we have a simple set of VMs for the target
    group with an endpoint configured for the HTTP protocol on port `5000` (the default
    port for ASP.NET Core).
  prefs: []
  type: TYPE_NORMAL
- en: The frontend is an **ASP.NET Core Blazor** application. As a result, it uses
    **SignalR** (which abstracts WebSocket communication) to provide real-time connectivity
    between the web browser and the server. As a result, we need to enable sticky
    sessions so that this can function properly. Sticky sessions will allow the client
    to continue to use the same VM, thus allowing the WebSocket to stay alive and
    not be disrupted by changing which web server it communicates with.
  prefs: []
  type: TYPE_NORMAL
- en: For the health probe, the frontend will use the root path of the web application,
    while the backend will use a special path that routes to a controller that’s been
    configured to respond to the health probe.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that our virtual network has been fully configured and our VMs have been
    set up behind load balancers, we need to think about what network traffic we want
    to allow through the system. In AWS, this can be controlled by creating security
    groups, which allow traffic to be sent between components of your architecture
    on specific ports using specific protocols.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in this process is to think through the logical stops for our
    network traffic as it makes its way through our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Logical components of our architecture](img/B21183_07_10..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Logical components of our architecture
  prefs: []
  type: TYPE_NORMAL
- en: The application components, including the frontend and the backend, are on this
    list, followed by the database. However, these aren’t the only places where our
    network traffic flows. Since we introduced load balancers in front of both the
    frontend and the backend, we have two additional stops for network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to think about how each component communicates with others.
    This includes both the port and protocol but also the direction of the traffic.
    To do this, we need to think about the network traffic from the perspective of
    each component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Frontend load balancer network traffic flow](img/B21183_07_11..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Frontend load balancer network traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of the frontend load balancer, we’ll be receiving traffic
    from the internet on port `80` using the HTTP protocol. This inbound traffic is
    called `5000` using the HTTP protocol. This outbound traffic is called **egress**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Frontend network traffic flow](img/B21183_07_12..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Frontend network traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of the frontend, we’ll be receiving traffic from the frontend
    load balancer on port `5000` using the HTTP protocol. The C# application code
    will make requests to the REST web API hosted in the backend, but we’ll be routing
    all our requests to the backend through the backend load balancer on port `80`
    using the HTTP protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Backend load balancer network traffic flow](img/B21183_07_13..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Backend load balancer network traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of the backend load balancer, we’ll be receiving traffic
    from the frontend on port `80` using the HTTP protocol. Due to the target group
    configuration, we’ll be forwarding those requests to the backend on port `5000`
    using the HTTP protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Backend network traffic flow](img/B21183_07_14..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Backend network traffic flow
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of the backend, we’ll be receiving traffic from the backend
    load balancer on port `5000` using the HTTP protocol. The C# application code
    will be making requests to the PostgreSQL database on port `5432` using the HTTPS
    protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Secrets such as database credentials or service access keys need to be stored
    securely. Each cloud platform has a service that provides this functionality.
    On AWS, this service is called **AWS** **Secrets Manager**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Secrets stored in AWS Secrets Manager can be accessed by VMs
    once they have the necessary IAM privileges](img/B21183_07_15..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Secrets stored in AWS Secrets Manager can be accessed by VMs once
    they have the necessary IAM privileges
  prefs: []
  type: TYPE_NORMAL
- en: 'You simply create secrets on this service using a consistent naming convention,
    then construct an IAM role that has permission to access these secrets. The following
    IAM policy will grant permission to just secrets that start with `fleetportal/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The values for `region` and `account-id` will need to be altered to reflect
    where the secrets were created. It’s important to note that an AWS account is
    typically used as a security boundary for an application and an environment. So,
    we would likely have separate AWS accounts for our solution’s development and
    production environments, as well as any other environments we may need. This will
    isolate our secrets manager secrets within the context of the AWS account and
    the region.
  prefs: []
  type: TYPE_NORMAL
- en: The two main attributes we use to grant permissions are `action` and `resource`.
    When implementing the principle of least privilege, it’s important to be as specific
    as possible about the actions that are required for a particular identity. If
    access is not required, don’t grant it. Likewise, we should ensure the resources
    we grant these permissions to are as narrow as possible. It’s easy to be lazy
    and leave `*` in the resources or the actions. Still, we need to be aware that
    a malicious attacker could use overly generous permissions to move laterally within
    our environments.
  prefs: []
  type: TYPE_NORMAL
- en: VMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have everything we need for our solution, we can finish by talking
    about where our application components will run: in VMs that have been provisioned
    using AWS EC2.'
  prefs: []
  type: TYPE_NORMAL
- en: When provisioning VMs on AWS, you have two options. First, you can provide static
    VMs. In this approach, you need to specify key characteristics for every VM. Alternatively,
    you can use an **AWS Auto Scaling group** to dynamically provision and manage
    the VMs. In this approach, you provide the Auto Scaling group with some configuration
    and parameters on when to scale up and when to scale down, at which point the
    Auto Scaling group will take care of everything else.
  prefs: []
  type: TYPE_NORMAL
- en: When provisioning a static VM on AWS, you need to associate it with an **AWS
    key pair** to ensure that you can connect to its operating system. This will allow
    your operators to perform diagnostics and update or patch the software and operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: All VMs need to be connected to a virtual network, so when you set up a static
    VM, you need to specify the network configuration. This can be accomplished by
    creating a network interface and associating it with the VM. The network interface
    connects the VM to the appropriate subnet, which is the place where you attach
    one or more security groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The internal configuration of your VM is controlled by two critical attributes:
    the VM image and the user data. As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239),
    the VM image can either be a vanilla installation of an operating system or it
    can be a fully configured version of your application. The decision of **build
    versus bake** is up to you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'User data allows you to run the *last mile* configuration when the VM starts
    up. This can be done using industry-standard `cloud-init` configuration to perform
    a wide variety of tasks such as setting up users/groups, setting up environment
    variables, or mounting disks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Resource VMs created statically](img/B21183_07_16..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Resource VMs created statically
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS can dynamically manage your VMs based on the load that they incur. This
    is done using an Auto Scaling group. This Auto Scaling group is responsible for
    provisioning the VMs. Consequently, this means that the Auto Scaling group needs
    to have the key characteristics that define your VM set on its launch template.
    The Auto Scaling group uses this launch template to specify the configuration
    of each VM that it provisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – VMs created and managed dynamically using an Auto-Scaling Group](img/B21183_07_17..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – VMs created and managed dynamically using an Auto-Scaling Group
  prefs: []
  type: TYPE_NORMAL
- en: Besides this launch template, the Auto Scaling group simply needs to be told
    what subnets the VMs should be provisioned into and under what circumstances it
    should provision or de-provision VMs from the set that it actively manages.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS has a cross-cutting service called **CloudWatch** that can capture logs
    and telemetry from various AWS services you consume within your solutions. We’ll
    be using this as the primary logging mechanism within this book. Many services
    support CloudWatch out of the box with minimal to no configuration to get it working.
    At the same time, other services and scenarios require permissions to be granted
    to allow that service to log in to CloudWatch.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on AWS, we need to come up with a plan for how to provision
    our environments and deploy our code.
  prefs: []
  type: TYPE_NORMAL
- en: VM configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our solution, we have two VM roles: the frontend role, which is responsible
    for handling web page requests from the end user’s web browser, and the backend
    role, which is responsible for handling REST API requests from the web application.
    Each of these roles has different code and a different configuration that needs
    to be set. Each will require its own Packer template to build a VM image that
    we can use to launch a VM on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Packer pipeline to build a VM image for the frontend](img/B21183_07_18..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Packer pipeline to build a VM image for the frontend
  prefs: []
  type: TYPE_NORMAL
- en: A GitHub Actions workflow that triggers off changes to the frontend application
    code and the frontend Packer template will execute `packer build` and create a
    new VM image for the solution’s frontend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the frontend and the backend will have an identical GitHub workflow that
    executes `packer build`. The key difference between the workflows is the code
    bases that they execute against. Both the frontend and the backend might have
    slightly different operating system configurations, and both will require different
    deployment packages for their respective application components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Packer pipeline to build a VM image for the backend](img/B21183_07_19..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Packer pipeline to build a VM image for the backend
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the application code will be baked into the VM image
    rather than copied to an already running VM. This means that to update the software
    running on the VMs, each VM will need to be restarted so that it has a new VM
    image containing the latest copy of the code.
  prefs: []
  type: TYPE_NORMAL
- en: This approach makes the VM image an immutable deployment artifact that is versioned
    and updated each time there is a release of the application code that needs to
    be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud environment configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the VM images have been built for both the frontend and the backend, we
    can execute the final workflow, which will both provision and deploy our solution
    to AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – VM images are used as inputs to the Terraform code, which provisions
    the environment on AWS](img/B21183_07_20..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – VM images are used as inputs to the Terraform code, which provisions
    the environment on AWS
  prefs: []
  type: TYPE_NORMAL
- en: The Terraform code base will have two input variables for the version of the
    VM image for both the frontend and the backend. When new versions of the application
    software need to be deployed, the input parameters for these versions will be
    incremented to reflect the target version for deployment. When the workflow is
    executed, `terraform apply` will simply replace the existing VMs with VMs using
    the new VM image.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a solid plan for how we will implement both the cloud architecture
    using AWS and the deployment architecture using GitHub Actions, let’s start building!
    In the next section, we’ll break down the HashiCorp configuration language code
    that we used to implement the Terraform and Packer solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our design in place, all we need to do is write the code that implements
    the design.
  prefs: []
  type: TYPE_NORMAL
- en: Packer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our solution has a frontend and a backend application component. Although the
    application code is radically different, the way we build a VM image is not.
  prefs: []
  type: TYPE_NORMAL
- en: AWS plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in [*Chapter 4*](B21183_04.xhtml#_idTextAnchor239), Packer
    – like Terraform – is an extensible command-line executable. Each cloud platform
    provides a plugin for Packer that encapsulates the integration with its services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Plugins need to be declared within a Packer solution. At the time of writing,
    the latest version of the AWS Packer plugin is `1.2.6`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AWS plugin for Packer provides an `amazon-ebs` builder that will generate
    an AMI by creating a new VM from a base image, executing the provisioners, taking
    an **Elastic Block Store** (**EBS**) disk image snapshot, and creating an **Amazon
    Machine Image** (**AMI**) from it. This behavior is controlled by the Amazon builder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The first input to the Amazon `amazon-ebs` builder is the base image to use
    when creating the initial VM against which the Packer template’s provisioners
    will be executed. The preceding code references the latest version of the Ubuntu
    `22.04` VM image within the target AWS region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `amazon-ebs` builder references the `amazon-ami` data source to ensure that
    the correct base image is used before the provisioners are executed. Here, `ami_name`
    is probably the most important attribute on this block as it dictates the version
    name that the VM image will be referenced by in `terraform` `apply` operations.
  prefs: []
  type: TYPE_NORMAL
- en: Operating system configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid access control issues, it’s a good idea to establish the context for
    the provisioners to be executed within:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is a standard `execute_command` parameter that can be used to set the context
    for all provisioners. It allows you to eliminate any unnecessary `sudo` commands
    within your installation scripts. The preceding `execution_command` parameter
    will allow your Packer template scripts to execute as a privileged user.
  prefs: []
  type: TYPE_NORMAL
- en: Our solution is built using ASP.NET Core. Therefore, we need to install .NET
    6.0 SDK for our solution to work properly on the VMs. Ubuntu, like other Debian-based
    distributions of Linux, uses the `apt` command-line application to perform package
    management. By default, Ubuntu includes several public repositories that include
    most of the common software packages. However, sometimes, you need to set up additional
    package repositories when the default repositories don’t work. Microsoft hosts
    a package repository for `apt`, that houses the correct software package we need
    to install .NET 6.0 on Ubuntu. Therefore, we need to add that repository before
    we can use `apt` to install .NET 6.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Packer template includes a file called `dotnet.pref` that has the following
    contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the Packer `file` provisioner to copy this file to the correct location
    on the VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, we execute the `install-dotnet6-prereq.sh` bash script, which downloads
    a `.deb` file and installs it using the `dpkg` tool. This registers the third-party
    repository hosted by Microsoft with the Debian package management tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can simply run `apt-get update -y` to get the latest version of the
    packages from all repositories, and we are ready to install .NET 6.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If we don’t include the `packages.microsoft.com` repository, then this `apt-get
    install` command will fail with an error message saying that the `dotnet-sdk-6.0`
    package could not be found.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a service in Linux
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most applications run as processes within Linux that run perpetually. This
    is often the case when the application needs to listen for network traffic – such
    as a web server. Another great benefit of setting up a service in Linux is that
    the operating system can auto-start the service every time the VM reboots. To
    do that, you need to set up a service definition file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This service file needs to be copied to the `/etc/systemd/system` folder. By
    running the `systemctl` command, it will be enabled so that the operating system
    will automatically start the service when the machine reboots. The `systemctl`
    command is also useful to `start`, `stop`, and check the `status` value of your
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s best practice to run services using their own identity. This allows you
    to grant the service access to only the resources on the VM that it needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code sets up a local user and group for the service to run under
    and changes the ownership of the application’s folder at `/var/www/fleet-portal`
    so that the service’s user account has sufficient access to the application’s
    executable and supporting files. Both the user and the application’s working directory
    are specified in the service definition file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the user is ready, we can install the service definition file and enable
    the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the operating system configuration, which can be baked into the
    VM image. Any additional configuration steps require more information from the
    cloud environment that Terraform provisions.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has an application code base that needs to
    be deployed. Since this is the first time we will be using the `aws` provider,
    we’ll look at the basic provider setup and how to configure the backend before
    we look at the nuts and bolts of each component of our architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Provider setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to specify all the providers that we intend to use in this solution
    within the `required_providers` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to configure the AWS provider to ensure that it uses the desired
    target region using the `primary_region` input variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes, you may want to add a secondary region in the future, so it’s a good
    idea to establish the primary region when you start the project. Even if you only
    deploy to one region, you still have a *primary region*.
  prefs: []
  type: TYPE_NORMAL
- en: The AWS provider does require some additional parameters to specify the credentials
    to use to connect to AWS, but because these are sensitive values, we don’t want
    to embed them into the code. We’ll pass those values in later when we automate
    the deployment using the standard AWS `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
    environment variables. It’s important to note that there are many different ways
    to configure the AWS provider to authenticate with AWS. I recommend using environment
    variables as it is a consistent approach across cloud platforms and other Terraform
    providers, and it integrates easily with different pipeline tools, such as GitHub
    Actions, which we’ll be using in the next section and future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Backend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because we will be using a CI/CD pipeline to provision and maintain our environment
    in the long term, we need to set up a remote backend for our Terraform state.
    Because our solution will be hosted on AWS, we’ll use the AWS **Simple Storage
    Service** (**S3**) backend to store our Terraform state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the AWS provider, we don’t want to hard code the backend configuration
    in our code, so we’ll simply set up a placeholder for the backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We’ll configure the backend’s parameters using the `-backend-config` parameters
    when we run `terraform init` in our CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to ensure that the AWS IAM identity you use to authenticate with
    AWS has access to this S3 bucket. Otherwise, you will get authentication errors.
  prefs: []
  type: TYPE_NORMAL
- en: Input variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s good practice to pass in short names that identify the application’s name
    and the application’s environment. This allows you to embed consistent naming
    conventions across the resources that make up your solution, which makes it easier
    to identify and track resources from the AWS Console.
  prefs: []
  type: TYPE_NORMAL
- en: The `primary_region`, `vpc_cidr_block`, and `az_count` input variables drive
    key architectural characteristics of the deployment. They can’t be hard-coded
    as it would limit the reusability of the Terraform code base.
  prefs: []
  type: TYPE_NORMAL
- en: The `vpc_cidr_block` input variable establishes the virtual network address
    space, which is often tightly regulated by an enterprise governance body. There
    is usually a process to ensure that teams across an organization do not use IP
    address ranges that conflict, thus making it impossible to allow those two applications
    to integrate or integrate with shared network resources within the enterprise
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: The `az_count` input variable allows us to configure how much redundancy we
    want within our solution. This will affect the high availability of the solution
    but also the cost of the deployment. As you can imagine, cost is also a tightly
    regulated characteristic of cloud infrastructure deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Consistent naming and tagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The AWS console is designed in such a way that it’s rather difficult to get
    an application-centric view of your deployment. This is why it’s extremely important
    to leave breadcrumbs within the resources that you deploy that indicate what application
    and environment they belong to. Almost all resources within the AWS provider have
    a `map` attribute called `tags`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You should make a habit of setting both the AWS console-recognized `Name` tag
    and a tagging scheme for your devices that establishes application and environment
    ownership of that resource. For our solution, we use two top-level input variables,
    `application_name` and `environment_name`, to set this context, and we’ll embed
    these values on all the resources that we provision.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS can create an application-centric view within the AWS console using something
    called a resource group. Unlike on other platforms, a resource group on AWS is
    not a strong boundary around a set of resources but a loosely coupled relationship
    between resources derived from a common tagging scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates an AWS resource group that creates a central location
    where you can access all of your related resources from one place. Simply adding
    `application` and `environment` tags to all your resources will include them.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because our solution is a standard three-tier architecture, we are configuring
    our virtual network into public and private subnets for the frontend and backend
    application components.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to distribute our VMs across the Availability Zones to ensure the high
    availability of our solution. Rather than hard-code the Availability Zones or
    just take the first two, we can randomly select the number of Availability Zones
    we want from the list of available ones for the given region using the `aws_availability_zones`
    data source and a `random_shuffle` resource from the `random` provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If the `az_count` input variable has a value of `2`, then the preceding code
    will randomly select two Availability Zones from the region of the current AWS
    provider. Remember that the AWS provider is scoped to a particular region, and
    when we initialized the provider, we used the `primary_region` input variable
    to set that value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than hard-code the address space for our subnets, it would be nice if
    we could calculate our subnets’ address space using HCL’s built-in functions.
    The `cidrsubnet` function allows us to take an address space and split it into
    smaller address spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will generate two maps, one for the public subnets and another
    for the private subnets. It accomplishes this by taking the randomly selected
    Availability Zones and using `cidrsubnet` to grab the next available block of
    `/24` or `256` IP addresses for each (this is more than enough for our application
    to scale to a huge number of VMs in each Availability Zone in both the frontend
    and the backend):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is the value that the `public_subnets` and `private_subnets`
    maps will have when evaluated with a `vpc_cidr_block` value of `10.0.0.0/16`,
    a `cidr_split_bits` value of `8` and an `az_count` value of `2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'By manipulating these input variables, we can reasonably size the virtual network
    and its corresponding subnets so that we don’t monopolize available address spaces
    for other applications that we may want to provision within the broader organization.
    For example, setting `vpc_cidr_block` to `10.0.0.0/22` allocates a total IP address
    count of `1024` to our application. With an `az_count` value of `2` and a `cidr_split_bits`
    value of `2`, we can allocate address space for our four subnets, each with `/24`
    and `256` IP addresses. This gives us sufficient room for our application to scale
    without over-allocating valuable IP address space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We create each subnet by iterating over the corresponding map of subnet address
    spaces. The preceding code demonstrates how we can use this map to set the correct
    Availability Zone and address space for each subnet.
  prefs: []
  type: TYPE_NORMAL
- en: Network routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As per our design, the public subnets route internet traffic to the internet
    gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We use the `aws_route_table` resource to define the route and then `aws_route_table_association`
    to link the route table to the corresponding subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The private subnets route their internet traffic to a NAT gateway, which is
    provisioned in each private subnet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Because each private subnet has its own NAT gateway, we need a route table
    for each subnet to route the traffic to the correct NAT gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, unlike the public subnets, which share the same route table, we
    need to iterate on the `private_subnets` map to create a different route table
    for each private subnet and associate it with the corresponding private subnet
    using the `each` symbol.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As per our design, we need two AWS ALB instances – one for the frontend and
    another for the backend. We’ll use the `aws_lb` resource and related resources
    with the `aws_lb` prefix to provision the target group and listener configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the sticky session configuration needed for the ASP.NET Core Blazor
    Web application’s WebSocket configuration is implemented by a nested `stickiness`
    block. Likewise, the health probe is implemented by a nested `health_check` block.
    This structure will be identical for both the frontend and the backend, but the
    configuration will differ slightly, with the backend not requiring sticky sessions
    and having a different path for the health probe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VMs are explicitly included in the target group using the `aws_lb_target_group_attachment`
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we are iterating over the corresponding `aws_instance` resource
    map and referencing the AWS EC2 instance ID using `each.value.id`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we must provision the AWS ALB itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we are dynamically constructing a list of subnets using the corresponding
    `aws_subnet` resource map. When a resource block is provisioned with a `count`
    value, that resource block becomes a list, while when it is provisioned with a
    `for_each` iterator, it becomes a map. This is an important detail to pay attention
    to when you want to reference it from other resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we must connect our AWS ALB to the target group using the listener:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As per our design, we have three logical components of our solution architecture
    through which network traffic will pass. Each needs its own security group and
    set of rules to allow ingress and egress traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: A security group is created using the `aws_security_group` resource and attached
    to a virtual network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all components within the architecture will need both ingress and egress
    rules, but it’s important to think about all the ways network traffic should be
    allowed to flow through the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code establishes the rules we designed for the frontend load balancer,
    which allows traffic in from the internet (for example, `0.0.0.0/0`) and allows
    traffic out to the frontend VMs (for example, `aws_security_group.frontend.id`).
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To allow our VMs to access our AWS Secrets Manager resources, we need to define
    an IAM role and associate it with our VMs. This will allow our VMs to operate
    under the security context defined by the IAM policies attached to this IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates the IAM role for the backend VMs, which need access
    to the PostgreSQL database’s connection string that we will store in AWS Secrets
    Manager. The IAM role itself doesn’t do anything unless there is a policy defined.
    We need to attach a policy definition to the role to grant specific privileges
    to the VMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code grants access to all VMs operating with this IAM role associated
    with accessing AWS Secrets Manager secrets that begin with the `fleet-ops/dev`
    prefix. We must build this prefix using our standard naming convention input variables,
    `application_name` and `environment_name`, which have `fleet-ops` and `dev` as
    values, respectively. When we provision the production version of the `fleet-ops`
    platform, the `environment_name` input variable will be set to `prod`, ensuring
    that the VMs in the `dev` environment don’t have access to the secrets in the
    `prod` environment. Deploying the different environments of our application into
    isolated AWS accounts would also create a more secure security boundary.
  prefs: []
  type: TYPE_NORMAL
- en: VMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When provisioning static VMs, we have much more control over the configuration
    of each machine. Some VMs have specific network and storage configurations to
    meet workload demands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates a network interface that we can then attach to a
    VM. Notice that we are iterating over the frontend subnets. This will ensure we
    have exactly one VM in each subnet (and consequently each Availability Zone).
    This network interface is where we attach the security group for VMs in the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we provision the VM using the `aws_instance` resource, taking care
    to use the correct instance type, network interface, and AWS AMI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: AWS has a cross-cutting service called CloudWatch that collects logs and telemetry
    across the various AWS services. To enable CloudWatch on your EC2 instances, you
    simply need to add the `monitoring` attribute and set it to `true`.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on the service and its available configuration options within the
    Terraform resources used to provision it, to activate CloudWatch, you might need
    to go through the process of provisioning additional resources and setting up
    additional IAM permissions to grant the respective resource to write to CloudWatch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to set up is an IAM policy that will allow the specific
    service access to assume an IAM role. In this case, we are granting VPC Flow Logs
    access to assume an IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use this policy when we set up the IAM role to grant the VPC Flow Logs
    service access to this particular IAM role. This will be important later when
    we link everything together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code allows VPC Flow Logs to assume this role, eventually granting
    it access to writing logs to CloudWatch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to set up another IAM policy that will grant access to write
    to CloudWatch logs. You can further narrow the scope of an access policy by narrowing
    the allowed actions and the allowed resources the policy grants access to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we do a good job of being specific about the types of
    operations we want to grant access to by giving specific operations such as `logs:PutLogEvents`.
    However, the resources are set to `*`, a very wide access level. We should consider
    narrowing that down to just the resources that we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to attach the policy to the IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have an IAM role that is allowed to write to CloudWatch and
    we have allowed VPC Flow Logs to assume this role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create a CloudWatch log group that will store the logs from
    VPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll connect VPC Flow Logs to the log group and assign the IAM role
    it should use to gain access to write to CloudWatch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code also links our VPC to the VPC Flow Logs service, thus completing
    the flow and placing the networking logs in the corresponding CloudWatch log group.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have implemented the Packer and Terraform solutions and have a
    working code base that will build VM images for both our frontend and backend
    application components while also provisioning our cloud environment into AWS.
    In the next section, we’ll dive into YAML and Bash and implement GitHub Actions
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base consisting of application code
    and an operating system configuration encapsulated within a Packer template. These
    two application components are then deployed into a cloud environment on AWS,
    which is defined within our Terraform code base.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an additional code base that we have yet to discuss: our automation
    pipelines. We will be implementing our automation pipelines using GitHub Actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Source code structure within our GitHub repository](img/B21183_07_21..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Source code structure within our GitHub repository
  prefs: []
  type: TYPE_NORMAL
- en: 'In GitHub Actions, automation pipelines are called workflows and they are stored
    in a particular folder within the source code repository, namely `/.github/workflows`.
    Each of our code bases is stored in a separate folder. Our solutions source code
    repository’s folder structure looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As per our design, we will have GitHub Actions workflows that will execute
    Packer and build VM images for both the frontend (for example, `packer-frontend.yaml`)
    and the backend (for example, `packer-backend.yaml`). We’ll also have workflows
    that will run `terraform plan` and `terraform apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Each folder path will allow us to control which GitHub Actions workflows should
    trigger so that we aren’t unnecessarily running workflows when no applicable changes
    have been made.
  prefs: []
  type: TYPE_NORMAL
- en: Because we are following GitFlow, we’ll have a main branch where the production
    version of all of our code will reside. Developers, whether they are working on
    updates to the application code (for example, C#), the operating system configuration
    (for example, the Packer template), or the cloud environment configuration (for
    example, the Terraform template), will create a branch off of `main` with the
    `feature/*` naming convention.
  prefs: []
  type: TYPE_NORMAL
- en: Once they’ve done this, they can submit a pull request. This indicates that
    the developer believes their code changes are ready to be merged back into the
    `main` branch – in other words, their code changes are ready for production!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – GitFlow’s pull request process](img/B21183_07_22..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – GitFlow’s pull request process
  prefs: []
  type: TYPE_NORMAL
- en: The pull request is a great time to perform some checks on our solution’s code.
    For the application code, this could take the form of a build, static code analysis,
    and unit or integration tests. Each of these actions tests a different aspect
    of the application code. The build (that is, compiling the C# code base) is one
    of the most basic tests that we can perform. It simply tests whether the application
    code is valid C# and is devoid of inherent language syntax errors. Static code
    analysis can cover a wide range of code quality checks, including readability
    and maintainability or security and vulnerability assessments. The unit and integration
    tests check the functionality of the software components working individually
    and together to accomplish the underlying business purpose of the software. Executing
    these tests regularly is known as **continuous integration** (**CI**) and is half
    of the famous and often elusive **CI/CD pipeline**, where **CD** stands for **continuous
    delivery**.
  prefs: []
  type: TYPE_NORMAL
- en: The CI pipeline cuts down on routine work surrounding the built-in quality of
    the application code. Without it, these checks would need to be performed by humans
    through exhaustive code reviews and manual testing. We still need to do code reviews
    and manual testing, but a good CI pipeline will reduce the effort that humans
    need to perform.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered what built-in quality controls we can put on application
    code, what can we do with our operating system and our cloud environment configuration?
    Is there a way to test IaC without provisioning the infrastructure? There is,
    but there are limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Packer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because the VM image acts as an immutable artifact that contains a versioned
    copy of the application code and operating system configuration, we need to update
    this artifact any time something changes in either the application code or the
    operating system configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This means that we need a trigger on both code bases that affect the final artifact
    for Packer, which includes the application code and the operating system configuration
    within the Packer template itself. With GitHub Actions, we can add a list of `paths`
    that will trigger our workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should build a new VM image every time there is a pull request and every
    time there is a push onto `main`. When Packer is executed, it is essentially doing
    a pretty rigorous integration test. Therefore, it’s useful to have it performed
    as part of our CI process. That means we need to have a VM image that is tested
    and verified to be production-ready before we push the code into the `main` branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – VM image versioning](img/B21183_07_23..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – VM image versioning
  prefs: []
  type: TYPE_NORMAL
- en: Our Packer workflow will generate a unique name and version for each VM image
    it produces. We can build tests into our Packer template to verify that the web
    server is running and listening on port `5000`. Using this version of the image,
    we can also launch a new VM and inspect the operating system’s configuration ourselves
    to make sure everything is in order.
  prefs: []
  type: TYPE_NORMAL
- en: When we are confident that the code changes to either the application code or
    the operating system configuration are fully functional, we can approve the pull
    request and merge it into the `main` branch. This will trigger a new version of
    the VM image from the production-ready code in the `main` branch. We can use the
    new version of this production-ready VM image to update our cloud environment
    configuration when we are ready to deploy these changes to our environments.
  prefs: []
  type: TYPE_NORMAL
- en: The GitHub Actions workflow needs some ground rules to be established that control
    the specific versions of software and key locations within the code base. It’s
    important to always be specific. This means using specific versions of software
    instead of relying on the internet Gods to decide which version you’ll use. This
    might work well when you are running things locally on your machine and are there
    to solve the inevitable problems and conflicts that arise, but for an automation
    pipeline, there is no human there to correct things as they are happening; there
    are only assumptions – assumptions about what version of the software you’re using.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use two pieces of software: the .NET SDK and Packer. Likewise, we have
    two code bases: the C# .NET code base for the application and the HCL code base
    for Packer. As such, we must establish where these code bases are very clearly
    and upfront. Setting pipeline variables for them is a very useful way of accomplishing
    this as it ensures they are featured prominently in the YAML file and are stored
    in a reusable variable in case they will be repeated multiple times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the triggers and some variables set for our workflow, we need
    to structure the jobs. For each Packer template, we will have two jobs: one that
    builds the C# .NET application code and produces a deployment package and another
    that runs `packer build` to produce the VM image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `build` job performs a pretty standard .NET build process, which includes
    restoring package dependencies from NuGet (the .NET package manager), building
    the code, running unit and integration tests, publishing a deployable artifact,
    and storing that artifact so that it can be used by future jobs within the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Packer workflow](img/B21183_07_24..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – Packer workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'The `packer` job immediately downloads the `.zip` file containing the deployment
    artifact and puts it into a location where the Packer template’s `file` provisioner
    expects it. Then, it generates a unique version of the name for the VM image that
    will be produced if successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: It does this by using Bash to generate the current year and month and appends
    `github.run_number` to ensure uniqueness if we happen to be running this pipeline
    more than once per day.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it obtains the public IP address for the VM on which the GitHub Actions
    workflow is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: It does this so that when it runs `packer build`, it can configure Packer’s
    plugin for AWS to poke a hole in the firewall to allow SSH traffic from the GitHub
    Actions machine to the temporary VM running on AWS where the Packer provisioners
    are executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it installs a specific version of Packer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it executes `packer build`, making sure to specify the `AWS_ACCESS_KEY_ID`
    and `AWS_SECRET_ACCESS_KEY` environment variables that the AWS plugin relies upon
    to authenticate to AWS’s REST APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: It also specifies two input variables to the Packer template using the `PKR_VAR_`
    prefixed environment variable technique so that it includes the image version
    and the build agent IP address, both of which were dynamically generated within
    the GitHub Actions workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With both of our VM images built and their versions input into our `tfvars`
    file, our Terraform automation pipeline is ready to take the reins and not only
    provision our environment but deploy our solution (although not technically).
    The deployment was technically done within the `packer build` process, with the
    physical deployment packages being copied to the home directory and the Linux
    service setup primed and ready. Terraform finishes the job by launching VMs using
    these images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that we only need to trigger the Terraform automation pipeline when
    the Terraform code base changes. This could include configuration changes to the
    resources simply be an updated VM image version within the `tfvars` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – The terraform apply workflow](img/B21183_07_25..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – The terraform apply workflow
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the Terraform pipeline is quite simple. We simply need to execute
    either `terraform plan` or `terraform apply`, depending on whether we want to
    evaluate or execute the changes for our cloud environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In keeping with the *always be specific* mantra, we must dutifully designate
    the version of Terraform that we want to use and specify the location for the
    Terraform code base using pipeline variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must install the particular version of Terraform using the `setup-terraform`
    GitHub Action published by HashiCorp, which will handle the details of its installation
    for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it executes `terraform apply` again, making sure to include the AWS
    credentials and the target backend location for the Terraform state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The backend configuration is set using the `-backend-config` command-line argument,
    which frees us from having to hardcode these settings in our source code.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we execute `terraform apply` twice. First, we perform a targeted
    apply on the `random_shuffle.az` resource, after which we perform a general apply.
    The targeted apply ensures that the Availability Zones we are targeting have been
    selected before we calculate the IP address space for our networks. The need for
    this is driven by the dynamic nature of calculating the address space using the
    `cidrsubnet` function. If we wanted to avoid this targeted apply approach, we
    could opt for a more hard-coded approach of the Availability Zones and the corresponding
    address spaces.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! With the completion of our Terraform GitHub Actions workflow, we
    have put the finishing touches on our end-to-end CI/CD pipeline. Our AWS-based
    solution will be up and running our VM cloud architecture in no time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a multi-tier cloud architecture using AWS and VMs,
    a fully operational GitFlow process, and an end-to-end CI/CD pipeline using GitHub
    Actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, our fearless leader at Söze Enterprises will be throwing
    us into turmoil with some big new ideas, and we’ll have to respond to his call
    to action. It turns out our CEO, Keyser, has been up late watching some YouTube
    videos about the next big thing – containers – and after talking with his pal
    Jeff on his superyacht, he has decided that we need to refactor our whole solution
    so that it can run on Docker and Kubernetes. Luckily, the good people at Amazon
    have a service that might help us out: AWS **Elastic Kubernetes** **Service**
    (**EKS**).'
  prefs: []
  type: TYPE_NORMAL
