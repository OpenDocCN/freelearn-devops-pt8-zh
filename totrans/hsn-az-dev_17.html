<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Big Data Storage - Azure Data Lake</h1>
                </header>
            
            <article>
                
<p>Sometimes, we have to store unlimited amounts of data. That scenario covers most big data platforms, where having even a soft limit for the maximum capacity could cause problems with the active development and maintenance of our application. Thanks to Azure Data Lake, we have limitless possibilities when it comes to storing both structured and unstructured data, all with an efficient security model and great performance.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Azure Data Lake Store fundamentals</li>
<li>Storing data in Azure Data Lake Store </li>
<li>Security features and concerns</li>
<li>Best practices for working with Azure Data Lake Store</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To perform the exercises in this chapter, you will need:</p>
<ul>
<li>Access to an Azure subscription</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding Azure Data Lake Store</h1>
                </header>
            
            <article>
                
<p>When considering your storage solution, you have to take into account the amount of data you want to store. Depending on your answer, you may choose a different option from services available in Azure—Azure Storage, Azure SQL, or Azure Cosmos DB. There is also a variety of databases available as images for VMs (such as Cassandra or MongoDB); the ecosystem is quite rich so everyone can find what they are looking for. The problem arises when you do not have an upper limit for the amount of data stored or, considering the characteristics of today's applications, that amount grows so rapidly that there is no possibility to declare a safe limit, which we will never hit. For those kinds of scenario, there is a separate kind of storage named Data Lakes. They allow you to store data in its natural format, so it does not imply any kind of structure over information stored. In Azure, a solution for that kind of problem is named Azure Data Lake Store<span>; </span>in this chapter, you will learn the basics of this service, which allows you to dive deeper into the service and adjust it to your needs.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Data Lake Store fundamentals</h1>
                </header>
            
            <article>
                
<p>Azure Data Lake Store is called a hyper-scale repository for data for a reason—there is no limit when it comes to storing files. It can have any format, be any size, and store information structured differently. This is also a great model for big data analytics as you can store files in the way that is the best for your processing services (some prefer a small number of big files, some prefer many small files – choose what suits you the most). This is not possible for other storage solutions such as relational, NoSQL, or graph databases, as they always have some restrictions when it comes to saving unstructured data. Let's check an example comparison between Azure Data Lake Store and Azure Storage:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td/>
<td><strong>AZDS</strong></td>
<td><strong>Azure Storage</strong></td>
</tr>
<tr>
<td><strong>Limits</strong></td>
<td>No file size/number of files limits</td>
<td>Maximum account capacity of 500 TBs, the maximum size of files</td>
</tr>
<tr>
<td><strong>Redundancy</strong></td>
<td>LRS</td>
<td>LRS/ZRS/GRS/RA-GRS</td>
</tr>
<tr>
<td><strong>API</strong></td>
<td>WebHDFS</td>
<td>Azure Blob Storage API</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The important thing here is the redundancy—for now, the only model which Azure Data Lake Store supports is LRS. That means that, in the event of a disaster, you may lose data stored inside a single data center. To avoid that, you will have to implement your own policy to copy data to a replica. In fact, you have available two models—synchronous replication, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/6e7bb730-f268-497e-afff-f01ac1910c0d.png" style="width:31.33em;height:18.75em;" width="466" height="278"/></p>
<p>Or you have asynchronous, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b4b4b501-74a0-4c00-9593-0513251b8e01.png" style="width:31.08em;height:20.42em;" width="514" height="337"/></p>
<p>There are some obvious pros and cons of both solutions:</p>
<ul>
<li><strong>Synchronous</strong>: Ensures that a copy of data was saved to a replica, more difficult to handle when considering duplicates, and lower performance.</li>
<li><strong>Asynchronous</strong>: Data can be lost (because you will not move data to a replica before a disaster), better performance (because you just save without waiting for replication), and easier to handle.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>While replication may look better for Azure Storage, remember that the Azure Data Lake Store filesystem is based on HDFS—this allows for a seamless integration with many OSS tools, such as:</p>
<ul>
<li>Apache Hive</li>
<li>Apache Storm</li>
<li>Apache Spark</li>
<li>MapReduce</li>
<li>Apache Pig</li>
<li> And many more...!</li>
</ul>
<p>This gives you a much better ecosystem, tool-wise. If you want to store data inside Azure Data Lake Store and prefer to use HDInsights to perform analysis and transformations over your files, instead of other Azure tools, you can easily connect to your instance and start working on them.</p>
<div class="packt_infobox">Note that for now, ADLS supports <span>HDInsight 3.2, 3.4, 3.5, and 3.6 distributions.</span></div>
<p>When it comes to accessing files stored inside an instance of Azure Data Lake Store, it leverages the POSIX-style permissions model; you basically operate on three different permissions, which can be applied to a file or a folder:</p>
<ul>
<li><strong>Read (R)</strong>: For reading data</li>
<li><strong>Write (W)</strong>: For writing data</li>
<li><strong>Execute (E)</strong>: Applicable to a folder, used to give read/write permissions in a folder context (such as creating children or listing files)</li>
</ul>
<p>We will cover more security concepts in the security section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating an Azure Data Lake Store instance</h1>
                </header>
            
            <article>
                
<p>To create an Azure Data Lake Store instance, you will need to search for <kbd>Azure Data Lake</kbd><em> </em>in the portal and fill in the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/cfd77d09-ca4d-4a30-b11b-9895e7882c57.png" style="width:23.83em;height:32.25em;" width="403" height="546"/></p>
<p>However, you need to take into consideration the following facts:</p>
<ul>
<li><span class="packt_screen">Location</span>: Currently there are four different locations available—<span class="packt_screen">Central US</span>, <span class="packt_screen">East US 2</span>, <span class="packt_screen">North Europe</span>, and <span class="packt_screen">West Europe</span>. Do remember that transferring data between DCs costs you extra money, so if you plan to use this service, plan your architecture carefully.</li>
<li><span class="packt_screen">Pricing package</span>: There are two pricing models available—<span class="packt_screen">Pay-as-You-Go</span> and fixed <span class="packt_screen">Monthly commitment</span>. They have pros and cons (fixed pricing is in general cheaper but it is not that flexible when your application grows, it is difficult sometimes to plan required capacity ahead), so try to understand as best you can the characteristics of your applications using that service to choose whatever suits you the most.</li>
<li><span class="packt_screen">Encryption settings</span>: By default, encryption of your data is <span class="packt_screen">Enabled</span> for a new account. While it is possible to disable it, in most cases you will stay with the default settings. What is more, there are two models of encryption—either you let the service  manage encryption keys for you, or you provide your own keys (stored inside Azure Key Vault).</li>
</ul>
<div class="packt_tip">Since it is a good idea to rotate encryption keys, you may face the issue when, due to a failure, even redundant copies of your data are inaccessible. While it is possible to recover from backup data, you will need an old key to decrypt it. Because of that, it is advisable to store a copy of old keys in case of unexpected outages.</div>
<p>When you click on the <span class="packt_screen">Create </span>button, your service will be provisioned—you can access it to see the overview:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/782a7941-5ce3-4aea-9220-ffdeb94d048a.png" style="width:38.08em;height:36.00em;" width="656" height="622"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Since it is freshly created, we cannot see different metrics which describe how much data we are storing. What is more, the current cost is 0 USD—this is, of course, something we expected as no file was uploaded to the service. From the UI perspective, there is not much that we can do for now; some additional features such as <span class="packt_screen">Firewall </span>will be described later in that chapter. Besides the portal, you can also easily access your instance of Azure Data Lake Store by using Microsoft Azure Storage Explorer:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a023fcf9-5bae-4f31-b7f1-2f872bb76899.png" width="1136" height="366"/></p>
<p>It makes things much easier when you have multiple files and folders and you try to navigate through them.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Storing data in Azure Data Lake Store</h1>
                </header>
            
            <article>
                
<p>Because Azure Data Lake Store is all about storing data, in this section of the chapter you will see how you can store different files, use permissions to restrict access to them, and organize your instance. The important thing to remember here is the fact that you are not limited to using big data tools to store or access data stored within a service—if you manage to communicate with the Azure Data Lake Store protocol, you can easily operate on files using C#, JavaScript, or any other kind of programming language.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using the Azure portal to navigate</h1>
                </header>
            
            <article>
                
<p>To get started with working with files in the Azure portal, you will have to click on the <span class="packt_screen">Data explorer</span><strong> </strong>button:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/14d96ccb-3be3-409b-a16d-10fb860d12af.png" style="width:37.17em;height:17.00em;" width="623" height="285"/></p>
<p>Once you click on it, you will see a new screen, where you are given many different options for creating a folder, uploading files, or changing access properties. While this tool is not the best way to manage thousands of files, it gives you some insight into what is stored and how:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/fc95ca29-aa07-461b-bd70-cf4fbaf0f1ca.png" width="1152" height="467"/></p>
<div class="packt_infobox">The downside of the UI available in the portal is the fact that it has a tendency to hang, especially if you have hundreds of files. Some options (such as deleting a folder) also tend to fail if you have stored gigabytes of data. In that scenario, it is better to either use PowerShell or custom procedures to perform an operation.</div>
<p>Now we will discuss options available on the UI.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Filter</h1>
                </header>
            
            <article>
                
<p>When you click on the <span class="packt_screen">Filter </span>button, you will see a screen that tells you what files you are interested in:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0c4d2f79-0835-4149-af31-6bc499ef594d.png" style="width:16.17em;height:16.83em;" width="300" height="312"/></p>
<p>It is the easiest way to quickly limit files displayed within a folder, but of course it has some caveats—for example, you cannot use a wildcard to filter only a specific file extension.</p>
<div class="packt_tip">To remove a filter, click on the <span class="packt_screen">Reset </span>button on the <span class="packt_screen">Filter </span>screen.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">New folder</h1>
                </header>
            
            <article>
                
<p>This simple option gives you the possibility to create a new folder:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/15eee2c6-2190-4bd4-b9e3-7f94d401553b.png" style="width:43.00em;height:13.50em;" width="853" height="267"/></p>
<p>Note that by default a new folder can be accessed only by you—to make it visible to others (and to allow them to read it), you will have to assign a particular group of users explicitly to it.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Upload</h1>
                </header>
            
            <article>
                
<p>With the <span class="packt_screen">Upload </span>function, you can upload files directly from your local machine to the cloud:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0e65e259-6129-43b9-a178-ee3f3b71cc19.png" style="width:43.92em;height:26.42em;" width="837" height="504"/></p>
<p>The files you choose will be uploaded to the folder you are currently browsing. There is also the possibility of allowing overwriting existing files; if you decide not to do so and upload a duplicate, you will see the following error:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/9c921855-8494-4b9f-b8eb-524a6fed15ad.png" width="835" height="326"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Access</h1>
                </header>
            
            <article>
                
<p>One of the most important features of Azure Data Lake Store is the ability to fully declare access to a specific resource stored inside it:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d36c0bb8-9efa-47c1-84c0-defca584e7e9.png" style="width:40.00em;height:36.83em;" width="567" height="522"/></p>
<p>By default, only you can access a file or a folder. To add a new user or a group, you can click on the <span class="packt_screen">+ Add</span><strong> </strong>button:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/71b4f4a8-fb81-43cc-8957-3b8f5fbb14ce.png" style="width:42.58em;height:24.08em;" width="610" height="346"/></p>
<p>There are important things I would like to cover here:</p>
<ul>
<li><span class="packt_screen">Permissions</span>: Remember that to grant somebody access to list files inside a folder, you will have to assign two permissions: <span class="packt_screen">Read </span>and <span class="packt_screen">Execute</span>. The same applies to creating children inside a folder.</li>
<li><span class="packt_screen">Add to</span>: It is possible to propagate a particular set of permissions, not only to a single folder but also to all folders inside it. This is especially helpful when you can quickly allow somebody to list files and folders inside some parent directory.</li>
<li><span class="packt_screen">Add as</span>: You can add a set of permissions, either as a default permission entry (which will be assigned by default to all other users of a folder) or as an access entry (which specifies how somebody can access it). You can also combine both to speed things up.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Files and folders</h1>
                </header>
            
            <article>
                
<p>Next to each file and folder visible, you can see an icon, which displays a menu with additional options:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/859cc1cb-fc6c-4335-8e2c-26204d145102.png" width="1015" height="259"/></p>
<p>In fact, these are the same options we have just covered—they just apply to a specific folder and file.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Microsoft Azure Storage Explorer</h1>
                </header>
            
            <article>
                
<p>Most of the preceding options can be performed using Microsoft Azure Storage Explorer:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b7acd5e9-c1d3-43e3-b1f6-64945ffec5c7.png" width="1193" height="498"/></p>
<p>Unfortunately, it does not give you the possibility to assign permissions to files and folders. It is, however, a much better option for browsing stored data—what is more it automatically displays a set of required permissions assigned to an item.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using SDKs</h1>
                </header>
            
            <article>
                
<p>The most flexible (and the most advanced) option to manage files and your Azure Data Lake Store instance is using an SDK for a language you are using. Currently, there are three different languages officially supported:</p>
<ul>
<li>.NET platform</li>
<li>Java</li>
<li>Python</li>
</ul>
<p>There is also the possibility of using a REST API, so basically you can connect with it using any language you want. Here, The following code snippet allows you to connect to the service:</p>
<pre><span>var client = AdlsClient.CreateClient("&lt;account-name"&gt;, &lt;credentials&gt;);</span></pre>
<p>There are two options available when it comes to authenticating to connect to your service:</p>
<ul>
<li>End-user authentication</li>
<li>Service-to-service authentication</li>
</ul>
<p>Both scenarios are described in detail in the documentation available in the <em>Further reading</em><strong> </strong>section. Whichever option you choose, you will end up with using a generated OAuth 2.0 token. Here, you can find a simple provider to a service that leverages the described methods and allows you to easily create a new folder and append data to a file:</p>
<pre>public class DataLakeProvider : IDisposable<br/>{<br/>   private readonly DataLakeStoreFileSystemManagementClient _client;<br/> <br/>    public DataLakeProvider(string clientId, string clientSecret)<br/>    {<br/>        var clientCredential = new ClientCredential(clientId, clientSecret);<br/>        var creds = ApplicationTokenProvider.LoginSilentAsync("domainId", clientCredential).Result;<br/>        _client = new DataLakeStoreFileSystemManagementClient(creds);<br/>    }<br/> <br/>    public Task CreateDirectory(string path)<br/>    {<br/>        return _client.FileSystem.MkdirsAsync("datalakeaccount", path);<br/>    }<br/> <br/>    public async Task AppendToFile(string destinationPath, string content)<br/>    {<br/>        using (var stream = new MemoryStream(Encoding.UTF8.GetBytes(content)))<br/>        {<br/>            await _client.FileSystem.ConcurrentAppendAsync("datalakeaccount", destinationPath, stream, appendMode: AppendModeType.Autocreate);<br/>        }<br/>    }<br/> <br/>    public void Dispose()<br/>    {<br/>        _client.Dispose();<br/>    }<br/>}</pre>
<p>You can read more about writing such a provider in the blog post mentioned in the <em>Further reading</em><strong> </strong>section.</p>
<div class="packt_tip">The important thing about using SDKs is the ability to abstract many operations and automate them—you can easily delete files recursively or dynamically create them. Such operations are unavailable when using UIs and most serious project developers would rather code stuff than rely on manual file management.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Security</h1>
                </header>
            
            <article>
                
<p>Azure Data Lake Store offers a bit of a different security model than other storage options available for Azure. In fact, it offers you a complex solution that consists of authentication, authorization, network isolation, data protection, and auditing. As it is designed to be the very base of data-driven systems, it has to extend common capabilities when it comes to securing who (or what) and how to access information stored. In this section, we will cover different security features available and describe them in detail, so you are familiar with them and know how to use them.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Authentication and authorization</h1>
                </header>
            
            <article>
                
<p>To authenticate who or what can access data stored, Azure Data Lake Store uses Azure Active Directory to know what the current entity accessing data is. To authorize it, it leverages both <strong>role-based access control</strong><em> </em>(<strong>RBAC</strong>), to secure the resource itself, and POSIX ACL to secure data.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>It is important to understand the distinction between these two terms:</p>
<ul>
<li><strong>Authentication</strong>: This determines who or what tries to access a particular resource.</li>
<li><strong>Authorization</strong>: This secures a resource by limiting access to it to those who have been assigned a particular set of permissions.</li>
</ul>
<div class="packt_tip">It is important to remember that if you have multiple subscriptions hosting different resources that would like to access Azure Data Lake Store, you have to assign the same Azure AD instance to all of them—if you fail to do so, some will not be able to access data, as only users and services defined within a directory assigned to ADLS can be authenticated and given access to it.</div>
<p>Let's check the difference between the RBAC and POSIX models.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">RBAC</h1>
                </header>
            
            <article>
                
<p>RBAC controls who can access an Azure resource. It is a separate set of roles and permissions, that has nothing to do with the data stored. To check this feature, click on the <span class="packt_screen">Access control (IAM)</span><strong> </strong>blade:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/30fc62be-cd45-4d9f-99c9-0c28c7902cd0.png" width="1226" height="585"/></p>
<p class="mce-root"/>
<p>On the preceding screen, you can see that I have three different apps (services) and one user assigned to the resource. They also have different roles:</p>
<ul>
<li><span class="packt_screen">Owner</span>: Full access including determining who can access the resource</li>
<li><span class="packt_screen">Contributor</span>: Full access excluding determining who can access the resource</li>
</ul>
<p>If you click the <span class="packt_screen">Roles </span>button, you will see a full list of possible roles for the ADLS:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d075c409-8ef7-4988-8f16-4c41ba0116ce.png" style="width:41.92em;height:38.00em;" width="591" height="536"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Using the <span class="packt_screen">Access Control (IAM)</span><strong> </strong>blade, you can easily control who can access your instance of Azure Data Lake Store and how—use it any time you want to change permissions or the set of users/services accessing it.</p>
<div class="packt_tip">A good idea is to manage groups rather than individual entities—this allows you to add/remove a user or an entity in one place (Azure AD) instead of browsing resources and their RBAC.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">POSIX ACL</h1>
                </header>
            
            <article>
                
<p>As described previously, you can manage access to data stored within your instance of ADLS by providing a set of permissions defined as <span class="packt_screen">Read</span>, <span class="packt_screen">Write</span>,<strong> </strong>and <span class="packt_screen">Execute</span>. They are the part of the POSIX <strong>access control list</strong> (<strong><span>ACL</span></strong>) that is a feature of Hadoop HDFS, which is the part of the engine of this Azure service. If you have used, for example FTP servers, you probably have worked with filesystem permissions; they were described as numbers or strings containing the letters <kbd>r</kbd>, <kbd>w</kbd>, <kbd>x</kbd>, and the character <kbd>-</kbd>. The following is an example:</p>
<ul>
<li><kbd>-rwx------</kbd><strong><span> </span></strong><span>is equal to</span> <kbd>0700</kbd> <span>and declares read, write, and execute permissions only for the owner.</span></li>
<li><kbd>-rwxrwxrwx</kbd><strong><span> </span></strong><span>is equal to</span> <kbd>0777</kbd> <span>and declares read, write, and execute permissions for everyone.</span></li>
<li><kbd>-rw-rw-rw-</kbd><strong><span> </span></strong><span>is equal to</span> <kbd>0666</kbd> <span>and declares read and write permissions for everyone.</span></li>
</ul>
<p>You can find more about the POSIX ACL model in the <em>Further reading</em><strong> </strong>section.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Network isolation</h1>
                </header>
            
            <article>
                
<p>I mentioned the <span class="packt_screen">Firewall </span>blade earlier, but we skipped it so you could learn something about it once you are familiar with the service. When you click on the <span class="packt_screen">Firewall </span>blade, you will see a screen that allows you to specify which IP address can access your instance:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/1902c0b2-ee17-4201-8ae4-4d129fcd0dbc.png" width="764" height="472"/></p>
<p>The important thing here is the ability to block other Azure services from accessing your data—this can be helpful if you have requirements that force you to disallow anyone from reading any information stored in ADLS. You can find out more about security features in the link provided in the <em>Further reading</em><strong> </strong>section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Best practices</h1>
                </header>
            
            <article>
                
<p>Azure Data Lake Store is a bit different when it comes to accessing data stored and performing read and writes. As this service is designed for storing petabytes of data, it is important to know the best practices for doing so, to avoid problems such as the need to reorganize all files or slow reads/writes. This also includes security features (as discussed earlier), as this is an important part of the whole solution. In this section, we will focus on multiple advice regarding ADLS, so you will use it consciously and leverage the best practices.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Performance</h1>
                </header>
            
            <article>
                
<p>One important feature of many storage solutions is their performance. In general, we expect that our databases will work without a problem whether the load is low or high and a single record is big or small. When it comes to ADLS, you have to take into account the following factors:</p>
<ul>
<li><strong>Parallelism</strong>: As stated in the documentation, it is important to ensure, that you provide a certain level of parallelism when performing reads/writes. The ideal number of threads per one core is defined as 8-12.</li>
<li><strong>File size</strong>: While different data analytics solutions may work differently with different file sizes, it is also important to know that ADLS also has an optimal file size to work with. As it is based on HDFS and leverages the POSIX model for permissions, it promotes bigger files (several hundred megabytes) instead of smaller ones to avoid problems with replication, connections, and authentication checks.</li>
<li><strong>I/O limits</strong>: While some hard limits when it comes to throughput are not enabled on Azure Data Lake Store, you still can face some problems when your jobs are very demanding, capacity-wise. It is important to remember that even in this service, you can still face some soft limits that can be removed after contacting Azure support. If you face a 429 error, throttling may be the case.</li>
<li><strong>Batching</strong>: As in many cases where you face high throughput, it may be beneficial to use batching to lower write operations. In ADLS, the optimal size for a batch is defined as 4 MBs – by performing writes of that size, you can lower the required IOPS and improve the overall performance of the service.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Security</h1>
                </header>
            
            <article>
                
<p>We discussed this topic a little previously, but here we summarize it. When using ADLS and considering its security features (such as authentication, authorization, and access to files), it is important to remember the following things:</p>
<ul>
<li><strong>Prefer groups over users/services</strong>: While, initially, it is easier to assign an individual user to a resource or a folder, you will quickly face problems when the number of people interested in data starts to grow rapidly. This is why it is better to use Azure AD groups to both determine RBAC access to the resource itself and POSIX ACL for files and folders. It also improves the performance of the solution, as it is quicker to check whether an entity belongs to a group than to traverse through a long list of users.</li>
<li><strong>The minimum set of permissions</strong>: As in other services, always start with a minimum set of permissions required by someone who accesses your instance of Azure Data Lake Store. Do not assign a <span class="packt_screen">Write </span>permission to somebody who only reads data, or <span class="packt_screen">Execute </span>to a service that reads only a single file in a folder.</li>
<li><strong>Enable the firewall</strong>: In general, you do not want to allow anyone to access data stored inside ADLS. To secure your solution, so that only a subset of IP addresses can access information, enable the firewall so anyone outside the list will be rejected. </li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Resiliency</h1>
                </header>
            
            <article>
                
<p>It is crucial to ensure, that your data is stored in a safe manner and will not be lost in the case of any issue inside the DC. As mentioned at the very beginning of this chapter, ADLS does not support geo-redundancy—you have to implement it on your own. To do so, you have to incorporate a tool that will allow you to replicate data in the way you need. There are three different tools mentioned in the documentation—Distcp, Azure Data Factory, and AdlsCopy, but of course, you can use any other tool that can connect to Azure Data Lake Store and integrate with the service. </p>
<div class="packt_tip">When considering DR or HA for Azure Data Lake Store, take into consideration factors such as RPO, inconsistency, and complex data merging problems in the event of performing a failover. Sometimes, it is better to wait for a service to recover instead of switching to the secondary replica.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data structure</h1>
                </header>
            
            <article>
                
<p>You will choose a different data structure for different use scenarios—for IoT data it will be very granular:</p>
<pre>{Vector1}/{Vector2}/{Vector3}/{YYYY}/{MM}/{DD}/{HH}/{mm}</pre>
<p>On the other hand, for storing user data, the structure may be completely different:</p>
<pre>{AppName}/{UserId}/{YYYY}/{MM}/{DD}</pre>
<p>It all depends on your current requirements. The data structure is extremely important when you plan to perform an analysis on the files stored—it directly affects the size of files and their number, which can further affect the possible toolset for your activities.</p>
<div class="packt_tip">Another important thing here is the legal requirements—if you use any kind of sensitive data as a folder or a filename, you will have to be able to perform a clean up efficiently if a user tells you that he/she wants to be forgotten or asks for an account to be removed.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you have learned a bit about Azure Data Lake Store, an Azure service designed to store an almost unlimited amount of data without affecting its structure. We have covered things such as data structure, security features, and best practices, so you should be able to get started on your own and build your very first solution based on this particular Azure component. Bear in mind that what can easily replace Azure Storage for example—it all depends on your requirements and expectations. If you're looking for a more flexible security model, better performance, and better limits, ADLS is for you. This ends this part of the book, which included services for storing data, monitoring services, and performing communication between them. In the next chapter, you will learn more about scaling, performance, and maintainability in Azure.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Which security model is better—managing security groups or individual entities, and why?</li>
<li>What is the difference between RBAC and POSIX ACL?</li>
<li>What is the maximum size of a file in ADLS?</li>
</ol>
<ol start="4">
<li>Which data structure is better—a single folder containing thousands of files or a hierarchy of folders containing several files each?</li>
<li>Can Azure Data Lake Store be used with any programming language?</li>
<li>What is the difference between ADLS and Azure Storage?</li>
<li>How do you ensure that your solution based on ADLS is geo-redundant?</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>End-user authentication: <a href="https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-end-user-authenticate-net-sdk">https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-end-user-authenticate-net-sdk</a></li>
<li>Service-to-service authentica<strong>tion</strong>: <a href="https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-net-sdk">https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-net-sdk</a></li>
<li>Writing a Data Lake Store provider: <a href="http://blog.codenova.pl/post/azure-functions-webjobs-and-data-lake-writing-a-custom-extension-2">http://blog.codenova.pl/post/azure-functions-webjobs-and-data-lake-writing-a-custom-extension-2</a></li>
<li>Data Lake Store operations .NET: <a href="https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-net-sdk">https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-net-sdk</a></li>
<li>POSIX ACL: <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists">https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists</a></li>
<li>Security overview: <a href="https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-security-overview">https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-security-overview</a></li>
<li>Best practices: <a href="https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices">https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices</a></li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>