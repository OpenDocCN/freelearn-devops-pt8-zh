- en: Chapter 4. Automating Complete Infrastructures with Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a complete CoreOS infrastructure on Digital Ocean with Terraform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning a three-tier infrastructure on Google Compute Engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning a GitLab CE + CI runners on OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Heroku Apps and Add-ons using Terraform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a scalable Docker Swarm cluster on bare metal with Packet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll describe complete infrastructures using Terraform, how
    it looks when everything is tied together, with a real project in mind. Most examples
    from previous chapters on Terraform were on Amazon Web Services, so to try to
    be more diverse and complete, this chapter is dedicated to other infrastructure
    services, namely Digital Ocean, Google Cloud, Heroku, and Packet. On Digital Ocean,
    we'll build a fully working and monitored CoreOS cluster with DNS dynamically
    updated. On Google Cloud, we'll build a three-tier infrastructure with two HTTP
    nodes behind a load balancer and an isolated MySQL managed database. Using OpenStack,
    we'll deploy a GitLab CE and two GitLab CI runners, using different storage solutions.
    We'll see how we can integrate and automate a Heroku environment. We'll end this
    chapter with a powerful and scalable Docker Swarm cluster on bare metal using
    Packet, capable of scaling hundreds of containers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Terraform version in use for this book is 0.7.4.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a complete CoreOS infrastructure on Digital Ocean with Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll build from scratch a fully working CoreOS cluster on Digital
    Ocean in their New York region, using Terraform and cloud-init. We'll add some
    latency monitoring as well with StatusCake, so we have a good foundation of using
    Terraform on Digital Ocean.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To step through this recipe, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A working Terraform installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Digital Ocean account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A StatusCake account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by creating the `digitalocean` provider (it only requires an API
    token) in a file named `providers.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare the `do_token` variable in a file named `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, don''t forget to set it in a private `terraform.tfvars` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Handling the SSH key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that we''ll need an SSH key to log into the cluster members. With Digital
    Ocean, the resource is named `digitalocean_ssh_key`. I propose that we name the
    SSH key file `iac_admin_sshkey` in the `keys` directory, but as you might prefer
    something else, let''s use a variable for that as well. Let''s write this in a
    `keys.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the related variable in `variables.tf`, with our suggested default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s now time to effectively override the value in the `terraform.tfvars`
    file if you feel like it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Creating the CoreOS cluster members
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here''s the core of our infrastructure: three nodes running in the New York
    City data center NYC1, with private networking enabled, no backups activated (set
    it to `true` if you feel like it!), the SSH key we previously created, and a cloud-init
    file to initiate configuration. A virtual machine at Digital Ocean is named a
    *droplet*, so the resource to launch a droplet is `digitalocean_droplet`. All
    variables'' names relate to what we just enumerated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare all the variables in the `variables.tf` file, with some good defaults
    (the smallest 512 MB droplet, a three-node cluster), and some defaults we''ll
    want to override (AMS3 data center or the stable CoreOS channel):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are our overridden values in `terraform.tfvars` (but feel free to put
    your own values, such as using another data center or CoreOS release):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Adding useful output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It would be awesome to automatically have a few auto-documented lines on how
    to connect to our CoreOS cluster. As we can do that with the Terraform outputs,
    let''s use this example for a start, in `outputs.tf`. This is constructing an
    SSH command line with dynamic information from Terraform that we''ll be able to
    use easily (it''s simply iterating over every `digitalocean_droplet.coreos.*`
    available):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic DNS Integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the attractive features of Digital Ocean is the easy DNS integration.
    For example, if our domain is `infrastructure-as-code.org` and we launch a *blog*
    droplet, we''ll end up registering it automatically under the public DNS name
    `blog.infrastructure-as-code.org`. Pretty easy and dynamic! To give Digital Ocean
    power on our domain, we need to go to our registrar (where we bought our domain),
    and configure our domain to be managed by Digital Ocean, using their own nameservers,
    which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ns1.digitalocean.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ns2.digitalocean.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ns3.digitalocean.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This prerequisite being done, let''s declare our domain in the `dns.tf` file
    using the `digitalocean_domain` resource, automatically using a `cluster_domainname`
    variable for the domain name, and an initial IP address matching, that we can
    either set to a value you already know or to an arbitrary droplet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the new variable in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to override it as necessary in `terraform.tfvars`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to register automatically every droplet in the DNS. By iterating
    over each droplet, and extracting their `name` and `ipv4_address` attributes,
    we''ll add this `digitalocean_record` resource into the mix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will automatically register every droplet under the name core-[1,2,3].mydomain.com,
    for easier access and reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you like, you can access the `fqdn` attribute of this resource right from
    the outputs (`outputs.tf`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Integrating cloud-init
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to build a fully working `cloud-config.yml` file for our CoreOS cluster.
    Refer to the cloud-init part of this book in [Chapter 5](ch05.html "Chapter 5. Provisioning
    the Last Mile with Cloud-Init"), *Provisioning the Last Mile with Cloud-Init*
    for more information on the `cloud-config.yml` file, and especially on configuring
    CoreOS with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need for a fully usable CoreOS cluster are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A working etcd cluster on the local network interface (`$private_ipv4`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A working fleet cluster on the local network interface (`$private_ipv4`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleet is a distributed init system. You can think of it as systemd for a whole
    cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To configure etcd, we first need to obtain a new token. This token is unique
    and can be distributed through different channels. It can be easily obtained through
    the [https://coreos.com/os/docs/latest/cluster-discovery.html](https://coreos.com/os/docs/latest/cluster-discovery.html)
    etcd service. Then we'll start 2 units—etcd and fleet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Note this URL carefully and copy paste it in the following `cloud-config.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This will be enough to start an etcd + fleet cluster on CoreOS. [Chapter 5](ch05.html
    "Chapter 5. Provisioning the Last Mile with Cloud-Init"), *Provisioning the Last*
  prefs: []
  type: TYPE_NORMAL
- en: '*Mile with Cloud-Init*, for in-depth details on cloud-init.'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating dynamic StatusCake monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can reuse our knowledge from previous chapters to easily integrate full latency
    monitoring to the hosts of our CoreOS cluster, using a free StatusCake account
    ([https://statuscake.com](https://statuscake.com)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by configuring the provider in `providers.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare the required variables in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Also, override with your own values in `terraform.tfvars`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use the `statuscake_test` resource to activate immediate latency
    (ping) monitoring on every droplet by iterating over each `digitalocean_droplet.coreos.*`
    resource value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s time to `terraform apply` this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Confirm that we can connect to a member using the command line from the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the etcd cluster health:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that all fleet members are all right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Enjoy, in less than a minute, you're ready to use a CoreOS cluster with basic
    monitoring, using only fully automated Terraform code!
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning a three-tier infrastructure on Google Compute Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll provision a ready to use, three-tier, load-balanced web infrastructure
    on Google Compute Engine, using two CentOS 7.2 servers for the web and one master
    Google MySQL instance. The MySQL instance will allow connections only from the
    two web servers (with valid credentials), and all three instances (SQL and HTTP)
    will be accessible from a single *corporate* network (our company''s network).
    The topology looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Provisioning a three-tier infrastructure on Google Compute Engine](img/B05671_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To step through this recipe, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A working Terraform installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Google Compute Engine account with a project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we need to do is to get our credentials from the console.
  prefs: []
  type: TYPE_NORMAL
- en: Generating API credentials for a Google project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Navigate to your Google Cloud project, and in the *API Manager*, select **Credentials**
    | **Create credentials** | **Service Account Key**. Now choose **Compute Engine
    default service account** from the dropdown list, in the JSON format. Save this
    file as `account.json` at the root of the infrastructure repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the variables to define our credentials file in `variables.tf`, store
    the region we''re running in, and the Google Compute project name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to override those values in `terraform.tfvars` if you want to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in a `providers.tf` file, add the `google` provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our `google` provider is now configured!
  prefs: []
  type: TYPE_NORMAL
- en: Creating Google Compute HTTP instances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here''s the checklist of our requirements for these HTTP hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: We want two of them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their type is `n1-standard-1` (3.75 GB of RAM, one vCPU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Their region and zone is: us-east1-d'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They are running CentOS 7.2 (official image is: centos-cloud/centos 7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default SSH username is `centos`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SSH key known to us is (`keys/admin_key`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want a fully updated system with Docker installed and running
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s define generic variables for all these requirements in a `variables.tf`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s override in `terraform.tfvars` the generic values we just set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Google Cloud instances are called from Terraform using the resource `google_compute_instance`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add what we already know in this resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This could be enough, but we want to go much farther.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we''ll later add a firewall, whose rule will apply to a target
    defined by its tags. Let''s add a tag right now, so we can use it later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to configure networking. It''s necessary in our case to have a public
    IPv4, because we need to access the servers by SSH from outside. We might have
    chosen to not have publicly exposed servers and use a bastion host instead. To
    create a network interface in our default network, mapped behind a public IPv4,
    add the following to the `google_compute_instance` resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s finish by connecting automatically to each instance and fully update
    it, then install, enable, and start Docker. We do this using the `remote-exec`
    provisioner, correctly configured with the right SSH username and private key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We're finally done, with our two instances automatically provisioned!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Google Compute Firewall rule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our goal is simple: we want to allow anyone (0.0.0.0/0) to access using HTTP
    (TCP port `80`) any instance with the tag `www` in the default network. To do
    this, let''s use the `google_compute_firewall` resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Load balancing Google Compute instances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To load balance requests across our two instances, we''ll need to create a
    *pool* of hosts, where membership will be handled by a simple health check: an
    HTTP `GET` on / every second, with an immediate timeout (`1` second), and removal
    after `3` errors. We can do this in a file named `pool.tf` with the `google_compute_http_health_check`
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to transform those values into variables for better tuning on your
    end!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define the pool, which is defined by the results of the health
    checks and instances inclusion. This is done using the `google_compute_target_pool`
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `self_link` attribute returns the URI of the resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have our pool of hosts with health checks, let''s create the load balancer
    itself. It''s done using the `google_compute_forwarding_rule` resource, simply
    pointing to the pool of hosts we created earlier. Add the following in a `loadbalancer.tf`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Creating a Google MySQL database instance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our typical target application needs a database to store and access data. We
    won't get into database replication here, but it can also be done quite simply
    with Terraform on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Double-check you have the SQL API activated in the Google Cloud Console: [https://console.cloud.google.com/apis/library](https://console.cloud.google.com/apis/library).
    By default, it isn''t.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a checklist of what we know about our MySQL database:'
  prefs: []
  type: TYPE_NORMAL
- en: It's running on us-east1 region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's running MySQL 5.6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's type is *D2* (1 GB of RAM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our own network and both HTTP servers can access it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want a database named `app_db`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want a user with a password to be allowed to connect from the HTTP servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s put all these variables in the `variables.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to override each generic value in the `terraform.tfvars`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build our database using the `google_sql_database_instance` resource
    in a `db.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `pricing_plan` `"PACKAGE"` is more interesting for a long-lasting database.
    Also, the `authorized_network` block doesn't currently support a `count` value,
    so we can't iterate dynamically over every HTTP host. For now, we have to duplicate
    the block, but that may very well change in a newer Terraform version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now create a database, using a `google_sql_database` resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finish by creating the SQL user with host restriction. Like the `authorized_network`
    block, the `google_sql_user` resource doesn''t support a count value yet, so we
    have to duplicate the code for each HTTP server for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Adding some useful outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It would be awesome to have some useful information such as IPs for all our
    instances and services and usernames and passwords. Let''s add some outputs in
    `outputs.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Here we are!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Simply deploy our application on the HTTP servers and we''re done! To test
    drive the load balancer and the HTTP instances, you can simply deploy the NGINX
    container on each server and see the traffic flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Provisioning a GitLab CE + CI runners on OpenStack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack is a very popular open source cloud computing solution. Many providers
    are based on it, and you can roll your own in your data center. In this example,
    we'll use the public OpenStack by OVH, located in Montreal, QC (Canada), but we
    can use any other OpenStack. There're differences in implementation for every
    custom deployment, but we'll stick with very stable features.
  prefs: []
  type: TYPE_NORMAL
- en: We'll launch one compute instance running Ubuntu LTS 16.04 for GitLab, with
    a dedicated block device for Docker, and two other compute instances for GitLab
    CI runners. Security will allow HTTP for everyone, but SSH only for a known IP
    from our corporate network. To store our builds or releases, we'll create a *container*,
    which is in OpenStack terminology—an object storage. The equivalent with AWS S3
    is a *bucket*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To step through this recipe, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A working Terraform installation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenStack account on any OpenStack provider (public or private). This recipe
    uses an account on OVH's public OpenStack ([https://www.ovh.com/us/](https://www.ovh.com/us/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Internet connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll create:'
  prefs: []
  type: TYPE_NORMAL
- en: Three compute instances (virtual machines)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One keypair
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One block storage device
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One security group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One object storage bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring the OpenStack provider
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s start by configuring the OpenStack provider. We need four pieces of
    information: a username, a password, an OpenStack tenant name, and an OpenStack
    authentication endpoint URL. To make the code very dynamic, let''s create variables
    for those in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to override the default values with your own in the `terraform.tfvars`
    file!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Now we're good to go.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a key pair on OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To authenticate ourselves on the instances, we need to provide the public part
    of the key pair to OpenStack. This is done using the `openstack_compute_keypair_v2`
    resource, specifying in which region we want the key, and where the key is. Let''s
    add both variables in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, override them in the `terraform.tfvars` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build our resource in the `keys.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Creating a security group on OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know our requirements are to allow HTTP (TCP/80) from anywhere, but SSH
    (TCP/22) only from one corporate network. Add it right now in `variables.tf` so
    we can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to override with your own network in `terraform.tfvars`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a first security group allowing HTTP for everyone in our region,
    using the `openstack_compute_secgroup_v2` resource in a `security.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the same pattern, create another security group to allow SSH only
    from our corporate network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Creating block storage volumes on OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our requirements, we want a dedicated volume to be available to our GitLab
    instance, for Docker. We decide this one will be `10` GB in size. This volume
    will be mounted by the compute instance under a dedicated device (likely `/dev/vdb`).
    The whole thing is done using the `openstack_blockstorage_volume_v2` resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a simple output in `outputs.tf` so we know the volume description, name,
    and size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We now have every requirement to launch our compute instances.
  prefs: []
  type: TYPE_NORMAL
- en: Creating compute instances on OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It''s now time to create the instances. We know they have to be Ubuntu 16.04,
    and we decide on a flavor name: a flavor is the type of the machine. It varies
    from every other OpenStack installation. In our case, it''s named `vps-ssd-1`.
    Let''s define some defaults in the `variables.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, override them with good values in `terraform.tfvars`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a compute instance, we use a resource named `openstack_compute_instance_v2`.
    This resource takes all the parameters we previously declared (name, image, flavor,
    SSH key, and security groups). Let''s try this in `instances.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'To attach the block storage volume we created, we need to add a `volume {}`
    block inside the resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, an optional but fun part is that the commands needed to format the volume,
    mount it at the right place, fully update the system, install Docker, and run
    the GitLab CE container. This is done using the `remote-exec` provisioner and
    requires a SSH username. Let''s set it as `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can just type in all the commands to be executed when the instance is
    ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a simple output in the `outputs.tf` file, so we easily know the GitLab
    instance public IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The runner instances are the same, but a little simpler, as they don''t need
    a local volume. However, we need to set the amount of runners we want in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Override the value to have more runners in `terraform.tfvars`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create our runner instances using the `openstack_compute_instance_v2`
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: This will launch a GitLab CI runner, so builds can be triggered by GitLab! (there's
    one last step of configuration, though. It's out of the scope of this book, but
    we need to register each runner to the main GitLab instance by executing `docker
    exec -it gitlab-runner gitlab-runner register` and answering the questions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following output to `outputs.tf` so we know all the IP addresses of
    our runners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Creating an object storage container on OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This one is very simple: it only requires a name and a region. As it''s to
    store releases, let''s call it `releases`, using the `openstack_objectstorage_container_v1`
    resource, in an `objectstorage.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a simple output in `outputs.tf` so we remember the `Object Storage` container
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Applying
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the end, do a `terraform apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Connect to the GitLab instance and enjoy the runners (after GitLab token registration)!
  prefs: []
  type: TYPE_NORMAL
- en: Managing Heroku apps and add-ons using Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Heroku is a popular **Platform-as-a-Service** (**PaaS**), where you have absolutely
    no control over the infrastructure. But even for such platforms, Terraform can
    automate and manage things for you, so Heroku can do the rest. We''ll create an
    app (a simple GitHub Hubot: [http://hubot.github.com/](http://hubot.github.com/)),
    but feel free to use your own. On top of this app, we''ll automatically plug a
    Heroku add-on (redis) and deploy everything.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To step through this recipe, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A working Terraform installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Heroku account ([https://www.heroku.com/](https://www.heroku.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional Slack Token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First things first: we need to define the Heroku provider. It consists of an
    e-mail address and an API key. Let''s create generic variables for that in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to override them in `terraform.tfvars`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create the Heroku provider with the information we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Creating a Heroku application with Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of clicking through Heroku to create an application, let''s do it right
    from Terraform. We want to run our app in Europe and we want Hubot to connect
    to Slack, so we need to provide a Slack token as well. Let''s start by creating
    default values in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create our first Heroku app with its variables using the `heroku_app`
    resource, in `heroku.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: That's it! As simple as it seems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add some output in `outputs.tf` so we have better information about our app,
    like the Heroku app URL and environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Adding Heroku add-ons using Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some add-ons need Redis to store data. Instead of going through the web application
    and enabling add-ons, let''s instead use the `heroku_addon` resource. It takes
    a reference to the app to link the add-on to, and a plan (`hobby-dev` is free,
    so let''s use that):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Using Heroku with Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It''s out of the scope of this book to show Heroku usage, but let''s apply
    this terraform code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don''t have an application ready to ship on Heroku, let''s try to deploy
    GitHub''s chat robot *Hubot*. It''s an easy application ready to use on Heroku.
    Quickly reading through the Hubot documentation, let''s install the Hubot generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new `hubot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Answer the questions and when you''re done, using the usual `heroku` command,
    add the Heroku git remote for our Heroku app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Now you can `git push heroku` and see your application being deployed, all using
    Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a scalable Docker Swarm cluster on bare metal with Packet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IaaS clouds have been popularized through heavy usage of virtual machines. Recent
    initiatives are targeting bare metal servers with an API, so we get the best of
    both worlds—on-demand servers through an API and incredible performance through
    direct access to the hardware. [https://www.packet.net/](https://www.packet.net/)
    is a bare metal IaaS provider ([https://www.scaleway.com/](https://www.scaleway.com/)
    is another) very well supported by Terraform with an awesome global network. Within
    minutes we have new hardware ready and connected to the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll build a fully automated and scalable Docker Swarm cluster, so we can
    operate highly scalable and performant workloads on bare metal: this setup can
    scale thousands of containers in just a few minutes. This cluster is composed
    of *Type 0* machines (4 cores and 8 GB RAM), for one manager and 2 nodes, totaling
    12 cores and 24 GB of RAM, but we can use more performant machines if we want:
    the same cluster with *Type 2* machines will have 72 cores and 768 GB of RAM (though
    the price will adapt accordingly).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To step through this recipe, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A working Terraform installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Packet.net account with an API key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by creating the `packet` provider, using the API key (an authentication
    token). Create the variable in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, be sure to override the value in `terraform.tfvars` with the real token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Creating a Packet project using Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Packet, like some other IaaS providers, uses the notion of *project* to group
    machines. Let''s create a project named `Docker Swarm Bare Metal Infrastructure`,
    since that''s what we want to do, in a `projects.tf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: This way, if you happen to manage multiple projects or customers, you can split
    them all into their own projects.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Packet SSH keys using Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To connect to the machines using SSH, we need at least one public key uploaded
    to our Packet account. Let''s create a variable to store it in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to override the value in `terraform.tfvars` if you use another
    name for the key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `packet_ssh_key` resource to create the SSH key on our Packet
    account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Bootstraping a Docker Swarm manager on Packet using Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll create two types of servers for this Docker Swarm cluster: managers
    and nodes. Managers are controlling what''s executed on the nodes. We''ll start
    by bootstrapping the Docker Swarm manager server, using the Packet service (more
    alternatives are available from Packet API):'
  prefs: []
  type: TYPE_NORMAL
- en: We want the cheapest server (`baremetal_0`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want the servers in Amsterdam (`ams1`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want the servers to run Ubuntu 16.04 (`ubuntu_16_04_image`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Default SSH user is `root`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Billing will be `hourly`, but that can be `monthly` as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s put generic information in `variables.tf` so we can manipulate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, override them in `terraform.tfvars` to match our values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a server with Packet, let''s use the `packet_device` resource, specifying
    the chosen plan, facility, operating system, billing, and the project in which
    it will run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's create two scripts that will execute when the server is ready. The
    first one will update Ubuntu (`update_os.sh`) while the second will install Docker
    (`install_docker.sh`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'This script will install and start Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call those scripts as a `remote-exec` provisioner inside the `packet_device`
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the system is fully provisioned and functional, with Docker running.
  prefs: []
  type: TYPE_NORMAL
- en: 'To initialize a Docker Swarm cluster, starting with Docker 1.12, we can just
    issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'A server at Packet has one interface sharing both public and private IP addresses.
    The private IP is the second one, and is available through the following exported
    attribute: `${packet_device.swarm_master.network.2.address}`. Let''s create another
    `remote-exec` provisioner, so the Swarm manager is initialized automatically,
    right after bootstrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have a Docker cluster running, with only one node—the manager
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to store the Swarm token, so the nodes can join. The token
    can be obtained with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll store this token in a simple file in our infrastructure repository (`worker.token`),
    so we can access it and version it. Let''s create a variable to store our token
    in a file in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'We will execute the previous `docker swarm` command through SSH when everything
    else is done, using a `local-exec` provisioner. As we can''t interact with the
    process, let''s skip the host key checking and other initial SSH checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: We're now done with the Docker Swarm manager!
  prefs: []
  type: TYPE_NORMAL
- en: Bootstraping Docker Swarm nodes on Packet using Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need nodes to join the swarm, so the workload can be spread. For convenience,
    the machine specs for the nodes will be the same as that of the master. Here''s
    what will happen:'
  prefs: []
  type: TYPE_NORMAL
- en: Two nodes are created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The token file is sent to each node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operating system is updated, and Docker is installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The node joins the swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start by creating a variable for the number of nodes we want, in `variables.tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Override that value as the cluster grows in `terraform.tfvars`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the nodes using the same `packet_device` resource we used for the master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a `file` provisioner to copy the token file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same update and Docker installation scripts as the master, create
    the same `remote-exec` provisioner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: The operating system is now fully updated and Docker is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we want to join the Docker Swarm cluster. To do this, we need two pieces
    of information: the token and the local IP of the master. We already have the
    token in a file locally, and Terraform knows the local IP of the swarm manager.
    So a trick is to create a simple script (I suggest you write a more robust one!),
    that reads the local token, and takes the local manager IP address as an argument.
    In a file named `scripts/join_swarm.sh`, enter the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we just have to send this file to the nodes using the `file` provisioner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Use it as a last step through a `remote-exec` provisioner, sending the local
    Docker master IP (`${packet_device.swarm_master.network.2.address}"`) as an argument
    to the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the whole infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Our cluster is running.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Docker Swarm cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using our Docker Swarm cluster is out of the scope of this book, but now we
    have it, let's take a quick look to scale a container to the thousands!
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify we have our 3 nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: We want a common network for our containers, and we want to scale to the thousands.
    So a typical /24 network won't be enough (that's the `docker network` default).
    Let's create a /16 overlay network, so we have room for scale!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Docker service that will simply launch an nginx container on this
    new overlay network, with 3 replicas (3 instances of the container running at
    the same time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify if it''s working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, accessing by HTTP any of the public IPs of the cluster, any container
    of any node can answer: we can make an HTTP request to node-1, and it can be a
    container on node-2 responding. Nice!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s scale our service now, from 3 replicas to 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: We just scaled to a hundred containers in a few seconds and split them on all
    3 bare metal machines.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you know you can scale, and with such a configuration you can push the
    `nginx` service to 500, 1000, or maybe more!
  prefs: []
  type: TYPE_NORMAL
