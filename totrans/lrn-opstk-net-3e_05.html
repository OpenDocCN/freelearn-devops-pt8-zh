<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building a Virtual Switching Infrastructure Using Open vSwitch</h1>
                </header>
            
            <article>
                
<p class="chapter-content">In <em><a href="05786c3c-b24e-40dc-82a7-ed6072eca14f.xhtml">Chapter 4</a>, Virtual Network Infrastructure Using Linux Bridges</em>, we looked at how the Linux bridge mechanism driver and agent build a virtual network infrastructure using different types of interfaces and Linux bridges. In this chapter, you will be introduced to the Open vSwitch mechanism driver and its respective agent, which utilizes Open vSwitch as the virtual switching technology that connect instances and hosts to the physical network.</p>
<p class="chapter-content">In this chapter, you will do the following:</p>
<ul>
<li>Discover how Open vSwitch is used to build a virtual network infrastructure</li>
<li>Visualize traffic flow through virtual switches</li>
<li>Deploy the Open vSwitch mechanism driver and agent on hosts</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Open vSwitch driver</h1>
                </header>
            
            <article>
                
<p>The Open vSwitch mechanism driver supports a range of traditional and overlay networking technologies, and has support for the following types of drivers:</p>
<ul>
<li>Local</li>
<li>Flat</li>
<li>VLAN</li>
<li>VXLAN</li>
<li>GRE</li>
</ul>
<p class="mce-root"/>
<p>Within OpenStack Networking, Open vSwitch operates as a software switch that uses virtual network bridges and flow rules to forward packets between hosts. Although it is capable of supporting many technologies and protocols, only a subset of Open vSwitch features are leveraged by OpenStack Networking.</p>
<p>The following are three main components of Open vSwitch:</p>
<ul>
<li><strong>Kernel module</strong>: The <kbd>openvswitch</kbd> kernel module is the equivalent of ASICs on a hardware switch. It is the data plane of the switch where all packet processing takes place.</li>
<li><strong>vSwitch daemon</strong>: The <kbd>ovs-vswitchd</kbd> daemon is a Linux process that runs in user space on every physical host and dictates how the kernel module will be programmed.</li>
<li><strong>Database server</strong>: An OpenStack/Open vSwitch implementation uses a local database on every physical host called the <strong>Open vSwitch Database Server</strong> (<strong>OVSDB</strong>), which maintains the configuration of the virtual switches.</li>
</ul>
<p>A high-level architecture diagram of the preceding components can be seen here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/08b22c1f-e238-4798-8af0-291cd2da79be.png" style="width:37.75em;height:26.58em;"/></div>
<p>The Neutron Open vSwitch agent, <kbd>neutron-openvswitch-agent</kbd>, is a service that's configured on hosts using the Open vSwitch mechanism driver and is responsible for managing the implementation of networks and related interfaces. The agent connects tap interfaces to Open vSwitch or Linux bridges, depending on the firewall configuration, and programs flows using utilities such as <kbd>ovs-vsctl</kbd> and <kbd>ovs-ofctl</kbd> based on data provided by the <kbd>neutron-server</kbd> service.</p>
<p>In an Open vSwitch-based network implementation, there are five distinct types of virtual networking devices, as follows:</p>
<ul>
<li>Tap devices</li>
<li>Linux bridges</li>
<li>Virtual ethernet cables</li>
<li>OVS bridges</li>
<li>OVS patch ports</li>
</ul>
<p>Tap devices and Linux bridges were described briefly in the previous section, and their use in an Open vSwitch-based network remains the same. Virtual Ethernet (<strong>veth</strong>) cables are virtual interfaces that mimic network patch cables. An Ethernet frame sent to one end of a veth cable is received by the other end, just like a real network patch cable. Neutron makes use of veth cables when making connections between network namespaces and Linux bridges, as well as when connecting Linux bridges to Open vSwitch switches.</p>
<p>Neutron connects interfaces used by DHCP or router namespaces and instances to OVS bridge ports. The ports themselves can be configured much like a physical switch port. Open vSwitch maintains information about connected devices, including MAC addresses and interface statistics.</p>
<p>Open vSwitch has a built-in port type that mimics the behavior of a Linux veth cable, but is optimized for use with OVS bridges. When connecting two Open vSwitch bridges, a port on each switch is reserved as a <strong>patch port</strong>. Patch ports are configured with a peer name that corresponds to the patch port on the other switch. Graphically, it looks something like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/08895d69-89b1-4519-9485-414488ea8d98.png" style="width:30.00em;height:9.75em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 5.1</div>
<p>In the preceding diagram, two OVS bridges are cross-connected via a patch port on each switch. Open vSwitch patch ports are used to connect Open vSwitch bridges to each other, while Linux veth interfaces are used to connect Open vSwitch bridges to Linux bridges, or Linux bridges to other Linux bridges.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic OpenvSwitch commands</h1>
                </header>
            
            <article>
                
<p>Open vSwitch includes utilities that can be used to manage virtual switches created by users, including those created by the OpenStack Networking agent. These commands are useful when troubleshooting issues that inevitably occur on the network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Base commands</h1>
                </header>
            
            <article>
                
<p>The majority of Open vSwitch configuration and troubleshooting can be accomplished with the following commands:</p>
<ul>
<li><kbd>ovs-vsctl</kbd>: A tool used to configure the <kbd>ovs-vswitchd</kbd> database</li>
<li><kbd>ovs-ofctl</kbd>: A tool used for monitoring and administering OpenFlow switches</li>
<li><kbd>ovs-dpctl</kbd>: A tool used to administer Open vSwitch data paths</li>
<li><kbd>ovs-appctl</kbd>: A tool used to query and manage Open vSwitch daemons</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ovs-vsctl</h1>
                </header>
            
            <article>
                
<p>The <kbd>ovs-vsctl</kbd> tool is used to configure and view OVS bridge/switch operations. With this tool, users can configure ports on a switch, create and delete virtual switches, create bonds, and manage VLAN tagging on ports.</p>
<p>Useful commands include the following:</p>
<ul>
<li><kbd>ovs-vsctl show</kbd>: Prints a brief overview of the switch database configuration, including ports, VLANs, and so on</li>
<li><kbd>ovs-vsctl list-br</kbd>: Prints a list of configured bridges</li>
<li><kbd>ovs-vsctl list-ports &lt;bridge&gt;</kbd>: Prints a list of ports on the specified bridge</li>
<li><kbd>ovs-vsctl list interface</kbd>: Prints a list of interfaces along with statistics and other data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ovs-ofctl</h1>
                </header>
            
            <article>
                
<p>The <kbd>ovs-ofctl</kbd> tool is used to monitor and administer OpenFlow switches. The Neutron Open vSwitch agent uses <kbd>ovs-ofctl</kbd> to program flows on the virtual switches that are used to dictate traffic flow, perform VLAN tagging, perform NAT, and more.</p>
<p>Useful commands include the following:</p>
<ul>
<li><kbd>ovs-ofctl show &lt;bridge&gt;</kbd>: Shows OpenFlow features, actions, and port descriptions for the specified bridge.</li>
<li><kbd>ovs-ofctl dump-flows &lt;bridge&gt; &lt;flow&gt;</kbd>: Prints the flow entries for the specified bridge. If the flow is specified, only that flow is shown.</li>
<li><kbd>ovs-ofctl dump-ports-desc &lt;bridge&gt;</kbd>: Prints port statistics for the specified bridge, including the state, peer, and speed of the interface.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ovs-dpctl</h1>
                </header>
            
            <article>
                
<p>The <kbd>ovs-dpctl</kbd> tool is used to administer and query Open vSwitch data paths. Unlike <kbd>ovs-ofctl</kbd>, <kbd>ovs-dpctl</kbd> reflects flows for packets that have been matched by actual traffic traversing the system.</p>
<p>Useful commands include the following:</p>
<ul>
<li><kbd>ovs-dpctl dump-flows</kbd>: Shows the flow table data for all flows traversing the system</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ovs-appctl</h1>
                </header>
            
            <article>
                
<p>The <kbd>ovs-appctl</kbd> tool is used to query and manage Open vSwitch daemons, including <kbd>ovs-vswitchd</kbd>, <kbd>ovs-controller</kbd>, and others.</p>
<p>Useful commands include the following:</p>
<ul>
<li><kbd>ovs-appctl bridge/dump-flows &lt;bridge&gt;</kbd>: Dumps OpenFlow flows on the specified bridge</li>
<li><kbd>ovs-appctl dpif/dump-flows &lt;bridge&gt;</kbd>: Dumps data path flows on the specified bridge</li>
<li><kbd>ovs-appctl ofproto/trace &lt;bridge&gt; &lt;flow&gt;</kbd>: Shows the entire flow field of a given flow, including the matched rule and the action taken</li>
</ul>
<div class="packt_infobox"><span>Many of these commands are used by the Neutron Open vSwitch agent to program virtual switches and can often be used by operators to troubleshoot network connectivity issues along the way. Familiarizing yourself with these commands and their output is highly recommended.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing traffic flow when using Open vSwitch</h1>
                </header>
            
            <article>
                
<p>When using the Open vSwitch driver, for an Ethernet frame to travel from the virtual machine instance to the physical network, it will pass through many different interfaces, including the following:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 164px">
<p><strong>Network Type</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 185.156px">
<p><strong>Interface Type</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 385.844px">
<p><strong>Interface Name</strong></p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>all</p>
</td>
<td style="width: 185.156px">
<p>tap</p>
</td>
<td style="width: 385.844px">
<p>tapN</p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>all</p>
</td>
<td style="width: 185.156px">
<p>bridge</p>
</td>
<td style="width: 385.844px">
<p>qbrXXXX (only used with the iptables firewall driver)</p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>all</p>
</td>
<td style="width: 185.156px">
<p>veth</p>
</td>
<td style="width: 385.844px">
<p><kbd>qvbXXXX</kbd>, <kbd>qvoXXXX</kbd> (only used with the iptables firewall driver)</p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>all</p>
</td>
<td style="width: 185.156px">
<p>vSwitch</p>
</td>
<td style="width: 385.844px">
<p>br-int</p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>flat, vlan</p>
</td>
<td style="width: 185.156px">
<p>vSwitch</p>
</td>
<td style="width: 385.844px">
<p>br-ex (user-configurable)</p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>vxlan, gre</p>
</td>
<td style="width: 185.156px">
<p>vSwitch</p>
</td>
<td style="width: 385.844px">
<p>br-tun</p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>flat, vlan</p>
</td>
<td style="width: 185.156px">
<p>patch</p>
</td>
<td style="width: 385.844px">
<p><kbd>int-br-ethX</kbd>, <kbd>phy-br-ethX</kbd></p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>vxlan, gre</p>
</td>
<td style="width: 185.156px">
<p>patch</p>
</td>
<td style="width: 385.844px">
<p>patch-tun, patch-int</p>
</td>
</tr>
<tr>
<td style="width: 164px">
<p>flat, vlan</p>
</td>
<td style="width: 185.156px">
<p>physical</p>
</td>
<td style="width: 385.844px">
<p>ethX (where X is the interface)</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The Open vSwitch bridge <kbd>br-int</kbd> is known as the <strong>integration bridge</strong>. The integration bridge is the central virtual switch that most virtual devices are connected to, including instances, DHCP servers, routers, and more. When Neutron security groups are enabled and the iptables firewall driver is used, instances are not directly connected to the integration bridge. Instead, instances are connected to individual Linux bridges that are cross-connected to the integration bridge using a veth cable.</p>
<div class="packt_infobox">The <kbd>openvswitch</kbd> firewall driver is an alternative driver that implements security group rules using OpenFlow rules, but this is outside the scope of this book.</div>
<p>The Open vSwitch bridge <kbd>br-ethX</kbd> is known as the <strong>provider bridge</strong>. The provider bridge provides connectivity to the physical network via a connected physical interface. The provider bridge is also connected to the integration bridge by a virtual patch cable which is provided by patch ports <kbd>int-br-ethX</kbd> and <kbd>phy-br-ethX</kbd>.</p>
<p>A visual representation of the architecture described here can be seen in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/d01dc49d-0c21-44e9-bb34-8b00bdc884c8.png" style="width:37.83em;height:47.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 5.2</div>
<p>In the preceding diagram, instances are connected to an individual Linux bridge via their respective tap interface. The Linux bridges are connected to the OVS integration bridge using a <strong>veth</strong> interface. OpenFlow rules on the integration bridge dictate how traffic is forwarded through the virtual switch. The integration bridge is connected to the provider bridge using an OVS patch cable. Lastly, the provider bridge is connected to the physical network interface, which allows traffic to enter and exit the host onto the physical network infrastructure.</p>
<p>When using the Open vSwitch driver, each controller, network, or compute node in the environment has its own integration bridge and provider bridge. The virtual switches across nodes are effectively cross-connected to one another through the physical network. More than one provider bridge can be configured on a host, but often requires the use of a dedicated physical interface per provider bridge.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying ports on the virtual switch</h1>
                </header>
            
            <article>
                
<p>Using the <kbd>ovs-ofctl show br-int</kbd> command, we can see a logical representation of the integration bridge. The following screenshot demonstrates the use of this command to show the switch ports of the integration bridge on <kbd>compute02</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/ea15fddd-3a0c-4ed0-85de-6f0bd970ac59.png" style="width:41.75em;height:34.58em;"/></div>
<p>The following are the components demonstrated in the preceding screenshot:</p>
<ul>
<li>Port number 1 is named <kbd>int-br-eth2</kbd> and is one end of an OVS patch cable. The other end connects to the provider bridge, <kbd>br-eth2</kbd> (not shown).</li>
<li>Port number 2 is named <kbd>patch-tun</kbd> and is one end of an OVS patch cable. The other end connects to the tunnel bridge, <kbd>br-tun</kbd> (not pictured).</li>
<li>Port number 3 is named <kbd>qvo3de035cc-79</kbd> and corresponds to Neutron port <kbd>3de035cc-79a9-4172-bb25-d4a7ea96325e</kbd>.</li>
<li>Port number 4 is named <kbd>qvoce30da31-3a</kbd>a and corresponds to Neutron port <kbd>ce30da31-3a71-4c60-a350-ac0453b24d7d</kbd>.</li>
<li>Port number 5 is named <kbd>qvoa943af89-8e</kbd> and corresponds to Neutron port <kbd><kbd>a943af89-8e21-4b1d-877f-abe946f6e565.</kbd></kbd></li>
<li>The LOCAL port named <kbd>br-int</kbd> is used internally by Open vSwitch and can be ignored.</li>
</ul>
<p>The following screenshot demonstrates the switch configuration in a graphical manner:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/190fb5f1-dbf1-4043-a3ad-38199335ffd6.png" style="width:34.08em;height:42.83em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Figure 5.3</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying the local VLANs associated with ports</h1>
                </header>
            
            <article>
                
<p>Every port on the integration bridge connected to an instance or other network resource is placed in a VLAN that is local to that virtual switch.</p>
<p>The Open vSwitch database on each host is independent of all other hosts, and the local VLAN database is not directly related to the physical network infrastructure. Instances in the same Neutron network on a particular host are placed in the same VLAN on the local integration bridge, but there is no VLAN ID consistency expected between hosts. That said, flow rules will be implemented on each host that maps the local VLAN ID to the ID associated with the respective Neutron network, allowing for traffic between hosts across the common VLAN. This behavior will be discussed in further detail later in this chapter.</p>
<p>Using the <kbd>ovs-vsctl show</kbd> command, you can identify the local VLAN tag of all ports on all virtual switches on the host. The following screenshot demonstrates this command in action on <kbd>compute02</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/14c94a42-eda9-4b2a-a887-53cb89438140.png" style="width:15.25em;height:38.83em;"/><br/></div>
<p>Connected to the integration bridge are three interfaces named <kbd>qvoce30da31-3a</kbd>, <kbd>qvoa943af89-8e</kbd>, and <kbd>qvo3de035cc-79</kbd>. Two of the interfaces are in the same network and reside in the same local VLAN. The other interface, <kbd>qvoa943af89-8e</kbd>, is in a different network and thus is a different VLAN.</p>
<div class="packt_infobox">The local VLAN IDs are arbitrarily assigned by the local Open vSwitch process and may change upon restart of the <kbd>openvswitch-switch</kbd> service or after a reboot.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Programming flow rules</h1>
                </header>
            
            <article>
                
<p>Unlike the Linux bridge architecture, the Open vSwitch driver does not use VLAN interfaces on the host to tag traffic. Instead, the Open vSwitch agent programs flow rules on the virtual switches that dictate how traffic traversing the switch should be manipulated before forwarding. When traffic traverses a virtual switch, flow rules on the switch can transform, add, or strip the VLAN tags before forwarding the traffic. In addition to this, flow rules can be added that drop traffic if it matches certain characteristics. Open vSwitch is capable of performing other types of actions on traffic, but those actions are outside the scope of this book.</p>
<p>Using the <kbd>ovs-ofctl dump-flows &lt;bridge&gt;</kbd> command, we can observe the flows that are currently programmed on the specified bridge. The Open vSwitch plugin agent is responsible for converting information about the network in the Neutron database to Open vSwitch flows, and constantly maintains the flows as changes are being made to the network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flow rules for VLAN networks</h1>
                </header>
            
            <article>
                
<p>In the following example, VLANs <kbd>40</kbd> and <kbd>42</kbd> represent two networks in the data center. Both VLANs have been trunked down to the <kbd>controller</kbd> and <kbd>compute</kbd> nodes, and Neutron networks have been configured that utilize those VLAN IDs.</p>
<p>On the physical switch, the necessary configuration to facilitate the networking described here will resemble the following:</p>
<pre>vlan 40<br/>    name VLAN_40<br/>vlan 42<br/>    name VLAN_42<br/><br/>interface Ethernet1/4<br/>    description Provider_Interface_eth2<br/>    switchport<br/>    switchport mode trunk<br/>    switchport trunk allowed vlan add 40,42<br/>    no shutdown </pre>
<p>When configured as a trunk port, the provider interface can support multiple VLAN networks. Traffic that enters physical interface <kbd>eth2</kbd> is processed by the flow rules on the <kbd>br-eth2</kbd> bridge it is connected to. Flow rules are processed in order of priority from highest to lowest. By default, <kbd>ovs-ofctl</kbd> returns flow entries in the same order that the virtual switch sends them. Using <kbd>--rsort</kbd>, it is possible to return the results in order of priority, from highest to lowest, to match the order in which packets are processed:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/d50adb5c-2202-4c79-88f4-128d13aa80e1.png"/></div>
<div class="packt_infobox">For readability, both the <kbd>duration</kbd> and <kbd>cookie</kbd> fields have been removed.</div>
<p>The first three rules specify a particular inbound port:</p>
<pre>in_port="phy-br-eth2" </pre>
<p>According to the diagram in Figure 5.3, traffic entering the bridge <kbd>br-eth2</kbd> from physical interface <kbd>eth2</kbd> does so through port 1, not the port named <kbd>phy-br-eth2</kbd>, so the first three rules do not apply. As a result, traffic is forwarded to the integration bridge via the fourth rule, where no particular port is specified:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/0f1662f0-b7d8-4009-be41-3699f8ec0639.png"/></div>
<p>Flows with an action of <kbd>NORMAL</kbd> instructs Open vSwitch to act as a learning switch, which means traffic will be forwarded out of all of the ports, other than the one where traffic was received, until the switch learns and updates its forwarding database. Traffic is forwarded out of the port that's connected to the integration bridge.</p>
<p class="mce-root"/>
<div class="packt_infobox">The forwarding database, or FDB table, is the equivalent of a CAM or MAC address table on a physical switch. This learning behavior is similar to that of a hardware switch that floods traffic out of all ports until it learns the proper path.</div>
<p>As traffic exits the provider bridge <kbd>br-eth2</kbd> and enters port 1 of the integration bridge <kbd>br-int</kbd>, it is evaluated by the flow rules on <kbd>br-int</kbd>, as shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/6c6defc2-2ee9-4974-8288-81e16d46edb4.png"/></div>
<p>Of immediate importance are the flow rules inspecting traffic sourced from the <kbd>int-br-eth2</kbd> interface, as that is where traffic enters the integration bridge from the provider bridge. The first rule shown here performs the action of modifying the VLAN ID of a packet from its original VLAN to a VLAN that is local to the integration bridge on the <kbd>compute</kbd> node when the original VLAN ID, as identified by the <kbd>dl_vlan</kbd> value, is 42:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/b80043b2-5731-443f-9fbd-91b99845b63e.png"/></div>
<p>When traffic tagged as VLAN 42 on the physical network is sent to an instance and forwarded through the provider bridge to the integration bridge, the VLAN tag is modified from 42 to local VLAN 1. The frame is then forwarded to Table 60 for additional processing, where the default action is NORMAL. As a result, the frame is forwarded to a port on <kbd>br-int</kbd>, which is connected to the instance that matches the destination MAC address.</p>
<p class="mce-root"/>
<p>The next rule performs a similar action when the data link VLAN is <kbd>40</kbd> by replacing it with local VLAN 2. If traffic matches the <kbd>drop</kbd> rule, it means that no other rules of a higher priority entering <kbd>int-br-eth2</kbd> and traffic will be dropped:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/b10e51cf-1b8e-4e84-8338-ea2bd1fd7b44.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Return traffic</h1>
                </header>
            
            <article>
                
<p>Return traffic from the instances through the integration bridge <kbd>br-int</kbd> may be processed by various flow rules that are used to inhibit ARP and MAC spoofing from instances. If the traffic is allowed, it is forwarded to Table 60 for additional processing and out to the provider bridge:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/eb4af537-a41d-45e0-a37d-34f7d66f882c.png"/></div>
<p>Once traffic hits the provider bridge <kbd>br-eth2</kbd>, it is processed by the flow rules as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/c165d624-1458-4123-99bf-b6cd32ec407d.png"/></div>
<p class="mce-root"/>
<p>If these rules look familiar, it's because they are the same flow rules on the provider bridge that we showed you earlier. This time, however, traffic from the integration bridge connected to port <kbd>phy-br-eth2</kbd> is processed by these rules.</p>
<p>The first flow rule on the provider bridge checks the VLAN ID in the Ethernet header, and if it is <kbd>1</kbd>, modifies it to <kbd>42</kbd> before forwarding the traffic to the physical interface. The second rule modifies the VLAN tag of the frame from <kbd>2</kbd> to <kbd>40</kbd> before it exits the bridge. All other traffic from the integration bridge not tagged as VLAN <kbd>1</kbd> or <kbd>2</kbd> is dropped.</p>
<div class="packt_infobox">Flow rules for a particular network will not exist on a bridge if there are no instances or resources in that network scheduled to that node. The Neutron <span class="NormalPACKTChar">Open vSwitch agent</span> on each node is responsible for creating the appropriate flow rules for virtual switches on the respective node.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flow rules for flat networks</h1>
                </header>
            
            <article>
                
<p>Flat networks in Neutron are untagged networks, meaning there is no 802.1q VLAN tag associated with the network when it is created. Internally, however, Open vSwitch treats flat networks similarly to VLAN networks when programming the virtual switches. Flat networks are assigned a local VLAN ID in the Open vSwitch database just like a VLAN network, and instances in the same flat network connected to the same integration bridge are placed in the same local VLAN. However, there is a difference between VLAN and flat networks that can be observed in the flow rules that are created on the integration and provider bridges. Instead of mapping the local VLAN ID to a physical VLAN ID, and vice versa, as traffic traverses the bridges, the local VLAN ID is added to or stripped from the Ethernet header by flow rules.</p>
<p>On the physical switch, the necessary configuration to facilitate the networking described here will resemble the following:</p>
<pre>vlan 200<br/>    name VLAN_200 
 
interface Ethernet1/4<br/>    description Provider_Interface_eth2<br/>    switchport<br/>    switchport mode trunk<br/>    switchport trunk native vlan 200<br/>    switchport trunk allowed vlan add 200<br/>    no shutdown</pre>
<p>Alternatively, the interface can also be configured as an access port:</p>
<pre>interface Ethernet1/4<br/>    description Provider_Interface_eth2<br/>    switchport<br/>    switchport mode access<br/>    switchport access vlan 200<br/>    no shutdown </pre>
<p> Only one flat network is supported per provider interface. When configured as a trunk port with a native VLAN, the provider interface can support a single flat network and multiple VLAN networks. When configured as an access port, the interface can only support a single flat network and any attempt to tag traffic will fail.</p>
<p>In this example, a flat network has been added in Neutron that has no VLAN tag:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/edec1bcf-184a-4aeb-baeb-32d07866335c.png" style="width:31.00em;height:30.83em;"/></div>
<p>On the physical switch, this network will correspond to the native (untagged) VLAN on the switch port connected to <kbd>eth2</kbd> of <kbd>compute02</kbd>. In this case, the native VLAN is 200. An instance has been spun up on the network <kbd>MyFlatNetwork</kbd>, which results in the following virtual switch configuration:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/86ddb0bf-dc44-4d2f-8ebf-786e164889c0.png" style="width:21.50em;height:32.17em;"/></div>
<p>Notice that the port associated with the instance has been assigned a local VLAN ID of 3, as identified by the <kbd>tag</kbd> value, even though it is a flat network. On the integration bridge, there now exists a flow rule that modifies the VLAN header of an incoming Ethernet frame when it has no VLAN ID set:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/c2129b44-84ff-4641-ad76-3b7674d57f77.png"/></div>
<div class="packt_infobox">TCI stands for <strong>Tag Control Information</strong>, and is a 2-byte field of the 802.1q header. For packets with an 802.1q header, this field contains VLAN information including the VLAN ID. For packets without an 802.1q header, also known as untagged, the <kbd>vlan_tci</kbd> value is set to zero (<kbd>0x0000</kbd>).</div>
<p>The result is that incoming traffic on the flat network is tagged as VLAN 3 and forwarded to instances connected to the integration bridge that reside in VLAN 3.</p>
<p>As return traffic from the instance is processed by flow rules on the provider bridge, the local VLAN ID is stripped and the traffic becomes untagged:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/3797747e-3fb4-4e52-a6dc-7921460f8ecb.png"/></div>
<p>The untagged traffic is then forwarded out to the physical interface <kbd>eth2</kbd> and processed by the physical switch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flow rules for overlay networks</h1>
                </header>
            
            <article>
                
<p>Overlay networks in a reference implementation of Neutron are ones that use either VXLAN or GRE to encapsulate virtual instance traffic between hosts. Instances connected to an overlay network are attached to the integration bridge and use a local VLAN mapped to that network, just like the other network types we have discussed so far. All instances on the same host are connected to the same local VLAN.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this example, an overlay network has been created with Neutron auto-assigning a segmentation ID of 39.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6db7405b-1a7c-47d4-8c7f-7b4627b034b1.png" style="width:42.08em;height:41.50em;"/></p>
<p>No changes are needed on the physical switching infrastructure to support this network, as the traffic will be encapsulated and forwarded through the overlay network interface, <kbd>eth1</kbd>.</p>
<p>An instance has been spun up on the network <kbd>MyOverlayNetwork</kbd>, which results in the following virtual switch configuration:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c1786c01-7aa6-4431-a02f-6e6862dcc591.png" style="width:20.92em;height:31.42em;"/></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><br/></div>
<p>Notice that the port associated with the instance has been assigned a local VLAN ID of 4, even though it is an overlay network. When an instance sends traffic to another instance or device in the same network, the integration bridge forwards the traffic out toward the tunnel bridge, <kbd>br-tun</kbd>, where the following flow rules are consulted:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/551c9f09-b81b-4760-959b-9aaa8d972ed1.png"/></p>
<p>The flows rules implemented on the tunnel bridge are unique, in that they specify a <strong>virtual tunnel endpoint</strong>, or VTEP, for every destination MAC address, including other instances and routers that are connected to the network. This behavior ensures that traffic is forwarded directly to the <kbd>compute</kbd> or <kbd>network</kbd> node where the destination resides and is not forwarded out on all ports of the bridge. Traffic that does not match is dropped.</p>
<p class="CDPAlignLeft CDPAlign">In this example, traffic to destination MAC address <kbd>fa:16:3e:f1:b0:49</kbd> is forwarded out to port <kbd>vxlan0a140064</kbd>, which, as we can see here, is mapped to a tunnel endpoint:</p>
<p class="CDPAlignCenter CDPAlign"><br/>
<img src="assets/d8b315d8-4b63-4939-8bdf-5632ab566eb2.png"/></p>
<div class="packt_figure CDPAlignCenter CDPAlign"/>
<p>The address <kbd>10.20.0.100</kbd> is the VXLAN tunnel endpoint for <kbd>controller01</kbd>, and the MAC address <kbd>fa:16:3e:f1:b0:49</kbd> belongs to the DHCP server in the <kbd>MyOverlayNetwork</kbd> network.</p>
<p>Return traffic to the instance is first processed by flow rules on the tunnel bridge and then forwarded to the integration bridge, where it is then forwarded to the instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flow rules for local networks</h1>
                </header>
            
            <article>
                
<p>Local networks in an Open vSwitch implementation behave similar to that of a Linux bridge implementation. Instances in local networks are connected to the integration bridge and can communicate with other instances in the same network and local VLAN. There are no flow rules created for local networks, however.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Traffic between instances in the same network remains local to the virtual switch, and by definition, local to the <kbd>compute</kbd> node on which they reside. This means that connectivity to services hosted on other nodes, such as DHCP and metadata, will be unavailable to any instance not on the same host as those services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the ML2 networking plugin</h1>
                </header>
            
            <article>
                
<p>The remainder of this chapter is dedicated to providing instructions on installing and configuring the Neutron Open vSwitch agent and the ML2 plugin for use with the Open vSwitch mechanism driver. In this book, <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd> will be the only nodes configured for use with Open vSwitch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the bridge interface</h1>
                </header>
            
            <article>
                
<p>In this installation, physical network interface <kbd>eth2</kbd> will be utilized as the <strong>provider interface</strong> for bridging purposes.</p>
<p>On <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd>, configure the <kbd>eth2</kbd> interface within the <kbd>/etc/network/interfaces</kbd> file as follows:</p>
<pre>auto eth2<br/>iface eth2 inet manual </pre>
<p>Close and save the file, and bring the interface up with the following command:</p>
<pre>    <strong># ip link set dev eth2 up</strong></pre>
<div class="packt_infobox">Because the interface will be used in a bridge, an IP address cannot be applied directly to the interface. If there is an IP address applied to <kbd>eth2</kbd>, it will become inaccessible once the interface is placed in a bridge. The bridge will be created later on in this chapter.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the overlay interface</h1>
                </header>
            
            <article>
                
<p>In this installation, physical network interface <kbd>eth1</kbd> will be utilized as the <strong>overlay interface </strong>for overlay networks using VXLAN. For VXLAN networking, this is the equivalent of the VXLAN tunnel endpoint, or VTEP. Neutron will be responsible for configuring some aspects of Open vSwitch once the initial network configuration has been completed.</p>
<p class="mce-root"/>
<p>On all hosts, configure the <kbd>eth1</kbd> interface within the <kbd>/etc/network/interfaces</kbd> file, if it has not already been done:</p>
<pre>auto eth1<br/>iface eth1 inet static<br/>  address 10.20.0.X/24 </pre>
<p>Use the following table for the appropriate address. Substitute the address with <kbd>X</kbd> where appropriate:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 319.097px">
<p><strong>Host</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 341.903px">
<p><strong>Address</strong></p>
</td>
</tr>
<tr>
<td style="width: 319.097px">
<p><kbd>compute02</kbd></p>
</td>
<td style="width: 341.903px">
<p>10.20.0.102</p>
</td>
</tr>
<tr>
<td style="width: 319.097px">
<p><kbd>compute03</kbd></p>
</td>
<td style="width: 341.903px">
<p>10.20.0.103</p>
</td>
</tr>
<tr>
<td style="width: 319.097px">
<p><kbd>snat01</kbd></p>
</td>
<td style="width: 341.903px">
<p>10.20.0.104</p>
</td>
</tr>
</tbody>
</table>
<p>Close and save the file, and bring the interface up with the following command:</p>
<pre>    <strong># ip link set dev eth1 up</strong></pre>
<p>Confirm that the interface is in an <kbd>UP</kbd> state and that the address has been set using the <kbd>ip addr show dev eth1</kbd> command. Ensure the <kbd>compute02</kbd> can communicate over the newly configured interface by pinging <kbd>controller01</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a8de050c-6945-49b8-96b7-e1971355a34b.png"/></div>
<p>Repeat this process for all of the nodes.</p>
<div class="packt_tip">If you experience any issues communicating across this interface, you <em>will</em> experience issues with VXLAN networks that have been created with OpenStack Networking. Any issues should be corrected before continuing.</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML2 plugin configuration options</h1>
                </header>
            
            <article>
                
<p>The ML2 plugin was initially installed in <em><a href="bf508e37-ce8a-4116-89db-e8f8a6abf0f4.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, Installing Neutron, </em>and was configured to support the Linux bridge mechanism driver in the previous chapter. It must be modified to support the Open vSwitch mechanism driver.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mechanism drivers</h1>
                </header>
            
            <article>
                
<p>Mechanism drivers are responsible for implementing networks described by the type driver. Mechanism drivers shipped with the ML2 plugin include <kbd>linuxbridge</kbd>, <kbd>openvswitch</kbd>, and <kbd>l2population</kbd>.</p>
<p>Update the ML2 configuration file on <kbd>controller01</kbd> and append <kbd>openvswitch</kbd> to the list of mechanism drivers:</p>
<pre>[ml2] <br/>... <br/>mechanism_drivers = linuxbridge,l2population,openvswitch </pre>
<div class="packt_infobox">The Neutron Open vSwitch agent requires specific configuration options, which will be discussed later on in this chapter.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flat networks</h1>
                </header>
            
            <article>
                
<p>The <kbd>flat_networks</kbd> configuration option defines interfaces that support the use of untagged networks, commonly referred to as native or access VLANs. This option requires that a provider label is specified. A <strong>provider label</strong> is an arbitrary label or name that is mapped to a physical interface or bridge on the host. These mappings will be discussed in further detail later on in this chapter.</p>
<p>In the following example, the <kbd>physnet1</kbd> interface has been configured to support a flat network:</p>
<pre>flat_networks = physnet1 </pre>
<p>Multiple interfaces can be defined using a comma-separated list:</p>
<pre>flat_networks = physnet1,physnet2</pre>
<p class="mce-root"/>
<div class="packt_infobox">Due to the lack of an identifier to segregate untagged traffic on the same interface, an interface can only support a single flat network.</div>
<p>In this environment, the <kbd>flat_networks</kbd> option can remain <em>unconfigured</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network VLAN ranges</h1>
                </header>
            
            <article>
                
<p>The <kbd>network_vlan_ranges</kbd> configuration option defines a range of VLANs that project networks will be associated with upon their creation when <kbd>tenant_network_types</kbd> is <kbd>vlan</kbd>. When the number of available VLANs reaches zero, tenants will no longer be able to create VLAN networks.</p>
<p>In the following example, VLAN IDs <kbd>40</kbd> through <kbd>43</kbd> are available for tenant network allocation:</p>
<pre>network_vlan_ranges = physnet1:40:43 </pre>
<p>Non-contiguous VLANs can be allocated by using a comma-separated list:</p>
<pre>network_vlan_ranges = physnet1:40:43,physnet1:51:55 </pre>
<p>In this installation, the provider label <kbd>physnet1</kbd> will be used with VLANs <kbd>40</kbd> through <kbd>43</kbd>. Those VLANs will be automatically assigned to <kbd>vlan</kbd> networks upon creation, unless overridden by a user with the <kbd>admin</kbd> role.</p>
<p>Update the ML2 configuration file on the <kbd>controller</kbd> node and add the following <kbd>network_vlan_ranges</kbd> to the <kbd>[ml2_type_vlan]</kbd> section if it doesn't already exist:</p>
<pre>[ml2_type_vlan] <br/>... <br/>network_vlan_ranges = physnet1:40:43 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tunnel ID ranges</h1>
                </header>
            
            <article>
                
<p>When GRE networks are created, each network is assigned a unique segmentation ID that is used to encapsulate traffic. As traffic traverses the Open vSwitch tunnel bridge, the segmentation ID is used to populate a field in the encapsulation header of the packet. For GRE packets, the <kbd>KEY</kbd> header field is used.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The <kbd>tunnel_id_ranges</kbd> configuration option found under <kbd>[ml2_type_gre]</kbd> is a comma-separated list of ID ranges that are available for tenant network allocation when <kbd>tunnel_type</kbd> is set to <kbd>gre</kbd>.</p>
<p>In the following example, segmentation IDs 1 through 1,000 are reserved for allocation to tenant networks upon creation:</p>
<pre>tunnel_id_ranges = 1:1000 </pre>
<p>The <kbd>tunnel_id_ranges</kbd> option supports non-contiguous IDs using a comma-separated list as follows:</p>
<pre>tunnel_id_ranges = 1:1000,2000:2500 </pre>
<p>GRE networks will not be configured as part of the exercises in this book, so <kbd>tunnel_id_ranges</kbd> can remain <em>unconfigured</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VNI Ranges</h1>
                </header>
            
            <article>
                
<p>When VXLAN networks are created, each network is assigned a unique segmentation ID that is used to encapsulate traffic.</p>
<p>The <kbd>vni_ranges</kbd> configuration option is a comma-separated list of ID ranges that are available for project network allocation when <kbd>tunnel_type</kbd> is set to <kbd>vxlan</kbd>.</p>
<p>In the following example, segmentation IDs 1 through 1,000 are reserved for allocation to tenant networks upon creation:</p>
<pre>vni_ranges = 1:1000 </pre>
<p>The <kbd>vni_ranges</kbd> option supports non-contiguous IDs using a comma-separated list as follows:</p>
<pre>vni_ranges = 1:1000,2000:2500 </pre>
<p>Update the ML2 configuration file on the <kbd>controller</kbd> node and add the following <kbd>vni_ranges</kbd> to the <kbd>[ml2_type_vxlan]</kbd> section if it doesn't already exist:</p>
<pre>[ml2_type_vxlan]<br/>... <br/>vni_ranges = 1:1000 </pre>
<div class="packt_tip">The 24-bit VNI field in the VXLAN header supports up to approximately 16 million unique identifiers.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Security groups</h1>
                </header>
            
            <article>
                
<p>The <kbd>enable_security_group</kbd> configuration option instructs Neutron to enable or disable security group-related API functions. This option is set to <kbd>true</kbd> by default.</p>
<p>The <kbd>enable_ipset</kbd> configuration option instructs Neutron to enable or disable the <kbd>ipset</kbd> extension for iptables when the <kbd>iptables_hybrid</kbd> firewall driver is used. The use of ipsets allows for the creation of firewall rules that match entire sets of addresses at once rather than having individual lines per address, making lookups very efficient compared to traditional linear lookups. This option is set to <kbd>true</kbd> by default.</p>
<div class="packt_tip">If at any time the ML2 configuration file is updated, you must restart the <kbd>neutron-server</kbd> service and respective Neutron agent for the changes to take effect.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the Open vSwitch driver and agent</h1>
                </header>
            
            <article>
                
<p>The Open vSwitch mechanism driver is included with the ML2 plugin, and was installed in <em><a href="bf508e37-ce8a-4116-89db-e8f8a6abf0f4.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, Installing Neutron</em>. The following sections will walk you through the configuration of OpenStack Networking so that you can utilize the Open vSwitch driver and agent.</p>
<div class="packt_infobox">While the Linux bridge and Open vSwitch agents and drivers can coexist in the same environment, they should not be installed and configured simultaneously on the same host.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the Open vSwitch agent</h1>
                </header>
            
            <article>
                
<p>To install the Open vSwitch agent, issue the following command on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd>:</p>
<pre># apt install neutron-plugin-openvswitch-agent</pre>
<p>Dependencies, such as Open vSwitch components <kbd>openvswitch-common</kbd> and <kbd>openvswitch-switch</kbd>, will be installed. If prompted to overwrite existing configuration files, type <kbd>N</kbd> at the <kbd>[default=N]</kbd> prompt.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the Open vSwitch agent configuration file</h1>
                </header>
            
            <article>
                
<p>The Open vSwitch agent uses a configuration file located at <kbd>/etc/neutron/plugins/ml2/openvswitch_agent.ini</kbd> <span>. The most</span> common <span>options can be seen as follows:</span></p>
<pre>[agent] <br/>... <br/>tunnel_types = ...<br/>l2_population = ... <br/>arp_responder = ...<br/>enable_distributed_routing = ...<br/><br/>[ovs] <br/>... <br/>integration_bridge = ... <br/>tunnel_bridge = ...<br/>local_ip = ...<br/>bridge_mappings = ...<br/><br/>[securitygroup]<br/>... <br/>firewall_driver = ... </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tunnel types</h1>
                </header>
            
            <article>
                
<p>The <kbd>tunnel_types</kbd> configuration option specifies the types of tunnels supported by the agent. The two available options are <kbd>gre</kbd> and/or <kbd>vxlan</kbd>. The default value is <kbd>None</kbd>, which disables tunneling.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Update the <kbd>tunnel_types</kbd> configuration option in the <kbd>[agent]</kbd> section of the Open vSwitch agent configuration file accordingly on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd>:</p>
<pre>[agent] <br/>... <br/>tunnel_types = vxlan </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">L2 population</h1>
                </header>
            
            <article>
                
<p>To enable support for the L2 population driver, the <kbd>l2_population</kbd> configuration option must be set to <kbd>true</kbd>. Update the <kbd>l2_population</kbd> configuration option in the <kbd>[vxlan]</kbd> section of the Open vSwitch agent configuration file accordingly on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd>:</p>
<pre>[agent] <br/>... <br/>l2_population = true</pre>
<p>An important feature of the L2 population driver is its ARP responder functionality, which avoids the broadcasting of ARP requests across the overlay network. Each <kbd>compute</kbd> node can proxy ARP requests from virtual machines and provide them with replies, all without traffic leaving the host.</p>
<p>To enable the ARP responder, update the following configuration option:</p>
<pre>[agent] <br/>... <br/>arp_responder = true </pre>
<p>The default <kbd>arp_responder</kbd> configuration is <kbd>false</kbd> and can remain <em>unchanged</em> for this environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VXLAN UDP port</h1>
                </header>
            
            <article>
                
<p>The default port for UDP traffic between VXLAN tunnel endpoints varies depending on the system. The Internet Assigned Numbers Authority, or IANA, has assigned UDP port 4789 for the purposes of VXLAN and that is the default port used by Open vSwitch. The Linux kernel, on the other hand, uses UDP port 8472 for VXLAN. To maintain compatibility with the hosts using the Linux bridge mechanism driver and <kbd>vxlan</kbd> kernel module, the port must be changed from its default.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To change the port number, update the following configuration option from 4789 to 8472:</p>
<pre>[agent] <br/>... <br/>vxlan_udp_port = 8472</pre>
<p>This change is typically unnecessary in a pure Open vSwitch-based environment, but is required for the environment described in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integration bridge</h1>
                </header>
            
            <article>
                
<p>The <kbd>integration_bridge</kbd> configuration option specifies the name of the integration bridge used on each node. There is a single integration bridge per node that acts as the virtual switch where all virtual machine VIFs, otherwise known as <strong>virtual network interfaces</strong>, are connected. The default name of the integration bridge is <kbd>br-int</kbd> and should not be modified.</p>
<div class="packt_infobox">Since the Icehouse release of OpenStack, the Open vSwitch agent automatically creates the integration bridge the first time the agent service is started. You do not need to add an interface to the integration bridge, as Neutron is responsible for connecting network devices to this virtual switch.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tunnel bridge</h1>
                </header>
            
            <article>
                
<p>The tunnel bridge is a virtual switch, similar to the integration and provider bridges, and is used to connect GRE and VXLAN tunnel endpoints. Flow rules exist on this bridge that are responsible for properly encapsulating and decapsulating tenant traffic as it traverses the bridge.</p>
<p>The <kbd>tunnel_bridge</kbd> configuration option specifies the name of the tunnel bridge. The default value is <kbd>br-tun</kbd> and should not be modified. It is not necessary to create this bridge manually since Neutron does this automatically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Local IP</h1>
                </header>
            
            <article>
                
<p>The <kbd>local_ip</kbd> configuration option specifies the local IP address on the node, which will be used to build the overlay network between hosts. Refer to <em><a href="961d71d1-9804-4af7-ad1f-8716e6dd5ac6.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a></em>, <em>Introduction to OpenStack Networking</em>, for ideas on how the overlay network should be architected. In this installation, all guest traffic through the overlay networks will traverse a dedicated network over the <kbd>eth1</kbd> interface that we configured earlier in this chapter.</p>
<p class="mce-root"/>
<p>Update the <kbd>local_ip</kbd> configuration option in the <kbd>[vxlan]</kbd> section of the Open vSwitch agent configuration file accordingly on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd><span>snat01</span></kbd>:</p>
<pre>[vxlan] <br/>... <br/>local_ip = 10.20.0.X </pre>
<p>The following table provides the interfaces and addresses to be configured on each host. Substitute these for <kbd>X</kbd> where appropriate:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 268px">
<p><strong>Hostname</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 180.351px">
<p><strong>Interface</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 381.649px">
<p><strong>IP Address</strong></p>
</td>
</tr>
<tr>
<td style="width: 268px">
<p>compute02</p>
</td>
<td style="width: 180.351px">
<p>eth1</p>
</td>
<td style="width: 381.649px">
<p>10.20.0.102</p>
</td>
</tr>
<tr>
<td style="width: 268px">
<p>compute03</p>
</td>
<td style="width: 180.351px">
<p>eth1</p>
</td>
<td style="width: 381.649px">
<p>10.20.0.103</p>
</td>
</tr>
<tr>
<td style="width: 268px">
<p>snat01</p>
</td>
<td style="width: 180.351px">
<p>eth1</p>
</td>
<td style="width: 381.649px">
<p>10.20.0.104</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bridge mappings</h1>
                </header>
            
            <article>
                
<p>The <kbd>bridge_mappings</kbd> configuration option describes the mapping of an artificial label to a virtual switch created with Open vSwitch. Unlike the Linux bridge driver that configures a separate bridge for every network, each with its own interface, the Open vSwitch driver uses a single virtual switch containing a single physical interface and uses flow rules to tag traffic if necessary.</p>
<p>When networks are created, they are associated with an interface label, such as <kbd>physnet1</kbd>. The label <kbd>physnet1</kbd> is then mapped to a bridge, such as <kbd>br-eth1</kbd>, which contains the physical interface <kbd>eth1</kbd>. The mapping of the label to the bridge interface is handled by the <kbd>bridge_mappings</kbd> option. This mapping can be observed as follows:</p>
<pre>bridge_mappings = physnet1:br-eth1 </pre>
<p>The chosen label(s) must be consistent between all nodes in the environment that are expected to handle traffic for a given network created with Neutron. However, the physical interface mapped to the label may be different. A difference in mappings is often observed when one node maps <kbd>physnet1</kbd> to a one gigabit-capable bridge, while another maps <kbd>physnet1</kbd> to a ten gigabit-capable bridge.</p>
<p>Multiple bridge mappings are allowed and can be added using a comma-separated list:</p>
<pre>bridge_mappings = physnet1:br-eth1,physnet2:br-eth2</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this installation process, <kbd>physnet1</kbd> will be used as the interface label and will map to the bridge <kbd>br-eth2</kbd>. Update the Open vSwitch agent configuration file accordingly on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd>: </p>
<pre>[ovs]<br/>...<br/>bridge_mappings = physnet1:br-eth2</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the bridges</h1>
                </header>
            
            <article>
                
<p>To configure a bridge with Open vSwitch, use the Open vSwitch utility <kbd>ovs-vsctl</kbd>. Create the bridge <kbd>br-eth2</kbd> on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd>, as follows:</p>
<pre>    # ovs-vsctl add-br br-eth2</pre>
<p>Use the <kbd>ovs-vsctl add-port</kbd> command to add physical interface <kbd>eth2</kbd> to the bridge like so:</p>
<pre>    # ovs-vsctl add-port br-eth2 eth2</pre>
<p>The configuration of the bridge should persist reboots. However, the bridge interface can also be configured in <kbd>/etc/network/interfaces</kbd> if necessary using the following syntax:</p>
<pre>auto br-eth2 <br/>allow-ovs br-eth2 <br/>iface br-eth2 inet manual <br/>    ovs_type OVSBridge<br/>    ovs_port seth2<br/><br/>allow-br-eth2 eth2 <br/>iface eth2 inet manual <br/>    ovs_bridge br-eth2 <br/>    ovs_type OVSPort </pre>
<div class="packt_infobox">Please note that the physical switch port connected to <kbd>eth2</kbd> must support 802.1q VLAN tagging if VLAN networks of any type are to be created. On many switches, the switch port can be configured as a trunk port.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Firewall driver</h1>
                </header>
            
            <article>
                
<p>The <kbd>firewall_driver</kbd> configuration option instructs Neutron to use a particular firewall driver for security group functionality.  Different firewall drivers may be configured based on the mechanism driver in use.</p>
<p class="mce-root"/>
<p>Update the ML2 configuration file on <kbd>compute02</kbd> and <kbd>compute03</kbd> and define the appropriate <kbd>firewall_driver</kbd> in the <kbd>[securitygroup]</kbd> section on a single line:</p>
<pre>[securitygroup] <br/>... <br/>firewall_driver = iptables_hybrid</pre>
<p>The <kbd>iptables_hybrid</kbd> firewall driver implements firewall rules using iptables and relies on the use of Linux bridges in-between the instance's tap interface and the integration bridge. The <kbd>openvswitch</kbd> firewall driver, on the other hand, implements firewall rules using OpenFlow and does not rely on Linux bridges or iptables. As of the Pike release of OpenStack, the <kbd>openvswitch</kbd> firewall driver is not production-ready and is not recommended.</p>
<p>If you do not want to use a firewall and want to disable the application of security group rules, set <kbd>firewall_driver</kbd> to <kbd>noop</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the DHCP agent to use the Open vSwitch driver</h1>
                </header>
            
            <article>
                
<p>For Neutron to properly connect DHCP namespace interfaces to the appropriate network bridge, the DHCP agent on the node hosting the agent must be configured to use the Open vSwitch interface driver, as shown here:</p>
<pre>[DEFAULT] <br/>... <br/>interface_driver = openvswitch </pre>
<p>In this environment, the DHCP agent is running on the <kbd>controller01</kbd> node utilizing the Linux bridge driver and agent, and the interface driver was configured to work with Linux bridges. No change is necessary at this time. For environments running only Open vSwitch, be sure to set the interface driver accordingly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Restarting services</h1>
                </header>
            
            <article>
                
<p>Now that the appropriate OpenStack configuration files have been modified to use Open vSwitch as the networking driver, certain services must be started or restarted for the changes to take effect.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The Open vSwitch network agent should be restarted on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd>:</p>
<pre># systemctl restart neutron-openvswitch-agent</pre>
<p>The following services should be restarted on the <kbd>controller</kbd> node:</p>
<pre># systemctl restart neutron-server</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Verifying Open vSwitch agents</h1>
                </header>
            
            <article>
                
<p>To verify that the Open vSwitch network agents have been properly checked in, issue the <kbd>openstack network agent list</kbd> command on the <kbd>controller</kbd> node:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/6d95c416-a899-4f59-9667-1d6bfccad549.png"/></div>
<p>The Open vSwitch agent on <kbd>compute02</kbd>, <kbd>compute03</kbd>, and <kbd>snat01</kbd> should now be visible in the output with a state of <kbd>UP</kbd>. If an agent is not present, or the state is <kbd>DOWN</kbd>, you will need to troubleshoot agent connectivity issues by observing log messages found in <kbd>/var/log/neutron/neutron-openvswitch-agent.log</kbd> on the respective host.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter saw us installing and configuring the Neutron Open vSwitch mechanism driver and agent on two <kbd>compute</kbd> nodes and a dedicated <kbd>network</kbd> node, which will be used for distributed virtual routing functions at a later time. Instances scheduled to <kbd>compute02</kbd> and <kbd>compute03</kbd> will leverage Open vSwitch virtual network components, while <kbd>compute01</kbd> and network services on <kbd>controller01</kbd> will leverage Linux bridges.</p>
<p>Both the Linux bridge and Open vSwitch drivers and agents for Neutron provide unique solutions to the same problem of connecting virtual machine instances to the network. The use of Open vSwitch relies on flow rules to determine how traffic in and out of the environment should be processed and requires both user space utilities and kernel modules to perform such actions. On the other hand, the use of Linux bridges requires the <kbd>8021q</kbd> and <kbd>bridge</kbd> kernel modules and relies on the use of VLAN and VXLAN interfaces on the host to bridge instances to the physical network.For simple environments, I recommend using the ML2 plugin and Linux bridge mechanism driver and agent, unless integration with OpenFlow controllers or the use of a third-party solution or plugin is required. Other Neutron technologies, such as distributed virtual routers, are only available when using the Open vSwitch driver and agent.</p>
<p>In the next chapter, you will be guided through the process of creating different types of networks to provide connectivity to instances. The process of creating networks is the same for both Linux bridge and Open vSwitch-based environments, but the underlying network implementation will vary based on the driver and agent in use.</p>


            </article>

            
        </section>
    </body></html>