- en: Introduction to OpenStack Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 网络简介
- en: In today's data centers, networks are composed of more devices than ever before.
    Servers, switches, routers, storage systems, and security appliances that once
    consumed rows and rows of data center space now exist as virtual machines and
    virtual network appliances. These devices place a large strain on traditional
    network management systems, as they are unable to provide a scalable and automated
    approach to managing next-generation networks. Users now expect more control and
    flexibility of the infrastructure with quicker provisioning, all of which OpenStack
    promises to deliver.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的数据中心，网络由比以往更多的设备组成。服务器、交换机、路由器、存储系统和安全设备，这些曾经占据数据中心大片空间的设备现在作为虚拟机和虚拟网络设备存在。这些设备给传统的网络管理系统带来了巨大压力，因为它们无法提供可扩展和自动化的方法来管理下一代网络。用户现在期望更高的基础设施控制和灵活性，快速的资源配置，而这些正是
    OpenStack 所承诺提供的。
- en: 'This chapter will introduce many features that OpenStack Networking provides,
    as well as various network architectures supported by OpenStack. Some topics that
    will be covered include the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 OpenStack 网络提供的许多功能，以及 OpenStack 支持的各种网络架构。涵盖的主题包括以下内容：
- en: Features of OpenStack Networking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack 网络功能
- en: Physical infrastructure requirements
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理基础设施要求
- en: Service separation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务分离
- en: What is OpenStack Networking?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 OpenStack 网络？
- en: OpenStack Networking is a pluggable, scalable, and API-driven system to manage
    networks in an OpenStack-based cloud. Like other core OpenStack components, OpenStack
    Networking can be used by administrators and users to increase the value and maximize
    the utilization of existing data center resources.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络是一个可插拔、可扩展的、基于 API 的系统，用于管理 OpenStack 基础云中的网络。与其他核心 OpenStack 组件一样，OpenStack
    网络可以供管理员和用户使用，以增加现有数据中心资源的价值并最大化利用。
- en: Neutron, the project name for the OpenStack Networking service, complements
    other core OpenStack services such as Compute (Nova), Image (Glance), Identity
    (Keystone), Block (Cinder), Object (Swift), and Dashboard (Horizon) to provide
    a complete cloud solution.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron，作为 OpenStack 网络服务的项目名称，补充了其他核心 OpenStack 服务，如计算（Nova）、镜像（Glance）、身份（Keystone）、块存储（Cinder）、对象存储（Swift）和仪表盘（Horizon），共同提供完整的云解决方案。
- en: OpenStack Networking exposes an application programmable interface (API) to
    users and passes requests to the configured network plugins for additional processing.
    Users are able to define network connectivity in the cloud, and cloud operators
    are allowed to leverage different networking technologies to enhance and power
    the cloud.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络为用户提供了一个应用程序可编程接口（API），并将请求传递给配置的网络插件进行额外处理。用户能够在云中定义网络连接，而云运营商则可以利用不同的网络技术来增强和驱动云服务。
- en: 'OpenStack Networking services can be split between multiple hosts to provide
    resiliency and redundancy, or they can be configured to operate on a single node.
    Like many other OpenStack services, Neutron requires access to a database for
    persistent storage of the network configuration. A simplified example of the architecture
    can be seen here:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络服务可以分布在多个主机之间，以提供弹性和冗余，或者可以配置为在单个节点上运行。像许多其他 OpenStack 服务一样，Neutron
    需要访问数据库以持久化存储网络配置。可以在此看到一个简化的架构示例：
- en: '![](img/b0e53890-f596-464d-89e3-686ed1c77c1f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0e53890-f596-464d-89e3-686ed1c77c1f.png)'
- en: Figure 1.1
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1
- en: In figure 1.1, the Neutron server connects to a database where the logical network
    configuration persists. The Neutron server can take API requests from users and
    services and communicate with agents via a message queue. In a typical environment,
    network agents will be scattered across controller and compute nodes and perform
    duties on their respective node.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 1.1 中，Neutron 服务器连接到一个数据库，该数据库存储着逻辑网络配置。Neutron 服务器可以接收用户和服务的 API 请求，并通过消息队列与代理进行通信。在典型环境中，网络代理会分布在控制节点和计算节点上，并在各自的节点上执行任务。
- en: Features of OpenStack Networking
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 网络功能
- en: OpenStack Networking includes many technologies you would find in the data center,
    including switching, routing, load balancing, firewalling, and virtual private
    networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络包括许多数据中心中常见的技术，包括交换、路由、负载均衡、防火墙和虚拟私人网络。
- en: These features can be configured to leverage open source or commercial software
    and provide a cloud operator with all the tools necessary to build a functional
    and self-contained cloud networking stack. OpenStack Networking also provides
    a framework for third-party vendors to build on and enhance the capabilities of
    the cloud.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能可以配置为利用开源或商业软件，提供给云操作员构建功能齐全且自包含的云网络栈所需的所有工具。OpenStack Networking 还为第三方厂商提供了一个框架，供其在此基础上构建并增强云的功能。
- en: Switching
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切换
- en: A **virtual switch** is defined as a software application or service that connects
    virtual machines to virtual networks at the data link layer of the OSI model,
    also known as layer 2\. Neutron supports multiple virtual switching platforms,
    including Linux bridges provided by the `bridge` kernel module and Open vSwitch.
    Open vSwitch, also known as OVS, is an open source virtual switch that supports
    standard management interfaces and protocols, including NetFlow, SPAN, RSPAN,
    LACP, and 802.1q VLAN tagging. However, many of these features are not exposed
    to the user through the OpenStack API. In addition to VLAN tagging, users can
    build overlay networks in software using L2-in-L3 tunneling protocols, such as
    GRE or VXLAN. Virtual switches can be used to facilitate communication between
    instances and devices outside the control of OpenStack, which include hardware
    switches, network firewalls, storage devices, bare-metal servers, and more.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟交换机**被定义为一种软件应用程序或服务，它在 OSI 模型的数据链路层（也称为第二层）连接虚拟机和虚拟网络。Neutron 支持多种虚拟交换平台，包括由
    `bridge` 内核模块提供的 Linux 桥接和 Open vSwitch。Open vSwitch，也称为 OVS，是一个开源虚拟交换机，支持标准管理接口和协议，包括
    NetFlow、SPAN、RSPAN、LACP 和 802.1q VLAN 标签。然而，许多这些功能并未通过 OpenStack API 提供给用户。除了
    VLAN 标签之外，用户还可以使用 L2-in-L3 隧道协议（如 GRE 或 VXLAN）在软件中构建覆盖网络。虚拟交换机可以用于促进实例与 OpenStack
    控制之外的设备之间的通信，这些设备包括硬件交换机、网络防火墙、存储设备、裸金属服务器等。'
- en: Additional information on the use of Linux bridges and Open vSwitch as switching
    platforms for OpenStack can be found in *[Chapter 4](05786c3c-b24e-40dc-82a7-ed6072eca14f.xhtml),*
    *Virtual Network Infrastructure Using Linux Bridges,* and *[Chapter 5](0763a131-4ab9-4b3e-8854-8646feae7937.xhtml),*
    *Building a Virtual Switching Infrastructure Using Open vSwitch,* respectively.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Linux 桥接和 Open vSwitch 作为 OpenStack 切换平台的更多信息，请参阅 *[第 4 章](05786c3c-b24e-40dc-82a7-ed6072eca14f.xhtml)，*
    *使用 Linux 桥接的虚拟网络基础设施* 和 *[第 5 章](0763a131-4ab9-4b3e-8854-8646feae7937.xhtml)，*
    *使用 Open vSwitch 构建虚拟交换基础设施*。
- en: Routing
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 路由
- en: OpenStack Networking provides routing and NAT capabilities through the use of
    IP forwarding, iptables, and network namespaces. Each network namespace has its
    own routing table, interfaces, and iptables processes that provide filtering and
    network address translation. By leveraging network namespaces to separate networks,
    there is no need to worry about overlapping subnets between networks created by
    users. Configuring a router within Neutron enables instances to interact and communicate
    with outside networks or other networks in the cloud.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Networking 通过使用 IP 转发、iptables 和网络命名空间提供路由和 NAT 功能。每个网络命名空间都有自己的路由表、接口和
    iptables 进程，提供过滤和网络地址转换。通过利用网络命名空间将网络分开，可以避免用户创建的网络之间的子网重叠问题。在 Neutron 中配置路由器可以使实例与外部网络或云中的其他网络进行交互和通信。
- en: More information on routing within OpenStack can be found in *[Chapter 10](371886b8-4c2a-49e9-90b8-8fe79217adb4.xhtml),
    Creating Standalone Routers with Neutron*, *[Chapter 11](bb8e64d5-76d5-4be8-b6b3-8ee9a520a439.xhtml),*
    *Router Redundancy Using VRRP*, and *[Chapter 12](b441728b-4377-43cf-b675-166266fef6c9.xhtml),
    Distributed Virtual Routers*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在 OpenStack 中路由的更多信息，请参阅 *[第 10 章](371886b8-4c2a-49e9-90b8-8fe79217adb4.xhtml)，使用
    Neutron 创建独立路由器*、* [第 11 章](bb8e64d5-76d5-4be8-b6b3-8ee9a520a439.xhtml)，* *使用
    VRRP 实现路由冗余*，以及 *[第 12 章](b441728b-4377-43cf-b675-166266fef6c9.xhtml)，分布式虚拟路由器*。
- en: Load balancing
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: First introduced in the Grizzly release of OpenStack, **Load Balancing as a
    Service (****LBaaS v2)** provides users with the ability to distribute client
    requests across multiple instances or servers. Users can create monitors, set
    connection limits, and apply persistence profiles to traffic traversing a virtual
    load balancer. OpenStack Networking is equipped with a plugin for LBaaS v2 that
    utilizes HAProxy in the open source reference implementation, but plugins are
    available that manage virtual and physical load-balancing appliances from third-party
    network vendors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**负载均衡即服务（LBaaS v2）** 最早在 OpenStack 的 Grizzly 版本中引入，提供了将客户端请求分发到多个实例或服务器的功能。用户可以创建监控器，设置连接限制，并为通过虚拟负载均衡器的流量应用持久化配置。OpenStack
    网络提供了一个用于 LBaaS v2 的插件，该插件在开源参考实现中使用 HAProxy，但也有插件可管理来自第三方网络供应商的虚拟和物理负载均衡设备。'
- en: More information on the use of load balancers within Neutron can be found in
    *[Chapter 13](71145e1b-825c-43ac-9993-24eceb7a0a26.xhtml),* *Load Balancing Traffic
    to Instances*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在 Neutron 中使用负载均衡器的更多信息，请参见 *[第13章](71145e1b-825c-43ac-9993-24eceb7a0a26.xhtml)，*
    *负载均衡流量到实例*。
- en: Firewalling
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防火墙
- en: 'OpenStack Networking provides two API-driven methods of securing network traffic
    to instances: security groups and **Firewall as a Service (FWaaS)**. Security
    groups find their roots in nova-network, the original networking stack for OpenStack
    built in to the Compute service, and are based on Amazon''s EC2 security groups.
    When using security groups in OpenStack, instances are placed into groups that
    share common functionality and rule sets. In a reference implementation, security
    group rules are implemented at the instance port level using drivers that leverage
    iptables or OpenFlow. Security policies built using FWaaS are also implemented
    at the port level, but can be applied to ports of routers as well as instances.
    The original FWaaS v1 API implemented firewall rules inside Neutron router namespaces,
    but that behavior has been removed in the v2 API.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络提供了两种 API 驱动的保护网络流量到实例的方法：安全组和 **防火墙即服务（FWaaS）**。安全组源于 nova-network，这是
    OpenStack 的原始网络堆栈，内置于计算服务中，且基于亚马逊的 EC2 安全组。在 OpenStack 中使用安全组时，实例被放置在共享公共功能和规则集的组中。在参考实现中，安全组规则通过利用
    iptables 或 OpenFlow 的驱动程序在实例端口级别实现。使用 FWaaS 构建的安全策略也在端口级别实现，但可以应用于路由器的端口以及实例。原始的
    FWaaS v1 API 在 Neutron 路由器的命名空间中实现防火墙规则，但在 v2 API 中已移除该行为。
- en: More information on securing instance traffic can be found in *[Chapter 8](240902fd-5108-446e-afa5-8122de12f0af.xhtml),
    Managing Security Groups*. The use of FWaaS is outside the scope of this book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有关保护实例流量的更多信息，请参见 *[第8章](240902fd-5108-446e-afa5-8122de12f0af.xhtml)，管理安全组*。本书的范围不包括
    FWaaS 的使用。
- en: Virtual private networks
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟私人网络
- en: A **virtual private network (VPN)** extends a private network across a public
    network such as the internet. A VPN enables a computer to send and receive data
    across public networks as if it were directly connected to the private network.
    Neutron provides a set of APIs to allow users to create IPSec-based VPN tunnels
    from Neutron routers to remote gateways when using the open source reference implementation.
    The use of VPN as a Service is outside the scope of this book.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟私人网络（VPN）** 扩展了一个私有网络，跨越公共网络如互联网。VPN 使计算机能够在公共网络上发送和接收数据，就像它直接连接到私有网络一样。当使用开源参考实现时，Neutron
    提供了一组 API，允许用户从 Neutron 路由器创建基于 IPSec 的 VPN 隧道到远程网关。本书的范围不包括作为服务的 VPN 的使用。'
- en: Network functions virtualization
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络功能虚拟化
- en: '**Network functions virtualization (****NFV)** is a network architecture concept
    that proposes virtualizing network appliances used for various network functions.
    These functions include intrusion detection, caching, gateways, WAN accelerators,
    firewalls, and more. Using SR-IOV, instances are no longer required to use para-virtualized
    drivers or to be connected to virtual bridges within the host. Instead, the instance
    is attached to a Neutron port that is associated with a **virtual function** (**VF**)
    in the NIC, allowing the instance to access the NIC hardware directly. Configuring
    and implementing SR-IOV with Neutron is outside the scope of this book.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络功能虚拟化（**NFV**）** 是一种网络架构概念，旨在将用于各种网络功能的网络设备虚拟化。这些功能包括入侵检测、缓存、网关、WAN加速器、防火墙等。使用SR-IOV后，实例不再需要使用准虚拟化驱动程序或连接到主机中的虚拟桥。相反，实例会附加到与NIC中**虚拟功能**（**VF**）关联的Neutron端口，从而使实例能够直接访问NIC硬件。配置和实现SR-IOV与Neutron的结合超出了本书的范围。'
- en: OpenStack Networking resources
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 网络资源
- en: OpenStack gives users the ability to create and configure networks and subnets
    and instruct other services, such as Compute, to attach virtual devices to ports
    on these networks. The Identity service gives cloud operators the ability to segregate
    users into projects. OpenStack Networking supports project-owned resources, including
    each project having multiple private networks and routers. Projects can be left
    to choose their own IP addressing scheme, even if those addresses overlap with
    other project networks, or administrators can place limits on the size of subnets
    and addresses available for allocation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack使用户能够创建和配置网络与子网，并指示其他服务（如计算服务）将虚拟设备连接到这些网络上的端口。身份服务使云管理员能够将用户划分为不同的项目。OpenStack网络支持项目拥有的资源，包括每个项目拥有多个私有网络和路由器。项目可以选择自己的IP地址方案，即使这些地址与其他项目的网络重叠，或者管理员可以对子网的大小和可分配地址进行限制。
- en: 'There are two types of networks that can be expressed in OpenStack:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack中可以表达两种类型的网络：
- en: '**Project/tenant network**: A virtual network created by a project or administrator
    on behalf of a project. The physical details of the network are not exposed to
    the project.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目/租户网络**：由项目或管理员代表项目创建的虚拟网络。网络的物理细节对项目不公开。'
- en: '**Provider network**: A virtual network created to map to a physical network.
    Provider networks are typically created to enable access to physical network resources
    outside of the cloud, such as network gateways and other services, and usually
    map to VLANs. Projects can be given access to provider networks.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供商网络**：一种虚拟网络，用于映射到物理网络。提供商网络通常是为了访问云外的物理网络资源，如网络网关和其他服务，通常映射到VLAN。可以为项目提供对提供商网络的访问。'
- en: The terms *project* and *tenant* are used interchangeably within the OpenStack
    community, with the former being the newer and preferred nomenclature.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenStack社区中，*项目*和*租户*这两个术语是可以互换使用的，前者是更新且更推荐的术语。
- en: A **project network** provides connectivity to resources in a project. Users
    can create, modify, and delete project networks. Each project network is isolated
    from other project networks by a boundary such as a VLAN or other segmentation
    ID. A **provider network**, on the other hand, provides connectivity to networks
    outside of the cloud and is typically created and managed by a cloud administrator.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**项目网络** 提供项目内资源的连接。用户可以创建、修改和删除项目网络。每个项目网络通过VLAN或其他分段ID等边界与其他项目网络隔离。**提供商网络**
    则提供对云外网络的连接，通常由云管理员创建和管理。'
- en: The primary differences between project and provider networks can be seen during
    the network provisioning process. Provider networks are created by administrators
    on behalf of projects and can be dedicated to a particular project, shared by
    a subset of projects, or shared by all projects. Project networks are created
    by projects for use by their instances and cannot be shared with all projects,
    though sharing with certain projects may be accomplished using role-based access
    control (**RBAC**) policies. When a provider network is created, the administrator
    can provide specific details that aren't available to ordinary users, including
    the network type, the physical network interface, and the network segmentation
    identifier, such as a VLAN ID or VXLAN VNI. Project networks have these same attributes,
    but users cannot specify them. Instead, they are automatically determined by Neutron.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 项目网络与提供商网络的主要区别在于网络配置过程中可见。提供商网络由管理员代表项目创建，可以专用于特定项目，或由部分项目共享，甚至所有项目共享。项目网络由项目创建，供其实例使用，不能与所有项目共享，但可以通过基于角色的访问控制（**RBAC**）策略与特定项目共享。创建提供商网络时，管理员可以提供普通用户无法获取的具体细节，包括网络类型、物理网络接口和网络分段标识符（如VLAN
    ID或VXLAN VNI）。项目网络具有相同的属性，但用户无法指定它们，这些属性由Neutron自动确定。
- en: 'There are other foundational network resources that will be covered in further
    detail later in this book, but are summarized in the following table for your
    convenience:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书后面将详细介绍其他基础网络资源，但在此表格中进行了总结以供参考：
- en: '| **Resource** | **Description** |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **资源** | **描述** |'
- en: '| Subnet | A block of IP addresses used to allocate ports created on the network.
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 子网 | 用于分配创建在网络上的端口的IP地址块。 |'
- en: '| Port | A connection point for attaching a single device, such as the virtual
    network interface card (vNIC) of a virtual instance, to a virtual network. Port
    attributes include the MAC address and the fixed IP address on the subnet. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 端口 | 将单个设备（例如虚拟实例的虚拟网络接口卡（vNIC））连接到虚拟网络的连接点。端口属性包括子网上的MAC地址和固定IP地址。 |'
- en: '| Router | A virtual device that provides routing between self-service networks
    and provider networks. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 路由器 | 提供自服务网络和提供商网络之间路由的虚拟设备。 |'
- en: '| Security group | A set of virtual firewall rules that control ingress and
    egress traffic at the port level. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 安全组 | 一组虚拟防火墙规则，控制端口级别的入站和出站流量。 |'
- en: '| DHCP | An agent that manages IP addresses for instances on provider and self-service
    networks. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| DHCP | 管理提供商和自服务网络上实例IP地址的代理。 |'
- en: '| Metadata | A service that provides data to instances during boot. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 元数据 | 在实例引导过程中向实例提供数据的服务。 |'
- en: Virtual network interfaces
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟网络接口
- en: OpenStack deployments are most often configured to use the libvirt KVM/QEMU
    driver to provide platform virtualization. When an instance is booted for the
    first time, OpenStack creates a port for each network interface attached to the
    instance. A virtual network interface called a **tap interface** is created on
    the compute node hosting the instance. The tap interface corresponds directly
    to a network interface within the guest instance and has the properties of the
    port created in Neutron, including the MAC and IP address. Through the use of
    a bridge, the host can expose the guest instance to the physical network. Neutron
    allows users to specify alternatives to the standard tap interface, such as Macvtap
    and SR-IOV, by defining special attributes on ports and attaching them to instances.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack部署通常配置为使用libvirt KVM/QEMU驱动程序进行平台虚拟化。当实例首次启动时，OpenStack为每个附加到实例的网络接口创建一个端口。在托管实例的计算节点上创建称为**tap接口**的虚拟网络接口。tap接口直接对应于客户实例内的网络接口，并具有Neutron创建的端口属性，包括MAC和IP地址。通过使用桥接，主机可以将客户实例暴露给物理网络。Neutron允许用户通过在端口上定义特殊属性并将其附加到实例来指定标准tap接口的替代方案，如Macvtap和SR-IOV。
- en: Virtual network switches
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟网络交换机
- en: OpenStack Networking supports many types of virtual and physical switches, and
    includes built-in support for Linux bridges and Open vSwitch virtual switches.
    This book will cover both technologies and their respective drivers and agents.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Networking支持多种虚拟和物理交换机类型，并内置支持Linux桥接和Open vSwitch虚拟交换机。本书将涵盖这两种技术及其各自的驱动程序和代理。
- en: The terms *bridge* and *switch* are often used interchangeably in the context
    of OpenStack Networking, and may be used in the same way throughout this book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 网络环境中，*bridge* 和 *switch* 这两个术语常常可以互换使用，并且在本书中也将以相同的方式使用。
- en: Overlay networks
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 叠加网络
- en: 'Neutron supports overlay networking technologies that provide network isolation
    at scale with little to no modification of the underlying physical infrastructure.
    To accomplish this, Neutron leverages L2-in-L3 overlay networking technologies
    such as GRE, VXLAN, and GENEVE. When configured accordingly, Neutron builds point-to-point
    tunnels between all network and compute nodes in the cloud using a predefined
    interface. These point-to-point tunnels create what is called a **mesh network**,
    where every host is connected to every other host. A cloud consisting of one combined
    controller and network node, and three compute nodes, would have a fully meshed
    overlay network that resembles figure 1.2:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 支持叠加网络技术，这些技术可以在不对底层物理基础设施做太多修改的情况下，实现大规模的网络隔离。为了实现这一点，Neutron 利用 L2-in-L3
    叠加网络技术，如 GRE、VXLAN 和 GENEVE。经过适当配置后，Neutron 会在云中的所有网络和计算节点之间建立点对点隧道，使用预定义的接口。这些点对点隧道构成了所谓的
    **mesh network**（网状网络），其中每个主机都与其他主机相连。由一个结合了控制器和网络节点的节点，以及三个计算节点组成的云，将拥有一个完全网状的叠加网络，类似于图
    1.2：
- en: '![](img/e7047532-54da-4781-b1b8-9f7dab00435c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7047532-54da-4781-b1b8-9f7dab00435c.png)'
- en: Figure 1.2
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2
- en: Using the overlay network pictured in figure 1.2, traffic between instances
    or other virtual devices on any given host will travel between layer 3 endpoints
    on each of the underlying hosts without regard for the layer 2 network beneath
    them. Due to encapsulation, Neutron routers may be needed to facilitate communication
    between different project networks as well as networks outside of the cloud.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图 1.2 中所示的叠加网络，任何给定主机上实例或其他虚拟设备之间的流量将穿越每个底层主机上的第 3 层端点，而不考虑其下方的第 2 层网络。由于封装，可能需要
    Neutron 路由器来促进不同项目网络之间的通信，以及与云外网络的通信。
- en: Virtual Extensible Local Area Network (VXLAN)
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟可扩展局域网（VXLAN）
- en: This book focuses primarily on VXLAN, an overlay technology that helps address
    scalability issues with VLANs. VXLAN encapsulates layer 2 Ethernet frames inside
    layer 4 UDP packets that can be forwarded or routed between hosts. This means
    that a virtual network can be transparently extended across a large network without
    any changes to the end hosts. In the case of OpenStack Networking, however, a
    VXLAN mesh network is commonly constructed only between nodes that exist in the
    same cloud.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要关注 VXLAN，这是一种叠加技术，帮助解决 VLAN 的可扩展性问题。VXLAN 将第 2 层以太网帧封装在第 4 层 UDP 数据包中，这些数据包可以在主机之间转发或路由。这意味着虚拟网络可以透明地跨越大规模网络扩展，而无需对终端主机进行任何修改。然而，在
    OpenStack 网络环境中，VXLAN 网状网络通常仅在同一云中的节点之间构建。
- en: Rather than use VLAN IDs to differentiate between networks, VXLAN uses a VXLAN
    Network Identifier (VNI) to serve as the unique identifier on a link that potentially
    carries traffic for tens of thousands of networks, or more. An 802.1q VLAN header
    supports up to 4,096 unique IDs, whereas a VXLAN header supports approximately
    16 million unique IDs. Within an OpenStack cloud, virtual machine instances are
    unaware that VXLAN is used to forward traffic between hosts. The VXLAN Tunnel
    Endpoint (VTEP) on the physical node handles the encapsulation and decapsulation
    of traffic without the instance ever knowing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用 VLAN ID 区分网络不同，VXLAN 使用 VXLAN 网络标识符（VNI）作为链路上的唯一标识符，这个链路可能承载成千上万甚至更多网络的流量。802.1q
    VLAN 头部支持最多 4,096 个唯一 ID，而 VXLAN 头部则支持大约 1600 万个唯一 ID。在 OpenStack 云中，虚拟机实例并不知道
    VXLAN 被用来在主机之间转发流量。物理节点上的 VXLAN 隧道端点（VTEP）负责流量的封装和解封装，实例对此并不知情。
- en: Because VXLAN network traffic is encapsulated, many network devices cannot participate
    in these networks without additional configuration, if at all. As a result, VXLAN
    networks are effectively isolated from other networks in the cloud and require
    the use of a Neutron router to provide access to connected instances. More information
    on creating Neutron routers begins in *[Chapter 10](371886b8-4c2a-49e9-90b8-8fe79217adb4.xhtml),
    Creating Standalone Routers with Neutron*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 VXLAN 网络流量被封装，许多网络设备在没有额外配置的情况下无法参与这些网络。结果，VXLAN 网络实际上与云中的其他网络隔离，并且需要使用 Neutron
    路由器来为连接的实例提供访问。有关创建 Neutron 路由器的更多信息，请参见 *[第10章](371886b8-4c2a-49e9-90b8-8fe79217adb4.xhtml)，使用
    Neutron 创建独立路由器*。
- en: While not as performant as VLAN or flat networks on some hardware, the use of
    VXLAN is becoming more popular in cloud network architectures where scalability
    and self-service are major drivers. Newer networking hardware that offers VXLAN
    offloading capabilities should be leveraged if you are considering implementing
    VXLAN-based overlay networks in your cloud.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在某些硬件上，VXLAN的性能不如VLAN或平面网络，但在云网络架构中，VXLAN的使用越来越普遍，尤其是在可扩展性和自助服务是主要驱动因素的情况下。如果你正在考虑在云中实现基于VXLAN的覆盖网络，应利用提供VXLAN卸载功能的较新网络硬件。
- en: More information on how VXLAN encapsulation works is described in RFC 7348,
    available at the following URL: [https://tools.ietf.org/html/rfc7348](https://tools.ietf.org/html/rfc7348)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有关VXLAN封装工作原理的更多信息，请参阅RFC 7348，详情见以下网址：[https://tools.ietf.org/html/rfc7348](https://tools.ietf.org/html/rfc7348)
- en: Generic Router Encapsulation (GRE)
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用路由器封装（GRE）
- en: A **GRE network** is similar to a VXLAN network in that traffic from one instance
    to another is encapsulated and sent over a layer 3 network. A unique segmentation
    ID is used to differentiate traffic from other GRE networks. Rather than use UDP
    as the transport mechanism, GRE uses IP protocol 47\. For various reasons, the
    use of GRE for encapsulating tenant network traffic has fallen out of favor now
    that VXLAN is supported by both Open vSwitch and Linux Bridge network agents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**GRE网络**类似于VXLAN网络，其中一个实例到另一个实例的流量被封装并通过第3层网络发送。使用唯一的分段ID来区分来自其他GRE网络的流量。与使用UDP作为传输机制不同，GRE使用IP协议47。由于各种原因，随着VXLAN被Open
    vSwitch和Linux Bridge网络代理支持，使用GRE封装租户网络流量的做法已不再流行。'
- en: More information on how GRE encapsulation works is described in RFC 2784 available
    at the following URL: [https://tools.ietf.org/html/rfc278](https://tools.ietf.org/html/rfc278)[4](https://tools.ietf.org/html/rfc278)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有关GRE封装工作原理的更多信息，请参阅RFC 2784，详情见以下网址：[https://tools.ietf.org/html/rfc278](https://tools.ietf.org/html/rfc278)[4](https://tools.ietf.org/html/rfc278)
- en: As of the Pike release of OpenStack, the Open vSwitch mechanism driver is the
    only commonly used driver that supports GRE.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从OpenStack的Pike版本开始，Open vSwitch机制驱动程序是唯一支持GRE的常用驱动程序。
- en: Generic Network Virtualization Encapsulation (GENEVE)
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用网络虚拟化封装（GENEVE）
- en: GENEVE is an emerging overlay technology that resembles VXLAN and GRE, in that
    packets between hosts are designed to be transmitted using standard networking
    equipment without having to modify the client or host applications. Like VXLAN,
    GENEVE encapsulates packets with a unique header and uses UDP as its transport
    mechanism. GENEVE leverages the benefits of multiple overlay technologies such
    as VXLAN, NVGRE, and STT, and may supplant those technologies over time. The Open
    Virtual Networking (OVN) mechanism driver relies on GENEVE as its overlay technology,
    which may speed up the adoption of GENEVE in later releases of OpenStack.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GENEVE是一种新兴的覆盖技术，它类似于VXLAN和GRE，旨在通过标准网络设备传输主机之间的分组，而无需修改客户端或主机应用程序。像VXLAN一样，GENEVE使用独特的头部封装分组，并使用UDP作为其传输机制。GENEVE利用了VXLAN、NVGRE和STT等多种覆盖技术的优势，并可能随着时间的推移取代这些技术。Open
    Virtual Networking（OVN）机制驱动程序依赖于GENEVE作为其覆盖技术，这可能会加速GENEVE在OpenStack后续版本中的采用。
- en: Preparing the physical infrastructure
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备物理基础设施
- en: 'Most OpenStack clouds are made up of physical infrastructure nodes that fit
    into one of the following four categories:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数OpenStack云由适合以下四类之一的物理基础设施节点组成：
- en: '**Controller node**: Controller nodes traditionally run the API services for
    all of the OpenStack components, including Glance, Nova, Keystone, Neutron, and
    more. In addition, controller nodes run the database and messaging servers, and
    are often the point of management of the cloud via the Horizon dashboard. Most
    OpenStack API services can be installed on multiple controller nodes and can be
    load balanced to scale the OpenStack control plane.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制节点**：控制节点通常运行所有OpenStack组件的API服务，包括Glance、Nova、Keystone、Neutron等。此外，控制节点还运行数据库和消息服务器，通常也是通过Horizon仪表板管理云的入口点。大多数OpenStack
    API服务可以安装在多个控制节点上，并且可以负载均衡以扩展OpenStack控制平面。'
- en: '**Network node**: Network nodes traditionally run DHCP and metadata services
    and can also host virtual routers when the Neutron L3 agent is installed. In smaller
    environments, it is not uncommon to see controller and network node services collapsed
    onto the same server or set of servers. As the cloud grows in size, most network
    services can be broken out between other servers or installed on their own server
    for optimal performance.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络节点**：网络节点通常运行 DHCP 和元数据服务，并且在安装了 Neutron L3 代理时，也可以承载虚拟路由器。在较小的环境中，控制节点和网络节点的服务往往会合并到同一台服务器或一组服务器上。随着云规模的增长，大多数网络服务可以拆分到其他服务器上，或者为了优化性能，安装在独立的服务器上。'
- en: '**Compute node**: Compute nodes traditionally run a hypervisor such as KVM,
    Hyper-V, or Xen, or container software such as LXC or Docker. In some cases, a
    compute node may also host virtual routers, especially when Distributed Virtual
    Routing (DVR) is configured. In proof-of-concept or test environments, it is not
    uncommon to see controller, network, and compute node services collapsed onto
    the same machine. This is especially common when using DevStack, a software package
    designed for developing and testing OpenStack code. All-in-one installations are
    not recommended for production use.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算节点**：计算节点通常运行如 KVM、Hyper-V 或 Xen 等虚拟化软件，或如 LXC 或 Docker 等容器软件。在某些情况下，计算节点还可能承载虚拟路由器，特别是当配置了分布式虚拟路由（DVR）时。在概念验证或测试环境中，控制、网络和计算节点的服务经常合并在同一台机器上。这在使用
    DevStack（一个用于开发和测试 OpenStack 代码的软件包）时尤其常见。对于生产环境，不建议使用一体化安装。'
- en: '**Storage node**: Storage nodes are traditionally limited to running software
    related to storage such as Cinder, Ceph, or Swift. Storage nodes do not usually
    host any type of Neutron networking service or agent and will not be discussed
    in this book.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储节点**：存储节点通常仅运行与存储相关的软件，如 Cinder、Ceph 或 Swift。存储节点通常不承载任何类型的 Neutron 网络服务或代理，本书将不讨论这一部分内容。'
- en: 'When Neutron services are broken out between many hosts, the layout of services
    will often resemble the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Neutron 服务分布在多个主机之间时，服务的布局通常类似于以下结构：
- en: '![](img/468a9df6-164e-4bd6-bd7e-7b5bb65b9f78.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/468a9df6-164e-4bd6-bd7e-7b5bb65b9f78.png)'
- en: Figure 1.3
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3
- en: In figure 1.3, the neutron API service `neutron-server` is installed on the
    Controller node, while Neutron agents responsible for implementing certain virtual
    networking resources are installed on a dedicated network node. Each compute node
    hosts a network plugin agent responsible for implementing the network plumbing
    on that host. Neutron supports a highly available API service with a shared database
    backend, and it is recommended that the cloud operator load balances traffic to
    the Neutron API service when possible. Multiple DHCP, metadata, L3, and LBaaS
    agents should be implemented on separate network nodes whenever possible. Virtual
    networks, routers, and load balancers can be scheduled to one or more agents to
    provide a basic level of redundancy when an agent fails. Neutron even includes
    a built-in scheduler that can detect failure and reschedule certain resources
    when a failure is detected.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 1.3 中，Neutron API 服务 `neutron-server` 安装在控制节点上，而负责实现某些虚拟网络资源的 Neutron 代理则安装在专用的网络节点上。每个计算节点上都运行一个网络插件代理，负责在该主机上实现网络连接。Neutron
    支持高可用的 API 服务，并具有共享的数据库后台，建议云操作员在可能的情况下对 Neutron API 服务进行流量负载均衡。应尽可能将多个 DHCP、元数据、L3
    和 LBaaS 代理部署在独立的网络节点上。虚拟网络、路由器和负载均衡器可以被调度到一个或多个代理上，以提供在代理失败时的基本冗余。Neutron 甚至包括一个内置调度器，能够在检测到故障时重新调度某些资源。
- en: Configuring the physical infrastructure
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置物理基础设施
- en: Before the installation of OpenStack can begin, the physical network infrastructure
    must be configured to support the networks needed for an operational cloud. In
    a production environment, this will likely include a dedicated management VLAN
    used for server management and API traffic, a VLAN dedicated to overlay network
    traffic, and one or more VLANs that will be used for provider and VLAN-based project
    networks. Each of these networks can be configured on separate interfaces, or
    they can be collapsed onto a single interface if desired.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始安装 OpenStack 之前，必须配置物理网络基础设施，以支持操作云所需的网络。在生产环境中，这通常包括用于服务器管理和 API 流量的专用管理
    VLAN，用于叠加网络流量的 VLAN，以及一个或多个将用于提供商和基于 VLAN 的项目网络的 VLAN。可以在单独的接口上配置这些网络，也可以将它们合并到一个接口上，具体取决于需求。
- en: 'The reference architecture for OpenStack Networking defines at least four distinct
    types of traffic that will be seen on the network:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络的参考架构定义了网络中至少四种不同类型的流量：
- en: Management
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理
- en: API
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API
- en: External
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部
- en: Guest
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户机
- en: These traffic types are often categorized as control plane or data plane, depending
    on the purpose, and are terms used in networking to describe the purpose of the
    traffic. In this case, **control plane** traffic is used to describe traffic related
    to management, API, and other non-VM related traffic. **Data plane** traffic,
    on the other hand, represents traffic generated by, or directed to, virtual machine
    instances.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些流量类型通常根据目的被分类为控制平面或数据平面，这些术语用于网络中描述流量的目的。在这种情况下，**控制平面**流量用于描述与管理、API 及其他非虚拟机相关的流量。**数据平面**流量则代表由虚拟机实例生成的或指向虚拟机实例的流量。
- en: Although I have taken the liberty of splitting out the network traffic onto
    dedicated interfaces in this book, it is not necessary to do so to create an operational
    OpenStack cloud. In fact, many administrators and distributions choose to collapse
    multiple traffic types onto single or bonded interfaces using VLAN tagging. Depending
    on the chosen deployment model, the administrator may spread networking services
    across multiple nodes or collapse them onto a single node. The security requirements
    of the enterprise deploying the cloud will often dictate how the cloud is built.
    The various network and service configurations will be discussed in the upcoming
    sections.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在本书中我已将网络流量分配到专用接口，但并不需要这样做才能创建一个可操作的 OpenStack 云。事实上，许多管理员和发行版选择将多种流量类型合并到单一或绑定的接口上，使用
    VLAN 标记。根据所选的部署模型，管理员可能将网络服务分布在多个节点上，或将它们集中到一个节点上。部署云的企业的安全要求通常会决定云的构建方式。接下来的章节将讨论各种网络和服务配置。
- en: Management network
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理网络
- en: The **management network**, also referred to as the **internal network** in
    some distributions, is used for internal communication between hosts for services
    such as the messaging service and database service, and can be considered as part
    of the control plane.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**管理网络**，在某些发行版中也称为**内部网络**，用于主机之间的内部通信，如消息服务和数据库服务，可以视为控制平面的一部分。'
- en: All hosts will communicate with each other over this network. In many cases,
    this same interface may be used to facilitate image transfers between hosts or
    some other bandwidth-intensive traffic. The management network can be configured
    as an isolated network on a dedicated interface or combined with another network
    as described in the following section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所有主机将在此网络上进行相互通信。在许多情况下，可能会使用相同的接口来促进主机之间的镜像传输或其他带宽密集型流量。管理网络可以配置为专用接口上的隔离网络，或如下一节所述，与其他网络合并。
- en: API network
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: API 网络
- en: The **API network** is used to expose OpenStack APIs to users of the cloud and
    services within the cloud and can be considered as part of the control plane.
    Endpoint addresses for API services such as Keystone, Neutron, Glance, and Horizon
    are procured from the API network.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**API 网络**用于将 OpenStack API 暴露给云的用户和云内服务，并可以视为控制平面的一部分。API 服务的端点地址，如 Keystone、Neutron、Glance
    和 Horizon，来自于 API 网络。'
- en: It is common practice to utilize a single interface and IP address for API endpoints
    and management access to the host itself over SSH. A diagram of this configuration
    is provided later in this chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常做法是利用单一接口和 IP 地址来访问 API 端点以及通过 SSH 访问主机本身。该配置的示意图将在本章后面提供。
- en: It is recommended, though not required, that you physically separate management
    and API traffic from other traffic types, such as storage traffic, to avoid issues
    with network congestion that may affect operational stability.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不是强制要求，但建议将管理和 API 流量与其他流量类型（如存储流量）物理分离，以避免可能影响操作稳定性的网络拥塞问题。
- en: External network
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部网络
- en: An **external network** is a provider network that provides Neutron routers
    with external network access. Once a router has been configured and attached to
    the external network, the network becomes the source of floating IP addresses
    for instances and other network resources attached to the router. IP addresses
    in an external network are expected to be routable and reachable by clients on
    a corporate network or the internet. Multiple external provider networks can be
    segmented using VLANs and trunked to the same physical interface. Neutron is responsible
    for tagging the VLAN based on the network configuration provided by the administrator.
    Since external networks are utilized by VMs, they can be considered as part of
    the data plane.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**外部网络**是一个提供给Neutron路由器外部网络访问的提供商网络。一旦路由器被配置并连接到外部网络，该网络就成为实例和附加到路由器的其他网络资源的浮动IP地址的来源。外部网络中的IP地址应当是可路由的，并且可以被公司网络或互联网中的客户端访问。多个外部提供商网络可以通过VLAN进行分段，并通过同一个物理接口进行中继。Neutron负责根据管理员提供的网络配置对VLAN进行标记。由于外部网络是由虚拟机使用的，它们可以视为数据平面的一部分。'
- en: Guest network
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户网络
- en: The **guest network** is a network dedicated to instance traffic. Options for
    guest networks include local networks restricted to a particular node, flat, or
    VLAN-tagged networks, or virtual overlay networks made possible with GRE, VXLAN,
    or GENEVE encapsulation. For more information on guest networks, refer to *[Chapter
    6](5a3df5cf-aebb-4c57-9f48-fa5419a5b2ae.xhtml), Building Networks with Neutron*.
    Since guest networks provide connectivity to VMs, they can be considered part
    of the data plane.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户网络**是专门为实例流量提供的网络。客户网络的选项包括仅限特定节点的本地网络、平面网络或VLAN标签网络，或通过GRE、VXLAN或GENEVE封装技术实现的虚拟覆盖网络。有关客户网络的更多信息，请参阅*[第6章](5a3df5cf-aebb-4c57-9f48-fa5419a5b2ae.xhtml)，使用Neutron构建网络*。由于客户网络为虚拟机提供连接，因此它们可以视为数据平面的一部分。'
- en: The physical interfaces used for external and guest networks can be dedicated
    interfaces or ones that are shared with other types of traffic. Each approach
    has its benefits and drawbacks, and they are described in more detail later in
    this chapter. In the next few chapters, I will define networks and VLANs that
    will be used throughout the book to demonstrate the various components of OpenStack
    Networking. Generic information on the configuration of switch ports, routers,
    or firewalls will also be provided.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 用于外部和客户网络的物理接口可以是专用接口，也可以是与其他类型流量共享的接口。每种方式都有其优缺点，后文将详细描述。在接下来的几章中，我将定义将在全书中使用的网络和VLAN，以展示OpenStack
    Networking的各个组件。还将提供关于交换机端口、路由器或防火墙配置的一般信息。
- en: Physical server connections
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理服务器连接
- en: The number of interfaces needed per host is dependent on the purpose of the
    cloud, the security and performance requirements of the organization, and the
    cost and availability of hardware. A single interface per server that results
    in a combined control and data plane is all that is needed for a fully operational
    OpenStack cloud. Many organizations choose to deploy their cloud this way, especially
    when port density is at a premium, the environment is simply used for testing,
    or network failure at the node level is a non-impacting event. When possible,
    however, it is recommended that you split control and data traffic across multiple
    interfaces to reduce the chances of network failure.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主机所需的接口数量取决于云的用途、组织的安全性和性能要求以及硬件的成本和可用性。每个服务器只需要一个接口，控制面和数据面合并在同一个接口中，这样就足以支撑一个完全运行的OpenStack云。许多组织选择以这种方式部署云，尤其是在端口密度有限、环境仅用于测试或节点级别的网络故障不会产生影响的情况下。然而，如果可能的话，建议将控制流量和数据流量分配到多个接口上，以减少网络故障的可能性。
- en: Single interface
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单一接口
- en: For hosts using a single interface, all traffic to and from instances as well
    as internal OpenStack, SSH management, and API traffic traverse the same physical
    interface. This configuration can result in severe performance penalties, as a
    service or guest can potentially consume all available bandwidth. A single interface
    is recommended only for non-production clouds.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用单一接口的主机，所有进出实例的流量以及内部OpenStack、SSH管理和API流量都会穿越同一个物理接口。这种配置可能导致严重的性能损失，因为某个服务或虚拟机可能会消耗所有可用带宽。单一接口仅推荐用于非生产环境的云。
- en: 'The following table demonstrates the networks and services traversing a single
    interface over multiple VLANs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了多个VLAN下通过单一接口穿越的网络和服务：
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **服务/功能** | **目的** | **接口** | **VLAN** |'
- en: '| SSH | Host management | eth0 | 10 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SSH | 主机管理 | eth0 | 10 |'
- en: '| APIs | Access to OpenStack APIs | eth0 | 15 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| APIs | 访问 OpenStack APIs | eth0 | 15 |'
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | eth0 | 20 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 覆盖网络 | 用于在主机之间隧道化覆盖（VXLAN、GRE、GENEVE）流量 | eth0 | 20 |'
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | eth0 | Multiple |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 客户机/外部网络 | 用于提供访问外部云资源和基于 VLAN 的项目网络 | eth0 | 多个 |'
- en: Multiple interfaces
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多个接口
- en: 'To reduce the likelihood of guest traffic impacting management traffic, segregation
    of traffic between multiple physical interfaces is recommended. At a minimum,
    two interfaces should be used: one that serves as a dedicated interface for management
    and API traffic (control plane), and another that serves as a dedicated interface
    for external and guest traffic (data plane). Additional interfaces can be used
    to further segregate traffic, such as storage.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为减少客户机流量对管理流量的影响，建议在多个物理接口之间进行流量隔离。至少应使用两个接口：一个作为专用接口处理管理和 API 流量（控制平面），另一个作为专用接口处理外部和客户机流量（数据平面）。可以使用额外的接口进一步隔离流量，例如存储流量。
- en: 'The following table demonstrates the networks and services traversing two interfaces
    with multiple VLANs:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了通过两个接口和多个 VLAN 的网络和服务：
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **服务/功能** | **目的** | **接口** | **VLAN** |'
- en: '| SSH | Host management | eth0 | 10 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| SSH | 主机管理 | eth0 | 10 |'
- en: '| APIs | Access to OpenStack APIs | eth0 | 15 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| APIs | 访问 OpenStack APIs | eth0 | 15 |'
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | eth1 | 20 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 覆盖网络 | 用于在主机之间隧道化覆盖（VXLAN、GRE、GENEVE）流量 | eth1 | 20 |'
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | eth1 | Multiple |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 客户机/外部网络 | 用于提供访问外部云资源和基于 VLAN 的项目网络 | eth1 | 多个 |'
- en: Bonding
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绑定
- en: 'The use of multiple interfaces can be expanded to utilize bonds instead of
    individual network interfaces. The following common bond modes are supported:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个接口可以扩展为使用绑定而非单独的网络接口。以下是支持的常见绑定模式：
- en: '**Mode 1 (active-backup)**: Mode 1 bonding sets all interfaces in the bond
    to a backup state while one interface remains active. When the active interface
    fails, a backup interface replaces it. The same MAC address is used upon failover
    to avoid issues with the physical network switch. Mode 1 bonding is supported
    by most switching vendors, as it does not require any special configuration on
    the switch to implement.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式 1（主动-备份）**：模式 1 绑定将绑定中的所有接口设置为备份状态，同时保持一个接口处于活动状态。当活动接口失败时，备份接口将替代它。故障切换时使用相同的
    MAC 地址，以避免物理网络交换机出现问题。大多数交换机厂商支持模式 1 绑定，因为它不需要在交换机上进行特殊配置。'
- en: '**Mode 4 (active-active)**: Mode 4 bonding involves the use of **aggregation
    groups**, a group in which all interfaces share an identical configuration and
    are grouped together to form a single logical interface. The interfaces are aggregated
    using the IEEE 802.3ad Link Aggregation Control Protocol (LACP). Traffic is load
    balanced across the links using methods negotiated by the physical node and the
    connected switch or switches. The physical switching infrastructure *must* be
    capable of supporting this type of bond. While some switching platforms require
    that multiple links of an LACP bond be connected to the same switch, others support
    technology known as **Multi-Chassis Link Aggregation (MLAG)** that allows multiple
    physical switches to be configured as a single logical switch. This allows links
    of a bond to be connected to multiple switches that provide hardware redundancy
    while allowing users the full bandwidth of the bond under normal operating conditions,
    all with no additional changes to the server configuration.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式 4（主动-主动）**：模式 4 绑定涉及使用 **聚合组**，该组中的所有接口共享相同的配置并组合成一个逻辑接口。接口通过 IEEE 802.3ad
    链路聚合控制协议（LACP）进行聚合。流量根据物理节点和连接交换机或交换机的协商方法在链路上进行负载均衡。物理交换基础设施 *必须* 支持这种类型的绑定。尽管一些交换平台要求将
    LACP 绑定的多个链路连接到同一交换机，但其他平台支持名为 **多机箱链路聚合（MLAG）** 的技术，该技术允许多个物理交换机被配置为一个逻辑交换机。这使得绑定的链路可以连接到多个交换机，提供硬件冗余，同时在正常操作条件下允许用户充分利用绑定的带宽，且无需对服务器配置进行额外更改。'
- en: Bonding can be configured within the Linux operating system using tools such
    as iproute2, ifupdown, and Open vSwitch, among others.The configuration of bonded
    interfaces is outside the scope of OpenStack and this book.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Bonding可以在Linux操作系统中通过诸如iproute2、ifupdown和Open vSwitch等工具进行配置。Bonded接口的配置超出了OpenStack和本书的范围。
- en: Bonding configurations vary greatly between Linux distributions. Refer to the
    respective documentation of your Linux distribution for assistance in configuring
    bonding.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Bonding配置在不同的Linux发行版之间差异很大。请参考你所使用的Linux发行版的相关文档以帮助配置bonding。
- en: 'The following table demonstrates the use of two bonds instead of two individual
    interfaces:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格演示了使用两个bond而非两个单独接口的情况：
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **服务/功能** | **目的** | **接口** | **VLAN** |'
- en: '| SSH | Host management | bond0 | 10 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SSH | 主机管理 | bond0 | 10 |'
- en: '| APIs | Access to OpenStack APIs | bond0 | 15 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| APIs | 访问OpenStack APIs | bond0 | 15 |'
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | bond1 | 20 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 覆盖网络 | 用于在主机之间隧道覆盖（VXLAN, GRE, GENEVE）流量 | bond1 | 20 |'
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | bond1 | Multiple |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 客户机/外部网络 | 用于提供对外部云资源的访问以及基于VLAN的项目网络 | bond1 | 多个 |'
- en: 'In this book, an environment will be built using three non-bonded interfaces:
    one for management and API traffic, one for VLAN-based provider or project networks,
    and another for overlay network traffic. The following interfaces and VLAN IDs
    will be used:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中将构建一个使用三种非bonded接口的环境：一个用于管理和API流量，一个用于基于VLAN的提供者或项目网络，另一个用于覆盖网络流量。以下接口和VLAN
    ID将被使用：
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **服务/功能** | **目的** | **接口** | **VLAN** |'
- en: '| SSH and APIs | Host management and access to OpenStack APIs | eth0 / ens160
    | 10 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SSH和API | 主机管理和访问OpenStack APIs | eth0 / ens160 | 10 |'
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | eth1 / ens192 | 20 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 覆盖网络 | 用于在主机之间隧道覆盖（VXLAN, GRE, GENEVE）流量 | eth1 / ens192 | 20 |'
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | eth2 / ens224 | 30,40-43 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 客户机/外部网络 | 用于提供对外部云资源的访问以及基于VLAN的项目网络 | eth2 / ens224 | 30,40-43 |'
- en: When an environment is virtualized in VMware, interface names may differ from
    the standard eth0, eth1, ethX naming convention. The interface names provided
    in the table reflect the interface naming convention seen on controller and compute
    nodes that exist as virtual machines, rather than bare-metal machines.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当环境在VMware中虚拟化时，接口名称可能与标准的eth0、eth1、ethX命名约定有所不同。表中提供的接口名称反映的是虚拟机中存在的控制节点和计算节点的接口命名约定，而不是裸机上的命名。
- en: Separating services across nodes
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨节点分离服务
- en: Like other OpenStack services, cloud operators can split OpenStack Networking
    services across multiple nodes. Small deployments may use a single node to host
    all services, including networking, compute, database, and messaging. Others might
    find benefit in using a dedicated controller node and a dedicated network node
    to handle guest traffic routed through software routers and to offload Neutron
    DHCP and metadata services. The following sections describe a few common service
    deployment models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 像其他OpenStack服务一样，云操作员可以将OpenStack网络服务分布到多个节点。小型部署可能会使用单个节点来托管所有服务，包括网络、计算、数据库和消息服务。其他用户可能会发现使用专用的控制节点和专用的网络节点来处理通过软件路由器路由的客户机流量，并卸载Neutron
    DHCP和元数据服务会更有优势。以下部分描述了一些常见的服务部署模型。
- en: Using a single controller node
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单个控制节点
- en: In an environment consisting of a single controller and one or more compute
    nodes, the controller will likely handle all networking services and other OpenStack
    services while the compute nodes strictly provide compute resources.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在由单个控制节点和一个或多个计算节点组成的环境中，控制节点通常负责处理所有网络服务和其他OpenStack服务，而计算节点严格提供计算资源。
- en: 'The following diagram demonstrates a controller node hosting all OpenStack
    management and networking services where the Neutron layer 3 agent is not utilized.
    Two physical interfaces are used to separate management (control plane) and instance
    (data plane) network traffic:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个控制节点托管所有OpenStack管理和网络服务的情况，其中没有使用Neutron层3代理。使用两个物理接口来分离管理（控制平面）和实例（数据平面）网络流量：
- en: '![](img/f21a4d7e-97ff-43ce-8703-dce66fd92435.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f21a4d7e-97ff-43ce-8703-dce66fd92435.png)'
- en: Figure 1.3
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3
- en: The preceding diagram reflects the use of a single combined controller/network
    node and one or more compute nodes, with Neutron providing only layer 2 connectivity
    between instances and external gateway devices. An external router is needed to
    handle routing between network segments.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示反映了使用单一的合并控制/网络节点和一个或多个计算节点的配置，其中Neutron仅提供实例与外部网关设备之间的第二层连接。需要外部路由器来处理网络段之间的路由。
- en: 'The following diagram demonstrates a controller node hosting all OpenStack
    management and networking services, including the Neutron L3 agent. Three physical
    interfaces are used to provide separate control and data planes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个控制节点承载所有OpenStack管理和网络服务，包括Neutron L3代理。三个物理接口用于提供独立的控制面和数据面：
- en: '![](img/4cc9d21e-6620-49fd-9b02-c8bec95dd0fe.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc9d21e-6620-49fd-9b02-c8bec95dd0fe.png)'
- en: Figure 1.4
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4
- en: The preceding diagram reflects the use of a single combined controller/network
    node and one or more compute nodes in a network configuration that utilizes the
    Neutron L3 agent. Software routers created with Neutron reside on the controller
    node, and handle routing between connected project networks and external provider
    networks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示反映了在使用Neutron L3代理的网络配置中，使用单一的合并控制/网络节点和一个或多个计算节点。使用Neutron创建的软件路由器驻留在控制节点上，处理连接的项目网络和外部提供商网络之间的路由。
- en: Using a dedicated network node
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用专用网络节点
- en: A network node is dedicated to handling most or all the OpenStack networking
    services, including the L3 agent, DHCP agent, metadata agent, and more. The use
    of a dedicated network node provides additional security and resilience, as the
    controller node will be at less risk of network and resource saturation. Some
    Neutron services, such as the L3 and DHCP agents and the Neutron API service,
    can be scaled out across multiple nodes for redundancy and increased performance,
    especially when distributed virtual routers are used.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个网络节点专门处理大部分或所有OpenStack网络服务，包括L3代理、DHCP代理、元数据代理等。使用专用网络节点能提供额外的安全性和韧性，因为控制节点在网络和资源饱和方面的风险较小。一些Neutron服务，如L3和DHCP代理以及Neutron
    API服务，可以在多个节点之间进行扩展，以实现冗余和提高性能，尤其是在使用分布式虚拟路由器时。
- en: 'The following diagram demonstrates a network node hosting all OpenStack networking
    services, including the Neutron L3, DHCP, metadata, and LBaaS agents. The Neutron
    API service, however, remains installed on the controller node. Three physical
    interfaces are used where necessary to provide separate control and data planes:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个网络节点承载所有OpenStack网络服务，包括Neutron L3、DHCP、元数据和LBaaS代理。然而，Neutron API服务仍然安装在控制节点上。必要时，使用三个物理接口来提供独立的控制面和数据面：
- en: '![](img/9788b045-b94f-4a20-90ee-85a5afa1db3d.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9788b045-b94f-4a20-90ee-85a5afa1db3d.png)'
- en: Figure 1.5
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5
- en: 'The environment built out in this book will be composed of five hosts, including
    the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中构建的环境将由五个主机构成，包括以下内容：
- en: A single controller node running all OpenStack network services and the Linux
    bridge network agent
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个控制节点运行所有OpenStack网络服务和Linux桥接网络代理
- en: A single compute node running the Nova compute service and the Linux bridge
    network agent
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个计算节点运行Nova计算服务和Linux桥接网络代理
- en: Two compute nodes running the Nova compute service and the Open vSwitch network
    agent
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个计算节点运行Nova计算服务和Open vSwitch网络代理
- en: A single network node running the Open vSwitch network agent and the L3 agent
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络节点运行Open vSwitch网络代理和L3代理
- en: Not all hosts are required should you choose not to complete the exercises described
    in the upcoming chapters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择不完成接下来章节中描述的练习，则并非所有主机都是必需的。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: OpenStack Networking offers the ability to create and manage different technologies
    found in a data center in a virtualized and programmable manner. If the built-in
    features and reference implementations are not enough, the pluggable architecture
    of OpenStack Networking allows for additional functionality to be provided by
    third-party commercial and open source vendors. The security requirements of the
    organization building the cloud as well as the use cases of the cloud, will ultimately
    dictate the physical layout and separation of services across the infrastructure
    nodes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Networking提供了以虚拟化和可编程的方式创建和管理数据中心中不同技术的能力。如果内建的功能和参考实现不够，OpenStack
    Networking的可插拔架构允许第三方商业和开源供应商提供额外的功能。构建云的组织的安全要求以及云的使用案例，最终将决定基础设施节点间服务的物理布局和分离。
- en: To successfully deploy Neutron and harness all it has to offer, it is important
    to have a strong understanding of core networking concepts. In this book, we will
    cover some fundamental network concepts around Neutron and build a foundation
    for deploying instances.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功部署Neutron并充分利用它的所有功能，理解核心网络概念非常重要。在本书中，我们将介绍一些与Neutron相关的基础网络概念，并为部署实例打下基础。
- en: In the next chapter, we will begin a package-based installation of OpenStack
    on the Ubuntu 16.04 LTS operating system. Topics covered include the installation,
    configuration, and verification of many core OpenStack projects, including Identity,
    Image, Dashboard, and Compute. The installation and configuration of base OpenStack
    Networking services, including the Neutron API, can be found in *[Chapter 3](bf508e37-ce8a-4116-89db-e8f8a6abf0f4.xhtml),*
    *Installing Neutron*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始在Ubuntu 16.04 LTS操作系统上进行基于软件包的OpenStack安装。涉及的内容包括多个核心OpenStack项目的安装、配置和验证，其中包括身份认证、镜像、仪表盘和计算服务。基础OpenStack网络服务的安装与配置，包括Neutron
    API，可以在*[第3章](bf508e37-ce8a-4116-89db-e8f8a6abf0f4.xhtml)，* *安装Neutron*中找到。
