- en: Introduction to OpenStack Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today's data centers, networks are composed of more devices than ever before.
    Servers, switches, routers, storage systems, and security appliances that once
    consumed rows and rows of data center space now exist as virtual machines and
    virtual network appliances. These devices place a large strain on traditional
    network management systems, as they are unable to provide a scalable and automated
    approach to managing next-generation networks. Users now expect more control and
    flexibility of the infrastructure with quicker provisioning, all of which OpenStack
    promises to deliver.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will introduce many features that OpenStack Networking provides,
    as well as various network architectures supported by OpenStack. Some topics that
    will be covered include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Features of OpenStack Networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical infrastructure requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service separation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is OpenStack Networking?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking is a pluggable, scalable, and API-driven system to manage
    networks in an OpenStack-based cloud. Like other core OpenStack components, OpenStack
    Networking can be used by administrators and users to increase the value and maximize
    the utilization of existing data center resources.
  prefs: []
  type: TYPE_NORMAL
- en: Neutron, the project name for the OpenStack Networking service, complements
    other core OpenStack services such as Compute (Nova), Image (Glance), Identity
    (Keystone), Block (Cinder), Object (Swift), and Dashboard (Horizon) to provide
    a complete cloud solution.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack Networking exposes an application programmable interface (API) to
    users and passes requests to the configured network plugins for additional processing.
    Users are able to define network connectivity in the cloud, and cloud operators
    are allowed to leverage different networking technologies to enhance and power
    the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenStack Networking services can be split between multiple hosts to provide
    resiliency and redundancy, or they can be configured to operate on a single node.
    Like many other OpenStack services, Neutron requires access to a database for
    persistent storage of the network configuration. A simplified example of the architecture
    can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0e53890-f596-464d-89e3-686ed1c77c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1
  prefs: []
  type: TYPE_NORMAL
- en: In figure 1.1, the Neutron server connects to a database where the logical network
    configuration persists. The Neutron server can take API requests from users and
    services and communicate with agents via a message queue. In a typical environment,
    network agents will be scattered across controller and compute nodes and perform
    duties on their respective node.
  prefs: []
  type: TYPE_NORMAL
- en: Features of OpenStack Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking includes many technologies you would find in the data center,
    including switching, routing, load balancing, firewalling, and virtual private
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: These features can be configured to leverage open source or commercial software
    and provide a cloud operator with all the tools necessary to build a functional
    and self-contained cloud networking stack. OpenStack Networking also provides
    a framework for third-party vendors to build on and enhance the capabilities of
    the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Switching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **virtual switch** is defined as a software application or service that connects
    virtual machines to virtual networks at the data link layer of the OSI model,
    also known as layer 2\. Neutron supports multiple virtual switching platforms,
    including Linux bridges provided by the `bridge` kernel module and Open vSwitch.
    Open vSwitch, also known as OVS, is an open source virtual switch that supports
    standard management interfaces and protocols, including NetFlow, SPAN, RSPAN,
    LACP, and 802.1q VLAN tagging. However, many of these features are not exposed
    to the user through the OpenStack API. In addition to VLAN tagging, users can
    build overlay networks in software using L2-in-L3 tunneling protocols, such as
    GRE or VXLAN. Virtual switches can be used to facilitate communication between
    instances and devices outside the control of OpenStack, which include hardware
    switches, network firewalls, storage devices, bare-metal servers, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Additional information on the use of Linux bridges and Open vSwitch as switching
    platforms for OpenStack can be found in *[Chapter 4](05786c3c-b24e-40dc-82a7-ed6072eca14f.xhtml),*
    *Virtual Network Infrastructure Using Linux Bridges,* and *[Chapter 5](0763a131-4ab9-4b3e-8854-8646feae7937.xhtml),*
    *Building a Virtual Switching Infrastructure Using Open vSwitch,* respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking provides routing and NAT capabilities through the use of
    IP forwarding, iptables, and network namespaces. Each network namespace has its
    own routing table, interfaces, and iptables processes that provide filtering and
    network address translation. By leveraging network namespaces to separate networks,
    there is no need to worry about overlapping subnets between networks created by
    users. Configuring a router within Neutron enables instances to interact and communicate
    with outside networks or other networks in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: More information on routing within OpenStack can be found in *[Chapter 10](371886b8-4c2a-49e9-90b8-8fe79217adb4.xhtml),
    Creating Standalone Routers with Neutron*, *[Chapter 11](bb8e64d5-76d5-4be8-b6b3-8ee9a520a439.xhtml),*
    *Router Redundancy Using VRRP*, and *[Chapter 12](b441728b-4377-43cf-b675-166266fef6c9.xhtml),
    Distributed Virtual Routers*.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First introduced in the Grizzly release of OpenStack, **Load Balancing as a
    Service (****LBaaS v2)** provides users with the ability to distribute client
    requests across multiple instances or servers. Users can create monitors, set
    connection limits, and apply persistence profiles to traffic traversing a virtual
    load balancer. OpenStack Networking is equipped with a plugin for LBaaS v2 that
    utilizes HAProxy in the open source reference implementation, but plugins are
    available that manage virtual and physical load-balancing appliances from third-party
    network vendors.
  prefs: []
  type: TYPE_NORMAL
- en: More information on the use of load balancers within Neutron can be found in
    *[Chapter 13](71145e1b-825c-43ac-9993-24eceb7a0a26.xhtml),* *Load Balancing Traffic
    to Instances*.
  prefs: []
  type: TYPE_NORMAL
- en: Firewalling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenStack Networking provides two API-driven methods of securing network traffic
    to instances: security groups and **Firewall as a Service (FWaaS)**. Security
    groups find their roots in nova-network, the original networking stack for OpenStack
    built in to the Compute service, and are based on Amazon''s EC2 security groups.
    When using security groups in OpenStack, instances are placed into groups that
    share common functionality and rule sets. In a reference implementation, security
    group rules are implemented at the instance port level using drivers that leverage
    iptables or OpenFlow. Security policies built using FWaaS are also implemented
    at the port level, but can be applied to ports of routers as well as instances.
    The original FWaaS v1 API implemented firewall rules inside Neutron router namespaces,
    but that behavior has been removed in the v2 API.'
  prefs: []
  type: TYPE_NORMAL
- en: More information on securing instance traffic can be found in *[Chapter 8](240902fd-5108-446e-afa5-8122de12f0af.xhtml),
    Managing Security Groups*. The use of FWaaS is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual private networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **virtual private network (VPN)** extends a private network across a public
    network such as the internet. A VPN enables a computer to send and receive data
    across public networks as if it were directly connected to the private network.
    Neutron provides a set of APIs to allow users to create IPSec-based VPN tunnels
    from Neutron routers to remote gateways when using the open source reference implementation.
    The use of VPN as a Service is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Network functions virtualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Network functions virtualization (****NFV)** is a network architecture concept
    that proposes virtualizing network appliances used for various network functions.
    These functions include intrusion detection, caching, gateways, WAN accelerators,
    firewalls, and more. Using SR-IOV, instances are no longer required to use para-virtualized
    drivers or to be connected to virtual bridges within the host. Instead, the instance
    is attached to a Neutron port that is associated with a **virtual function** (**VF**)
    in the NIC, allowing the instance to access the NIC hardware directly. Configuring
    and implementing SR-IOV with Neutron is outside the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack Networking resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack gives users the ability to create and configure networks and subnets
    and instruct other services, such as Compute, to attach virtual devices to ports
    on these networks. The Identity service gives cloud operators the ability to segregate
    users into projects. OpenStack Networking supports project-owned resources, including
    each project having multiple private networks and routers. Projects can be left
    to choose their own IP addressing scheme, even if those addresses overlap with
    other project networks, or administrators can place limits on the size of subnets
    and addresses available for allocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of networks that can be expressed in OpenStack:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Project/tenant network**: A virtual network created by a project or administrator
    on behalf of a project. The physical details of the network are not exposed to
    the project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provider network**: A virtual network created to map to a physical network.
    Provider networks are typically created to enable access to physical network resources
    outside of the cloud, such as network gateways and other services, and usually
    map to VLANs. Projects can be given access to provider networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The terms *project* and *tenant* are used interchangeably within the OpenStack
    community, with the former being the newer and preferred nomenclature.
  prefs: []
  type: TYPE_NORMAL
- en: A **project network** provides connectivity to resources in a project. Users
    can create, modify, and delete project networks. Each project network is isolated
    from other project networks by a boundary such as a VLAN or other segmentation
    ID. A **provider network**, on the other hand, provides connectivity to networks
    outside of the cloud and is typically created and managed by a cloud administrator.
  prefs: []
  type: TYPE_NORMAL
- en: The primary differences between project and provider networks can be seen during
    the network provisioning process. Provider networks are created by administrators
    on behalf of projects and can be dedicated to a particular project, shared by
    a subset of projects, or shared by all projects. Project networks are created
    by projects for use by their instances and cannot be shared with all projects,
    though sharing with certain projects may be accomplished using role-based access
    control (**RBAC**) policies. When a provider network is created, the administrator
    can provide specific details that aren't available to ordinary users, including
    the network type, the physical network interface, and the network segmentation
    identifier, such as a VLAN ID or VXLAN VNI. Project networks have these same attributes,
    but users cannot specify them. Instead, they are automatically determined by Neutron.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other foundational network resources that will be covered in further
    detail later in this book, but are summarized in the following table for your
    convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Resource** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Subnet | A block of IP addresses used to allocate ports created on the network.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Port | A connection point for attaching a single device, such as the virtual
    network interface card (vNIC) of a virtual instance, to a virtual network. Port
    attributes include the MAC address and the fixed IP address on the subnet. |'
  prefs: []
  type: TYPE_TB
- en: '| Router | A virtual device that provides routing between self-service networks
    and provider networks. |'
  prefs: []
  type: TYPE_TB
- en: '| Security group | A set of virtual firewall rules that control ingress and
    egress traffic at the port level. |'
  prefs: []
  type: TYPE_TB
- en: '| DHCP | An agent that manages IP addresses for instances on provider and self-service
    networks. |'
  prefs: []
  type: TYPE_TB
- en: '| Metadata | A service that provides data to instances during boot. |'
  prefs: []
  type: TYPE_TB
- en: Virtual network interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack deployments are most often configured to use the libvirt KVM/QEMU
    driver to provide platform virtualization. When an instance is booted for the
    first time, OpenStack creates a port for each network interface attached to the
    instance. A virtual network interface called a **tap interface** is created on
    the compute node hosting the instance. The tap interface corresponds directly
    to a network interface within the guest instance and has the properties of the
    port created in Neutron, including the MAC and IP address. Through the use of
    a bridge, the host can expose the guest instance to the physical network. Neutron
    allows users to specify alternatives to the standard tap interface, such as Macvtap
    and SR-IOV, by defining special attributes on ports and attaching them to instances.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network switches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking supports many types of virtual and physical switches, and
    includes built-in support for Linux bridges and Open vSwitch virtual switches.
    This book will cover both technologies and their respective drivers and agents.
  prefs: []
  type: TYPE_NORMAL
- en: The terms *bridge* and *switch* are often used interchangeably in the context
    of OpenStack Networking, and may be used in the same way throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neutron supports overlay networking technologies that provide network isolation
    at scale with little to no modification of the underlying physical infrastructure.
    To accomplish this, Neutron leverages L2-in-L3 overlay networking technologies
    such as GRE, VXLAN, and GENEVE. When configured accordingly, Neutron builds point-to-point
    tunnels between all network and compute nodes in the cloud using a predefined
    interface. These point-to-point tunnels create what is called a **mesh network**,
    where every host is connected to every other host. A cloud consisting of one combined
    controller and network node, and three compute nodes, would have a fully meshed
    overlay network that resembles figure 1.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7047532-54da-4781-b1b8-9f7dab00435c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2
  prefs: []
  type: TYPE_NORMAL
- en: Using the overlay network pictured in figure 1.2, traffic between instances
    or other virtual devices on any given host will travel between layer 3 endpoints
    on each of the underlying hosts without regard for the layer 2 network beneath
    them. Due to encapsulation, Neutron routers may be needed to facilitate communication
    between different project networks as well as networks outside of the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Extensible Local Area Network (VXLAN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book focuses primarily on VXLAN, an overlay technology that helps address
    scalability issues with VLANs. VXLAN encapsulates layer 2 Ethernet frames inside
    layer 4 UDP packets that can be forwarded or routed between hosts. This means
    that a virtual network can be transparently extended across a large network without
    any changes to the end hosts. In the case of OpenStack Networking, however, a
    VXLAN mesh network is commonly constructed only between nodes that exist in the
    same cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than use VLAN IDs to differentiate between networks, VXLAN uses a VXLAN
    Network Identifier (VNI) to serve as the unique identifier on a link that potentially
    carries traffic for tens of thousands of networks, or more. An 802.1q VLAN header
    supports up to 4,096 unique IDs, whereas a VXLAN header supports approximately
    16 million unique IDs. Within an OpenStack cloud, virtual machine instances are
    unaware that VXLAN is used to forward traffic between hosts. The VXLAN Tunnel
    Endpoint (VTEP) on the physical node handles the encapsulation and decapsulation
    of traffic without the instance ever knowing.
  prefs: []
  type: TYPE_NORMAL
- en: Because VXLAN network traffic is encapsulated, many network devices cannot participate
    in these networks without additional configuration, if at all. As a result, VXLAN
    networks are effectively isolated from other networks in the cloud and require
    the use of a Neutron router to provide access to connected instances. More information
    on creating Neutron routers begins in *[Chapter 10](371886b8-4c2a-49e9-90b8-8fe79217adb4.xhtml),
    Creating Standalone Routers with Neutron*.
  prefs: []
  type: TYPE_NORMAL
- en: While not as performant as VLAN or flat networks on some hardware, the use of
    VXLAN is becoming more popular in cloud network architectures where scalability
    and self-service are major drivers. Newer networking hardware that offers VXLAN
    offloading capabilities should be leveraged if you are considering implementing
    VXLAN-based overlay networks in your cloud.
  prefs: []
  type: TYPE_NORMAL
- en: More information on how VXLAN encapsulation works is described in RFC 7348,
    available at the following URL: [https://tools.ietf.org/html/rfc7348](https://tools.ietf.org/html/rfc7348)
  prefs: []
  type: TYPE_NORMAL
- en: Generic Router Encapsulation (GRE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **GRE network** is similar to a VXLAN network in that traffic from one instance
    to another is encapsulated and sent over a layer 3 network. A unique segmentation
    ID is used to differentiate traffic from other GRE networks. Rather than use UDP
    as the transport mechanism, GRE uses IP protocol 47\. For various reasons, the
    use of GRE for encapsulating tenant network traffic has fallen out of favor now
    that VXLAN is supported by both Open vSwitch and Linux Bridge network agents.
  prefs: []
  type: TYPE_NORMAL
- en: More information on how GRE encapsulation works is described in RFC 2784 available
    at the following URL: [https://tools.ietf.org/html/rfc278](https://tools.ietf.org/html/rfc278)[4](https://tools.ietf.org/html/rfc278)
  prefs: []
  type: TYPE_NORMAL
- en: As of the Pike release of OpenStack, the Open vSwitch mechanism driver is the
    only commonly used driver that supports GRE.
  prefs: []
  type: TYPE_NORMAL
- en: Generic Network Virtualization Encapsulation (GENEVE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GENEVE is an emerging overlay technology that resembles VXLAN and GRE, in that
    packets between hosts are designed to be transmitted using standard networking
    equipment without having to modify the client or host applications. Like VXLAN,
    GENEVE encapsulates packets with a unique header and uses UDP as its transport
    mechanism. GENEVE leverages the benefits of multiple overlay technologies such
    as VXLAN, NVGRE, and STT, and may supplant those technologies over time. The Open
    Virtual Networking (OVN) mechanism driver relies on GENEVE as its overlay technology,
    which may speed up the adoption of GENEVE in later releases of OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the physical infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most OpenStack clouds are made up of physical infrastructure nodes that fit
    into one of the following four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Controller node**: Controller nodes traditionally run the API services for
    all of the OpenStack components, including Glance, Nova, Keystone, Neutron, and
    more. In addition, controller nodes run the database and messaging servers, and
    are often the point of management of the cloud via the Horizon dashboard. Most
    OpenStack API services can be installed on multiple controller nodes and can be
    load balanced to scale the OpenStack control plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network node**: Network nodes traditionally run DHCP and metadata services
    and can also host virtual routers when the Neutron L3 agent is installed. In smaller
    environments, it is not uncommon to see controller and network node services collapsed
    onto the same server or set of servers. As the cloud grows in size, most network
    services can be broken out between other servers or installed on their own server
    for optimal performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute node**: Compute nodes traditionally run a hypervisor such as KVM,
    Hyper-V, or Xen, or container software such as LXC or Docker. In some cases, a
    compute node may also host virtual routers, especially when Distributed Virtual
    Routing (DVR) is configured. In proof-of-concept or test environments, it is not
    uncommon to see controller, network, and compute node services collapsed onto
    the same machine. This is especially common when using DevStack, a software package
    designed for developing and testing OpenStack code. All-in-one installations are
    not recommended for production use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage node**: Storage nodes are traditionally limited to running software
    related to storage such as Cinder, Ceph, or Swift. Storage nodes do not usually
    host any type of Neutron networking service or agent and will not be discussed
    in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When Neutron services are broken out between many hosts, the layout of services
    will often resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/468a9df6-164e-4bd6-bd7e-7b5bb65b9f78.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3
  prefs: []
  type: TYPE_NORMAL
- en: In figure 1.3, the neutron API service `neutron-server` is installed on the
    Controller node, while Neutron agents responsible for implementing certain virtual
    networking resources are installed on a dedicated network node. Each compute node
    hosts a network plugin agent responsible for implementing the network plumbing
    on that host. Neutron supports a highly available API service with a shared database
    backend, and it is recommended that the cloud operator load balances traffic to
    the Neutron API service when possible. Multiple DHCP, metadata, L3, and LBaaS
    agents should be implemented on separate network nodes whenever possible. Virtual
    networks, routers, and load balancers can be scheduled to one or more agents to
    provide a basic level of redundancy when an agent fails. Neutron even includes
    a built-in scheduler that can detect failure and reschedule certain resources
    when a failure is detected.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the physical infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the installation of OpenStack can begin, the physical network infrastructure
    must be configured to support the networks needed for an operational cloud. In
    a production environment, this will likely include a dedicated management VLAN
    used for server management and API traffic, a VLAN dedicated to overlay network
    traffic, and one or more VLANs that will be used for provider and VLAN-based project
    networks. Each of these networks can be configured on separate interfaces, or
    they can be collapsed onto a single interface if desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reference architecture for OpenStack Networking defines at least four distinct
    types of traffic that will be seen on the network:'
  prefs: []
  type: TYPE_NORMAL
- en: Management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These traffic types are often categorized as control plane or data plane, depending
    on the purpose, and are terms used in networking to describe the purpose of the
    traffic. In this case, **control plane** traffic is used to describe traffic related
    to management, API, and other non-VM related traffic. **Data plane** traffic,
    on the other hand, represents traffic generated by, or directed to, virtual machine
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Although I have taken the liberty of splitting out the network traffic onto
    dedicated interfaces in this book, it is not necessary to do so to create an operational
    OpenStack cloud. In fact, many administrators and distributions choose to collapse
    multiple traffic types onto single or bonded interfaces using VLAN tagging. Depending
    on the chosen deployment model, the administrator may spread networking services
    across multiple nodes or collapse them onto a single node. The security requirements
    of the enterprise deploying the cloud will often dictate how the cloud is built.
    The various network and service configurations will be discussed in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Management network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **management network**, also referred to as the **internal network** in
    some distributions, is used for internal communication between hosts for services
    such as the messaging service and database service, and can be considered as part
    of the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: All hosts will communicate with each other over this network. In many cases,
    this same interface may be used to facilitate image transfers between hosts or
    some other bandwidth-intensive traffic. The management network can be configured
    as an isolated network on a dedicated interface or combined with another network
    as described in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: API network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **API network** is used to expose OpenStack APIs to users of the cloud and
    services within the cloud and can be considered as part of the control plane.
    Endpoint addresses for API services such as Keystone, Neutron, Glance, and Horizon
    are procured from the API network.
  prefs: []
  type: TYPE_NORMAL
- en: It is common practice to utilize a single interface and IP address for API endpoints
    and management access to the host itself over SSH. A diagram of this configuration
    is provided later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended, though not required, that you physically separate management
    and API traffic from other traffic types, such as storage traffic, to avoid issues
    with network congestion that may affect operational stability.
  prefs: []
  type: TYPE_NORMAL
- en: External network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **external network** is a provider network that provides Neutron routers
    with external network access. Once a router has been configured and attached to
    the external network, the network becomes the source of floating IP addresses
    for instances and other network resources attached to the router. IP addresses
    in an external network are expected to be routable and reachable by clients on
    a corporate network or the internet. Multiple external provider networks can be
    segmented using VLANs and trunked to the same physical interface. Neutron is responsible
    for tagging the VLAN based on the network configuration provided by the administrator.
    Since external networks are utilized by VMs, they can be considered as part of
    the data plane.
  prefs: []
  type: TYPE_NORMAL
- en: Guest network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **guest network** is a network dedicated to instance traffic. Options for
    guest networks include local networks restricted to a particular node, flat, or
    VLAN-tagged networks, or virtual overlay networks made possible with GRE, VXLAN,
    or GENEVE encapsulation. For more information on guest networks, refer to *[Chapter
    6](5a3df5cf-aebb-4c57-9f48-fa5419a5b2ae.xhtml), Building Networks with Neutron*.
    Since guest networks provide connectivity to VMs, they can be considered part
    of the data plane.
  prefs: []
  type: TYPE_NORMAL
- en: The physical interfaces used for external and guest networks can be dedicated
    interfaces or ones that are shared with other types of traffic. Each approach
    has its benefits and drawbacks, and they are described in more detail later in
    this chapter. In the next few chapters, I will define networks and VLANs that
    will be used throughout the book to demonstrate the various components of OpenStack
    Networking. Generic information on the configuration of switch ports, routers,
    or firewalls will also be provided.
  prefs: []
  type: TYPE_NORMAL
- en: Physical server connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of interfaces needed per host is dependent on the purpose of the
    cloud, the security and performance requirements of the organization, and the
    cost and availability of hardware. A single interface per server that results
    in a combined control and data plane is all that is needed for a fully operational
    OpenStack cloud. Many organizations choose to deploy their cloud this way, especially
    when port density is at a premium, the environment is simply used for testing,
    or network failure at the node level is a non-impacting event. When possible,
    however, it is recommended that you split control and data traffic across multiple
    interfaces to reduce the chances of network failure.
  prefs: []
  type: TYPE_NORMAL
- en: Single interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For hosts using a single interface, all traffic to and from instances as well
    as internal OpenStack, SSH management, and API traffic traverse the same physical
    interface. This configuration can result in severe performance penalties, as a
    service or guest can potentially consume all available bandwidth. A single interface
    is recommended only for non-production clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table demonstrates the networks and services traversing a single
    interface over multiple VLANs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  prefs: []
  type: TYPE_TB
- en: '| SSH | Host management | eth0 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| APIs | Access to OpenStack APIs | eth0 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | eth0 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | eth0 | Multiple |'
  prefs: []
  type: TYPE_TB
- en: Multiple interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To reduce the likelihood of guest traffic impacting management traffic, segregation
    of traffic between multiple physical interfaces is recommended. At a minimum,
    two interfaces should be used: one that serves as a dedicated interface for management
    and API traffic (control plane), and another that serves as a dedicated interface
    for external and guest traffic (data plane). Additional interfaces can be used
    to further segregate traffic, such as storage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table demonstrates the networks and services traversing two interfaces
    with multiple VLANs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  prefs: []
  type: TYPE_TB
- en: '| SSH | Host management | eth0 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| APIs | Access to OpenStack APIs | eth0 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | eth1 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | eth1 | Multiple |'
  prefs: []
  type: TYPE_TB
- en: Bonding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The use of multiple interfaces can be expanded to utilize bonds instead of
    individual network interfaces. The following common bond modes are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mode 1 (active-backup)**: Mode 1 bonding sets all interfaces in the bond
    to a backup state while one interface remains active. When the active interface
    fails, a backup interface replaces it. The same MAC address is used upon failover
    to avoid issues with the physical network switch. Mode 1 bonding is supported
    by most switching vendors, as it does not require any special configuration on
    the switch to implement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mode 4 (active-active)**: Mode 4 bonding involves the use of **aggregation
    groups**, a group in which all interfaces share an identical configuration and
    are grouped together to form a single logical interface. The interfaces are aggregated
    using the IEEE 802.3ad Link Aggregation Control Protocol (LACP). Traffic is load
    balanced across the links using methods negotiated by the physical node and the
    connected switch or switches. The physical switching infrastructure *must* be
    capable of supporting this type of bond. While some switching platforms require
    that multiple links of an LACP bond be connected to the same switch, others support
    technology known as **Multi-Chassis Link Aggregation (MLAG)** that allows multiple
    physical switches to be configured as a single logical switch. This allows links
    of a bond to be connected to multiple switches that provide hardware redundancy
    while allowing users the full bandwidth of the bond under normal operating conditions,
    all with no additional changes to the server configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bonding can be configured within the Linux operating system using tools such
    as iproute2, ifupdown, and Open vSwitch, among others.The configuration of bonded
    interfaces is outside the scope of OpenStack and this book.
  prefs: []
  type: TYPE_NORMAL
- en: Bonding configurations vary greatly between Linux distributions. Refer to the
    respective documentation of your Linux distribution for assistance in configuring
    bonding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table demonstrates the use of two bonds instead of two individual
    interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  prefs: []
  type: TYPE_TB
- en: '| SSH | Host management | bond0 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| APIs | Access to OpenStack APIs | bond0 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | bond1 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | bond1 | Multiple |'
  prefs: []
  type: TYPE_TB
- en: 'In this book, an environment will be built using three non-bonded interfaces:
    one for management and API traffic, one for VLAN-based provider or project networks,
    and another for overlay network traffic. The following interfaces and VLAN IDs
    will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service/function** | **Purpose** | **Interface** | **VLAN** |'
  prefs: []
  type: TYPE_TB
- en: '| SSH and APIs | Host management and access to OpenStack APIs | eth0 / ens160
    | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay network | Used to tunnel overlay (VXLAN, GRE, GENEVE) traffic between
    hosts | eth1 / ens192 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| Guest/external network(s) | Used to provide access to external cloud resources
    and for VLAN-based project networks | eth2 / ens224 | 30,40-43 |'
  prefs: []
  type: TYPE_TB
- en: When an environment is virtualized in VMware, interface names may differ from
    the standard eth0, eth1, ethX naming convention. The interface names provided
    in the table reflect the interface naming convention seen on controller and compute
    nodes that exist as virtual machines, rather than bare-metal machines.
  prefs: []
  type: TYPE_NORMAL
- en: Separating services across nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like other OpenStack services, cloud operators can split OpenStack Networking
    services across multiple nodes. Small deployments may use a single node to host
    all services, including networking, compute, database, and messaging. Others might
    find benefit in using a dedicated controller node and a dedicated network node
    to handle guest traffic routed through software routers and to offload Neutron
    DHCP and metadata services. The following sections describe a few common service
    deployment models.
  prefs: []
  type: TYPE_NORMAL
- en: Using a single controller node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an environment consisting of a single controller and one or more compute
    nodes, the controller will likely handle all networking services and other OpenStack
    services while the compute nodes strictly provide compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates a controller node hosting all OpenStack
    management and networking services where the Neutron layer 3 agent is not utilized.
    Two physical interfaces are used to separate management (control plane) and instance
    (data plane) network traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f21a4d7e-97ff-43ce-8703-dce66fd92435.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram reflects the use of a single combined controller/network
    node and one or more compute nodes, with Neutron providing only layer 2 connectivity
    between instances and external gateway devices. An external router is needed to
    handle routing between network segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates a controller node hosting all OpenStack
    management and networking services, including the Neutron L3 agent. Three physical
    interfaces are used to provide separate control and data planes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cc9d21e-6620-49fd-9b02-c8bec95dd0fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram reflects the use of a single combined controller/network
    node and one or more compute nodes in a network configuration that utilizes the
    Neutron L3 agent. Software routers created with Neutron reside on the controller
    node, and handle routing between connected project networks and external provider
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Using a dedicated network node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A network node is dedicated to handling most or all the OpenStack networking
    services, including the L3 agent, DHCP agent, metadata agent, and more. The use
    of a dedicated network node provides additional security and resilience, as the
    controller node will be at less risk of network and resource saturation. Some
    Neutron services, such as the L3 and DHCP agents and the Neutron API service,
    can be scaled out across multiple nodes for redundancy and increased performance,
    especially when distributed virtual routers are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates a network node hosting all OpenStack networking
    services, including the Neutron L3, DHCP, metadata, and LBaaS agents. The Neutron
    API service, however, remains installed on the controller node. Three physical
    interfaces are used where necessary to provide separate control and data planes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9788b045-b94f-4a20-90ee-85a5afa1db3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment built out in this book will be composed of five hosts, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A single controller node running all OpenStack network services and the Linux
    bridge network agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single compute node running the Nova compute service and the Linux bridge
    network agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two compute nodes running the Nova compute service and the Open vSwitch network
    agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single network node running the Open vSwitch network agent and the L3 agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all hosts are required should you choose not to complete the exercises described
    in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking offers the ability to create and manage different technologies
    found in a data center in a virtualized and programmable manner. If the built-in
    features and reference implementations are not enough, the pluggable architecture
    of OpenStack Networking allows for additional functionality to be provided by
    third-party commercial and open source vendors. The security requirements of the
    organization building the cloud as well as the use cases of the cloud, will ultimately
    dictate the physical layout and separation of services across the infrastructure
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: To successfully deploy Neutron and harness all it has to offer, it is important
    to have a strong understanding of core networking concepts. In this book, we will
    cover some fundamental network concepts around Neutron and build a foundation
    for deploying instances.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin a package-based installation of OpenStack
    on the Ubuntu 16.04 LTS operating system. Topics covered include the installation,
    configuration, and verification of many core OpenStack projects, including Identity,
    Image, Dashboard, and Compute. The installation and configuration of base OpenStack
    Networking services, including the Neutron API, can be found in *[Chapter 3](bf508e37-ce8a-4116-89db-e8f8a6abf0f4.xhtml),*
    *Installing Neutron*.
  prefs: []
  type: TYPE_NORMAL
