- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Containerize with AWS – Building Solutions with AWS EKS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS容器化——使用AWS EKS构建解决方案
- en: In the previous chapter, we built and automated our solution on AWS while utilizing
    **Elastic Cloud Compute** (**EC2**). We built VM images with Packer and provisioned
    our VMs using Terraform. In this chapter, we’ll follow a similar path, but instead
    of working with VMs, we’ll look at hosting our application in containers within
    a Kubernetes cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们在AWS上构建并自动化了我们的解决方案，同时使用了**弹性云计算**（**EC2**）。我们使用Packer构建了虚拟机镜像，并通过Terraform对虚拟机进行配置。在本章中，我们将沿着类似的路径前进，但这次，我们不再使用虚拟机，而是将把我们的应用程序托管在Kubernetes集群中的容器里。
- en: 'To achieve this, we’ll need to alter our approach by ditching Packer and replacing
    it with Docker to create a deployable artifact for our application. Once again,
    we will be using the `aws` provider for Terraform, but this time, we’ll be introducing
    something new: the `kubernetes` provider for Terraform, which will provision to
    the Kubernetes cluster after our AWS infrastructure has been provisioned using
    the `aws` provider for Terraform.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们需要改变策略，放弃Packer，改用Docker来创建我们应用程序的可部署制品。我们仍将使用`aws`提供程序来运行Terraform，但这次，我们将引入一个新的内容：`kubernetes`提供程序，它将在使用`aws`提供程序为Terraform配置完AWS基础设施后，将资源配置到Kubernetes集群中。
- en: Again, with this approach, we will only focus on the new and different. I’ll
    call out where we are building on previous chapters and when something is legitimately
    new.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，在这种方法中，我们将仅关注新的和不同的部分。我会指出我们在哪些部分是在之前章节的基础上构建的，以及哪些内容是完全新的。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及以下主题：
- en: Laying the foundation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打下基础
- en: Designing the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Building the solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: Automating the deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化部署
- en: Laying the foundation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打下基础
- en: Our story continues through the lens of Söze Enterprises, founded by the enigmatic
    Turkish billionaire Keyser Söze. Our team has been hard at work building the next-generation
    autonomous vehicle orchestration platform. Previously, we had hoped to leapfrog
    the competition by leveraging Amazon’s rock-solid platform, leveraging our team’s
    existing skills, and focusing on feature development. The team was just getting
    into their groove when a curveball came down from above.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的故事通过Söze Enterprises的视角继续进行，该公司由神秘的土耳其亿万富翁Keyser Söze创办。我们的团队一直在努力构建下一代自动驾驶汽车编排平台。此前，我们希望通过利用亚马逊坚如磐石的平台，借助团队现有的技能，专注于功能开发，从而超越竞争对手。团队刚刚开始步入正轨时，却迎来了一个意外的挑战。
- en: It turns out, over the weekend, our elusive executive was influenced by a rendezvous
    with Andy Jassy, the CEO of AWS, while scuba diving amid the rare and exotic marine
    life off the coast of the Galápagos. Keyser heard about the more efficient resource
    utilization leading to improved cost optimization and faster deployment and rollback
    times, and he was hooked. His new autonomous vehicle platform needed to harness
    the power of the cloud, and container-based architecture was the way to do it.
    So, he decided to accelerate his plans to adopt cloud-native architecture!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，在周末，我们那位难以捉摸的高管受到了与AWS首席执行官Andy Jassy的会面影响，他们在加拉帕戈斯群岛海岸附近潜水时，接触到了稀有而奇特的海洋生物。Keyser听说了更高效的资源利用方式，这带来了更好的成本优化以及更快的部署和回滚时间，他被吸引住了。他的新型自动驾驶平台需要利用云的力量，而基于容器的架构就是实现这一目标的方式。所以，他决定加快采用云原生架构的计划！
- en: The news of transitioning to a container-based architecture means reevaluating
    their approach, diving into new technologies, and possibly even reshuffling team
    dynamics. For the team, containers were always the long-term plan, but now, things
    need to be sped up, which will require a significant investment in time, resources,
    and training.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 转向基于容器的架构的消息意味着需要重新评估他们的方法，深入了解新技术，甚至可能重新调整团队的动态。对于团队来说，容器一直是长期计划，但现在，事情需要加快进度，这将需要大量的时间、资源和培训投资。
- en: As the team scrambles to adjust their plans, they can’t help but feel a mix
    of excitement and apprehension. They know that they are part of something groundbreaking
    under Keyser’s leadership. His vision for the future of autonomous vehicles is
    bold and transformative. And while his methods may be unconventional, they have
    learned that his instincts are often right. In this chapter, we’ll explore this
    transformation from VMs to containers using AWS.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当团队忙于调整计划时，他们不禁感到一种兴奋与不安的交织。他们知道，在Keyser的领导下，他们正在参与一项开创性的工作。他对于自动驾驶未来的愿景大胆而具有变革性。尽管他的做法可能不拘一格，但他们已经学会了他的直觉往往是正确的。在本章中，我们将探讨从VM到容器的转变，使用AWS来实现这一目标。
- en: Designing the solution
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: 'As we saw in the previous chapter, where we built our solution using VMs using
    AWS EC2, we had full control over the operating system configuration through the
    VM images we provisioned with Packer. Now that we will be transitioning to hosting
    our solution on AWS **Elastic Kubernetes Service** (**EKS**), we’ll need to introduce
    a new tool to replace VM images with container images – **Docker**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中所见，我们使用AWS EC2构建了解决方案，并通过Packer配置的VM镜像完全控制操作系统。现在，我们将转向在AWS **弹性Kubernetes服务**（**EKS**）上托管我们的解决方案，我们需要引入一个新工具，将VM镜像替换为容器镜像——**Docker**：
- en: '![Figure 8.1 – Logical architecture for the autonomous vehicle platform](img/B21183_08_1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 自动驾驶平台的逻辑架构](img/B21183_08_1.jpg)'
- en: Figure 8.1 – Logical architecture for the autonomous vehicle platform
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 自动驾驶平台的逻辑架构
- en: 'Our application architecture, comprising a frontend, a backend, and a database,
    will remain the same, but we will need to provision different resources with Terraform
    and harness new tools from Docker and Kubernetes to automate the deployment of
    our solution to this new infrastructure:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用架构，包括前端、后端和数据库，将保持不变，但我们需要使用Terraform来配置不同的资源，并利用Docker和Kubernetes的新工具来自动化将解决方案部署到这一新基础设施中：
- en: '![Figure 8.2 – Source control structure of our repository](img/B21183_08_2.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 我们代码库的源代码控制结构](img/B21183_08_2.jpg)'
- en: Figure 8.2 – Source control structure of our repository
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 我们代码库的源代码控制结构
- en: In this solution, we’ll have seven parts. We still have the application code
    and Dockerfiles (replacing the Packer-based VM images) for both the frontend and
    backend. We still have GitHub Actions to implement our CI/CD process, but now
    we have two Terraform code bases – one for provisioning the underlying infrastructure
    to AWS and another for provisioning our application to the Kubernetes cluster
    hosted on EKS. Then, we have the two code bases for our application’s frontend
    and backend.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个解决方案中，我们将有七个部分。我们依然保留前端和后端的应用代码和Dockerfile（替代基于Packer的VM镜像）。我们仍然使用GitHub
    Actions来实施我们的CI/CD流程，但现在我们有两个Terraform代码库——一个用于配置AWS的基础设施，另一个用于将我们的应用部署到EKS上托管的Kubernetes集群中。然后，我们还有两个代码库，分别用于应用的前端和后端。
- en: Cloud architecture
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云架构
- en: In the previous chapter, our cloud-hosting solution was a set of dedicated VMs.
    In this chapter, our objective is to leverage AWS EKS to use a shared pool of
    VMs that are managed by Kubernetes to host our application. To achieve this, we’ll
    be using some new resources that are geared toward container-based workloads.
    However, much of the networking, load balancing, and other components will largely
    be the same.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们的云托管解决方案是由一组专用VM组成的。在本章中，我们的目标是利用AWS EKS，使用由Kubernetes管理的共享VM池来托管我们的应用。为了实现这一目标，我们将使用一些新的资源，这些资源专为基于容器的工作负载设计。然而，网络、负载均衡及其他组件大部分将保持不变。
- en: Virtual network
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: 'Recalling our work in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with
    EC2 instances and virtual networks, setting up a **virtual private cloud** (**VPC**)
    for AWS EKS follows a similar process. The core network is still there, with all
    the pomp and circumstance, from subnets – both public and private – to all the
    minutia of route tables, internet gateways, and NAT gateways, the virtual network
    we’ll build for our EKS cluster will largely be the same as the one we created
    previously. The only difference will be how we use it:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在[*第7章*](B21183_07.xhtml#_idTextAnchor365)中使用EC2实例和虚拟网络的工作，为AWS EKS设置**虚拟私有云**（**VPC**）遵循类似的流程。核心网络依然存在，依旧充满了繁琐的细节，从子网——包括公有和私有子网——到路由表、互联网网关和NAT网关等一系列细节，我们为EKS集群构建的虚拟网络将与我们之前创建的网络大致相同。唯一的不同是我们如何使用它：
- en: '![Figure 8.3 – AWS virtual network architecture](img/B21183_08_3.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – AWS 虚拟网络架构](img/B21183_08_3.jpg)'
- en: Figure 8.3 – AWS virtual network architecture
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – AWS 虚拟网络架构
- en: Previously, we used the public subnets for our frontend VMs and the private
    subnets for our backend. As we learned in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278),
    when we introduce Kubernetes into the mix, we’ll be transitioning to a shared
    pool of VMs that host our application as pods. These VMs will be hosted in the
    private subnets and a load balancer will be hosted in the public subnets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们为前端 VM 使用了公共子网，而为后端使用了私有子网。正如我们在[*第五章*](B21183_05.xhtml#_idTextAnchor278)中所学到的，当我们将
    Kubernetes 引入时，我们将过渡到一个共享的 VM 池，托管我们作为 Pod 运行的应用程序。这些 VM 将托管在私有子网中，负载均衡器将托管在公共子网中。
- en: Container registry
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器注册表
- en: Building on our exploration of container architecture in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278),
    we know that we need to build container images and we need to store them in a
    container registry. For that purpose, AWS offers **Elastic Container Registry**
    (**ECR**). This is a private container registry, unlike public registries such
    as Docker Hub, which we looked at in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在[*第五章*](B21183_05.xhtml#_idTextAnchor278)中对容器架构的探讨，我们知道我们需要构建容器镜像，并将它们存储在容器注册表中。为此，AWS
    提供了**弹性容器注册表**（**ECR**）。这是一个私有容器注册表，不同于我们在[*第五章*](B21183_05.xhtml#_idTextAnchor278)中提到的公共注册表，例如
    Docker Hub。
- en: 'We’ll need to utilize the Docker command-line utility to build and push images
    to ECR. To be able to do that, we need to grant an identity the necessary permissions.
    As we saw in the previous chapter, when we build VM images using Packer, we’ll
    likely have a GitHub Actions workflow that builds and pushes the container images
    to ECR. The identity that the GitHub Actions workflow executes under will need
    permission to do that. Once these Docker images are in ECR, the final step is
    to grant our cluster access to pull images from the registry:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要利用 Docker 命令行工具来构建和推送镜像到 ECR。为了实现这一点，我们需要授予某个身份必要的权限。正如我们在上一章所看到的，当我们使用
    Packer 构建 VM 镜像时，我们可能会有一个 GitHub Actions 工作流，用于构建并将容器镜像推送到 ECR。GitHub Actions
    工作流执行的身份需要具备这些权限。将这些 Docker 镜像上传到 ECR 后，最后一步是授予我们的集群访问权限，从注册表中拉取镜像：
- en: '![Figure 8.4 – IAM policy giving a group access to push container images to
    ECR](img/B21183_08_4.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – IAM 策略授予一个组推送容器镜像到 ECR 的权限](img/B21183_08_4.jpg)'
- en: Figure 8.4 – IAM policy giving a group access to push container images to ECR
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – IAM 策略授予一个组推送容器镜像到 ECR 的权限
- en: We’ll set up an IAM group that we’ll grant this permission to. This will allow
    us to add the user for the GitHub Action, as well as any other human users who
    want to push images directly from the command line. In AWS, IAM policies are extremely
    flexible; they can be declared independently or inline with the identity they
    are being attached to. This allows us to create reusable policies that can be
    attached to multiple identities. In this case, we’ll define the policy that grants
    access to push images to this ECR and then attach it to the group. Then, membership
    in the group will grant users access to these permissions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置一个 IAM 组，并授予该组相应权限。这样，我们就可以将 GitHub Action 的用户以及任何其他希望直接从命令行推送镜像的用户添加到该组。在
    AWS 中，IAM 策略非常灵活；它们可以独立声明，或者与它们附加的身份内联声明。这使得我们可以创建可重用的策略，并将其附加到多个身份上。在这个案例中，我们将定义一个授予推送镜像到
    ECR 的权限的策略，然后将其附加到该组。接着，加入该组的成员将获得这些权限。
- en: The final step is to grant access to the cluster such that it can pull images
    from our ECR when it schedules pods within the nodes. To do that, we can use a
    built-in AWS policy called `AmazonEC2ContainerRegistryReadOnly`. We’ll need to
    reference it using its fully qualified ARN, which is `arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly`.
    Built-in policies have a common `arn:aws:iam::aws:policy` prefix that identifies
    them as published by AWS and not published by any specific user within their AWS
    account. When we publish our own policies, the fully qualified ARN will include
    our account number.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是授予集群访问权限，以便它在节点内调度 Pod 时可以从我们的 ECR 拉取镜像。为此，我们可以使用 AWS 提供的内建策略 `AmazonEC2ContainerRegistryReadOnly`。我们需要使用其完全限定的
    ARN 来引用它，即 `arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly`。内建策略的
    ARN 前缀通常是 `arn:aws:iam::aws:policy`，这标识它们是由 AWS 发布的，而非某个具体用户在其 AWS 账户内发布的。当我们发布自己的策略时，完全限定的
    ARN 会包含我们的账户号码。
- en: Load balancing
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'Unlike in the previous chapter, where we provisioned and configured our own
    AWS **Application Load Balancer** (**ALB**), when using Amazon EKS, one of the
    advantages is that EKS takes on much of the responsibility of provisioning and
    configuring load balancers. We can direct and influence its actions using Kubernetes
    annotations but this is largely taken care of for us. In our solution, to keep
    things simple, we’ll be using NGINX as our ingress controller and configuring
    it to set up an AWS **Network Load Balancer** (**NLB**) for us:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章中我们配置和管理自己的 AWS **应用负载均衡器**（**ALB**）不同，使用 Amazon EKS 的一个优点是 EKS 承担了大部分配置和管理负载均衡器的责任。我们可以使用
    Kubernetes 注解来引导和影响其操作，但大部分工作已由 EKS 为我们处理。在我们的解决方案中，为了简化流程，我们将使用 NGINX 作为入口控制器，并配置它为我们设置一个
    AWS **网络负载均衡器**（**NLB**）：
- en: '![Figure 8.5 – Elastic load balancer working with an NGINX ingress controller
    to route traffic to our application’s pods](img/B21183_08_5.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – 弹性负载均衡器与 NGINX 入口控制器协同工作，将流量路由到我们应用的 Pod](img/B21183_08_5.jpg)'
- en: Figure 8.5 – Elastic load balancer working with an NGINX ingress controller
    to route traffic to our application’s pods
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – 弹性负载均衡器与 NGINX 入口控制器协同工作，将流量路由到我们应用的 Pod
- en: 'To delegate this responsibility to EKS, we need to grant it the necessary IAM
    permissions to provision and manage these resources. Therefore, we’ll need to
    provision an IAM policy and attach it to the EKS cluster. We can do this using
    an IAM role that has been assigned to the cluster’s node group:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此责任委托给 EKS，我们需要授予其配置和管理这些资源所需的 IAM 权限。因此，我们需要配置一个 IAM 策略并将其附加到 EKS 集群。我们可以使用分配给集群节点组的
    IAM 角色来完成此操作：
- en: '![Figure 8.6 – IAM policy allowing EKS to provision and manage elastic load
    balancers](img/B21183_08_6.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.6 – 允许 EKS 配置和管理弹性负载均衡器的 IAM 策略](img/B21183_08_6.jpg)'
- en: Figure 8.6 – IAM policy allowing EKS to provision and manage elastic load balancers
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – 允许 EKS 配置和管理弹性负载均衡器的 IAM 策略
- en: Then, we provision Kubernetes resources (for example, services and ingress controllers)
    and annotate them to inform the specific configuration of our elastic load balancers
    that we want EKS to enact on our behalf.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为 Kubernetes 资源（例如服务和入口控制器）配置资源，并添加注解，以告知我们希望 EKS 代表我们实施的弹性负载均衡器的具体配置。
- en: Network security
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: There are many ways to host services on Kubernetes and make them accessible
    outside of the cluster. In our solution, we’ll be using an AWS elastic load balancer
    to allow external traffic into our cluster through our NGINX controller. There
    are other options, such as NodePort, which allow you to access a pod directly
    through an exposed port on the node. This would require public access to the cluster’s
    nodes and is not the preferred method from both security and scalability perspectives.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上托管服务并使其可在集群外部访问有很多种方式。在我们的解决方案中，我们将使用 AWS 弹性负载均衡器，通过我们的 NGINX
    控制器将外部流量引入集群。还有其他选项，例如 NodePort，允许你通过节点上暴露的端口直接访问 Pod。这需要集群节点的公共访问，而且从安全性和可扩展性角度来看，这并不是首选方法。
- en: If we want access to the cluster using `kubectl`, then we need to turn on public
    endpoint access. This is useful when you’re developing something small on your
    own but not ideal when you’re working in an enterprise context. You will most
    likely have the private network infrastructure in place so that you never have
    to enable the public endpoint.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望使用`kubectl`访问集群，那么我们需要开启公共端点访问。这在你自己开发一些小项目时很有用，但在企业环境中并不理想。你很可能已经有了私有网络基础设施，因此永远不需要启用公共端点。
- en: Secrets management
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机密管理
- en: Incorporating secrets into pods within an Amazon EKS cluster can be achieved
    through various methods, each with its advantages and disadvantages. As we did
    with VMs in the previous chapter, the method that we will explore is using AWS
    Secrets Manager secrets. Kubernetes has a built-in approach using Kubernetes Secrets.
    This method is straightforward and integrated directly into Kubernetes, but it
    has limitations in terms of security since secrets are encoded in Base64 and can
    be accessed by anyone with cluster access.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将机密信息集成到 Amazon EKS 集群中的 Pod 可以通过多种方式实现，每种方式都有其优缺点。正如我们在上一章中处理虚拟机时所做的那样，我们将探索的方式是使用
    AWS Secrets Manager 中的机密信息。Kubernetes 也有一种内置的方法，使用 Kubernetes Secrets。这种方法简单明了，直接集成到
    Kubernetes 中，但在安全性方面有局限性，因为机密信息是用 Base64 编码的，任何具有集群访问权限的人都可以访问。
- en: 'Integration with AWS Secrets Manager can help solve this problem but to access
    our secrets stored there, we need to enable our Kubernetes deployments to authenticate
    with AWS **Identity and Access Management** (**IAM**). This is often referred
    to as Workload Identity and it is an approach that is relatively common across
    cloud platforms:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS Secrets Manager 的集成可以帮助解决这个问题，但为了访问我们存储在其中的机密，我们需要使我们的 Kubernetes 部署能够与
    AWS **身份与访问管理** (**IAM**) 进行身份验证。这通常被称为工作负载身份，它是一个在各大云平台中相对常见的方法：
- en: '![Figure 8.7 – AWS EKS with Workload Identity](img/B21183_08_7.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – AWS EKS 与工作负载身份](img/B21183_08_7.jpg)'
- en: Figure 8.7 – AWS EKS with Workload Identity
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – AWS EKS 与工作负载身份
- en: To set up Workload Identity on EKS, we need to configure the cluster with an
    **OpenID Connect** (**OIDC**) provider. Then, we must set up an IAM role that
    has a policy that allows a Kubernetes service account to assume the role. This
    IAM role can then be granted access to any AWS permissions and resources that
    the Kubernetes deployment needs access to, including Secrets Manager secrets.
    The last thing we need to do is provision a Kubernetes service account by the
    same name within Kubernetes and give it a special annotation to connect it to
    the IAM role.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 EKS 上设置工作负载身份，我们需要使用 **OpenID Connect** (**OIDC**) 提供程序来配置集群。然后，我们必须设置一个
    IAM 角色，该角色有一个策略，允许 Kubernetes 服务账户假设该角色。然后，可以为此 IAM 角色授予对 Kubernetes 部署所需的任何 AWS
    权限和资源的访问权限，包括 Secrets Manager 的机密。最后，我们需要在 Kubernetes 中使用相同名称配置一个 Kubernetes 服务账户，并为其添加特殊注释以将其连接到
    IAM 角色。
- en: 'Once this is done, our Kubernetes deployments will be allowed to access our
    AWS Secrets Manager secrets but they won’t be using that access. The final step
    is to configure the Kubernetes deployment to pull in the secrets and make them
    accessible to our application code running in the pods:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，我们的 Kubernetes 部署将被允许访问 AWS Secrets Manager 的机密，但它们不会使用该访问权限。最后一步是配置
    Kubernetes 部署以拉取机密并使其对运行在 Pod 中的应用程序代码可访问：
- en: '![Figure 8.8 – AWS EKS Secrets Manager integration](img/B21183_08_8.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.8 – AWS EKS 与 Secrets Manager 集成](img/B21183_08_8.jpg)'
- en: Figure 8.8 – AWS EKS Secrets Manager integration
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 – AWS EKS 与 Secrets Manager 集成
- en: Kubernetes has a common practice of doing this using volume mounts. As a result,
    there is a common Kubernetes provider known as the secrets store **Container Storage
    Interface** (**CSI**) provider. This is a cloud-agnostic technique that integrates
    Kubernetes with external secret stores, such as AWS Secrets Manager. This method
    offers enhanced security and scalability, but it requires more setup and maintenance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 通常使用卷挂载的方式来实现这一点。因此，存在一个名为 secrets store **容器存储接口** (**CSI**) 提供程序的常见
    Kubernetes 提供程序。这是一种与云平台无关的技术，可以将 Kubernetes 与外部秘密存储（如 AWS Secrets Manager）集成。此方法提供了增强的安全性和可扩展性，但需要更多的设置和维护。
- en: 'To get this working, we need to deploy two components to our EKS cluster: the
    secrets store CSI driver and then the AWS provider for this driver that will allow
    it to interface with AWS Secrets Manager. Both of these components can be deployed
    to our EKS cluster with `SecretProviderClass`. This is a type of resource that
    connects to AWS Secrets Manager through the CSI driver to access specific secrets.
    It connects to specific secrets in Secrets Manager using the service account that
    we granted access to via the IAM role and its permissions.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其正常工作，我们需要将两个组件部署到我们的 EKS 集群中：秘密存储 CSI 驱动程序，然后是该驱动程序的 AWS 提供程序，允许它与 AWS Secrets
    Manager 进行交互。这两个组件可以通过 `SecretProviderClass` 部署到我们的 EKS 集群中。这是一种资源类型，通过 CSI 驱动程序连接到
    AWS Secrets Manager，从而访问特定的机密。它使用我们通过 IAM 角色及其权限授予访问权限的服务账户连接到 Secrets Manager
    中的特定机密。
- en: Kubernetes cluster
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 集群
- en: Amazon EKS offers a managed Kubernetes service that streamlines the deployment
    and management of containerized applications on AWS. The EKS cluster is the central
    figure of this architecture. EKS handles the heavy lifting of setting up, operating,
    and maintaining the Kubernetes control plane and nodes, which are essentially
    EC2 instances. When setting up an EKS cluster, users define node groups, which
    manifest as collections of EC2 instances that the EKS service is responsible for
    provisioning and managing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EKS 提供了一项托管的 Kubernetes 服务，简化了容器化应用程序在 AWS 上的部署和管理。EKS 集群是此架构的核心。EKS
    负责设置、操作和维护 Kubernetes 控制平面和节点，这些节点本质上是 EC2 实例。在设置 EKS 集群时，用户定义节点组，这些节点组表现为 EC2
    实例集合，EKS 服务负责为其配置和管理。
- en: There are several options for node groups that can host your workloads. The
    most common examples are AWS-managed and self-managed node groups. AWS-managed
    node groups are essentially on-demand EC2 instances that are allocated for the
    EKS cluster. AWS simplifies the management of these nodes but this imposes some
    restrictions on what AWS features can be used. Self-managed nodes are also essentially
    on-demand EC2 instances but they provide greater control over the features and
    configuration options available to them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种选项可以用来承载您的工作负载的节点组。最常见的例子是 AWS 管理的节点组和自管理的节点组。AWS 管理的节点组本质上是为 EKS 集群分配的按需
    EC2 实例。AWS 简化了这些节点的管理，但这也会对可以使用的 AWS 特性施加一些限制。自管理节点本质上也是按需 EC2 实例，但它们提供了更大的控制权，允许对可用的特性和配置选项进行更精细的调整。
- en: 'A great way to optimize for cost is to use a Fargate node group. This option
    takes advantage of AWS’ serverless compute engine and removes the need to provision
    and manage EC2 instances. However, this is probably more suitable for unpredictable
    workloads rather than those that require a steady state. In those situations,
    you can take advantage of a combination of autoscaling and spot and reserved instances
    to reap significant discounts and cost reduction:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 优化成本的一个好方法是使用 Fargate 节点组。这个选项利用了 AWS 的无服务器计算引擎，省去了配置和管理 EC2 实例的需求。然而，这可能更适用于不可预测的工作负载，而不是那些需要保持稳定状态的工作负载。在这种情况下，您可以利用自动扩展、现货实例和预留实例的组合，从而获得显著的折扣和成本降低：
- en: '![Figure 8.9 – Anatomy of an AWS EKS cluster](img/B21183_08_9.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – AWS EKS 集群结构](img/B21183_08_9.jpg)'
- en: Figure 8.9 – Anatomy of an AWS EKS cluster
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – AWS EKS 集群结构
- en: IAM policies are a major part of the configuration of EKS due to the nature
    of the service and how we delegate responsibility to it to manage AWS resources.
    This is similar to what we do with AWS Auto Scaling groups but even more so. IAM
    policies are attached to the cluster and individual node groups. Depending on
    the capabilities you want to enable within your cluster and your node groups,
    you might need additional policies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: IAM 策略是 EKS 配置的重要部分，因为服务的特性以及我们如何委派责任来管理 AWS 资源。这与我们在 AWS 自动扩展组中的做法类似，但更加重要。IAM
    策略附加在集群和单独的节点组上。根据您希望在集群和节点组中启用的功能，您可能需要额外的策略。
- en: The `AmazonEKSClusterPolicy` policy grants the cluster access to control the
    internal workings of the cluster itself, including node groups, CloudWatch logging,
    and access control within the cluster.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`AmazonEKSClusterPolicy` 策略授予集群访问权限，以控制集群内部的工作方式，包括节点组、CloudWatch 日志和集群内的访问控制。'
- en: The `AmazonEKSVPCResourceController` policy grants the cluster access to manage
    network resources such as network interfaces, IP address assignment, and security
    group attachments to the VPC.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`AmazonEKSVPCResourceController` 策略授予集群访问权限，以管理网络资源，例如网络接口、IP 地址分配和安全组附加到 VPC
    的操作。'
- en: There are four policies (`AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`,
    `AmazonEC2ContainerRegistryReadOnly`, and `CloudWatchAgentServerPolicy`) that
    are essential for the operation of EKS worker nodes. These policies absolutely
    must be attached to the IAM role that you assign to your EKS node group. They
    grant access to the EKS cluster’s control plane and let nodes within the node
    group integrate with the core infrastructure provided by the cluster, including
    the network, container registries, and CloudWatch. As described previously, we
    also added an optional policy to allow the EKS cluster to manage elastic load
    balancers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个策略（`AmazonEKSWorkerNodePolicy`、`AmazonEKS_CNI_Policy`、`AmazonEC2ContainerRegistryReadOnly`
    和 `CloudWatchAgentServerPolicy`）对于 EKS 工作节点的运行至关重要。这些策略必须附加到您分配给 EKS 节点组的 IAM
    角色上。它们授予访问 EKS 集群控制平面的权限，并允许节点组内的节点与集群提供的核心基础设施集成，包括网络、容器注册表和 CloudWatch。如前所述，我们还添加了一个可选的策略，允许
    EKS 集群管理弹性负载均衡器。
- en: Deployment architecture
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署架构
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on AWS, we need to come up with a plan on how to provision
    our environments and deploy our code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对在 AWS 上构建的解决方案的云架构有了一个清晰的了解，我们需要制定计划，如何配置环境并部署代码。
- en: Cloud environment configuration
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云环境配置
- en: 'Building upon the methodology we established in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365)
    for provisioning EC2 instances, our approach to provisioning the AWS EKS environment
    will follow a similar pattern. The core of this process lies in utilizing GitHub
    Actions, which will remain unchanged in its fundamental setup and operation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在[*第七章*](B21183_07.xhtml#_idTextAnchor365)中为 EC2 实例配置建立的方法，我们在配置 AWS EKS
    环境时将采用类似的模式。此过程的核心在于利用 GitHub Actions，其基本设置和操作将保持不变：
- en: '![Figure 8.10 – The Terraform code provisions the environment on AWS](img/B21183_08_10.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – Terraform 代码在 AWS 上配置环境](img/B21183_08_10.jpg)'
- en: Figure 8.10 – The Terraform code provisions the environment on AWS
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – Terraform 代码在 AWS 上配置环境
- en: However, instead of provisioning EC2 instances as we did previously, the Terraform
    code will be tailored to set up the necessary components for an EKS environment.
    This includes the creation of an EKS cluster and an ECR. The GitHub Action will
    automate the execution of this Terraform code, following the same workflow pattern
    we used before.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与我们之前配置 EC2 实例不同，Terraform 代码将根据 EKS 环境的需要进行定制。这包括创建 EKS 集群和 ECR。GitHub Action
    将自动执行此 Terraform 代码，遵循我们之前使用的相同工作流模式。
- en: By reusing the GitHub Actions workflow with different Terraform scripts, we
    maintain consistency in our deployment process while adapting to the different
    infrastructure requirements of the EKS environment. This step will need to be
    executed in a standalone mode to ensure certain prerequisites are there, such
    as the container registry. Only once the container registry is provisioned can
    we build and push container images to it for our frontend and backend application
    components.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过复用 GitHub Actions 工作流并使用不同的 Terraform 脚本，我们在保持部署过程一致性的同时，适应了 EKS 环境不同的基础设施需求。此步骤需要在独立模式下执行，以确保某些先决条件已到位，例如容器注册表。只有在容器注册表配置完成后，我们才能构建并将容器镜像推送到容器注册表中，用于前端和后端应用组件。
- en: This step will also provision the EKS cluster that hosts the Kubernetes control
    plane. We’ll use this in the final step in conjunction with the container images
    to deploy our application.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤还将配置托管 Kubernetes 控制平面的 EKS 集群。我们将在最后一步与容器镜像一起使用它来部署我们的应用程序。
- en: Container configuration
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器配置
- en: 'Unlike Packer, which doesn’t rely on any existing infrastructure to provision
    the application deployment artifacts (for example, the AMIs built by Packer),
    our container images need to have a container registry before they can be provisioned:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Packer 不同，Packer 不依赖任何现有基础设施来配置应用程序部署工件（例如，由 Packer 构建的 AMI），我们的容器镜像在配置之前需要有一个容器注册表：
- en: '![Figure 8.11 – Docker pipeline to build a container image for the frontend](img/B21183_08_11.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 构建前端容器镜像的 Docker 流水线](img/B21183_08_11.jpg)'
- en: Figure 8.11 – Docker pipeline to build a container image for the frontend
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 构建前端容器镜像的 Docker 流水线
- en: The workflow is very similar to that of Packer in that we combine the application
    code and a template that stores the operating system configuration. In this case,
    it stores a Dockerfile rather than a Packer template.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流与 Packer 非常相似，我们将应用程序代码与存储操作系统配置的模板结合。在这种情况下，它存储的是 Dockerfile，而不是 Packer
    模板。
- en: Kubernetes configuration
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 配置
- en: 'Once we’ve published container images for both the frontend and backend, we’re
    ready to complete the deployment by adding a final step that executes Terraform
    using the Kubernetes provider so that it will deploy our application to the EKS
    cluster:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们发布了前端和后端的容器镜像，就可以通过添加一个最终步骤来完成部署，该步骤使用 Kubernetes 提供程序执行 Terraform，从而将我们的应用程序部署到
    EKS 集群：
- en: '![Figure 8.12 – Container images as inputs to terraform code, which provisions
    the environment on EKS’ Kubernetes control plane](img/B21183_08_12.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – 容器镜像作为 terraform 代码的输入，terraform 代码在 EKS 的 Kubernetes 控制平面上配置环境](img/B21183_08_12.jpg)'
- en: Figure 8.12 – Container images as inputs to terraform code, which provisions
    the environment on EKS’ Kubernetes control plane
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 容器镜像作为 terraform 代码的输入，terraform 代码在 EKS 的 Kubernetes 控制平面上配置环境
- en: We will output key pieces of information from the previous Terraform step that
    provisioned the AWS infrastructure. This will include details about the ECR repositories
    and the EKS cluster. We can use these as inputs for the final Terraform execution
    step where we use the Kubernetes provider. We have separated this step into separate
    Terraform workspaces to decouple it from the AWS infrastructure. This recognizes
    the hard dependency between the Kubernetes control plane layer and the underlying
    infrastructure. It allows us to independently manage the underlying infrastructure
    without making changes to the Kubernetes deployments, as well as make changes
    that are isolated within the Kubernetes control plane that will speed up the release
    process.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输出前一个 Terraform 步骤中配置 AWS 基础设施的关键信息。这将包括关于 ECR 仓库和 EKS 集群的详细信息。我们可以将这些作为最终
    Terraform 执行步骤的输入，在该步骤中我们将使用 Kubernetes 提供程序。我们已经将此步骤分离到不同的 Terraform 工作空间，以将其与
    AWS 基础设施解耦。这考虑到了 Kubernetes 控制平面层和底层基础设施之间的硬依赖关系。它使我们能够独立管理底层基础设施，而不需要更改 Kubernetes
    部署，同时可以在 Kubernetes 控制平面中进行隔离的更改，从而加快发布过程。
- en: In this section, we reviewed the key changes in our architecture as we transitioned
    from VM-based architecture to container-based architecture. In the next section,
    we’ll get tactical in building the solution, but we’ll be careful to build on
    the foundations we built in the previous chapter when we first set up our solution
    on AWS using VMs powered by EC2.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们回顾了架构中的关键变化，即从基于虚拟机的架构转向基于容器的架构。在下一部分中，我们将采取战术性的步骤来构建解决方案，但我们会小心地基于上一章中我们第一次在
    AWS 上使用 EC2 启动虚拟机时所构建的基础，继续构建。
- en: Building the solution
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: In this section, we’ll be taking our theoretical knowledge and applying it to
    a tangible, functioning solution while harnessing the power of Docker, Terraform,
    and Kubernetes on the AWS platform. Some parts of this process will require significant
    change, such as when we provision our AWS infrastructure using Terraform; other
    parts will have minor changes, such as the Kubernetes configuration that we use
    to deploy our application to our Kubernetes cluster; and some will be completely
    new, such as the process to build and push our Docker images to our container
    registry.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将把理论知识应用于一个具体的、可运行的解决方案，同时利用 AWS 平台上的 Docker、Terraform 和 Kubernetes
    的力量。这个过程的某些部分将需要重大更改，例如我们使用 Terraform 配置 AWS 基础设施时；其他部分将会有一些小的更改，例如我们用来将应用程序部署到
    Kubernetes 集群的 Kubernetes 配置；还有一些部分将是全新的，例如构建并推送 Docker 镜像到容器注册表的过程。
- en: Docker
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker
- en: As we saw in the previous chapter, where we built VM images in Packer, there
    is a certain amount of operating system configuration that needs to be set up.
    With Docker, we are doing largely the same thing but we are doing it for a specific
    process. This means much of the work that we did in setting up the service in
    Linux is eliminated because the container runtime controls when the application
    is running or not. This is fundamentally different than configuring the Linux
    operating system to run an executable as a service. As a result, much of this
    boilerplate is eliminated.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，当我们在 Packer 中构建虚拟机镜像时，需要进行一定的操作系统配置。在 Docker 中，我们基本上做的是相同的事情，但我们是为特定的进程进行配置。这意味着我们在设置
    Linux 服务时所做的很多工作被省略了，因为容器运行时控制着应用程序是否运行。这与配置 Linux 操作系统将可执行文件作为服务运行是根本不同的。因此，许多冗余的配置被省略了。
- en: 'Another major difference is that with the Packer image, we build the application
    outside of Packer and we drop a zipped artifact containing the application as
    part of the Packer build. With Docker, we’ll build the application and produce
    the artifact within the container build process. After this process is complete,
    we’ll follow a similar process where we drop the deployment package into a clean
    container image layer to eliminate any residual build artifacts:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要区别是，使用 Packer 镜像时，我们在 Packer 外部构建应用程序，并将一个包含应用程序的压缩工件作为 Packer 构建的一部分。而使用
    Docker 时，我们将在容器构建过程中构建应用程序并生成工件。这个过程完成后，我们将遵循一个类似的过程，将部署包放入干净的容器镜像层中，以消除任何残留的构建工件：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following line sets the base image for the build stage. It uses the official
    Microsoft .NET SDK image (version `6.0`) from the **Microsoft Container** **Registry**
    (**MCR**):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下这一行设置了构建阶段的基础镜像。它使用了官方的 Microsoft .NET SDK 镜像（版本 `6.0`），来自**Microsoft 容器**
    **注册表**（**MCR**）：
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Before we build the project, we need to resolve its dependencies. The `dotnet
    restore` command will do this by pulling all the dependencies from NuGet (the
    .NET package manager):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, we execute the `dotnet publish` command, which creates the binaries for
    the project. The `-c Release` option specifies that the build should be optimized
    for production. We drop the files into the `out` folder to be picked up by a future
    step:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We start a new build stage with the .NET runtime image as the base and we copy
    the binaries that we built from the previous stage to this new one. This will
    ensure that any intermediate build artifacts are not layered into the container
    image:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we set the startup command for the container. When the container starts,
    it will run `dotnet FleetPortal.dll`, which starts our ASP.NET application. It
    will start listening for incoming web server traffic.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base of application code that needs
    to be deployed. However, with a Kubernetes solution, the infrastructure is simplified
    in that we only need a Kubernetes cluster (and a few other things). The important
    piece is the configuration within the Kubernetes platform itself.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: As a result, much of the Terraform setup is very similar to what we did in the
    previous chapter, so we will only focus on new resources needed for our solution.
    You can check the full source code for this book on GitHub if you want to work
    with the complete solution.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Container registry
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll set up repositories for both the frontend and backend of our application
    using AWS ECR. To simplify the dynamic creation of our ECR repositories, we can
    set up a local variable called `repository_list` that has constants for the two
    container images we need repositories for:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we’ll use a `for` expression to generate a map from this list that we
    can then use to create a corresponding ECR repository using the `for_each` iterator:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we’ll set up an IAM group that we can grant access to push container
    images to:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we need to generate an IAM policy that grants access to each of the ECR
    repositories and attach it to the IAM group we created previously:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we must grant access to this group. We’ll be granting access to the
    identities of developers on our team or the GitHub Actions workflows that will
    be pushing new images as part of our CI/CD process:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Kubernetes cluster
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that our container registry is all set up and we can push images to it,
    we need to set up our Kubernetes cluster. That’s where AWS EKS comes in. The cluster’s
    configuration is relatively simple but there’s quite a bit of work we need to
    do with IAM to make it all work.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we provision our EKS cluster, we need to set up the IAM role that it
    will use to interact with the rest of the AWS platform. This is not a role that
    our nodes or Kubernetes deployments will use. It’s the role that EKS will use
    to enact configuration changes made to the cluster across all the AWS resources
    that are being used:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As a result, the EKS service will assume this role. Hence, the `assume` policy
    needs to allow a principal of the `Service` type with `eks.amazonaws.com` as its
    identifier:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With this role, we are going to enable EKS to provision and manage the resources
    that it needs within our AWS account. As a result, we need to attach the built-in
    `AmazonEKSClusterPolicy` and `AmazonEKSVPCResourceController` policies:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code is an example of how to do this for one of the policies.
    You could create an `aws_iam_role_policy_attachment` resource for each of the
    policies or use an iterator over a collection of the policies that we need to
    attach.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that this IAM role is ready, we can set up our cluster using the `aws_eks_cluster`
    resource:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: A significant portion of the configuration is done within the `vpc_config` block,
    which references many of the same structures that we provisioned in the previous
    chapter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that you might want to keep in mind is how important the IAM policies
    are for enabling this EKS cluster to be successfully provisioned. Since there
    is no direct relationship between the IAM role’s policy attachments, you should
    ensure that IAM role permissions are created before we attempt to provision the
    EKS cluster. The following code demonstrates the use of the `depends_on` attribute,
    which allows us to define this relationship explicitly:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The EKS cluster is just the control plane. For our cluster to have utility,
    we need to add worker nodes. We can do this by adding one or more node groups.
    These node groups will be composed of a collection of EC2 instances that will
    be enlisted as worker nodes. These nodes also need their own IAM role:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A key difference is that because this role will be assumed by the worker nodes,
    which are EC2 instances, the IAM role’s `assume` policy needs to align with this
    fact.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as before with our EKS cluster, which needed an IAM role to be set up
    as a prerequisite, the same is true for our node group. Now that the node group’s
    IAM role is ready, we can use the following code to create an EKS node group associated
    with the previously defined cluster. It specifies the desired, minimum, and maximum
    sizes of the node group, along with other configurations, such as the AMI type
    and disk size:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Again, just like with the EKS cluster, the IAM role’s policy attachments are
    critical to making the node group functional. Therefore, you need to make sure
    that all policy attachments are attached to the IAM role before you start provisioning
    our node group. As we discussed in the previous section, there are four policies
    (, `AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`, `AmazonEC2ContainerRegistryReadOnly`,
    and `CloudWatchAgentServerPolicy`) that are essential for the operation of EKS
    worker nodes:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you add additional features to your EKS cluster, you may introduce additional
    IAM policies that grant the cluster and its worker nodes different permissions
    within AWS. When you do, don’t forget to also include these policies in these
    `depends_on` attributes to ensure smooth operations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Logging and monitoring
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can enable CloudWatch logging on the cluster by simply adding the `enabled_cluster_log_types`
    attribute to the `aws_eks_cluster` resource:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This attribute takes one or more different log types. I’d recommend checking
    the documentation for all the different options supported. Next, we need to provision
    a CloudWatch log group for the cluster:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This requires a specific naming convention and it needs to match the name you
    use for your cluster. Therefore, it’s a good idea to extract the value you pass
    to the `name` attribute of the `aws_eks_cluster` resource as a local variable
    so that you can use it in two places.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Workload identity
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the cluster provisioned, we need to get the OIDC issuer certificate from
    the cluster so that we can use it to configure the OpenID Connect provider with
    AWS IAM. The following code uses the `tls_certificate` data source from the `tls`
    utility provider, which we covered in [*Chapter 3*](B21183_03.xhtml#_idTextAnchor185),
    to obtain additional metadata about the certificate:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With this additional metadata, we can use the `aws_iam_openid_connect_provider`
    resource to connect the cluster to the AWS IAM OIDC provider by referencing `sts.amazonaws.com`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We’ve already set up several IAM roles, including one for the EKS cluster and
    another for the worker nodes of the cluster. Therefore, I won’t reiterate the
    creation of the `aws_iam_role` resource for the workload identity. However, this
    new role does need to have a very distinct assumption policy. The workload identity
    IAM role needs to reference the OIDC provider and a yet-to-be-provisioned Kubernetes
    service account:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As you can see, in the preceding code, the service account follows a very specific
    naming convention: `system:serviceaccount:<namespace>:<service-account-name>`.
    We replace `<namespace>` with the name of the Kubernetes namespace and likewise,
    we replace `<service-account-name>` with the name of the service account. It’s
    important to point out that we are referencing resources that do not exist yet.
    As such, the reference to them within the workload identity IAM role’s assumption
    policy is a pointer or a placeholder to this yet-to-be-created resource. Both
    the Kubernetes namespace and the service account are resources that will need
    to be created within the Kubernetes control plane. We’ll tackle that in the next
    section using the `kubernetes` Terraform provider.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have an IAM role for our workload identity, we simply need to grant
    it access to the AWS resources we want it to use. Therefore, we will use the `aws_iam_policy_document`
    data source once more to generate an IAM policy that we will attach to the workload
    identity’s IAM role. This is where we have the opportunity to grant it access
    to any resource in AWS that our application code will need. For our solution,
    we’ll start with access to AWS Secrets Manager secrets by granting it access to
    read secrets using the `secretsmanager:GetSecretValue` action:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This policy will grant the IAM role access to the secrets within this account.
    We could further refine its access by enhancing the `*` wildcard path to ensure
    that it has access to only certain secrets. This can be done by implementing a
    naming convention that uses a unique prefix for your secrets. The `application_name`
    and `environment_name` variables are a perfect way to implement this naming convention
    and to tighten access to your Kubernetes workloads to AWS Secrets Manager.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we just need to provision secrets to Secrets Manager with the right naming
    convention:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'AWS Secrets Manager uses a parent resource called `aws_secretsmanager_secret`
    as a logical placeholder for the secret itself but recognizes that the secret’s
    value might change over time:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Those different values for the secret are stored in `aws_secretsmanager_secret_version`
    resources. You can generate complex secrets using the `random` provider but it’s
    probably more common to obtain `secret_string` from the outputs of other resources.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), we introduced Kubernetes
    architecture and automation techniques using YAML and **HashiCorp Configuration
    Language** (**HCL**). In our solutions in this book, we will be using the Terraform
    provider for Kubernetes to automate our application’s deployment. This allows
    us to both parameterize the Kubernetes configurations that would otherwise be
    trapped in hard-coded YAML files and provision a combination of Kubernetes primitives
    and Helm charts with the same deployment process.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Provider setup
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ironically, the first thing we need to do to set up the `kubernetes` provider
    is initialize the `aws` provider so that we can get information about our EKS
    cluster. We can do that using the data sources provided and a single input variable:
    the cluster’s name. Of course, the AWS region is also an implied parameter to
    this operation but it is part of the `aws` provider configuration rather than
    inputs to the data sources themselves:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We’ll use both the `aws_eks_cluster` and `aws_eks_cluster_auth` data sources
    to grab the data we need to initialize the `kubernetes` provider:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Interestingly, the Helm provider setup is pretty much identical to the Kubernetes
    provider configuration. It seems a bit redundant, but it’s relatively straightforward:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Namespace
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating the Kubernetes namespace is extremely simple:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This will act as the logical container for all of the Kubernetes resources that
    we provision for our application.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Service account
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we built one-half of this bridge when we set up the
    OpenID Connect provider configuration within AWS and we specified the Kubernetes
    namespace and service account name ahead of time. Now, we’ll finish constructing
    this bridge by provisioning `kubernetes_service_account` and ensuring that `namespace`
    and `name` match our AWS configuration:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We also need to add an annotation that references the unique identifier (or
    ARN) for the workload identity’s IAM role. We can set this up as an output variable
    in our Terraform workspace that provisions the AWS infrastructure and routes its
    value to an input variable on the Terraform workspace for our Kubernetes configuration.
    This is a great example of how the `kubernetes` provider for Terraform can be
    a useful way of configuring Kubernetes resources that require tight coupling with
    the cloud platform.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Secrets store CSI driver
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the service account set up, our application is one step closer to being
    able to access our secrets in AWS Secrets Manager. However, before we can do that,
    we need to set up the secrets store CSI driver. As we discussed previously, this
    is a common Kubernetes component that provides a standard mechanism for using
    volume mounts as a way to distribute remotely managed secrets to workloads running
    in Kubernetes. The driver is extremely flexible and can be extended through providers
    that act as adapters for different external secret management systems.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the secrets store CSI driver Helm chart:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can optionally enable secret synchronization by using the `syncSecret.enabled`
    attribute to make the secrets accessible from Kubernetes secrets. This makes it
    extremely convenient to inject the secrets into our application’s pods without
    customized code to retrieve them from the mounted volume.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to install the AWS provider for the CSI driver:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Both of these Helm charts provision several different Kubernetes resources to
    your cluster under the `kube-system` namespace. If you encounter errors, interrogating
    the pods hosting these components is a good place for you to start debugging your
    configuration.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Secret provider class
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we’ve installed both the CSI driver and its AWS provider, we are ready
    to connect to AWS Secrets Manager. So far, we have only enabled this ability;
    we haven’t exercised it by accessing secrets.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: That’s what the `SecretProviderClass` resource is for. It connects to a specific
    set of secrets within AWS Secrets Manager. You’ll notice that the way this type
    of resource is provisioned is different than other resources in Kubernetes. While
    other resource types have a corresponding Terraform resource, `SecretProviderClass`
    uses a `kubernetes_manifest` resource.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s because this resource type is managed through a Kubernetes **custom
    resource definition** (**CRD**); it’s not a built-in type within Kubernetes:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The structure of `SecretProviderClass` has two parts. First, `parameters` is
    where we declare what secrets we want to bring in:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, `objectName` corresponds to either the relative name of the Secrets Manager
    secret or a fully qualified ARN for the secret. Next, `objectType` indicates what
    CSI driver provider should be used to access the secret, while `objectVersionLabel`
    allows us to select a specific version of the secret within Secrets Manager. For
    AWS, to access the latest version (probably the most common use case), you need
    to specify `AWSCURRENT` as the value.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, there is a collection of `secretObjects` that’s used to define corresponding
    Kubernetes secret objects:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: These `secretObjects` will later be used in the deployment specification of
    our application to create environment variables for each secret within the pods.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Kubernetes deployment is one of the most significant resources that we have
    to provision within Kubernetes. As a result, it can be rather intimidating as
    there are several rather complex nested sections. The most important thing going
    on in the deployment is the container specification. This sets up the actual runtime
    environment for our pods.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important piece of information is the container image we want to use
    in our pods. To configure this, we need to construct the fully qualified path
    to the container image stored in our ECR. To do that, we need two pieces of information.
    First, we need the AWS account number and second, we need the AWS region name
    where our ECR repository is provisioned to:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The AWS account number can easily be obtained from the `aws_caller_identity`
    data source. This is an extremely simple data source that provides contextual
    information about the AWS account and IAM identity that Terraform is using with
    the `aws` provider. As a result, to create this data source, you simply create
    it without any parameters:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is a common pattern for accessing Terraform provider authentication context
    and cloud platform provisioning scope – in this case, what AWS account and what
    region we are provisioning to.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the version of the same YAML code converted into HCL using an input
    variable to set different attributes on the entity:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The local variable we use for the container image name is the fully qualified
    path to our container image within ECR. It follows the `<account>.dkr.ecr.<region>.amazonaws.com/<repository>:<tag>`
    structure. Here, `<account>` is the AWS account number, which can be accessed
    using the `aws_caller_identity` data source. Then, `<region>` is the AWS region,
    which is accessible from the input variables. Finally, `<repository>` is the ECR
    repository name and `<version>` is the tag for the specific version of the container
    image.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: We can set `service_account_name` by referencing other Kubernetes resources
    provisioned within this Terraform workspace. This is a key difference between
    using YAML and the `kubernetes` provider for Terraform. If we were using YAML
    this, would have to be hard-coded, whereas with HCL, we can reference other resources
    within the Terraform workspace.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'To reference an AWS Secrets Manager secret, we need to modify the `container`
    block so that it includes another `env` block:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This allows us to reference one of the `secretObjects` objects we declared within
    `SecretProviderClass` and give it an environment variable name that our application
    code can reference to access the secret.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Service
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Kubernetes service is primarily a network routing mechanism. It defines
    the port on which the service should be exposed to external clients and what port
    the network traffic should be forwarded to on the pods:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Here, `selector` specifies which pods traffic should be forwarded to and it
    should match the corresponding pods, with the `app` label set to the same value
    as the service’s selector.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMap
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we know from [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), the ConfigMap
    resource is a great way to pass non-sensitive configuration settings to your pods:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Often, the Terraform workspace that provisions the infrastructure will output
    several different values that need to be included in a Kubernetes ConfigMap (URIs,
    AWS ARNs, DNS, and so on).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ingress controller is a component of Kubernetes that routes external network
    traffic into the cluster. It works in conjunction with a Kubernetes ingress, which
    defines specific rules that route traffic for specific services. This is very
    similar to the structure of the CSI driver and `SecretProviderClass`. One provides
    the foundational subsystem, thus enabling the capability, while the other implements
    a specific configuration using that underlying subsystem.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular ingress controllers is a load balancer called NGINX.
    We can set up the NGINX ingress controller using a Helm chart. The components
    that are deployed by this Helm chart are why we needed an additional IAM policy
    that allows our EKS cluster to configure AWS ELB resources. That’s because the
    Kubernetes configurations of the ingress controller and ingress resources will
    be interpreted by EKS and manifested as the provisioning and configuration of
    AWS ELB resources. This means that instead of explicitly configuring ELB resources
    using the `aws` Terraform provider, you will be annotating Kubernetes deployments
    and the necessary ELB resources will be provisioned and configured on your behalf.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is install the NGINX ingress controller using
    a Helm chart:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will install NGINX and deploy a Kubernetes service for NGINX running under
    the namespace we specified. The next step is to configure an ingress for our application:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'An ingress resource is pretty simple. You need to set the namespace and specify
    what ingress controller you want to use. Then, you need to specify paths so that
    you can route network traffic to the correct Kubernetes services:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'It’s also pretty important to establish explicit `depends_on` statements for
    the Kubernetes services for the frontend and backend application deployments as
    well as the ingress controller since we don’t reference it directly within the
    HCL configuration:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now that we’ve built out the three components of our architecture, in the next
    section, we’ll move on to how we can automate the deployment using Docker to build
    and publish the container images and then Terraform to provision our infrastructure
    and deploy our solution to Kubernetes.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Automating the deployment
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll shift our focus from building our application and its
    environment to implementing deployment automations to efficiently provision our
    solution to AWS. Container-based architectures involve three core deployment motions.
    First, we must create and publish container images to a container registry. Next,
    we must provision the Kubernetes cluster environment where containers will be
    hosted. Finally, we must deploy the Kubernetes resources that will create the
    containers within Kubernetes pods and reference the container images we published.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like the VM image that we built with Packer in the previous chapter, the container
    image acts as an immutable artifact that contains a versioned copy of the application
    code and operating system configuration. We need to update this artifact every
    time something changes in either the application code or the operating system
    configuration:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Just like with Packer, we need to trigger a new container image to be built
    every time the application code and the operating system are configured within
    the Dockerfile itself. With GitHub Actions, we can add a list of `paths` that
    will trigger our workflow:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – VM image versioning](img/Image97144.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – VM image versioning
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the triggers and some variables set for our workflow, we need
    to structure `jobs`. For each Packer template, we will have two jobs: one that
    builds the C# .NET application code and produces a deployment package and another
    that runs `packer build` to produce the VM image:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The `build` job performs a pretty standard .NET build process, which includes
    restoring package dependencies from NuGet (the .NET package manager), building
    the code, running unit and integration tests, publishing a deployable artifact,
    and storing that artifact so that it can be used by future jobs within the pipeline:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Docker workflow](img/B21183_08_14.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Docker workflow
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The `docker` job immediately runs Terraform to obtain outputs of the ECR container
    repository that we want to target. We don’t have to run Terraform here but we
    could explicitly specify the ECR repository’s fully qualified path.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, it generates a unique version of the name for the container image that
    will be produced if successful. We’ll generate this image version based on the
    current date and the GitHub Action’s run number. This will guarantee that the
    image version is unique so that we don’t have to manually set it or worry about
    conflicts when pushing to the repository:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, we need to set up Docker:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, we must configure our AWS credentials using an official AWS GitHub Action.
    We’ll use an AWS access key and secret access key specified by the GitHub environment
    settings:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Once the credential has been configured, we can use the `amazon-ecr-login`
    action to connect to ECR:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, we’ll build and push the image using an official Docker GitHub Action.
    It’s important to note that this action is not specific to AWS. It uses standard
    container registry protocols to communicate with ECR using the fully qualified
    path to the ECR repository that we specify in the `tags` parameter:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Both of our application components (the frontend and the backend) will have
    a repository, so the registry endpoint will be different depending on which container
    image we’re pushing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we comprehensively covered
    the process of creating a Terraform GitHub Action that authenticates with AWS.
    Therefore, we won’t be delving into it any further. I encourage you to refer back
    to [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) to review the process.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we automate Kubernetes with Terraform, we are just running `terraform apply`
    again with a different root module. This time, the root module will configure
    the `kubernetes` and `helm` providers in addition to the `aws` provider. However,
    we won’t create new resources with the `aws` provider; we will only obtain data
    sources from existing resources we provisioned in the previous `terraform apply`
    command that provisioned the infrastructure to AWS.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the GitHub Action that executes this process will look strikingly
    similar to how we executed Terraform with AWS. Some of the variables might change
    to include things such as the container image details and cluster information.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we designed, built, and automated the deployment of a complete
    and end-to-end solution using container-based architecture. We built onto the
    foundations from [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), where we worked
    with the foundational infrastructure of AWS VPCs but layered on AWS EKS to host
    our application in containers. In the next and final step in our AWS journey,
    we’ll be looking at serverless architecture, moving beyond the underlying infrastructure,
    and letting the platform itself take our solution to new heights.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
