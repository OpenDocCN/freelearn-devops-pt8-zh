- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containerize with AWS – Building Solutions with AWS EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we built and automated our solution on AWS while utilizing
    **Elastic Cloud Compute** (**EC2**). We built VM images with Packer and provisioned
    our VMs using Terraform. In this chapter, we’ll follow a similar path, but instead
    of working with VMs, we’ll look at hosting our application in containers within
    a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we’ll need to alter our approach by ditching Packer and replacing
    it with Docker to create a deployable artifact for our application. Once again,
    we will be using the `aws` provider for Terraform, but this time, we’ll be introducing
    something new: the `kubernetes` provider for Terraform, which will provision to
    the Kubernetes cluster after our AWS infrastructure has been provisioned using
    the `aws` provider for Terraform.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, with this approach, we will only focus on the new and different. I’ll
    call out where we are building on previous chapters and when something is legitimately
    new.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laying the foundation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our story continues through the lens of Söze Enterprises, founded by the enigmatic
    Turkish billionaire Keyser Söze. Our team has been hard at work building the next-generation
    autonomous vehicle orchestration platform. Previously, we had hoped to leapfrog
    the competition by leveraging Amazon’s rock-solid platform, leveraging our team’s
    existing skills, and focusing on feature development. The team was just getting
    into their groove when a curveball came down from above.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out, over the weekend, our elusive executive was influenced by a rendezvous
    with Andy Jassy, the CEO of AWS, while scuba diving amid the rare and exotic marine
    life off the coast of the Galápagos. Keyser heard about the more efficient resource
    utilization leading to improved cost optimization and faster deployment and rollback
    times, and he was hooked. His new autonomous vehicle platform needed to harness
    the power of the cloud, and container-based architecture was the way to do it.
    So, he decided to accelerate his plans to adopt cloud-native architecture!
  prefs: []
  type: TYPE_NORMAL
- en: The news of transitioning to a container-based architecture means reevaluating
    their approach, diving into new technologies, and possibly even reshuffling team
    dynamics. For the team, containers were always the long-term plan, but now, things
    need to be sped up, which will require a significant investment in time, resources,
    and training.
  prefs: []
  type: TYPE_NORMAL
- en: As the team scrambles to adjust their plans, they can’t help but feel a mix
    of excitement and apprehension. They know that they are part of something groundbreaking
    under Keyser’s leadership. His vision for the future of autonomous vehicles is
    bold and transformative. And while his methods may be unconventional, they have
    learned that his instincts are often right. In this chapter, we’ll explore this
    transformation from VMs to containers using AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in the previous chapter, where we built our solution using VMs using
    AWS EC2, we had full control over the operating system configuration through the
    VM images we provisioned with Packer. Now that we will be transitioning to hosting
    our solution on AWS **Elastic Kubernetes Service** (**EKS**), we’ll need to introduce
    a new tool to replace VM images with container images – **Docker**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Logical architecture for the autonomous vehicle platform](img/B21183_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Logical architecture for the autonomous vehicle platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application architecture, comprising a frontend, a backend, and a database,
    will remain the same, but we will need to provision different resources with Terraform
    and harness new tools from Docker and Kubernetes to automate the deployment of
    our solution to this new infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Source control structure of our repository](img/B21183_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Source control structure of our repository
  prefs: []
  type: TYPE_NORMAL
- en: In this solution, we’ll have seven parts. We still have the application code
    and Dockerfiles (replacing the Packer-based VM images) for both the frontend and
    backend. We still have GitHub Actions to implement our CI/CD process, but now
    we have two Terraform code bases – one for provisioning the underlying infrastructure
    to AWS and another for provisioning our application to the Kubernetes cluster
    hosted on EKS. Then, we have the two code bases for our application’s frontend
    and backend.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, our cloud-hosting solution was a set of dedicated VMs.
    In this chapter, our objective is to leverage AWS EKS to use a shared pool of
    VMs that are managed by Kubernetes to host our application. To achieve this, we’ll
    be using some new resources that are geared toward container-based workloads.
    However, much of the networking, load balancing, and other components will largely
    be the same.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recalling our work in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) with
    EC2 instances and virtual networks, setting up a **virtual private cloud** (**VPC**)
    for AWS EKS follows a similar process. The core network is still there, with all
    the pomp and circumstance, from subnets – both public and private – to all the
    minutia of route tables, internet gateways, and NAT gateways, the virtual network
    we’ll build for our EKS cluster will largely be the same as the one we created
    previously. The only difference will be how we use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – AWS virtual network architecture](img/B21183_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – AWS virtual network architecture
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we used the public subnets for our frontend VMs and the private
    subnets for our backend. As we learned in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278),
    when we introduce Kubernetes into the mix, we’ll be transitioning to a shared
    pool of VMs that host our application as pods. These VMs will be hosted in the
    private subnets and a load balancer will be hosted in the public subnets.
  prefs: []
  type: TYPE_NORMAL
- en: Container registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building on our exploration of container architecture in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278),
    we know that we need to build container images and we need to store them in a
    container registry. For that purpose, AWS offers **Elastic Container Registry**
    (**ECR**). This is a private container registry, unlike public registries such
    as Docker Hub, which we looked at in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll need to utilize the Docker command-line utility to build and push images
    to ECR. To be able to do that, we need to grant an identity the necessary permissions.
    As we saw in the previous chapter, when we build VM images using Packer, we’ll
    likely have a GitHub Actions workflow that builds and pushes the container images
    to ECR. The identity that the GitHub Actions workflow executes under will need
    permission to do that. Once these Docker images are in ECR, the final step is
    to grant our cluster access to pull images from the registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – IAM policy giving a group access to push container images to
    ECR](img/B21183_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – IAM policy giving a group access to push container images to ECR
  prefs: []
  type: TYPE_NORMAL
- en: We’ll set up an IAM group that we’ll grant this permission to. This will allow
    us to add the user for the GitHub Action, as well as any other human users who
    want to push images directly from the command line. In AWS, IAM policies are extremely
    flexible; they can be declared independently or inline with the identity they
    are being attached to. This allows us to create reusable policies that can be
    attached to multiple identities. In this case, we’ll define the policy that grants
    access to push images to this ECR and then attach it to the group. Then, membership
    in the group will grant users access to these permissions.
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to grant access to the cluster such that it can pull images
    from our ECR when it schedules pods within the nodes. To do that, we can use a
    built-in AWS policy called `AmazonEC2ContainerRegistryReadOnly`. We’ll need to
    reference it using its fully qualified ARN, which is `arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly`.
    Built-in policies have a common `arn:aws:iam::aws:policy` prefix that identifies
    them as published by AWS and not published by any specific user within their AWS
    account. When we publish our own policies, the fully qualified ARN will include
    our account number.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike in the previous chapter, where we provisioned and configured our own
    AWS **Application Load Balancer** (**ALB**), when using Amazon EKS, one of the
    advantages is that EKS takes on much of the responsibility of provisioning and
    configuring load balancers. We can direct and influence its actions using Kubernetes
    annotations but this is largely taken care of for us. In our solution, to keep
    things simple, we’ll be using NGINX as our ingress controller and configuring
    it to set up an AWS **Network Load Balancer** (**NLB**) for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Elastic load balancer working with an NGINX ingress controller
    to route traffic to our application’s pods](img/B21183_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Elastic load balancer working with an NGINX ingress controller
    to route traffic to our application’s pods
  prefs: []
  type: TYPE_NORMAL
- en: 'To delegate this responsibility to EKS, we need to grant it the necessary IAM
    permissions to provision and manage these resources. Therefore, we’ll need to
    provision an IAM policy and attach it to the EKS cluster. We can do this using
    an IAM role that has been assigned to the cluster’s node group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – IAM policy allowing EKS to provision and manage elastic load
    balancers](img/B21183_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – IAM policy allowing EKS to provision and manage elastic load balancers
  prefs: []
  type: TYPE_NORMAL
- en: Then, we provision Kubernetes resources (for example, services and ingress controllers)
    and annotate them to inform the specific configuration of our elastic load balancers
    that we want EKS to enact on our behalf.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many ways to host services on Kubernetes and make them accessible
    outside of the cluster. In our solution, we’ll be using an AWS elastic load balancer
    to allow external traffic into our cluster through our NGINX controller. There
    are other options, such as NodePort, which allow you to access a pod directly
    through an exposed port on the node. This would require public access to the cluster’s
    nodes and is not the preferred method from both security and scalability perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: If we want access to the cluster using `kubectl`, then we need to turn on public
    endpoint access. This is useful when you’re developing something small on your
    own but not ideal when you’re working in an enterprise context. You will most
    likely have the private network infrastructure in place so that you never have
    to enable the public endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incorporating secrets into pods within an Amazon EKS cluster can be achieved
    through various methods, each with its advantages and disadvantages. As we did
    with VMs in the previous chapter, the method that we will explore is using AWS
    Secrets Manager secrets. Kubernetes has a built-in approach using Kubernetes Secrets.
    This method is straightforward and integrated directly into Kubernetes, but it
    has limitations in terms of security since secrets are encoded in Base64 and can
    be accessed by anyone with cluster access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Integration with AWS Secrets Manager can help solve this problem but to access
    our secrets stored there, we need to enable our Kubernetes deployments to authenticate
    with AWS **Identity and Access Management** (**IAM**). This is often referred
    to as Workload Identity and it is an approach that is relatively common across
    cloud platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – AWS EKS with Workload Identity](img/B21183_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – AWS EKS with Workload Identity
  prefs: []
  type: TYPE_NORMAL
- en: To set up Workload Identity on EKS, we need to configure the cluster with an
    **OpenID Connect** (**OIDC**) provider. Then, we must set up an IAM role that
    has a policy that allows a Kubernetes service account to assume the role. This
    IAM role can then be granted access to any AWS permissions and resources that
    the Kubernetes deployment needs access to, including Secrets Manager secrets.
    The last thing we need to do is provision a Kubernetes service account by the
    same name within Kubernetes and give it a special annotation to connect it to
    the IAM role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, our Kubernetes deployments will be allowed to access our
    AWS Secrets Manager secrets but they won’t be using that access. The final step
    is to configure the Kubernetes deployment to pull in the secrets and make them
    accessible to our application code running in the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – AWS EKS Secrets Manager integration](img/B21183_08_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – AWS EKS Secrets Manager integration
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has a common practice of doing this using volume mounts. As a result,
    there is a common Kubernetes provider known as the secrets store **Container Storage
    Interface** (**CSI**) provider. This is a cloud-agnostic technique that integrates
    Kubernetes with external secret stores, such as AWS Secrets Manager. This method
    offers enhanced security and scalability, but it requires more setup and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get this working, we need to deploy two components to our EKS cluster: the
    secrets store CSI driver and then the AWS provider for this driver that will allow
    it to interface with AWS Secrets Manager. Both of these components can be deployed
    to our EKS cluster with `SecretProviderClass`. This is a type of resource that
    connects to AWS Secrets Manager through the CSI driver to access specific secrets.
    It connects to specific secrets in Secrets Manager using the service account that
    we granted access to via the IAM role and its permissions.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon EKS offers a managed Kubernetes service that streamlines the deployment
    and management of containerized applications on AWS. The EKS cluster is the central
    figure of this architecture. EKS handles the heavy lifting of setting up, operating,
    and maintaining the Kubernetes control plane and nodes, which are essentially
    EC2 instances. When setting up an EKS cluster, users define node groups, which
    manifest as collections of EC2 instances that the EKS service is responsible for
    provisioning and managing.
  prefs: []
  type: TYPE_NORMAL
- en: There are several options for node groups that can host your workloads. The
    most common examples are AWS-managed and self-managed node groups. AWS-managed
    node groups are essentially on-demand EC2 instances that are allocated for the
    EKS cluster. AWS simplifies the management of these nodes but this imposes some
    restrictions on what AWS features can be used. Self-managed nodes are also essentially
    on-demand EC2 instances but they provide greater control over the features and
    configuration options available to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great way to optimize for cost is to use a Fargate node group. This option
    takes advantage of AWS’ serverless compute engine and removes the need to provision
    and manage EC2 instances. However, this is probably more suitable for unpredictable
    workloads rather than those that require a steady state. In those situations,
    you can take advantage of a combination of autoscaling and spot and reserved instances
    to reap significant discounts and cost reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Anatomy of an AWS EKS cluster](img/B21183_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Anatomy of an AWS EKS cluster
  prefs: []
  type: TYPE_NORMAL
- en: IAM policies are a major part of the configuration of EKS due to the nature
    of the service and how we delegate responsibility to it to manage AWS resources.
    This is similar to what we do with AWS Auto Scaling groups but even more so. IAM
    policies are attached to the cluster and individual node groups. Depending on
    the capabilities you want to enable within your cluster and your node groups,
    you might need additional policies.
  prefs: []
  type: TYPE_NORMAL
- en: The `AmazonEKSClusterPolicy` policy grants the cluster access to control the
    internal workings of the cluster itself, including node groups, CloudWatch logging,
    and access control within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The `AmazonEKSVPCResourceController` policy grants the cluster access to manage
    network resources such as network interfaces, IP address assignment, and security
    group attachments to the VPC.
  prefs: []
  type: TYPE_NORMAL
- en: There are four policies (`AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`,
    `AmazonEC2ContainerRegistryReadOnly`, and `CloudWatchAgentServerPolicy`) that
    are essential for the operation of EKS worker nodes. These policies absolutely
    must be attached to the IAM role that you assign to your EKS node group. They
    grant access to the EKS cluster’s control plane and let nodes within the node
    group integrate with the core infrastructure provided by the cluster, including
    the network, container registries, and CloudWatch. As described previously, we
    also added an optional policy to allow the EKS cluster to manage elastic load
    balancers.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a good idea of what our cloud architecture is going to look
    like for our solution on AWS, we need to come up with a plan on how to provision
    our environments and deploy our code.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud environment configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Building upon the methodology we established in [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365)
    for provisioning EC2 instances, our approach to provisioning the AWS EKS environment
    will follow a similar pattern. The core of this process lies in utilizing GitHub
    Actions, which will remain unchanged in its fundamental setup and operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – The Terraform code provisions the environment on AWS](img/B21183_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – The Terraform code provisions the environment on AWS
  prefs: []
  type: TYPE_NORMAL
- en: However, instead of provisioning EC2 instances as we did previously, the Terraform
    code will be tailored to set up the necessary components for an EKS environment.
    This includes the creation of an EKS cluster and an ECR. The GitHub Action will
    automate the execution of this Terraform code, following the same workflow pattern
    we used before.
  prefs: []
  type: TYPE_NORMAL
- en: By reusing the GitHub Actions workflow with different Terraform scripts, we
    maintain consistency in our deployment process while adapting to the different
    infrastructure requirements of the EKS environment. This step will need to be
    executed in a standalone mode to ensure certain prerequisites are there, such
    as the container registry. Only once the container registry is provisioned can
    we build and push container images to it for our frontend and backend application
    components.
  prefs: []
  type: TYPE_NORMAL
- en: This step will also provision the EKS cluster that hosts the Kubernetes control
    plane. We’ll use this in the final step in conjunction with the container images
    to deploy our application.
  prefs: []
  type: TYPE_NORMAL
- en: Container configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike Packer, which doesn’t rely on any existing infrastructure to provision
    the application deployment artifacts (for example, the AMIs built by Packer),
    our container images need to have a container registry before they can be provisioned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Docker pipeline to build a container image for the frontend](img/B21183_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Docker pipeline to build a container image for the frontend
  prefs: []
  type: TYPE_NORMAL
- en: The workflow is very similar to that of Packer in that we combine the application
    code and a template that stores the operating system configuration. In this case,
    it stores a Dockerfile rather than a Packer template.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we’ve published container images for both the frontend and backend, we’re
    ready to complete the deployment by adding a final step that executes Terraform
    using the Kubernetes provider so that it will deploy our application to the EKS
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Container images as inputs to terraform code, which provisions
    the environment on EKS’ Kubernetes control plane](img/B21183_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Container images as inputs to terraform code, which provisions
    the environment on EKS’ Kubernetes control plane
  prefs: []
  type: TYPE_NORMAL
- en: We will output key pieces of information from the previous Terraform step that
    provisioned the AWS infrastructure. This will include details about the ECR repositories
    and the EKS cluster. We can use these as inputs for the final Terraform execution
    step where we use the Kubernetes provider. We have separated this step into separate
    Terraform workspaces to decouple it from the AWS infrastructure. This recognizes
    the hard dependency between the Kubernetes control plane layer and the underlying
    infrastructure. It allows us to independently manage the underlying infrastructure
    without making changes to the Kubernetes deployments, as well as make changes
    that are isolated within the Kubernetes control plane that will speed up the release
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we reviewed the key changes in our architecture as we transitioned
    from VM-based architecture to container-based architecture. In the next section,
    we’ll get tactical in building the solution, but we’ll be careful to build on
    the foundations we built in the previous chapter when we first set up our solution
    on AWS using VMs powered by EC2.
  prefs: []
  type: TYPE_NORMAL
- en: Building the solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll be taking our theoretical knowledge and applying it to
    a tangible, functioning solution while harnessing the power of Docker, Terraform,
    and Kubernetes on the AWS platform. Some parts of this process will require significant
    change, such as when we provision our AWS infrastructure using Terraform; other
    parts will have minor changes, such as the Kubernetes configuration that we use
    to deploy our application to our Kubernetes cluster; and some will be completely
    new, such as the process to build and push our Docker images to our container
    registry.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, where we built VM images in Packer, there
    is a certain amount of operating system configuration that needs to be set up.
    With Docker, we are doing largely the same thing but we are doing it for a specific
    process. This means much of the work that we did in setting up the service in
    Linux is eliminated because the container runtime controls when the application
    is running or not. This is fundamentally different than configuring the Linux
    operating system to run an executable as a service. As a result, much of this
    boilerplate is eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another major difference is that with the Packer image, we build the application
    outside of Packer and we drop a zipped artifact containing the application as
    part of the Packer build. With Docker, we’ll build the application and produce
    the artifact within the container build process. After this process is complete,
    we’ll follow a similar process where we drop the deployment package into a clean
    container image layer to eliminate any residual build artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line sets the base image for the build stage. It uses the official
    Microsoft .NET SDK image (version `6.0`) from the **Microsoft Container** **Registry**
    (**MCR**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we build the project, we need to resolve its dependencies. The `dotnet
    restore` command will do this by pulling all the dependencies from NuGet (the
    .NET package manager):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we execute the `dotnet publish` command, which creates the binaries for
    the project. The `-c Release` option specifies that the build should be optimized
    for production. We drop the files into the `out` folder to be picked up by a future
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We start a new build stage with the .NET runtime image as the base and we copy
    the binaries that we built from the previous stage to this new one. This will
    ensure that any intermediate build artifacts are not layered into the container
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we set the startup command for the container. When the container starts,
    it will run `dotnet FleetPortal.dll`, which starts our ASP.NET application. It
    will start listening for incoming web server traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in our design, our solution is made up of two application components:
    the frontend and the backend. Each has a code base of application code that needs
    to be deployed. However, with a Kubernetes solution, the infrastructure is simplified
    in that we only need a Kubernetes cluster (and a few other things). The important
    piece is the configuration within the Kubernetes platform itself.'
  prefs: []
  type: TYPE_NORMAL
- en: As a result, much of the Terraform setup is very similar to what we did in the
    previous chapter, so we will only focus on new resources needed for our solution.
    You can check the full source code for this book on GitHub if you want to work
    with the complete solution.
  prefs: []
  type: TYPE_NORMAL
- en: Container registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we’ll set up repositories for both the frontend and backend of our application
    using AWS ECR. To simplify the dynamic creation of our ECR repositories, we can
    set up a local variable called `repository_list` that has constants for the two
    container images we need repositories for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we’ll use a `for` expression to generate a map from this list that we
    can then use to create a corresponding ECR repository using the `for_each` iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll set up an IAM group that we can grant access to push container
    images to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to generate an IAM policy that grants access to each of the ECR
    repositories and attach it to the IAM group we created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we must grant access to this group. We’ll be granting access to the
    identities of developers on our team or the GitHub Actions workflows that will
    be pushing new images as part of our CI/CD process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that our container registry is all set up and we can push images to it,
    we need to set up our Kubernetes cluster. That’s where AWS EKS comes in. The cluster’s
    configuration is relatively simple but there’s quite a bit of work we need to
    do with IAM to make it all work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we provision our EKS cluster, we need to set up the IAM role that it
    will use to interact with the rest of the AWS platform. This is not a role that
    our nodes or Kubernetes deployments will use. It’s the role that EKS will use
    to enact configuration changes made to the cluster across all the AWS resources
    that are being used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the EKS service will assume this role. Hence, the `assume` policy
    needs to allow a principal of the `Service` type with `eks.amazonaws.com` as its
    identifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With this role, we are going to enable EKS to provision and manage the resources
    that it needs within our AWS account. As a result, we need to attach the built-in
    `AmazonEKSClusterPolicy` and `AmazonEKSVPCResourceController` policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is an example of how to do this for one of the policies.
    You could create an `aws_iam_role_policy_attachment` resource for each of the
    policies or use an iterator over a collection of the policies that we need to
    attach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that this IAM role is ready, we can set up our cluster using the `aws_eks_cluster`
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: A significant portion of the configuration is done within the `vpc_config` block,
    which references many of the same structures that we provisioned in the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that you might want to keep in mind is how important the IAM policies
    are for enabling this EKS cluster to be successfully provisioned. Since there
    is no direct relationship between the IAM role’s policy attachments, you should
    ensure that IAM role permissions are created before we attempt to provision the
    EKS cluster. The following code demonstrates the use of the `depends_on` attribute,
    which allows us to define this relationship explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The EKS cluster is just the control plane. For our cluster to have utility,
    we need to add worker nodes. We can do this by adding one or more node groups.
    These node groups will be composed of a collection of EC2 instances that will
    be enlisted as worker nodes. These nodes also need their own IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: A key difference is that because this role will be assumed by the worker nodes,
    which are EC2 instances, the IAM role’s `assume` policy needs to align with this
    fact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as before with our EKS cluster, which needed an IAM role to be set up
    as a prerequisite, the same is true for our node group. Now that the node group’s
    IAM role is ready, we can use the following code to create an EKS node group associated
    with the previously defined cluster. It specifies the desired, minimum, and maximum
    sizes of the node group, along with other configurations, such as the AMI type
    and disk size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, just like with the EKS cluster, the IAM role’s policy attachments are
    critical to making the node group functional. Therefore, you need to make sure
    that all policy attachments are attached to the IAM role before you start provisioning
    our node group. As we discussed in the previous section, there are four policies
    (, `AmazonEKSWorkerNodePolicy`, `AmazonEKS_CNI_Policy`, `AmazonEC2ContainerRegistryReadOnly`,
    and `CloudWatchAgentServerPolicy`) that are essential for the operation of EKS
    worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you add additional features to your EKS cluster, you may introduce additional
    IAM policies that grant the cluster and its worker nodes different permissions
    within AWS. When you do, don’t forget to also include these policies in these
    `depends_on` attributes to ensure smooth operations.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can enable CloudWatch logging on the cluster by simply adding the `enabled_cluster_log_types`
    attribute to the `aws_eks_cluster` resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This attribute takes one or more different log types. I’d recommend checking
    the documentation for all the different options supported. Next, we need to provision
    a CloudWatch log group for the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This requires a specific naming convention and it needs to match the name you
    use for your cluster. Therefore, it’s a good idea to extract the value you pass
    to the `name` attribute of the `aws_eks_cluster` resource as a local variable
    so that you can use it in two places.
  prefs: []
  type: TYPE_NORMAL
- en: Workload identity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the cluster provisioned, we need to get the OIDC issuer certificate from
    the cluster so that we can use it to configure the OpenID Connect provider with
    AWS IAM. The following code uses the `tls_certificate` data source from the `tls`
    utility provider, which we covered in [*Chapter 3*](B21183_03.xhtml#_idTextAnchor185),
    to obtain additional metadata about the certificate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With this additional metadata, we can use the `aws_iam_openid_connect_provider`
    resource to connect the cluster to the AWS IAM OIDC provider by referencing `sts.amazonaws.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve already set up several IAM roles, including one for the EKS cluster and
    another for the worker nodes of the cluster. Therefore, I won’t reiterate the
    creation of the `aws_iam_role` resource for the workload identity. However, this
    new role does need to have a very distinct assumption policy. The workload identity
    IAM role needs to reference the OIDC provider and a yet-to-be-provisioned Kubernetes
    service account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, in the preceding code, the service account follows a very specific
    naming convention: `system:serviceaccount:<namespace>:<service-account-name>`.
    We replace `<namespace>` with the name of the Kubernetes namespace and likewise,
    we replace `<service-account-name>` with the name of the service account. It’s
    important to point out that we are referencing resources that do not exist yet.
    As such, the reference to them within the workload identity IAM role’s assumption
    policy is a pointer or a placeholder to this yet-to-be-created resource. Both
    the Kubernetes namespace and the service account are resources that will need
    to be created within the Kubernetes control plane. We’ll tackle that in the next
    section using the `kubernetes` Terraform provider.'
  prefs: []
  type: TYPE_NORMAL
- en: Secrets management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have an IAM role for our workload identity, we simply need to grant
    it access to the AWS resources we want it to use. Therefore, we will use the `aws_iam_policy_document`
    data source once more to generate an IAM policy that we will attach to the workload
    identity’s IAM role. This is where we have the opportunity to grant it access
    to any resource in AWS that our application code will need. For our solution,
    we’ll start with access to AWS Secrets Manager secrets by granting it access to
    read secrets using the `secretsmanager:GetSecretValue` action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This policy will grant the IAM role access to the secrets within this account.
    We could further refine its access by enhancing the `*` wildcard path to ensure
    that it has access to only certain secrets. This can be done by implementing a
    naming convention that uses a unique prefix for your secrets. The `application_name`
    and `environment_name` variables are a perfect way to implement this naming convention
    and to tighten access to your Kubernetes workloads to AWS Secrets Manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we just need to provision secrets to Secrets Manager with the right naming
    convention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'AWS Secrets Manager uses a parent resource called `aws_secretsmanager_secret`
    as a logical placeholder for the secret itself but recognizes that the secret’s
    value might change over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Those different values for the secret are stored in `aws_secretsmanager_secret_version`
    resources. You can generate complex secrets using the `random` provider but it’s
    probably more common to obtain `secret_string` from the outputs of other resources.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), we introduced Kubernetes
    architecture and automation techniques using YAML and **HashiCorp Configuration
    Language** (**HCL**). In our solutions in this book, we will be using the Terraform
    provider for Kubernetes to automate our application’s deployment. This allows
    us to both parameterize the Kubernetes configurations that would otherwise be
    trapped in hard-coded YAML files and provision a combination of Kubernetes primitives
    and Helm charts with the same deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: Provider setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ironically, the first thing we need to do to set up the `kubernetes` provider
    is initialize the `aws` provider so that we can get information about our EKS
    cluster. We can do that using the data sources provided and a single input variable:
    the cluster’s name. Of course, the AWS region is also an implied parameter to
    this operation but it is part of the `aws` provider configuration rather than
    inputs to the data sources themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use both the `aws_eks_cluster` and `aws_eks_cluster_auth` data sources
    to grab the data we need to initialize the `kubernetes` provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Interestingly, the Helm provider setup is pretty much identical to the Kubernetes
    provider configuration. It seems a bit redundant, but it’s relatively straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Namespace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating the Kubernetes namespace is extremely simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This will act as the logical container for all of the Kubernetes resources that
    we provision for our application.
  prefs: []
  type: TYPE_NORMAL
- en: Service account
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous section, we built one-half of this bridge when we set up the
    OpenID Connect provider configuration within AWS and we specified the Kubernetes
    namespace and service account name ahead of time. Now, we’ll finish constructing
    this bridge by provisioning `kubernetes_service_account` and ensuring that `namespace`
    and `name` match our AWS configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We also need to add an annotation that references the unique identifier (or
    ARN) for the workload identity’s IAM role. We can set this up as an output variable
    in our Terraform workspace that provisions the AWS infrastructure and routes its
    value to an input variable on the Terraform workspace for our Kubernetes configuration.
    This is a great example of how the `kubernetes` provider for Terraform can be
    a useful way of configuring Kubernetes resources that require tight coupling with
    the cloud platform.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets store CSI driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the service account set up, our application is one step closer to being
    able to access our secrets in AWS Secrets Manager. However, before we can do that,
    we need to set up the secrets store CSI driver. As we discussed previously, this
    is a common Kubernetes component that provides a standard mechanism for using
    volume mounts as a way to distribute remotely managed secrets to workloads running
    in Kubernetes. The driver is extremely flexible and can be extended through providers
    that act as adapters for different external secret management systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the secrets store CSI driver Helm chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can optionally enable secret synchronization by using the `syncSecret.enabled`
    attribute to make the secrets accessible from Kubernetes secrets. This makes it
    extremely convenient to inject the secrets into our application’s pods without
    customized code to retrieve them from the mounted volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to install the AWS provider for the CSI driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Both of these Helm charts provision several different Kubernetes resources to
    your cluster under the `kube-system` namespace. If you encounter errors, interrogating
    the pods hosting these components is a good place for you to start debugging your
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Secret provider class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we’ve installed both the CSI driver and its AWS provider, we are ready
    to connect to AWS Secrets Manager. So far, we have only enabled this ability;
    we haven’t exercised it by accessing secrets.
  prefs: []
  type: TYPE_NORMAL
- en: That’s what the `SecretProviderClass` resource is for. It connects to a specific
    set of secrets within AWS Secrets Manager. You’ll notice that the way this type
    of resource is provisioned is different than other resources in Kubernetes. While
    other resource types have a corresponding Terraform resource, `SecretProviderClass`
    uses a `kubernetes_manifest` resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s because this resource type is managed through a Kubernetes **custom
    resource definition** (**CRD**); it’s not a built-in type within Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The structure of `SecretProviderClass` has two parts. First, `parameters` is
    where we declare what secrets we want to bring in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, `objectName` corresponds to either the relative name of the Secrets Manager
    secret or a fully qualified ARN for the secret. Next, `objectType` indicates what
    CSI driver provider should be used to access the secret, while `objectVersionLabel`
    allows us to select a specific version of the secret within Secrets Manager. For
    AWS, to access the latest version (probably the most common use case), you need
    to specify `AWSCURRENT` as the value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, there is a collection of `secretObjects` that’s used to define corresponding
    Kubernetes secret objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: These `secretObjects` will later be used in the deployment specification of
    our application to create environment variables for each secret within the pods.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Kubernetes deployment is one of the most significant resources that we have
    to provision within Kubernetes. As a result, it can be rather intimidating as
    there are several rather complex nested sections. The most important thing going
    on in the deployment is the container specification. This sets up the actual runtime
    environment for our pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important piece of information is the container image we want to use
    in our pods. To configure this, we need to construct the fully qualified path
    to the container image stored in our ECR. To do that, we need two pieces of information.
    First, we need the AWS account number and second, we need the AWS region name
    where our ECR repository is provisioned to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The AWS account number can easily be obtained from the `aws_caller_identity`
    data source. This is an extremely simple data source that provides contextual
    information about the AWS account and IAM identity that Terraform is using with
    the `aws` provider. As a result, to create this data source, you simply create
    it without any parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This is a common pattern for accessing Terraform provider authentication context
    and cloud platform provisioning scope – in this case, what AWS account and what
    region we are provisioning to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the version of the same YAML code converted into HCL using an input
    variable to set different attributes on the entity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The local variable we use for the container image name is the fully qualified
    path to our container image within ECR. It follows the `<account>.dkr.ecr.<region>.amazonaws.com/<repository>:<tag>`
    structure. Here, `<account>` is the AWS account number, which can be accessed
    using the `aws_caller_identity` data source. Then, `<region>` is the AWS region,
    which is accessible from the input variables. Finally, `<repository>` is the ECR
    repository name and `<version>` is the tag for the specific version of the container
    image.
  prefs: []
  type: TYPE_NORMAL
- en: We can set `service_account_name` by referencing other Kubernetes resources
    provisioned within this Terraform workspace. This is a key difference between
    using YAML and the `kubernetes` provider for Terraform. If we were using YAML
    this, would have to be hard-coded, whereas with HCL, we can reference other resources
    within the Terraform workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reference an AWS Secrets Manager secret, we need to modify the `container`
    block so that it includes another `env` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to reference one of the `secretObjects` objects we declared within
    `SecretProviderClass` and give it an environment variable name that our application
    code can reference to access the secret.
  prefs: []
  type: TYPE_NORMAL
- en: Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Kubernetes service is primarily a network routing mechanism. It defines
    the port on which the service should be exposed to external clients and what port
    the network traffic should be forwarded to on the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Here, `selector` specifies which pods traffic should be forwarded to and it
    should match the corresponding pods, with the `app` label set to the same value
    as the service’s selector.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we know from [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), the ConfigMap
    resource is a great way to pass non-sensitive configuration settings to your pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Often, the Terraform workspace that provisions the infrastructure will output
    several different values that need to be included in a Kubernetes ConfigMap (URIs,
    AWS ARNs, DNS, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ingress controller is a component of Kubernetes that routes external network
    traffic into the cluster. It works in conjunction with a Kubernetes ingress, which
    defines specific rules that route traffic for specific services. This is very
    similar to the structure of the CSI driver and `SecretProviderClass`. One provides
    the foundational subsystem, thus enabling the capability, while the other implements
    a specific configuration using that underlying subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular ingress controllers is a load balancer called NGINX.
    We can set up the NGINX ingress controller using a Helm chart. The components
    that are deployed by this Helm chart are why we needed an additional IAM policy
    that allows our EKS cluster to configure AWS ELB resources. That’s because the
    Kubernetes configurations of the ingress controller and ingress resources will
    be interpreted by EKS and manifested as the provisioning and configuration of
    AWS ELB resources. This means that instead of explicitly configuring ELB resources
    using the `aws` Terraform provider, you will be annotating Kubernetes deployments
    and the necessary ELB resources will be provisioned and configured on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is install the NGINX ingress controller using
    a Helm chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This will install NGINX and deploy a Kubernetes service for NGINX running under
    the namespace we specified. The next step is to configure an ingress for our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'An ingress resource is pretty simple. You need to set the namespace and specify
    what ingress controller you want to use. Then, you need to specify paths so that
    you can route network traffic to the correct Kubernetes services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s also pretty important to establish explicit `depends_on` statements for
    the Kubernetes services for the frontend and backend application deployments as
    well as the ingress controller since we don’t reference it directly within the
    HCL configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve built out the three components of our architecture, in the next
    section, we’ll move on to how we can automate the deployment using Docker to build
    and publish the container images and then Terraform to provision our infrastructure
    and deploy our solution to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll shift our focus from building our application and its
    environment to implementing deployment automations to efficiently provision our
    solution to AWS. Container-based architectures involve three core deployment motions.
    First, we must create and publish container images to a container registry. Next,
    we must provision the Kubernetes cluster environment where containers will be
    hosted. Finally, we must deploy the Kubernetes resources that will create the
    containers within Kubernetes pods and reference the container images we published.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like the VM image that we built with Packer in the previous chapter, the container
    image acts as an immutable artifact that contains a versioned copy of the application
    code and operating system configuration. We need to update this artifact every
    time something changes in either the application code or the operating system
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like with Packer, we need to trigger a new container image to be built
    every time the application code and the operating system are configured within
    the Dockerfile itself. With GitHub Actions, we can add a list of `paths` that
    will trigger our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – VM image versioning](img/Image97144.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – VM image versioning
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the triggers and some variables set for our workflow, we need
    to structure `jobs`. For each Packer template, we will have two jobs: one that
    builds the C# .NET application code and produces a deployment package and another
    that runs `packer build` to produce the VM image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The `build` job performs a pretty standard .NET build process, which includes
    restoring package dependencies from NuGet (the .NET package manager), building
    the code, running unit and integration tests, publishing a deployable artifact,
    and storing that artifact so that it can be used by future jobs within the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Docker workflow](img/B21183_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Docker workflow
  prefs: []
  type: TYPE_NORMAL
- en: The `docker` job immediately runs Terraform to obtain outputs of the ECR container
    repository that we want to target. We don’t have to run Terraform here but we
    could explicitly specify the ECR repository’s fully qualified path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, it generates a unique version of the name for the container image that
    will be produced if successful. We’ll generate this image version based on the
    current date and the GitHub Action’s run number. This will guarantee that the
    image version is unique so that we don’t have to manually set it or worry about
    conflicts when pushing to the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to set up Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we must configure our AWS credentials using an official AWS GitHub Action.
    We’ll use an AWS access key and secret access key specified by the GitHub environment
    settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the credential has been configured, we can use the `amazon-ecr-login`
    action to connect to ECR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll build and push the image using an official Docker GitHub Action.
    It’s important to note that this action is not specific to AWS. It uses standard
    container registry protocols to communicate with ECR using the fully qualified
    path to the ECR repository that we specify in the `tags` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Both of our application components (the frontend and the backend) will have
    a repository, so the registry endpoint will be different depending on which container
    image we’re pushing.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), we comprehensively covered
    the process of creating a Terraform GitHub Action that authenticates with AWS.
    Therefore, we won’t be delving into it any further. I encourage you to refer back
    to [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365) to review the process.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we automate Kubernetes with Terraform, we are just running `terraform apply`
    again with a different root module. This time, the root module will configure
    the `kubernetes` and `helm` providers in addition to the `aws` provider. However,
    we won’t create new resources with the `aws` provider; we will only obtain data
    sources from existing resources we provisioned in the previous `terraform apply`
    command that provisioned the infrastructure to AWS.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the GitHub Action that executes this process will look strikingly
    similar to how we executed Terraform with AWS. Some of the variables might change
    to include things such as the container image details and cluster information.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we designed, built, and automated the deployment of a complete
    and end-to-end solution using container-based architecture. We built onto the
    foundations from [*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), where we worked
    with the foundational infrastructure of AWS VPCs but layered on AWS EKS to host
    our application in containers. In the next and final step in our AWS journey,
    we’ll be looking at serverless architecture, moving beyond the underlying infrastructure,
    and letting the platform itself take our solution to new heights.
  prefs: []
  type: TYPE_NORMAL
