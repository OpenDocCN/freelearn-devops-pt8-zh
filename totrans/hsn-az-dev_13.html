<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Real-Time Data Analysis - Azure Stream Analytics</h1>
                </header>
            
            <article>
                
<p>While some Azure components enable us to deliver data to the cloud, in most cases we also need something that is designed for analyzing and querying streamed data. One such service is Azure Stream Analytics, a real-time data analysis tool, which is able to read all messages sent through, for example, Event Hub, and transform, and save them using one of the predefined outputs.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Working with Azure Stream Analytics</li>
<li>Available input and output types</li>
<li>Querying data using the query language</li>
<li>Ensuring the correct order of incoming data and performing checkpoints or replays</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To perform the exercises in this chapter, you will need:</p>
<ul>
<li>Visual Studio 2017 instance</li>
<li>An Azure subscription</li>
<li>Azure Stream Analytics tools—<a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-install">https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-install</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Stream Analytics introduction</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed Azure Event Hub, which is a solution for receiving and processing thousands of messages per second, by introducing the implementation of event processor hosts. While it is great for workloads such as big data pipelines or IoT scenarios, it is not a solution to everything, especially if you want to avoid hosting VMs. Scaling such architectures can be cumbersome and nonintuitive; this is why there is Azure Stream Analytics, which is an event-processing engine designed for high volumes of data. It fills a gap where other services such as Event Hub or IoT Hub do not perform well (or where to do so they require much more skill and/or more sophisticated architecture), particularly for real-time analytics, anomaly detection, and geospatial analytics. It is an advanced tool for advanced tasks, which will greatly improve your cloud and message-processing skills.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stream ingestions versus stream analysis</h1>
                </header>
            
            <article>
                
<p>To get started, we will compare two topics:</p>
<ul>
<li><strong>Stream ingestion</strong>: This is a process where you introduce a service/API for receiving messages from your producers. Such a service is designed to ingest data only—it does nothing more (such as transforming or analyzing). To perform any kind of analysis of ingested data, you have to introduce your own processors.</li>
<li><strong>Stream analysis</strong>: This is a process where you actually analyze the data. You search for anomalies, duplicates, or malformed data, process it, and push it further to other services for storing, presenting, and triggering other actions.</li>
</ul>
<p>To make things even clearer, we can take a look at the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b15af9bc-e1b6-4bf3-9539-95324c45e3d0.png" style="width:37.83em;height:14.42em;" width="543" height="207"/></p>
<p>It shows the four steps of data processing:</p>
<ul>
<li><strong>Produce</strong>: Where data is actually produced by different services, devices, and clients</li>
<li><strong>Ingest</strong>: This is when the data is consumed from different sources</li>
<li><strong>Analyze</strong>: During this step data is analyzed, transformed, and routed to appropriate services and components</li>
<li><strong>Use</strong>: Storing, displaying, and processing data further in other services, such as PowerBI, Azure Functions, and many others</li>
</ul>
<p>While Azure Event Hub or Azure IoT Hub is a part of the ingest step, Azure Stream Analytics is responsible for <strong>analyzing</strong>.</p>
<div class="packt_tip">Note that you are not limited to Azure services when it comes to ingesting data. In such a scenario, you can also use any kind of queue or API, as long as it is capable of processing thousands of events per second.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Stream Analytics concepts</h1>
                </header>
            
            <article>
                
<p>In Azure Stream Analytics, the most important concept is a <strong>stream</strong>. You can think about it as a flow of many events carrying data—they do not necessarily have to be the same or share schema. Analyzing such a stream is not a trivial task. If you have to decode hundreds of thousands of events, the process has to be quick, robust, and reliable. We will discuss the main concepts of this service to verify whether it is capable of acting as our analyzing solution and the main events processor:</p>
<ul>
<li><strong>Fully managed</strong>: Azure Stream Analytics is a fully managed platform as a service(PaaS), so you do not have to worry about provisioning resources and scaling it—the runtime will take care of that, so you can focus on providing optimal queries for data analysis.</li>
<li><strong>An SQL-based query language</strong>: To analyze data, Azure Stream Analytics uses an SQL-based query language, which enables developers to build advanced procedures quickly, which extract from a stream exactly what they want. Additionally, you can bring your own extensions such as ML solutions or user-defined aggregates to perform extra calculations, using tools unavailable to the service.</li>
<li><strong>Performance</strong>: Azure Stream Analytics is focused on <strong>streaming units </strong>(<span><strong>SUs</strong>) </span>instead of some hardcoded values of CPUs or memory. This is because it is designed to provide stable performance and recurrent execution time. What is more, thanks to this concept, you can easily scale your solution to meet your demands.</li>
<li><strong>Low cost of ownership</strong>: In Azure Stream Analytics you pay only for what you choose. As pricing depends on the number of SUs per hour, there is no additional cost to be incorporated in the overall payment.</li>
</ul>
<p>There are also some extra technical concepts (such as input/output types, checkpoints, or replays), which we will cover in the next parts of this chapter. To see the big picture of the whole pipeline using Azure Stream Analytics, please check the following image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/1e589130-614e-4d10-a8d3-2198eb66b992.png" style="width:43.58em;height:33.67em;" width="571" height="441"/></p>
<p>Of course, there could be other references on this picture (additional services, user functions, and analyzers), but for the sake of simplicity, I did not include them.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Input and output types</h1>
                </header>
            
            <article>
                
<p>Azure Stream Analytics offers a seamless integration with some native Azure services, such as Azure Event Hub, Azure IoT Hub, or Azure Blob Storage. Additionally, it can be easily configured to output data to an SQL database, Blob, or Event Azure Data Lake Store. To leverage those possibilities, you will have to define both input and output types, which you are interested in. This allows for data to be easily ingested (in the form of a stream), so a job, which you will write, can work on thousands of events, analyzing and processing them. In this section, you will learn how to get started with Azure Stream Analytics and to define both the inputsand outputs.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Create Azure Stream Analytics in Azure portal</h1>
                </header>
            
            <article>
                
<p>To get started, you will need to create an instance of Azure Stream Analytics. To do so, you have to click on <span class="packt_screen">+ Create a resource</span><strong> </strong>and search for <kbd>Stream Analytics job</kbd>. This will display a form, where you can enter all the necessary data to create a service:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/5e62ebf9-4ae7-4a9e-9fd8-6cd399f8efa2.png" style="width:29.17em;height:36.42em;" width="378" height="472"/></p>
<p>There are two fields, which at first you might overlook:</p>
<ul>
<li><span class="packt_screen">Hosting environment</span>: Azure Stream Analytics can be hosted in two ways: as a native Azure service or deployed to an on-premise IoT Edge gateway device. IoT Edge is a topic beyond the scope of this book, so the natural choice will be <span class="packt_screen">Cloud</span>.</li>
<li><span class="packt_screen">Streaming units (1 to 120)</span>: You have to select how many <span>SUs</span><span> </span>you would like to provision for a job to process your events. The number of required SUs depends on the characteristics of your job, and additionally may vary depending on the input type of your choice. There is a link in the <em>Further reading</em><strong> </strong>section, which describes in detail how many SUs you may need for your job.</li>
</ul>
<div class="packt_infobox">Remember that you will pay <span>€0.093/hour for each SU you choose, even when it is not working on a job.</span></div>
<p>Once you click <span class="packt_screen">Create </span>and open the <span class="packt_screen">Overview </span>blade, you will see an empty dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/989b4ee6-dfd4-4c1a-883a-91dcb9a90482.png" style="width:59.83em;height:46.17em;" width="804" height="620"/></div>
<p>As you can see, both <span class="packt_screen">Inputs </span>and <span class="packt_screen">Outputs</span><strong> </strong>are empty for now—we have to change this, so we can use them in our query. Both of the features are available on the left, in the <span class="packt_screen">JOB TOPOLOGY</span><strong> </strong>section:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/eb0061d0-855e-43dc-89ac-b6deb2a3dc9d.png" width="120" height="182"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adding an input</h1>
                </header>
            
            <article>
                
<p>To add an input, click on the <span class="packt_screen">Inputs </span>blade. It will display an empty screen, where you have two possibilities:</p>
<ul>
<li><span class="packt_screen">+ Add stream input</span>:<span> Here you can add a link to </span>services that enable you to ingest a stream. Currently available Azure components are Azure Event Hub, Azure IoT Hub, and Azure Blob Storage. The inputs can live (or not) in the same subscription, and such a connection supports compression (so you can pass <span>a compressed stream</span> using, for example, GZip or deflate).</li>
<li><span class="packt_screen">+ Add reference input</span>: Instead of ingesting data from a real-time stream, you can also use Azure Blob Storage and add a reference to it, so you can ingest so-called reference data. In that scenario, Azure Stream Analytics will load the whole data into memory, so it can perform lookups on it. It is an ideal solution for static or slowly changing data, and supports data up to the maximum size of 300 MB</li>
</ul>
<p>Here you can find an example of configuring <span class="packt_screen">Event Hub</span> as an input:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/79d74fab-61f3-495c-8db9-9616807fe09d.png" style="width:27.08em;height:51.00em;" width="389" height="733"/></p>
<p>Depending on your choices (whether you have an Event Hub in your subscription or not, whether it exists or not), there will be different options available. In the previous example, I configured a new hub (which was nonexistent) to be the source of my data. There are some fields, however, which I would like to cover now:</p>
<ul>
<li><span class="packt_screen">Event Hub consumer group</span>: If you would like to make Azure Stream Analytics read data from the very beginning, enter a consumer group here. By default, it will use <kbd>$Default</kbd>, which is the default consumer group in Azure Event Hub.</li>
<li><span class="packt_screen">Event serialization format</span>: You can choose from JSON, Avro, and CSV. This allows you to deserialize events automatically, based on the used serialization format.</li>
<li><span class="packt_screen">Event compression type</span>: If you are using GZip or Deflate, here you can choose the right option, so the input will be automatically deserialized.</li>
</ul>
<div class="packt_infobox">Note that you need an actual Azure Event Hub namespace to be able to  create a hub from Azure Stream Analytics automatically.</div>
<p>After filling all the required fields, you will be able to click on the <span class="packt_screen">Create </span>button to initialize the creation of a new input. Of course, you can add more than just one input as they will all be available in the input stream, so you will be able to work with the incoming events. Before you start your job, you will need at least one output, which we are about to add now.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adding an output</h1>
                </header>
            
            <article>
                
<p>To add an output, you have to click on the <span class="packt_screen">Outputs </span>blade. It is similar to the <span class="packt_screen">Inputs </span>one, but there are different kinds of output available:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/6df0fb16-6d94-4ad7-9180-e9f97ea21ce8.png" style="width:37.50em;height:31.00em;" width="513" height="424"/></p>
<p>As you can see, there are many different types of output available, which makes Azure Stream Analytics so flexible when it comes to pushing ingested data to different services. We can divide them into different categories:</p>
<ul>
<li><span class="packt_screen">Storage</span>: SQL database, Blob storage, Table storage, Cosmos DB, and Data Lake Store</li>
<li><span class="packt_screen">Reporting</span>: Power BI</li>
<li><span class="packt_screen">Compute</span>: Azure Functions</li>
<li><span class="packt_screen">Messaging</span>: Event Hub, Service Bus</li>
</ul>
<p>Depending on the category, you will have different options for what you can do with the processed events:</p>
<ul>
<li><span class="packt_screen">Storage</span>: Storing data for further operations, archiving, and event log</li>
<li><span class="packt_screen">Reporting</span>: Near real-time reports </li>
<li><span class="packt_screen">Compute</span>: An easy solution for achieving unlimited integration capabilities</li>
<li><span class="packt_screen">Messaging</span>: Pushing events further for different pipelines and systems</li>
</ul>
<p>Here you can find a configuration for integrating Azure Table storage as an output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/05d434e1-3b7e-4cdf-8b23-5bc4a81c4c3d.png" style="width:31.67em;height:49.58em;" width="389" height="609"/></p>
<p>Available fields depend heavily on the selected output type, so I will not focus on them in this chapter. You can find a reference to them in the <em>Further reading</em><strong> </strong>section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Stream Analytics query language</h1>
                </header>
            
            <article>
                
<p>The strength of Azure Stream Analytics, besides the rich selection of Azure services that seamlessly integrate with it, lies in its query language, which allows you to analyze an input stream easily and output it to a required service. As it is an SQL-like language, it should be intuitive and easy to learn for most developers using this service. Even if you are not familiar with SQL, the many examples available and its simple syntax should make it easy for you.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Writing a query</h1>
                </header>
            
            <article>
                
<p>In the Azure portal, the query window for Azure Stream Analytics can be found either in the <span class="packt_screen">Overview </span>or <span class="packt_screen">Query </span>blade:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ccdb9cfe-a9f2-4d77-9823-e423acf8c87c.png" width="889" height="320"/></p>
<p>In the preceding example, you can see a simple SQL-like query, which performs the following three things:</p>
<ul>
<li>Selects data from the input using the given alias</li>
<li>Chooses the particular columns</li>
<li>Pushes them into a specific output</li>
</ul>
<p>You can also click on the <span class="packt_screen">Edit query</span><strong> </strong>link, so you will be routed to the <span class="packt_screen">Query </span>screen:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/7414db1b-be82-4085-9504-6cd0af031699.png" width="951" height="351"/></p>
<p>As you can see, to be able to actually work with a query, you will need both an input and an output, as without them you will not be able to save it. In general, a query consists of three elements:</p>
<ul>
<li><kbd>SELECT</kbd>: Where you are selecting columns from the input you are interested in</li>
<li><kbd>INTO</kbd>: Where you are telling the engine which output you are interested in</li>
<li><kbd>FROM</kbd>: Where you are selecting an input from which data should be fetched</li>
</ul>
<p>Of course, the preceding statements are not the only ones, which are available—you can use plenty of different options, such as <span class="packt_screen">GROUP BY</span>, <span class="packt_screen">LIKE</span>, or <span class="packt_screen">HAVING</span>. It all depends on the input stream and the schema of incoming data. For some jobs, you may only need to perform a quick transformation and extract the necessary columns; for others, you might require more sophisticated syntax for getting exactly what you want. You will find common query patterns in the link in the <em>Further reading</em><strong> </strong>section. In the preceding example, in the <kbd>SELECT</kbd> part of the query, I have selected three columns, which are available when analyzing Azure Event Hub events. What is more, I used the <kbd>AS </kbd> construct to tell the engine to actually rename fields to match those defined in the <span class="packt_screen">Outputs </span>section. When I run my job, I can see that it actually passes events to my table:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/79802b23-11fa-4186-8a01-2d5182fb8ea7.png" width="1233" height="300"/></p>
<p>However, there are some problems with the current setup:</p>
<ul>
<li>We rely on the Event Hub fields, which might change in the future.</li>
<li>We are missing the actual data of an event.</li>
<li>There are duplicated columns.</li>
</ul>
<p>Let's assume each event has the following structure:</p>
<pre> {"Id":"165e0206-8198-4f21-8a6d-ad2041031603","Date":"2018-09-02T12:17:48.3817632+02:00"}</pre>
<p>Of course, particular data changes over time. We can quickly change the query:</p>
<pre>SELECT<br/>    PartitionId,<br/>    Id,<br/>    Date<br/>INTO<br/>    [handsonazure-tablestorage]<br/>FROM<br/>    [handsonazure-alias]</pre>
<p><span>And adapt the</span> configuration to change the output a little bit:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/9668fd25-804b-4ef7-8cfa-e45792b367cf.png" width="1065" height="278"/></p>
<p>However, the basic constructs are only a few percent of the overall capability of the service. There are also inbuilt functions, which can be easily used in each query to enhance it, as follows:</p>
<ol>
<li>Mathematical functions:</li>
</ol>
<pre style="padding-left: 90px;"><span class="hljs-keyword">SELECT</span><span> </span><span class="hljs-keyword">FLOOR</span><span>(</span><span class="hljs-keyword">input</span><span>.x) </span><span class="hljs-keyword">AS</span><span> </span><span class="hljs-string">"The FLOOR of the variable x"</span><span> </span><span class="hljs-keyword">FROM</span><span> </span><span class="hljs-keyword">input<br/>SELECT<span> </span>SQUARE<span>(</span>input<span>.x) </span>AS<span> </span><span class="hljs-string">"The SQUARE of the variable x"</span><span> </span>FROM<span> </span>input<span> </span><br/></span></pre>
<ol start="2">
<li>Aggregate functions:</li>
</ol>
<pre style="padding-left: 90px;"><span class="hljs-keyword">SELECT</span><span> </span><span class="hljs-keyword">COUNT</span><span>(*) </span><span class="hljs-keyword">FROM</span><span> </span><span class="hljs-keyword">Input<br/>SELECT<span> </span>SUM<span> (Income) </span>FROM<span> </span>Input<br/>SELECT<span> </span>AVG<span> (Income) </span>FROM<span> </span>Input<br/></span></pre>
<ol start="3">
<li>Analytic functions:</li>
</ol>
<pre style="padding-left: 90px;"><span class="hljs-keyword">SELECT</span><span> ISFIRST(mi, </span><span class="hljs-number">10</span><span>) </span><span class="hljs-keyword">as</span><span> </span><span class="hljs-keyword">first</span><span> </span><span class="hljs-keyword">FROM</span><span> </span><span class="hljs-keyword">Input</span><span> </span></pre>
<ol start="4">
<li>Geospatial functions:</li>
</ol>
<pre style="padding-left: 90px;"><span class="hljs-keyword">SELECT</span><span> ST_DISTANCE(</span><span class="hljs-keyword">input</span><span>.pos1, </span><span class="hljs-keyword">input</span><span>.pos2) </span><span class="hljs-keyword">FROM</span><span> </span><span class="hljs-keyword">input</span><span> </span></pre>
<ol start="5">
<li>String functions:</li>
</ol>
<pre style="padding-left: 90px;"><span class="hljs-keyword">SELECT</span><span> </span><span class="hljs-keyword">SUBSTRING</span><span> (SerialNumber ,</span><span class="hljs-number">1</span><span>,</span><span class="hljs-number">3</span><span> ), </span><span class="hljs-keyword">FROM</span><span> </span><span class="hljs-keyword">Input</span></pre>
<p>In addition to these, there are some more such as record functions, date/time functions, conversion, or array functions. The preceding examples are of course not all the available functions. You can find them all in the <em>Further reading</em><strong> </strong>section. The important thing here is that some functions are deterministic (this means that they always return the same result if the same input values are used), and some are not—this is especially important when handling high loads and trying to avoid possible anomalies.</p>
<div class="packt_tip">Remember, you can merge different streams of data and push them to a single output (or vice versa—have a single input and distribute it to multiple outputs). This is a very powerful feature of this service, which makes ingesting and processing data much easier.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Event ordering, checkpoints, and replays</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we covered some basic topics of Azure Stream Analytics: how to configure inputs and outputs, querying data, and using the service. In the last part of this chapter, I will show you its more advanced features such as event ordering, checkpoints, and replays, which ensure that events are processed exactly in a way you would expect. These topics are in fact common subjects in many different messaging solutions, so you will be able to use knowledge from this chapter in your other projects.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Event ordering</h1>
                </header>
            
            <article>
                
<p>There are two concepts of events when it comes to their ordering:</p>
<ul>
<li>Application (or event) time</li>
<li>Arrival time</li>
</ul>
<p>There is a clear distinction between them:</p>
<ul>
<li><strong>Application time</strong>: This is a timestamp when an event was generated on the client (or application) side. It tells you exactly when it occurred.</li>
<li><strong>Arrival time</strong>: This is a system timestamp, which is not present in the original payload. It tells you when an event was received by a service and picked up for processing.</li>
</ul>
<p>Depending on the input type, arrival time and application time will be different properties (<kbd>EventEnqueuedUtcTime </kbd>or <kbd>EnqueuedTime </kbd>for arrival time, whereas application time, in general, will be a generic property). What you have to remember is, depending on the selected scenario, you can process events as they come but out of order, or in order but delayed. This can be easily described using the following event sequence:</p>
<ol>
<li><strong>Arrival</strong>: <kbd>2018-09-02T12:17:49</kbd> <strong>Application</strong>: <kbd>2018-09-02T12:17:48</kbd></li>
<li><strong>Arrival</strong>: <kbd>2018-09-02T12:17:50</kbd> <strong>Application</strong>:<kbd> 2018-09-02T12:17:44</kbd></li>
<li><strong>Arrival</strong>: <kbd>2018-09-02T12:17:51</kbd> <strong>Application</strong>:<strong> </strong><kbd>2018-09-02T12:17:46</kbd></li>
</ol>
<p>If you process events as they come into the stream, they will be processed <strong>out of order</strong>—in fact, they occurred in a different order, so there is a possibility that some data will be outdated. The other option is to sort events by application time; in such a scenario, the process will be delayed, but the order will be preserved.</p>
<div class="packt_tip">Whether you need to or not, processing events in order depends on the data schema and characteristics of the processed events. Processing them in order is more time-consuming, but sometimes you just cannot do it the other way.</div>
<p>Azure Stream Analytics has a feature named <span class="packt_screen">Event ordering</span>, which allows you to make a decision about what to do with events, which are either out of order or outdated:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/053f17d5-6cee-4585-a4ce-77f90fc5ddf0.png" width="687" height="448"/></div>
<p>There are two options available:</p>
<ul>
<li><span class="packt_screen">Events that arrive late</span>: This one allows you to process outdated events (for which the application time does not match the one processed as the last one) within a defined time window.</li>
<li><span class="packt_screen">Out of order events</span>: It is possible that Azure Stream Analytics consider some of your events to be  out of order (this situation could happen, for instance, if your senders' clocks are skewed). Here you can set a time window, during which this situation is acceptable).</li>
</ul>
<p>Additionally, you can define an action, which will be performed if an event either arrived late or was out of order—<span>for <span class="packt_screen">Drop</span>, it will simply be removed, and if you select <span class="packt_screen">Adjust</span>, processing</span> will be suspended for some time when such situations occur.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Checkpoints and replays</h1>
                </header>
            
            <article>
                
<p>In fact, Azure Stream Analytics is a stateful service, which is able to track the event-processing progress. This makes it suitable for the following:</p>
<ul>
<li>Job recovery</li>
<li>Stateful query logic</li>
<li>Different job start modes (now, custom, and when last stopped)</li>
</ul>
<p>Of course, there is a difference between what is possible after the checkpoint and when a replay is necessary. There are situations when the data stored within a checkpoint is not enough, and the whole replay is required; however, this may differ depending on your query. In fact, it depends on the query parallelization factor and can be described using the following formula:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><span><em>[The input event rate] x [The gap length] / [Number of processing partitions]</em></span></p>
<p>The more processors you have, the faster you can recover when something goes wrong. A good rule of thumb is to introduce more SUs in case your job fails and you have to close the gap quickly.</p>
<div class="packt_tip">The important thing to consider when replaying data is the use of window functions in your queries (tumbling, hopping, sliding, or session)—they allow you to process data in different kinds of windows, but complicate the replay mechanism.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered Azure Stream Analytics, a service for processing streams of data in near real time. You have learned what the available inputs and outputs are and how to configure them. What is more, you were able to write your first query, and check how the query language works for analyzing and processing incoming events. If you need a PaaS  that can quickly read and transform events and push them to many different Azure services, Azure Stream Analytics is for you.</p>
<p>In the next chapter, we will go through Azure Service Bus, an enterprise-class messaging solution that is in fact the foundation of Azure Event Hub, which we discussed previously.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the payment model for Azure Stream Analytics?</li>
<li>What is the difference between a stream and the reference output?</li>
<li>What is the difference between application and arrival time?</li>
<li>Which query construct do you need to select an ID from an input and push it to an output?</li>
<li>Can you process different inputs in the same query?</li>
<li>When is an event considered out of order?</li>
<li>Is it possible to get a substring from a property in a query? If so, which function can be used for that?</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>Scaling and <span>SUs</span>: <a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption">https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption</a></li>
<li>Different output types: <a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs">https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs</a></li>
<li>Common query patterns: <a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns">https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns</a></li>
<li>Window functions: <a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions">https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions</a></li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>