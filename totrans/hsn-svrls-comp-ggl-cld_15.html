<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building a PDF Conversion Service</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we have discussed the relative merits of working with serverless computing on Google Cloud. Over the course of this chapter and the next, we will look at a case study to explore how a solution might be deployed. Working through the examples will illustrate how to use many of the techniques we've previously discussed. </p>
<p>In order to do this, we will use an example case study based on Pet Theory, a hypothetical veterinary practice that is making the transition to using serverless technology on Google Cloud. </p>
<p>Over the course of this chapter, we will discuss the following topics:</p>
<ul>
<li>Designing a document service</li>
<li>Developing a d<span>ocument service</span></li>
<li><span>Developing a Cloud Run service</span></li>
<li>Securing service access</li>
<li>Testing the document access</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To complete the exercises in this chapter, you will require a Google Cloud project or a Qwiklabs account.</p>
<p class="mce-root">You can find the code files of this chapter in the GitHub repository for this book, under the <span><kbd>ch11</kbd> </span>subdirectory, at <a href="https://github.com/PacktPublishing/Hands-on-Serverless-Computing-with-Google-Cloud/tree/master/ch11">https://github.com/PacktPublishing/Hands-on-Serverless-Computing-with-Google-Cloud/tree/master/ch11</a>.</p>
<div class="mce-root packt_infobox">While you are going through the code snippets in this book, you will notice that, in a few instances, a few lines from the codes/outputs have been removed and replaced with ellipses (<kbd>...</kbd>). The use of ellipses is only to show relevant code/output. The complete code is available on GitHub at the link mentioned previously.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pet Theory case study overview</h1>
                </header>
            
            <article>
                
<p>For our case study, we will be exploring the Pet Theory veterinary practice. In this scenario, the business is looking to transition to serverless technology and provide some practical methods to incorporate Google Cloud and its products within the business. </p>
<p>The full Pet Theory case study incorporates a number of different scenarios that demonstrate how to resolve typical real-world issues with serverless technology. To view the complete scenario, visit the Qwiklabs site and search for <em>Serverless Workshop: Pet Theory Quest</em> (<a href="https://www.qwiklabs.com/quests/98">https://www.qwiklabs.com/quests/98</a>) to see the associated labs.</p>
<p>Over the course of this chapter and the next, we will be working through two lab examples that illustrate the power and flexibility of serverless technologies. In the first example in this chapter, we will look at how Pet Theory deals with moving to a unified document process based on the automatic conversion of documents. To begin our review, we will outline what the proposed architecture is attempting to achieve and the component roles and requirements involved.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing a document service</h1>
                </header>
            
            <article>
                
<p>Pet Theory has an issue with its existing process for document management. Currently, they use multiple document formats within the business and want to rationalize this to use a single unified approach. After some consultation, they decided to transition to <strong>Portable Document Format</strong> (<strong>PDF</strong>) so that they could continue to use rich media when sending information electronically to their clients and suppliers.</p>
<p>In terms of requirements, the Pet Theory team have decided they need a system capable of the following:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 34%">
<p><strong>Requirement</strong></p>
</td>
<td style="width: 29.3621%">
<p><strong>Component</strong></p>
</td>
</tr>
<tr>
<td style="width: 34%">
<p>Storing document information</p>
</td>
<td style="width: 29.3621%">
<p>Storage</p>
</td>
</tr>
<tr>
<td style="width: 34%">
<p>Handling processing requests</p>
</td>
<td style="width: 29.3621%">
<p>Processing</p>
</td>
</tr>
<tr>
<td style="width: 34%">
<p>Securing service access</p>
</td>
<td style="width: 29.3621%">
<p>Security</p>
</td>
</tr>
<tr>
<td style="width: 34%">
<p>Processing document conversions</p>
</td>
<td style="width: 29.3621%">
<p>Service</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Now we have the high-level requirements for the application, we should add to our understanding by describing each requirement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing document information</h1>
                </header>
            
            <article>
                
<p>From the preceding information, we know that storage is a key component in our architecture, as the documents will need to be stored somewhere. We also know that <span>Google Cloud</span> has a number of products that would suit this type of requirement. </p>
<p>Typically, the obvious choice for storage is to use a disk or shared filesystem to persist the data to be processed. However, in this instance, this type of storage may not be the best option for the situation presented. The general remit is to minimize the maintenance of the infrastructure required, and creating a traditional database on managed infrastructure would not meet this requirement.  </p>
<p>Thinking about what we have learned regarding <span>Google Cloud</span> Storage, we know that short-term object storage can be used for transient information. We also know that Cloud Storage provides an event framework that supports interaction with serverless computing. On that basis, Cloud Storage seems like a good candidate that is capable of meeting our storage requirement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling processing requests</h1>
                </header>
            
            <article>
                
<p>Processing requests can be handled in a number of ways. In this example, we are going to build a solution that can scale on demand. To that end, we want to decouple the storage requirement from the processing component, meaning we need to add some middleware capable of efficiently handling the information. </p>
<p>In our application, we have already decided to utilize Cloud Storage and therefore we will need a mechanism to asynchronously communicate this service with the backend processing to be performed. The application needs to work autonomously as much as possible to minimize both user interaction and the potential for errors. </p>
<p>Adding Cloud Pub/Sub to the application enables the application to define a consistent and scalable messaging service. The service can consume information from Cloud Storage and propagate this information to the component performing the backend processing. At this point, you may be thinking that Cloud Tasks would be a good alternative for this solution. However, the use case for Cloud Tasks states that it is better suited to a scenario where control of execution timing (that is, rate-limiting) is required. In our scenario, we do not need that level of control over execution. We have a need for general data ingestion and distribution for which the Cloud Pub/Sub product is better suited.</p>
<p>When using Cloud Pub/Sub, we will need to provide information relating to a topic that provides the information to be transported. In our application, this will contain information relating to the file that's uploaded to the Cloud Storage bucket. To consume information relating to a topic, we will also create a subscription that will receive an event notification based on a new message being added to the topic. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Securing service access</h1>
                </header>
            
            <article>
                
<p>As part of our requirements, we also need to implement security permissions for service access to ensure that only authorized accounts can invoke the new service. Our assumption with this service is that we can use an account to manage the necessary permissions. Over the course of the chapters relating to Cloud Functions and Cloud Run, we have seen how valuable these service accounts are for non-interactive solutions. To achieve this requirement, we can use service accounts that will be allocated the necessary permissions associated with the appropriate roles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Processing document conversations</h1>
                </header>
            
            <article>
                
<p>In order to select a service capable of performing the document conversion, take a moment to recap the relative merits of the serverless options available on <span>Google Cloud</span>:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Product</strong></p>
</td>
<td>
<p><strong>Scenario</strong></p>
</td>
<td>
<p><strong>Typical use case</strong></p>
</td>
</tr>
<tr>
<td>
<p>App Engine</p>
</td>
<td>
<p>Serverless HTTP applications</p>
</td>
<td>
<p>Web applications</p>
</td>
</tr>
<tr>
<td>
<p>Cloud Functions</p>
</td>
<td>
<p>Serverless functions and events</p>
</td>
<td>
<p>Event-driven functions</p>
</td>
</tr>
<tr>
<td>
<p>Cloud Run</p>
</td>
<td>
<p>Serverless HTTP containers</p>
</td>
<td>
<p>Fully managed HTTP request-response containers</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In addition to the original requirements, the Pet Theory team want to be able to do the following:</p>
<ul>
<li style="font-weight: 400">Process information in a scalable manner</li>
<li style="font-weight: 400">Leverage existing code/application where practical</li>
<li style="font-weight: 400">Minimize application maintenance</li>
<li style="font-weight: 400">Utilize in-house knowledge and skills as much as possible</li>
</ul>
<p>A team member has identified that the LibreOffice application can handle the conversion of documents to PDF. Packaging this application as a Docker image will allow the team to easily reuse and standardize the integration within their project. </p>
<p>From the preceding paragraphs, it seems that Cloud Run is a good fit for the current requirements. As we have learned, Cloud Run supports stateless HTTP and custom libraries. Furthermore, the established integration with services such as Cloud Pub/Sub means this is perfect for our proposed solution. Once the initial build has been tested and verified, the solution can be further enhanced by utilizing Google Developer tools such as Cloud Build to achieve continuous integration.</p>
<p>Awesome job! We have walked through a high-level analysis of requirements, broken it down into components, and now have the essence of a solution. To confirm our understanding, the following diagram covers the solution we have designed:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-847 image-border" src="assets/b86663ee-371e-4ded-a94e-a1f1dfd66bdf.png" style=""/></div>
<p>In this diagram, we have defined three distinct stages to be run on <span>Google Cloud</span> that represent the components we previously defined in order to manage our document processing. Our architecture can be described by the following steps:</p>
<ol>
<li style="font-weight: 400">A source document uploads to system storage (that is, a Cloud Storage bucket).</li>
<li style="font-weight: 400">A life cycle event is triggered and generates a new payload for the Pub/Sub topic.</li>
<li style="font-weight: 400">A Pub/Sub subscription polls for new data notifications.</li>
<li style="font-weight: 400">A Cloud Run service processes the uploaded content and creates a PDF.</li>
</ol>
<p>Note that in the preceding diagram, the main boxes are general abstractions to indicate service isolation. For our simple service, this should help clarify each stage of processing and the responsible component. In addition, we can also see that the majority of processing does not require the creation of code to handle storage event notifications and the message queue data objects. Our efforts are largely focused on the creation of the service for which we have sensibly opted for a preexisting application to handle the PDF conversion. </p>
<p>Based on this discussion, we now have a general understanding of the PDF creation process for documents that are submitted to our service. In the next section, we will start to look at the practical elements of developing the PDF service in order to fulfill the requirements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a document service</h1>
                </header>
            
            <article>
                
<p>In the previous section, we outlined an architecture for our serverless application. During this analysis phase, we worked out the high-level components required and then theorized on the type of activities required. Creating a document service requires the creation of a Cloud Run service to consume information from a Cloud Pub/Sub subscription. As we have chosen to minimize the code development process, our productivity has been significantly increased. Code for complex notifications and message queues has been deferred to existing mechanisms managed by <span>Google Cloud</span>. Instead, we will concentrate on building only the specific element needed for our requirements, for example, PDF conversion. </p>
<div class="packt_infobox"><br/>
In your Google Cloud project, open Cloud Shell and make sure a clone of the lab repository for <kbd>Chap11</kbd> is available.  </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing document information</h1>
                </header>
            
            <article>
                
<p>To get the project started, we need to create the storage for the application. In the following diagram, we can see that the solution uses two storage buckets. The first bucket stores document uploads while the second stores the output, that is, the processed<span> PDF </span>file:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-848 image-border" src="assets/e0a3ed35-cc7d-43a5-b518-aa6f7c3a9d14.png" style=""/></div>
<p>Creating storage buckets on Google Cloud is straightforward, and at this point should be a familiar activity, whether it's performed from the console or Cloud Shell. For this example, we will use Cloud Shell to complete the task:</p>
<ol>
<li style="font-weight: 400">Create a multi-region storage bucket for uploaded files:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong><span>gsutil mb gs://$GOOGLE_CLOUD_PROJECT-upload</span></strong></pre>
<ol start="2">
<li style="font-weight: 400">Create a multi-region storage bucket for processed files:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong><span>gsutil mb gs://$GOOGLE_CLOUD_PROJECT-processed</span></strong></pre>
<p class="mce-root" style="padding-left: 60px"><span>Google Cloud Storage provides the ability to enable a notification event linked to Pub/Sub. The event notification system is extremely powerful and will be used to raise a new message for each document that's deposited in the upload bucket.</span></p>
<p style="padding-left: 60px">Take a look at the following architecture diagram, which shows that we automatically receive a notification when data is added to the upload bucket. Linking Cloud Storage and Cloud Pub/Sub together in this way provides a sensible usage pattern that can be used in other projects to indicate the availability of data:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-849 image-border" src="assets/535fa8df-7e39-4a46-8162-bb332025837c.png" style=""/></div>
<p style="padding-left: 60px">Now, whenever a file is deposited in the upload bucket, a new Pub/Sub topic message will be queued, indicating that new content has been made available. </p>
<p style="padding-left: 60px">To set up the notification mechanism on the existing bucket, we use the <kbd>gsutil</kbd> command. The command will need to know what notification is to be triggered and also that a new topic is to be created for the upload bucket. </p>
<ol start="3">
<li style="font-weight: 400">Create a Pub/Sub topic notification called <kbd>new-doc</kbd> when a file is added to the upload bucket:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong><span>gsutil notification create -t new-doc -f json -e OBJECT_FINALIZE gs://$GOOGLE_CLOUD_PROJECT-upload</span></strong></pre>
<p>In the preceding command, we use the <kbd>OBJECT_FINALIZE</kbd> command, which indicates a new object being presented to a Google Cloud Storage bucket. Take note that the information generated uses the JSON format to pass information to the <kbd>new-doc</kbd> topic.</p>
<div class="packt_infobox"><br/>
We will only receive a notification about the file that's been uploaded after deploying the PDF service (as there is no active subscription for this new topic).</div>
<p><span>Great work! We now have the data storage and stream processing services up and running. Next, we will see how to build a PDF service using Cloud Run.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a Cloud Run service</h1>
                </header>
            
            <article>
                
<p>Building a service can be both daunting and challenging. In this section, we will walk through an example of how to build a service based on an existing application. In this instance, we will be using LibreOffice to create a PDF. </p>
<p>One thing to mention about this example is how an external application can easily be integrated to build a simple service. Applications (specifically Linux) encompass a lot of versatility, which means they typically offer a great way to extend and incorporate these within a solution. For example, Inkscape can be used to convert SVG into PNG, while Calibre can convert EPUB into PDF. The bottom line, before going down the route of developing an application, is to investigate what is possible using already-existing applications.</p>
<p>Going back to the creation of our service, to build our application, we will encapsulate LibreOffice in a Docker image. The information for the service is provided through a Cloud Pub/Sub subscription that encapsulates the Cloud Storage notification event.</p>
<p>Our application will be built using Node.js and will demonstrate how to access the message queue information. Although the code base is relatively short, it demonstrates how to integrate external applications with Cloud Run. Remember when exploring alternative applications that Cloud Run applications are meant to be stateless, as well as utilize HTTP. </p>
<p>The document service receives HTTP through the subscription interface to Cloud Pub/Sub, which means this provides a straightforward mechanism to exchange data between products. Additionally, state information is not required for the request-response life cycle associated with the data exchange. Processing the file with LibreOffice will, however, use the <kbd>/tmp</kbd> directory allocated to Cloud Run to temporarily hold information on the output file. </p>
<p>To enable Cloud Run to perform document conversions, we are going to consume notification messages derived from Cloud Storage, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-850 image-border" src="assets/dc539cab-8e33-4ab6-8531-3d0cfcede04b.png" style=""/></div>
<p>Building the Cloud Run service incorporates a Node.js application to process the information, as well as the creation of a Docker container. As in previous examples, we start with the <kbd>package.json</kbd> file and install packages to enable access to additional resources (for example, Cloud Storage and Express.js).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing the service  </h1>
                </header>
            
            <article>
                
<p>First, we need to populate our configuration files with the correct information:</p>
<ol>
<li style="font-weight: 400">Amend the <kbd>package.json</kbd> file to add a <kbd>start</kbd> script, as follows:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">...<br/>"scripts": {<br/> "start": "node index.js",<br/> "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"<br/> },<br/>...</pre>
<ol start="2">
<li style="font-weight: 400">Add the packages used by the conversion process:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>npm install express</strong><br/><strong>npm install body-parser</strong><br/><strong>npm install child_process</strong><br/><strong>npm install @google-cloud/storage</strong></pre>
<ol start="3">
<li style="font-weight: 400">Edit<span> the <kbd>index.js</kbd> file to add the required references and additional code. </span>Add the following package requirements to the top of the code file:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">const {promisify} = require('util');<br/>const {Storage} = require('@google-cloud/storage');<br/>const exec = promisify(require('child_process').exec);<br/>const storage = new Storage();</pre>
<ol start="4">
<li style="font-weight: 400">Replace the existing <kbd>app.post</kbd> function with the code outlined, as follows:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">app.post('/', async (req, res) =&gt; {<br/> try {<br/> const file = decodeBase64Json(req.body.message.data);<br/> await downloadFile(file.bucket, file.name);<br/> const pdfFileName = await convertFile(file.name);<br/> await uploadFile(process.env.PDF_BUCKET, pdfFileName);<br/> await deleteFile(file.bucket, file.name);<br/> }<br/> catch (ex) {<br/> console.log(`Error: ${ex}`);<br/> }<br/> res.set('Content-Type', 'text/plain');<br/> res.send('\n\nOK\n\n');<br/>})</pre>
<ol start="5">
<li style="font-weight: 400">Add the <kbd>downloadFile</kbd> <span>function:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px">async function downloadFile(bucketName, fileName) {<br/> const options = {destination: `/tmp/${fileName}`};<br/> await storage.bucket(bucketName).file(fileName).download(options);<br/>}</pre>
<p class="mce-root" style="padding-left: 60px"><span>The next function is where the magic happens in our program. From the following code snippet, we can see that LibreOffice is called in a headless state (that is, with no graphical interface) to generate a PDF. The resultant file is stored in the <kbd>/tmp</kbd> directory for postprocessing.</span></p>
<ol start="6">
<li style="font-weight: 400">Add the <kbd>convertFile</kbd> function:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">async function convertFile(fileName) {<br/> const cmd = 'libreoffice --headless --convert-to pdf --outdir /tmp ' +<br/> `"/tmp/${fileName}"`;<br/> console.log(cmd);<br/> const { stdout, stderr } = await exec(cmd);<br/> if (stderr) {<br/> throw stderr;<br/> }<br/> console.log(stdout);<br/> pdfFileName = fileName.replace(/\.\w+$/, '.pdf');<br/> return pdfFileName;<br/>}</pre>
<ol start="7">
<li style="font-weight: 400">Add the <kbd>deleteFile</kbd> function:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">async function deleteFile(bucketName, fileName) {<br/> await storage.bucket(bucketName).file(fileName).delete();<br/>}</pre>
<ol start="8">
<li style="font-weight: 400">Add the <kbd>uploadFile</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">async function uploadFile(bucketName, fileName) {<br/> await storage.bucket(bucketName).upload(`/tmp/${fileName}`);<br/>}</pre>
<p>Nice work! We now have an application capable of generating a PDF from an object passed to it. The next step is to create a Docker image that will run the application when the container is run.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the service</h1>
                </header>
            
            <article>
                
<p>To create a Dockerfile manifest for the application, follow these steps:</p>
<ol start="1">
<li style="font-weight: 400">Add the definition to install the <kbd>libreoffice</kbd> package to the Dockerfile:</li>
</ol>
<pre style="padding-left: 60px">FROM node:12<br/>RUN apt-get update -y \<br/> &amp;&amp; apt-get install -y libreoffice \<br/> &amp;&amp; apt-get clean<br/>WORKDIR /usr/src/app<br/>COPY package.json package*.json ./<br/>RUN npm install --only=production<br/>COPY . .<br/>CMD [ "npm", "start" ]</pre>
<ol start="2">
<li style="font-weight: 400">From the command line, build the service, create an image from the manifest, and store it in <kbd>gcr.io</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>gcloud builds submit \</strong><br/><strong> --tag gcr.io/$GOOGLE_CLOUD_PROJECT/pdf-converter</strong></pre>
<ol start="3">
<li style="font-weight: 400">Once the build process has successfully completed, deploy the service:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>gcloud beta run deploy pdf-converter \</strong><br/><strong> --image gcr.io/$GOOGLE_CLOUD_PROJECT/pdf-converter \</strong><br/><strong> --platform managed \</strong><br/><strong> --region us-central1 \</strong><br/><strong> --memory=2Gi \</strong><br/><strong> --no-allow-unauthenticated \</strong><br/><strong> --set-env-vars PDF_BUCKET=$GOOGLE_CLOUD_PROJECT-processed</strong></pre>
<p style="padding-left: 60px">The preceding properties for Cloud Run provide an overview of the key attributes associated with a typical deployment. We have increased the memory allocated for Cloud Run as the conversion process is memory intensive. </p>
<ol start="4">
<li style="font-weight: 400">Create a new environment variable to hold the <kbd>SERVICE_URL</kbd> parameters:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>SERVICE_URL=$(gcloud beta run services describe pdf-converter --platform managed --region us-central1 --format "value(status.url)")</strong></pre>
<p class="mce-root"><span>We now have an image built and stored in Google Cloud. The image is maintained in the Container Registry and the artifact can be deployed with additional properties to specify additional memory and the region in which it will be run. Notably, when the instance is run, we need to specify an environment variable that names the bucket that data will be output to. </span></p>
<p>As the final step for this build and deployment section, we capture the service URL assigned to the deployed Cloud Run instance. Capturing the service URL enables the user to access the service via an environment variable. We will use this environment variable later on in this exercise. </p>
<p>Congratulations – there now exists a service on Google Cloud capable of automatically creating a PDF based on an uploaded file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Securing service access</h1>
                </header>
            
            <article>
                
<p>With the service enabled successfully, we will turn our attention to securing access. To achieve this, we will be creating a new service account with the specific task of managing the invocation of the new service.</p>
<p>Thinking back to the original design, the service is actually invoked by Cloud Pub/Sub rather than a user. As we have chained together a series of events, we can take advantage of this to minimize the external sources that are able to initiate our new service. The following steps illustrate how to create a service account tasked with the invocation of a new Cloud Run PDF service instance. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a service account </h1>
                </header>
            
            <article>
                
<p>To create a service account, follow these simple steps:</p>
<ol>
<li style="font-weight: 400">Create a new service account:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>gcloud iam service-accounts create pubsub-cloud-run-invoker --display-name "PubSub Cloud Run Invoker"</strong></pre>
<ol start="2">
<li style="font-weight: 400">Give the service account permission to invoke Cloud Run:</li>
</ol>
<pre style="padding-left: 60px"><strong>gcloud beta run services add-iam-policy-binding pdf-converter --member=serviceAccount:pubsub-cloud-run-invoker@$GOOGLE_CLOUD_PROJECT.iam.gserviceaccount.com --role=roles/run.invoker --region us-central1</strong></pre>
<ol start="3">
<li style="font-weight: 400">Create an environment variable to hold the <kbd>PROJECT_ID</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>PROJECT_NUMBER=$(gcloud config get-value project)</strong></pre>
<ol start="4">
<li style="font-weight: 400">Allow the project to create Cloud Pub/Sub authentication tokens:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member=serviceAccount:service-$PROJECT_NUMBER@gcp-sa-pubsub.iam.gserviceaccount.com --role=roles/iam.serviceAccountTokenCreator</span></strong></pre>
<p>Congratulations – using a service account to manage resources follows best practice guidelines laid out by Google Cloud. Following these simple steps ensures that the identity and access permissions are limited to only the service account requiring access. </p>
<p>Now that we have a backend service constrained to a service account invocation, we can set up the message queue to process requests.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling processing requests</h1>
                </header>
            
            <article>
                
<p>The final development task is to add a subscription to the solution. The subscription is used to consume information from the selected topic on Cloud Pub/Sub. A simple mechanism such as this allows information to be channeled between different services with minimal effort.</p>
<p>In the following diagram, we bind the push endpoint for Pub/Sub to the <kbd>SERVICE_URL</kbd> and tie the invocation of the function to the service account we created earlier. Fortunately, initiating a processing request like this is straightforward on <span>Google Cloud</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-851 image-border" src="assets/d89a5e0a-b664-4d6d-8ead-71b14f0788a1.png" style=""/></div>
<p>To initiate a subscription on an existing Cloud Pub/Sub topic, do the following:</p>
<ol>
<li style="font-weight: 400">Create a new Pub/Sub subscription bound to the <kbd>SERVICE_URL</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>gcloud beta pubsub subscriptions create pdf-conv-sub --topic new-doc --push-endpoint=$SERVICE_URL --push-auth-service-account=pubsub-cloud-run-invoker@$GOOGLE_CLOUD_PROJECT.iam.gserviceaccount.com</strong></pre>
<ol start="2">
<li>Now <span>that a subscription has been declared, the information that's passed to the <kbd>new-doc</kbd> topic will be automatically pushed to the subscriber by Cloud Pub/Sub:</span></li>
</ol>
<pre style="padding-left: 60px">{<br/>  "message": {<br/>    "attributes": {<br/>      "key": "value"<br/>    },<br/>    "data": "V2VsY29tZSB0byBHb29nbGUgQ2xvdWQgU2VydmVybGVzcwo=",<br/>    "messageId": "123456789012"<br/>  },<br/>  "subscription": "projects/[PROJECT_ID]/subscriptions/[SUBSCRIPTION_ID]"<br/>}</pre>
<p style="padding-left: 60px">The message object indicates both a subscription and message. A subscription key/value pair defines the project and subscription ID to be used in the message queue. Additionally, a message object that details the key/value pairs for the attributes, data, and message ID allocated to the data to be sent. </p>
<ol start="3">
<li>In the preceding example, once the message/data has been successfully retrieved, the endpoint will use base64 to decode the message passed.</li>
</ol>
<div class="packt_infobox">Both the <kbd>PROJECT_ID</kbd> and <kbd>SUBSCRIPTION_ID</kbd> values need to be valid for the Google Cloud project being used. </div>
<div class="packt_tip">In case you are wondering, the data value displayed in the preceding example is a base64-encoded message. To encode a message, use the base64 application; for example, <kbd>echo "Welcome to Google Cloud Serverless" | base64</kbd>.<br/>
<br/>
To decode a message, add <kbd>-d</kbd> to the application command line; for example, <kbd>echo "V2VsY29tZSB0byBHb29nbGUgQ2xvdWQgU2VydmVybGVzcwo=" | base64 -d</kbd></div>
<p>The HTTPS request will be passed to the predefined endpoint and acknowledged on receipt. If the object passed is not acknowledged as successful, a retry mechanism will be employed, indicating the message needs to be resent. The retry process will be marshaled by a default acknowledgment deadline, which means the endpoint needs to respond before this timeout is in effect. </p>
<p>When working with Cloud Pub/Sub, it is useful to note that Google Cloud serverless systems (that is, App Engine, Cloud Functions, and Cloud Run) use the push mechanism. As flow control is automatically established, the client only needs to indicate success/failure on message processing.</p>
<p>Congratulations—the create-a-PDF service has now been designed and developed using Google Cloud serverless technologies. That concludes the development phase, and we can now progress to testing our new service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the document service</h1>
                </header>
            
            <article>
                
<p>Once the service has been successfully configured, we can commence testing the service. Before we begin that activity, let's take a moment to view the service architecture that has been built:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-852 image-border" src="assets/3b6781b2-fc94-4b6b-8850-1ce5044cb2a7.png" style=""/></div>
<p>Despite the simplicity of our service, the constituent components remove much of the complexity associated with processing information. The use of Cloud Storage negates the need to work with a database and provides an effective event notification system for content changes that are made to the storage.</p>
<p>Once the data has been uploaded, the event notification generates a new Cloud Pub/Sub message containing relevant information on the file uploaded. Again, we are not required to do any processing to achieve this result. The <span class="packt_screen">Topic</span>/<span class="packt_screen">Subscription</span> mechanism provides all the message-processing capability required. </p>
<p>Finally, the message that a new file has been uploaded reaches our PDF backend service. This is the point at which we see the inclusion of the image we built earlier. When Cloud Run executes, it receives the payload from Cloud Pub/Sub before seamlessly processing the information. </p>
<p>Why was it worth recounting what we did? Well, it should indicate how little we actually need to test in terms of the end-to-end process we've defined. Basically, we need to confirm the following:</p>
<ul>
<li style="font-weight: 400">That the Storage Service uses a managed activity</li>
<li style="font-weight: 400">That the Stream Processing Service uses a managed activity</li>
<li style="font-weight: 400">That the PDF Service is based on our code</li>
</ul>
<p>So, how and what would we test with the PDF service?</p>
<p>Basic testing of the service can be performed by checking the service is online and by adding documents to the specified upload bucket.</p>
<p>To test the PDF service is active, we will use the cURL program to retrieve an authorization token:</p>
<ol>
<li style="font-weight: 400">Initially, test that the service has been deployed successfully:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>curl -X POST -H "Authorization: Bearer $(gcloud auth print-identity-token)" $SERVICE_URL</strong></pre>
<p style="padding-left: 60px">Now that we know the service is online, we can begin to test the service. Remember, we have added a Pub/Sub notification event to our upload storage. Once data is added, the message queue will be updated and send a request to the service to transform the data file. </p>
<ol start="2">
<li style="font-weight: 400">Test the conversion process by uploading some public Qwiklabs example files:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>gsutil -m cp gs://spls/gsp644/* gs://$GOOGLE_CLOUD_PROJECT-upload</strong></pre>
<ol start="3">
<li style="font-weight: 400">To confirm the files have been processed, check the processed Cloud Storage bucket:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-853 image-border" src="assets/d4911469-6b9b-44c6-9698-3034dc9f6a47.png" style=""/></div>
<p>In the preceding screenshot, you will note that the original files have now been transformed into PDFs. Take a moment to also check the original upload folder on Cloud Storage and notice that the upload folder is now empty. What happened to all those files that were uploaded? Well, the application includes a hygiene element that also helpfully removes the files once they've successfully processed. Therefore, the upload folder only contains files that have not been converted.</p>
<p>There was a lot of work involved in this scenario; however, none of the techniques presented should be unfamiliar now that you've worked through the various chapters in this book. Congratulations on successfully creating the PDF service – learning how to build the components services will enable more complex systems to be developed over time. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we detailed the process for fulfilling project requirements with Google Cloud serverless technologies. As part of this process, we broke down the initial customer requirements and matched those to pre-existing Google Cloud products. Taking this approach significantly shortened our development cycle and minimized the level of testing required. </p>
<p>Working with serverless architectures on Google Cloud presents a number of opportunities to build some exciting applications. As we have seen in this chapter, the design and development process can be extremely rewarding. In most instances, working with the system to minimize both the code developed and the complexity of the application is time well spent. Our application example clearly demonstrates how using existing packages can significantly increase overall productivity and deliver on customer requirements. Hopefully, this chapter has ignited your imagination and provided the inspiration needed to build the next great application.  </p>
<p>In the next chapter, we will introduce a more advanced example in which multiple services are run at the same time.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">What command is used to access Google Cloud Storage from the command line? </li>
<li style="font-weight: 400">When would Stackdriver Logging be the most useful? </li>
<li style="font-weight: 400">What is the purpose of a Docker manifest file? </li>
<li style="font-weight: 400">Where does Cloud Build store the images it creates?</li>
<li style="font-weight: 400">The <kbd>curl</kbd> command can test GET and POST requests? (True or False)</li>
<li style="font-weight: 400">Why would you use the <kbd>gsutil</kbd> <span>command </span>with the <kbd>-m</kbd> <span>parameter?</span></li>
<li style="font-weight: 400">What type of permission is required to call a service? </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Connecting to Cloud Storage Buckets</strong>: <a href="https://cloud.google.com/compute/docs/disks/gcs-buckets">https://cloud.google.com/compute/docs/disks/gcs-buckets</a></li>
<li><strong>Stackerdriver Logging</strong>: <a href="https://cloud.google.com/logging/docs/view/logs_index">https://cloud.google.com/logging/docs/view/logs_index</a></li>
<li><strong>Qwiklabs Serverless Workshop</strong>: <a href="http://qwiklabs.com/quests/98">qwiklabs.com/quests/98</a></li>
</ul>


            </article>

            
        </section>
    </body></html>