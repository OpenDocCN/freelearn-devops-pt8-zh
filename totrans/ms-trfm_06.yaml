- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Connecting It All Together – GitFlow, GitOps, and CI/CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GitOps** is a contemporary approach to software development and operations
    that strives to make the management of infrastructure and applications easier
    and more efficient. It achieves this by using **Git** as the primary source of
    truth and adopting a declarative approach wherever possible. This methodology
    integrates the principles of version control and continuous delivery to optimize
    the software development life cycle and facilitate better teamwork between development
    and operations teams—and sometimes a fusion of the two disciplines into a true
    **DevOps** team.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding key concepts of GitOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging GitHub for source control management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging GitHub Actions for **continuous integration/continuous deployment**
    (**CI/CD**) pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding key concepts of GitOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways of implementing GitOps, and we’ll look at several in this
    chapter, but at its core, GitOps is about applying the software development life
    cycle to both application source code and infrastructure configuration—or **infrastructure
    as code** (**IaC**). The Git repository becomes the source of truth for what is
    in production, what *was* in production, and what *will* be in production soon.
    In order to do so, the Git repository will have to include configuration files,
    application code, infrastructure definitions, and deployment manifests—everything
    needed to reproduce a fully working version of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Declarative representations are preferred over compiled artifacts, but when
    source code is compiled into artifacts, they need to be versioned and tied back
    to a commit within the Git repository itself. Tools such as Terraform, Docker,
    and Kubernetes interpret these declarative files and automatically apply changes
    to the system to conform to the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Any changes to the Git repository are automatically and continuously applied
    to the target environment, no matter where the environment sits in the life cycle—a
    development, staging, or production environment. This automated process ensures
    consistency and reduces the risk of manual errors.
  prefs: []
  type: TYPE_NORMAL
- en: This can be achieved through a **push** or a **pull** model, which we first
    saw in the previous chapter when looking at different CI/CD pipeline approaches
    for Kubernetes-based solutions. Due to Kubernetes’s influence within the GitOps
    space, it is often a foregone conclusion that the goal is to establish a pull
    model. However, a pull model is not required to implement GitOps. There are many
    ways to implement GitOps, and each approach has distinct trade-offs that should
    be evaluated in your specific context.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you use the push model or the pull model, one of the big advantages
    of using GitOps is that it provides transparency and visibility into the changes
    made to the system by keeping a log of all deployments and updates through the
    normal source control management process. The Git commit history is transformed
    into an audit trail that makes it easier to understand what changes were made
    when they occurred, and by whom. The combination of the complete configuration
    and code to produce an end-to-end working system and a versioned copy makes it
    relatively easy to roll back to a previous state in the event of issues. Of course,
    stateful portions of your systems will likely need additional engineering to ensure
    both new deployments and rollbacks are uneventful.
  prefs: []
  type: TYPE_NORMAL
- en: Using this approach can improve software delivery processes, resulting in greater
    efficiency, reliability, and scalability while simultaneously encouraging collaboration
    between development, operations, and other teams. This is the key reason why adopting
    this approach is critical to enabling a DevOps culture within an organization.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the heavy reliance on Git—traditionally a software development tool—team
    members without an application development background can tend to struggle. Therefore,
    if you come from a non-developer background such as a system administrator, network
    or security engineer, or other infrastructure discipline, it’s very important
    that you take the time to learn basic Git commands and a **Gitflow** process,
    as this knowledge will be critical for you to be effective on the team.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform—and tools like it—are a critical component to a GitOps toolchain as
    the use of IaC is an important pillar of this approach, but it’s important to
    remember that Terraform is often just one ingredient in the grand recipe with
    the source control and pipelining tool playing the key role in facilitating the
    process. That’s why, in this book, we’ll be setting up sophisticated architectures
    using Terraform and CI/CD pipelines to provision them. Before we can get to that,
    we need to firmly understand what a CI/CD pipeline is and how to build one, which
    is what we will look at in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CI/CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A CI/CD pipeline is an automated set of steps and processes that help software
    development teams build, test, and deploy their applications quickly and reliably.
    It is a fundamental component when implementing a GitOps process as it takes on
    the critical role of facilitating the continuous flow of changes from development
    to production, ensuring that new code is automatically integrated, tested, and
    delivered to end users as a working system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Overview of the anatomy of a CI/CD pipeline](img/B21183_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Overview of the anatomy of a CI/CD pipeline
  prefs: []
  type: TYPE_NORMAL
- en: As its very name might suggest, a CI/CD pipeline actually consists of two processes
    that are stitched together. First, the **continuous integration** pipeline, which
    is responsible for building and ensuring the built-in quality of the application
    code of the system, and second, the **continuous deployment** pipeline, which
    is responsible for deploying that application code into its environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CI/CD pipeline aggregates these two historically distinct processes: **integration
    testing** and **deployment**. However, by combining them, it provides a systematic
    and automated approach to continuously delivering new features and bug fixes to
    users, reducing the time and risk associated with manual deployments. This, in
    turn, fosters a culture of collaboration, frequent feedback, and rapid innovation
    within development teams.'
  prefs: []
  type: TYPE_NORMAL
- en: A CI/CD pipeline that uses Terraform to provision infrastructure and deploys
    the latest code version to that infrastructure typically has two objectives. First,
    produce a version of the software that has been tested and verified to have satisfactory
    levels of built-in quality. Second, provision an environment—whatever that looks
    like—to host the application that is compatible and meets the software’s requirements
    to function correctly and efficiently. The third and final step is to deploy the
    application to that environment.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline makes no judgments about how robust your cloud architecture might
    be. Depending on your needs, you may opt to sacrifice certain qualities of your
    solution architecture for expediency or cost. The pipeline’s job is to provision
    whatever environment you tell it you need and to deploy the software to that environment,
    so once the pipeline has completed, your application is ready to accept incoming
    traffic from users.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll dig deeper into the internal structure of a CI/CD
    pipeline and discuss the mechanics of what is going on along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we learned about the fundamental principles of GitOps
    and that the CI/CD pipeline is grounded on a version control system such as Git,
    where developers commit their code changes. We can configure a CI/CD pipeline
    to trigger when certain key events take place within the code base, such as changes
    being pushed to a specific branch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once certain key events take place within the version control system, such
    as a developer pushing changes to a particular branch or path, the CI/CD pipeline
    is triggered. It will pull the latest code, build the application, and run a series
    of automated tests to verify the functionality and integrity of the application
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Anatomy of a CI/CD pipeline](img/B21183_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Anatomy of a CI/CD pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Various tests, including unit tests, integration tests, and sometimes even acceptance
    tests, can be conducted to ensure that the code meets quality standards and does
    not introduce regressions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests** operate on individual components and use mocks to isolate the
    tests’ outcomes around a single component by injecting placeholders for the component’s
    downstream dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Unit tests are isolated on a single component](img/B21183_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Unit tests are isolated on a single component
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration tests** operate across two or more components. They can use mocks
    or not, and their focus is on the reliability of interactions between components.
    Sometimes, for very intricate or complex components, you might want integration
    tests that focus on the various use cases surrounding them while keeping other
    components’ outputs predictable using mocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Integration tests are focused on two or more components and
    how they interact](img/B21183_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Integration tests are focused on two or more components and how
    they interact
  prefs: []
  type: TYPE_NORMAL
- en: '**System tests** introduce real-world dependencies, such as databases or messaging
    subsystems, into the mix and allow you to achieve much more realistic coverage
    across a system without fully deploying it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – System tests](img/B21183_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – System tests
  prefs: []
  type: TYPE_NORMAL
- en: System tests have a broader focus, often introducing real-world dependencies
    such as databases and external systems
  prefs: []
  type: TYPE_NORMAL
- en: 'An **end-to-end test** is one where you provide the entire host environment
    for the application—as it would be in production—and execute tests that mimic
    an actual client application or end user as closely as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – End-to-end tests](img/B21183_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – End-to-end tests
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end tests attempt to mimic, as closely as possible, actual end-user activity
    with the system fully operational, end to end.
  prefs: []
  type: TYPE_NORMAL
- en: It depends on the requirements of the particular application and organization,
    what kind of testing, and how much needs to be done on an application. Terraform
    can also play a crucial role in the continuous integration process by provisioning
    **just-in-time** (**JIT**) environments for system or end-to-end testing environments.
    Terraform allows you to dynamically create an environment fit for purpose, execute
    your tests, and then shut everything down.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the level of reliability that you want in your release process,
    you could opt for a deeper and more robust level of testing before the continuous
    deployment process is initiated.
  prefs: []
  type: TYPE_NORMAL
- en: After the continuous integration process is successfully completed, the application
    is packaged into a deployment package (e.g., a Docker container or a JAR file)
    that contains all the necessary dependencies and configurations and is ready to
    be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: During the continuous deployment process, both the Git source code and this
    deployment package are used to provide the environment and deploy the package
    to the target environment. Terraform is crucial in provisioning or updating the
    required infrastructure, such as virtual machines, containers, or serverless resources.
    As we looked at in the previous chapters, Terraform can also optionally perform
    the application deployment through a pre-built virtual machine image or a Kubernetes
    deployment with pre-built container images.
  prefs: []
  type: TYPE_NORMAL
- en: After deployment, the CD pipeline can run additional verification tests to ensure
    that the application runs correctly in the target environment by utilizing health
    checks built into the application and infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the architecture, the outcome of the CD pipeline is that it applies
    environment-specific configurations—usually derived from Terraform outputs, which
    contain vital configuration details—to the artifact, thus, customizing it for
    the target environment. These configurations might include database connection
    strings, API endpoints, or other settings that differ between environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, Terraform plays an essential role in this process but is not
    the only player on the field. Each step in this process is equally important and
    plays a critical role in consistently releasing software with built-in quality.
    In this book, we will review three architectures and three corresponding techniques
    for deployment for each of the three paradigms of cloud hosting: virtual machines,
    containers, and serverless. These solutions will be built using GitHub as the
    source control repository and GitHub Actions as the tool we use to implement our
    CI/CD pipelines. Depending on the architecture of the software and how it is hosted
    within the environment, the deployment technique may vary.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll look at the source control management aspects of
    GitOps, which include the developer workflows that add structure to our DevOps
    teams that are executing in this manner.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging GitHub for source control management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitHub is just one option for source control management software. We’ll be using
    it in this book, but it’s important for you to understand that the concepts and
    patterns implemented using GitHub are consistent no matter what source control
    provider you end up using for your projects. There may be small differences between
    the syntax and mechanisms that implement and execute pipelines, but the source
    control management system is just `git` under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: An important part of source control management is how to use it in a structured
    way on a team—large or small. These are conventions that your team can use so
    that you have consistent expectations across the team about how new features are
    shepherded through your development process and into production.
  prefs: []
  type: TYPE_NORMAL
- en: Gitflow is a common model that uses a combination of well-known, long-lived,
    and consistent naming conventions for short-lived branches. As we will see in
    the next subsection, it is highly customizable and a bit of a *Choose Your Own
    Adventure*, which is why it has become one of the most common operating models
    for development teams, no matter the size.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also look at a miniature variant called GitHub flow, which is an example
    of trunk-based development. This model advocates for keeping the `main` branch
    always deployable and minimizing the use of long-lived branches. Instead of creating
    long-lived stable branches for various purposes and designs, developers work directly
    on the `main` branch using only short-lived `feature` branches that are quickly
    merged back into `main`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll take a closer look at Gitflow to see what the developer
    experience would look like and how it would integrate with the automation systems
    that we build using Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Gitflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gitflow is one of the most popular branching models and workflows used by development
    teams around the world. Its prolific nature has led to the development of different
    variations and adaptations to suit different development environments and teams’
    preferences. At its core, Gitflow leverages a `main` branch to indicate production
    quality code and a `develop` branch that grants development teams a safe place
    to merge and perform integration testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Gitflow at its simplest](img/B21183_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Gitflow at its simplest
  prefs: []
  type: TYPE_NORMAL
- en: In Gitflow, `main` is the main branch representing the production-ready code.
    Only code that is ready for production should live in this branch. Features that
    are under development are created by individual developers on their own `feature/*`
    branch and then merged into a shared `develop` branch that acts a bit like a staging
    environment before being merged into `main`.
  prefs: []
  type: TYPE_NORMAL
- en: However, as mentioned before, Gitflow is highly customizable and there have
    been several extensions to this core model developed over the years with varying
    levels of adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, `release` branches are used for preparing and testing releases, starting
    from `develop` and merging back into both `develop` and `main`. This can give
    a team greater control over when and how they release a set of features into production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real world happens fast. As a result, sometimes critical changes need to
    be made rapidly to production to fix a specific issue. That’s when `hotfix` branches
    are used by starting from `main` and merging back into both `develop` and then
    `main` once a hotfix has been fully tested:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Gitflow extended](img/B21183_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Gitflow extended
  prefs: []
  type: TYPE_NORMAL
- en: 'Gitflow is highly customizable:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main`: Production only code (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`release`: Release staging (2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`develop`: Integrating testing (3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature/*`: Feature development (4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hotfix/*`: Critical patches to production (5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gitflow does not dictate a specific versioning scheme, but it is common to
    use semantic versioning (e.g., `{MAJOR}.{MINOR}.{PATCH}`) to indicate the significance
    of changes made in each release. Gitflow does provide a clear separation of tasks,
    making it suitable for larger teams and projects that require strict control over
    the development and release process. However, this structure can be overwhelming
    for smaller teams or experimental projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Gitflow integration with CI/CD pipelines](img/B21183_06_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Gitflow integration with CI/CD pipelines
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gitflow process has several key events where automation might be triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature/*` branch into `develop`. This often triggers a CI/CD pipeline that
    includes application code with built-in quality, unit, and integration tests.
    The merge of this pull request initiates a release pipeline that is deployed to
    the development environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`develop` branch into `release`. This usually includes additional testing,
    such as system and even end-to-end tests. The merge of this pull request initiates
    a release pipeline that deploys to the staging or release environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`release` into `main`. This usually includes additional variations of end-to-end
    tests that check performance or load and may include upgrade or version testing.
    The merge of this pull request initiates a release pipeline that deploys to the
    production environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`hotfix/*` branch into `main`. This would likely execute a smaller catalog
    of test suites but would likely include version or upgrade testing. The merge
    of this pull request initiates a release pipeline that deploys to the production
    environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s important to point out that this is probably the most extensive configuration
    of Gitflow, but humans being humans, I’m sure somebody out there has come up with
    an even more complex incarnation of Gitflow. In the next section, let’s look at
    something a little more simple and lightweight by going back and taking a look
    at Trunk-Based Development using GitHub flow.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve discussed, GitHub flow is the little brother of Gitflow. It’s much
    more simple and lightweight and perfect for small teams or experimentation. It
    focuses on only one branch—`main`—with new features being introduced for individual
    `feature/*` branches. Developers create `feature` branches from `main`, work on
    their changes, and then submit pull requests to merge them back into the `main`
    branch. Releases are often tagged from `main` after thorough testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – GitHub flow for small teams or experiments](img/B21183_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – GitHub flow for small teams or experiments
  prefs: []
  type: TYPE_NORMAL
- en: The main difference is that there is no official process around creating staging
    branches such as `develop` or `release` branches where integration testing is
    performed. The responsibility for integration testing resides on the individual
    developer of the feature within their own `feature` branch—in essence, taking
    individual responsibility for their changes working in production.
  prefs: []
  type: TYPE_NORMAL
- en: This also means that we have fewer key events which a CI/CD pipeline will trigger
    from. We only have a pull request from `feature/*` into `main` and then merge
    into `main` to trigger events. Additional testing can be performed on the `feature/*`
    branches themselves or teams can optionally introduce a manual trigger for a production
    release, which allows for more time to perform testing on `main`.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, GitHub flow is great for smaller teams that don’t have
    dedicated teams focused on integration testing!
  prefs: []
  type: TYPE_NORMAL
- en: Each variation of Gitflow has its strengths and weaknesses, and the choice of
    workflow depends on the project’s specific needs, team size, development process,
    and the tools or platforms used for version control. It’s essential to evaluate
    the requirements and preferences of the team and project to select the most suitable
    branching model. I’ll go over a few of these options in this book in more detail,
    but for the most part, I will use GitHub Flow to keep things simple in my examples.
  prefs: []
  type: TYPE_NORMAL
- en: Using GitHub Actions for CI/CD pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GitHub Actions** is a CI/CD service offered by GitHub that provides a platform
    for you to implement automation around your source control management process
    no matter what workflow you choose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to hook into GitHub Actions, you need to define YAML files that specify
    the tasks that you want to be automated. These files are called `.github/workflows`
    directory of your source code repository. The basic anatomy of a workflow consists
    of jobs. Jobs have steps. Steps can be a simple script that you execute or something
    more complex packaged together called an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code has two jobs: `build` and `test`. The jobs are grouped under
    the `jobs:` section and each job has steps grouped under the `steps:` section.
    You can customize the image that your job runs on using the `runs-on` attribute.
    This allows you to specify a container image that is customized to your needs
    with the correct Linux distribution or software configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: By default, a step simply executes a bash script using the `run` attribute,
    but you can utilize an action by specifying the action type with the `uses` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute Terraform, you simply need it installed on your agent. This can
    be done easily using an action provided by HashiCorp called `hashicorp\setup-terraform@v2`.
    The following code snippet demonstrates how to do this while specifying the specific
    version of Terraform that you want to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are additional attributes, but they are more for edge cases and are beyond
    the scope of this book. I recommend you check out the documentation for the action
    to check out all the different options available: [https://github.com/hashicorp/setup-terraform](https://github.com/hashicorp/setup-terraform).'
  prefs: []
  type: TYPE_NORMAL
- en: You must always store sensitive data as secrets to ensure that the data is not
    exposed in the logs. This can easily be accomplished by leveraging GitHub environments
    or other secret management services.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machine workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building automation pipelines that provision virtual-machine-hosted workloads,
    your toolchain should consist of something that can be used to set up the initial
    configuration of the virtual machine, provision the virtual machine, and make
    updates to the virtual machine’s configuration over time. The tools that we will
    cover in this book for these purposes are Packer, Terraform, and Ansible, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Packer build pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed when we looked at developing Packer templates, developers write
    and commit Packer configuration files using **HashiCorp Configuration Language**
    (**HCL**) to their Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: An independent pipeline is triggered when changes are pushed to the version
    control system affecting the folder where the Packer configuration files are stored.
    Within that pipeline, Packer is utilized to build virtual machine images for each
    server role (e.g., frontend, backend, and database). Packer is configured with
    the latest configurations for each role within the application, including the
    necessary software and settings unique to each layer. After successfully building
    each image, Packer creates machine images optimized for the cloud provider of
    choice (e.g., **Amazon Machine Images** (**AMIs**) for **Amazon Web Services**
    (**AWS**) or Azure Managed Images for Azure).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, Packer can fail due to transient issues with the virtual machine
    itself or just bugs within your script. You can use a **debug mode** within Packer
    that will allow you to pause the build process on the temporary virtual machine.
    This will allow you to connect to the machine, execute the command that failed
    manually, and troubleshoot the issues within the environment itself.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the target cloud platform, the generated machine images are stored
    in an artifact repository or directly in the cloud provider’s image repository
    for later use by Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform apply pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the virtual machine images are published to an image repository, Terraform
    simply needs to reference the correct image in order to provision a virtual machine
    with the right one. Similar to the Packer Build Pipeline, developers commit Terraform
    configuration files to their Git repository, and a separate pipeline is triggered
    whenever changes are pushed to the folder where the Terraform configuration is
    stored.
  prefs: []
  type: TYPE_NORMAL
- en: The Terraform configuration defines the network infrastructure, including subnets,
    security groups, and load balancers, needed for all the virtual machines within
    the solution. Terraform pulls the Packer-built machine images from the artifact
    repository or cloud provider’s image repository and provisions the required number
    of virtual machines for each role, setting up any load balancers necessary to
    distribute the load across multiple servers to ensure high availability and fault
    tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform can sometimes fail either for transient issues but also potential
    race conditions between resources that you are trying to provision that are implicitly
    dependent upon each other. We’ll go into more advanced troubleshooting scenarios
    in [*Chapter 17*](B21183_17.xhtml#_idTextAnchor700), but for now, it’s important
    to recognize that Terraform is idempotent, which means you can run it over and
    over again to reach a desired state—so, sometimes, just re-running the job can
    get you past the initial issue you faced.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible apply pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, after Terraform applies the infrastructure changes and the virtual
    machines are set up using the Packer images, the environment is primed and ready.
    However, the environment is not yet fully operational as there will likely be
    certain configuration changes that need to be made specific to the environment
    that were not available during the Packer image build phase. This is what I call
    *last mile* configuration—where we put the last touches on the environment by
    applying any configuration settings only known after Terraform `apply` executes.
    There are different options for performing these last-mile configuration changes.
    You can use Terraform to dynamically configure user data to pass directly to the
    virtual machine, or you can use another tool to do the job.
  prefs: []
  type: TYPE_NORMAL
- en: Since most virtual machines also need some routine maintenance performed, it’s
    good to consider a configuration management tool that can make updates to your
    environment without having to shut down or reboot virtual machines by changing
    the version of the Packer image used. That’s where tools such as Ansible come
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible can be used as a configuration management tool to perform the last mile
    configuration on the virtual machines in addition to performing ongoing maintenance
    on the machines. Ansible scripts are applied to the deployed virtual machines
    to set environment-specific values, configure services, and perform other necessary
    tasks. In doing so, the environment is now ready for operators to perform routine
    maintenance using the already established Ansible configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Like Terraform, Ansible is idempotent and can fall prey to similar transient
    errors. However, like Packer, Ansible is invoking change within the operating
    system itself. As a result, you just need to connect to one of these virtual machines
    and troubleshoot the commands that failed when Ansible executed its scripts.
  prefs: []
  type: TYPE_NORMAL
- en: By employing this approach, a virtual-machine-based solution can efficiently
    be provisioned and operated over the lifespan of the application. This allows
    for reproducible, scalable, and automated deployments and provides the necessary
    flexibility for different environments while ensuring consistent and reliable
    setups for each role within the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Container workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building automation pipelines that provision container-based workloads,
    your toolchain should consist of something that can be used to set the initial
    configuration of the various containers that need to be deployed, provision the
    Kubernetes cluster to host the containers and the underlying infrastructure that
    supports the Kubernetes cluster’s operations, and then finally provision Kubernetes
    resources to the Kubernetes control pane using Kubernetes’ REST API through a
    variety of different options.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the immutability of the container images and their lightweight and speedy
    nature, it’s easy to implement sophisticated rolling updates to roll out new versions
    of the container image across existing deployments. Therefore, the mechanics around
    provisioning and maintaining container-based workloads are really about building
    new container images and referencing the desired image within your Kubernetes
    configuration to invoke an update to the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Docker build pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed when we looked at the principles around Docker and how it works,
    developers write and commit Docker files using their Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: An independent pipeline is triggered when changes are pushed to the version
    control system, affecting the folder where the Docker configuration files are
    stored. Within that pipeline, Docker is utilized to build container images for
    each server role (e.g., frontend, backend, and database) within the application.
    Docker is configured with the latest configurations for each role within the application,
    including the necessary software and settings unique to each layer. The Docker
    image that is produced acts as our deployment package. As a result, it is versioned
    and stored in a Package repository called a container registry (which we discussed
    in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278)). Once the new Docker image
    is there, we can reference it from the Kubernetes configuration and trigger a
    deployment in Kubernetes in a myriad of ways.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes manifest update pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this pipeline, developers modify the manifests to reference the new version
    of the Docker image that was built and published in the previous step and submit
    a pull request to update the change. The trigger we use can be either a push model
    or a pull model. If you recall, in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278),
    *Container-Based Architectures*, we discussed several different methods for implementing
    a push model in this manner. Some options use `kubectl` and Kubernetes YAML manifests,
    and others use a Helm Chart with a set of YAML manifests that have been turned
    into a more dynamic template by using Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, using the pull model, we could use a continuous deployment agent
    hosted on the Kubernetes cluster itself, such as ArgoCD, that would pick up on
    changes within the Git repository and apply them to the cluster. Because ArgoCD
    is continuously monitoring the Git repository containing the Kubernetes manifests
    (or Helm Charts), whenever a new commit is made to the repository, it will automatically
    trigger a deployment process. ArgoCD isn’t doing any magic; it is simply using
    `kubectl apply` to apply the latest version of the manifests to the Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform apply pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have discussed in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), due
    to Kubernetes architecture, the Kubernetes cluster is often a shared resource
    where multiple teams will deploy their own workloads by targeting their own namespace
    within the cluster. That’s why it’s often the case that this pipeline may be managed
    by a different team than the ones that own the Docker Build and Kubernetes Manifest
    pipelines. This pipeline is owned by the team responsible for provisioning and
    maintaining the Kubernetes cluster. Their responsibility is to ensure that the
    cluster is up and running and ready to accept deployments from ArgoCD.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform could optionally be used to manage Kubernetes resources on the cluster,
    but as we addressed in [*Chapter 5*](B21183_05.xhtml#_idTextAnchor278), this may
    not be ideal in all situations due to team and organizational dynamics. It’s best
    to consider your specific context and make the right decision for your team and
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, Terraform is simply used to provision the Kubernetes cluster
    and surrounding infrastructure on the cloud platform of choice. Developers will
    commit Terraform configuration files to their Git repository, and the pipeline
    is triggered whenever changes are pushed to the folder where the Terraform configuration
    is stored.
  prefs: []
  type: TYPE_NORMAL
- en: This approach allows developers to focus on code development and testing without
    worrying about the underlying infrastructure and deployment process. The development
    teams can rely on an isolated environment within the Kubernetes cluster that they
    deploy to and really only need to maintain their code base and the Docker file
    used to configure their application.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In serverless architecture, the deployment process can be greatly simplified.
    You typically have two main pipelines to manage the serverless framework and surrounding
    services and the actual function code themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform apply pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This pipeline is responsible for provisioning the underlying infrastructure
    required to support the serverless workloads. It uses Terraform to define and
    manage the infrastructure components. The pipeline may create resources such as
    load balancers, API gateways, event triggers, and other logical components that
    serve as the foundation for serverless functions. These are often lightweight
    cloud services that are extremely quick to provision.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless deployment pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This pipeline is responsible for deploying individual serverless functions to
    the target platform (e.g., AWS Lambda or Azure Functions). Each serverless function
    typically has its own pipeline to handle its deployment, testing, and versioning.
    This maintains autonomy between the different components and allows teams to organize
    ownership that aligns with how they manage their code base. The pipeline really
    only involves packaging the function code, defining the configuration, and deploying
    it to the cloud platform of choice.
  prefs: []
  type: TYPE_NORMAL
- en: The serverless approach simplifies the deployment and management of code, and
    developers can focus more on writing the application logic while relying on automated
    deployment pipelines to handle infrastructure provisioning and serverless function
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a ton of tools out there to help improve Terraform code in terms of
    beauty, functionality, and maintainability. I won’t boil the ocean here but I
    will mention some critical tools that are absolutely required for any Terraform
    continuous integration process.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During development, you should install the HashiCorp Terraform plugin for Visual
    Studio Code. This will enable a ton of helpful productivity features within your
    editor but it will also automatically execute Terraform’s built-in formatting
    function, `terraform fmt`, on saving each file. This will drastically help promote
    consistent formatting within your code base. This is a proactive approach that
    is dependent on the developer to take steps to configure their development environment
    properly.
  prefs: []
  type: TYPE_NORMAL
- en: In order to verify each developer is employing this technique to keep your project’s
    Terraform code neat and tidy, you need to use a linter as part of your pull request
    process. Adding `tflint` to your pull request process will help prevent poorly
    formatted code from ever making it into your `main` branch!
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the code is formatted properly, we should generate some documentation
    for our modules. This is useful whether you are writing root modules or reusable
    modules. The `terraform-docs` tool, when pointed at a Terraform module director,
    will generate a markdown `README` file that documents the key aspects of your
    Terraform module, including version requirements for both Terraform and the providers
    you employ, as well as details on the input and output variables. This tool is
    ideal to set up as a pre-commit operation to ensure that your documentation is
    automatically generated every time the code is merged. It reads annotations that
    are built-in to HCL, such as `description`, `type`, `required`, and any default
    values.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more at [https://terraform-docs.io/user-guide/introduction/](https://terraform-docs.io/user-guide/introduction/).
  prefs: []
  type: TYPE_NORMAL
- en: Security scanning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Checkov** is a static code analyzer that can scan your Terraform plan files
    to detect security and compliance violations. It has thousands of built-in policies
    spanning many platforms but most importantly including the cloud platforms that
    we explore in this book: AWS, Azure, and Google Cloud. However, at the time of
    writing, the policy coverage is most comprehensive for AWS, with both Azure and
    Google Cloud with significantly less coverage.'
  prefs: []
  type: TYPE_NORMAL
- en: You can read more at [https://github.com/bridgecrewio/checkov](https://github.com/bridgecrewio/checkov).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the basic concepts of source control management,
    including detailed breakdowns of different branching and workflow strategies that
    are used by teams large and small. We looked at how our automation systems, namely
    our CI/CD pipelines, would integrate with these processes at key events.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move conceptual knowledge and start working on
    our first solution, which is to leverage virtual machines on the first public
    cloud, AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Building Solutions on AWS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Armed with the conceptual knowledge of Terraform and architectural concepts
    that transcend the implementation details of the major public cloud platforms,
    we’ll explore building solutions on **Amazon Web Services** (**AWS**) with three
    cloud computing paradigms: virtual machines, containers with Kubernetes, and serverless
    with AWS Lambda.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21183_07.xhtml#_idTextAnchor365), *Getting Started on AWS –
    Building Solutions with AWS EC2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), *Containerize with AWS – Building
    Solutions with AWS EKS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21183_09.xhtml#_idTextAnchor446), *Go Serverless with AWS –
    Building Solutions with AWS Lambda*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
