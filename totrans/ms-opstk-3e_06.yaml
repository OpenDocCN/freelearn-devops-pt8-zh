- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: OpenStack Networking – Connectivity and Managed Service Options
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack网络 – 连接性和托管服务选项
- en: “It always seems impossible until it’s done.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “事情总是看起来不可能，直到它完成。”
- en: – Nelson Mandela
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 纳尔逊·曼德拉
- en: Neutron, the OpenStack networking service, has evolved throughout different
    OpenStack releases, offering more capabilities and features. Cloud architects
    can design their OpenStack networking stack with multiple topologies, based on
    their existing networking resources and requirements. Unlike the former Nova networking
    service, which was bound only to basic networking offerings, OpenStack tenants
    can expect more granular networking control and flexibility with Neutron. Cloud
    and network administrators can offer more advanced networking features to tenants,
    such as routing, firewalls, load balancing, and private networking.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron是OpenStack的网络服务，随着不同版本的发布不断发展，提供了更多的功能和特性。云架构师可以根据现有的网络资源和需求，设计多种拓扑结构的OpenStack网络堆栈。与之前仅提供基础网络服务的Nova网络服务不同，OpenStack租户可以在Neutron中获得更细粒度的网络控制和灵活性。云和网络管理员可以为租户提供更高级的网络功能，如路由、防火墙、负载均衡和私有网络。
- en: 'In this chapter, we will walk through the rich updates that have been made
    to OpenStack networking by covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍OpenStack网络的丰富更新，涵盖以下主题：
- en: The Neutron architecture and its core service interactions in OpenStack
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack中Neutron架构及其核心服务交互
- en: Neutron plugins and supported drivers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neutron插件和支持的驱动程序
- en: Existing Neutron agents and required network types
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的Neutron代理和所需的网络类型
- en: Designing and implementing an OpenStack virtual switching layout using **Open
    vSwitch** ( **OVS** ) and **Open Virtual Network** ( **OVN** ) mechanisms as a
    **Software-Defined Networking** ( **SDN** ) implementation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Open vSwitch**（**OVS**）和**Open Virtual Network**（**OVN**）机制设计并实现OpenStack虚拟交换布局，作为**软件定义网络**（**SDN**）的实现
- en: Learning about and implementing routing, using virtual routers to interconnect
    tenant networks and provide external connectivity
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解并实现路由，使用虚拟路由器互联租户网络并提供外部连接
- en: Integrating the load balancer as a service feature using the emerging Octavia
    project
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新兴的Octavia项目将负载均衡作为服务功能进行集成
- en: Exploring Neutron’s core components
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Neutron的核心组件
- en: 'OpenStack networking has evolved through different releases to build an advanced
    and functioning cloud networking stack. Like many other OpenStack services, Neutron
    is composed of several services that can be stretched across multiple hosts, as
    shown in the following diagram:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack网络通过不同版本的发布不断发展，构建了一个先进且功能完善的云网络堆栈。像许多其他OpenStack服务一样，Neutron由多个服务组成，可以跨多个主机进行扩展，正如以下图所示：
- en: '![Figure 6.1 – The OpenStack Neutron core architecture](img/B21716_06_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – OpenStack Neutron核心架构](img/B21716_06_01.jpg)'
- en: Figure 6.1 – The OpenStack Neutron core architecture
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – OpenStack Neutron核心架构
- en: 'These components can be briefly broken down as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件可以简要地分解如下：
- en: '**Neutron server** : This acts as an API portal that receives API requests
    generated by services or end users and forwards them to the next process – in
    this case, the Neutron agents through the messaging queue service. The other part
    of the server interaction within an OpenStack ecosystem is access to the database
    to update the network objects for each API request.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neutron服务器**：它充当一个API门户，接收由服务或最终用户生成的API请求，并将其转发到下一个处理环节——在这种情况下，通过消息队列服务将请求传递给Neutron代理。OpenStack生态系统中服务器交互的另一个部分是访问数据库，以便更新每个API请求的网络对象。'
- en: '**Neutron agents** : Neutron’s architecture relies heavily on different types
    of agents that will be installed in other hosts to handle different networking
    features. The plugin agent, denoted as **neutron-*-agent** , is the process that
    handles virtual switching capabilities in each compute node. This type can be
    described as a **layer 2** ( **L2** ) agent. A very common L2 agent is OVS, which
    provides L2 connectivity via the ML2 mechanism driver. The second type of Neutron
    agent is the **layer 3** ( **L3** ) agent, denoted as **neutron-l3-agent** , which
    is installed in the network node and deals with instances of network access for
    L3, such as NAT, firewall, and **virtual private network** ( **VPN** ) capabilities.
    The DHCP agent, denoted as **neutron-dhcp-agent** , manages instances of DHCP
    configuration for each tenant network (a **dnsmasq** configuration).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neutron 代理**：Neutron 架构在很大程度上依赖于安装在其他主机上的不同类型的代理，以处理不同的网络功能。插件代理，标记为 **neutron-*-agent**，是处理每个计算节点中虚拟交换功能的进程。这种类型可以被描述为
    **第二层**（**L2**）代理。一种非常常见的 L2 代理是 OVS，它通过 ML2 机制驱动程序提供 L2 连通性。第二种 Neutron 代理是 **第三层**（**L3**）代理，标记为
    **neutron-l3-agent**，安装在网络节点上，处理 L3 网络访问的实例，如 NAT、防火墙和 **虚拟私人网络**（**VPN**）功能。DHCP
    代理，标记为 **neutron-dhcp-agent**，管理每个租户网络的 DHCP 配置实例（**dnsmasq** 配置）。'
- en: Depending on the nature of the network function request, the Neutron service
    will reach the agents deployed on the network and/or compute nodes. Cloud operators
    should list which network functions the infrastructure supports, which will decide
    the choice of plugins and agents that can be installed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据网络功能请求的性质，Neutron 服务将与部署在网络节点和/或计算节点上的代理进行交互。云运营商应列出基础设施支持的网络功能，这将决定可以安装的插件和代理的选择。
- en: Demystifying Neutron agents
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭示 Neutron 代理的秘密
- en: 'Neutron relies on agent services that interact with **neutron-server** through
    the queuing message service to implement virtual networking for L2 and L3 connectivity,
    DHCP, and routing services. Different agents will be deployed in the network and
    compute nodes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 依赖与 **neutron-server** 通过排队消息服务交互的代理服务，以实现 L2 和 L3 连通性、DHCP 和路由服务的虚拟网络。不同的代理将在网络和计算节点上部署：
- en: '**L2 agent** : Deployed on network and compute nodes to implement L2 networking
    connectivity for instances and virtual networks, such as routers.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 代理**：部署在网络节点和计算节点上，为实例和虚拟网络（如路由器）实现 L2 网络连通性。'
- en: '**L3 agent** : Deployed on the network node and, optionally, on compute nodes
    to perform L3 routing between different types of networks, such as tenant and
    external networks. L3 can also implement more advanced networking capabilities,
    such as firewalls and VPNs.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L3 代理**：部署在网络节点上，并可选地部署在计算节点上，用于在不同类型的网络之间执行 L3 路由，例如租户网络和外部网络。L3 还可以实现更高级的网络功能，如防火墙和
    VPN。'
- en: '**DHCP agent** : Deployed on the network node to run the DHCP service for tenant
    networks.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DHCP 代理**：部署在网络节点上，为租户网络运行 DHCP 服务。'
- en: '**Plugin agent** : Deployed on the network node to process data packets on
    virtual networks that depend on the implemented Neutron plugin.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件代理**：部署在网络节点上，处理依赖于已实现 Neutron 插件的虚拟网络中的数据包。'
- en: '**Metadata agent** : Deployed on the network node. The metadata service provided
    by Nova enables the retrieval of instance information, such as hostname and IP
    addresses, upon a metadata HTTP request ( **169.254.169.254** ). The Neutron metadata
    agent facilitates the forwarding of metadata to the Nova metadata service through
    its internal proxy component with additional information, such as instance and
    tenant IDs.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据代理**：部署在网络节点上。Nova 提供的元数据服务使得可以通过元数据 HTTP 请求（**169.254.169.254**）检索实例信息，如主机名和
    IP 地址。Neutron 元数据代理通过其内部代理组件将元数据转发到 Nova 元数据服务，并附加额外的信息，如实例和租户 ID。'
- en: The proper function of the deployed Neutron agent will vary, based on the types
    of the designed networks in the OpenStack environment. The next section will briefly
    cover the different categories of networks that should exist in our initial design
    draft.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的 Neutron 代理的正常功能将根据 OpenStack 环境中设计的网络类型而有所不同。下一节将简要介绍我们初步设计草稿中应存在的不同网络类别。
- en: Network categories
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络类别
- en: 'It is essential to go through the network types to enable traffic flow, based
    on their usage. The following glossary covers the network categories that are
    used mainly with the latest releases of OpenStack:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 必须了解不同的网络类型以启用流量流动，具体取决于它们的使用方式。以下术语表涵盖了主要与 OpenStack 最新版本一起使用的网络类别：
- en: '**Provider networks** : Created and managed by cloud operators who define a
    set of network attributes, such as the network’s type – for example, **VXLAN**
    , **Generic Routing Encapsulation** ( **GRE** ), or **flat** . Cloud operators
    provide and configure the underlying infrastructure, such as the physical network
    interface designated for the traffic flow.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供商网络**：由云操作员创建和管理，操作员定义一组网络属性，例如网络类型——例如，**VXLAN**、**通用路由封装**（**GRE**）或**平面**。云操作员提供并配置底层基础设施，例如指定用于流量流动的物理网络接口。'
- en: '**Self-service networks** : Referred to as **tenant networks** , these are
    created by cloud users. Self-service networks are self-contained and fully isolated
    from other networks in a multi-tenancy environment managed by Neutron. Tenants
    are restricted to creating virtual networks with what was predefined by the cloud
    operators in terms of network types. For example, a cloud user cannot implement
    GRE networks if the cloud operator does not provide the option. Cloud users do
    not have access to the underlying Neutron configuration.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自服务网络**：也称为 **租户网络**，这些网络由云用户创建。自服务网络是独立的，完全与由 Neutron 管理的多租户环境中的其他网络隔离。租户只能根据云操作员预定义的网络类型来创建虚拟网络。例如，如果云操作员未提供该选项，则云用户无法实现
    GRE 网络。云用户无法访问底层的 Neutron 配置。'
- en: '**External provider networks** : This refers to a provider network for specific
    external network connectivity. Cloud operators configure an external routing device
    to access the internet.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部提供商网络**：指特定外部网络连接的提供商网络。云操作员配置外部路由设备以访问互联网。'
- en: The next section will cover the most interesting capabilities of Neutron by
    exploring the concept of plugins and their latest mode of usage.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将通过探讨插件的概念及其最新使用模式，涵盖 Neutron 最具吸引力的功能。
- en: The core of networking – Neutron plugins
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络核心 – Neutron 插件
- en: 'Neutron supports the use of plugins and drivers, which use different software
    and hardware technologies provided by open source communities or vendor solutions.
    There are two types of Neutron plugins:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 支持使用插件和驱动程序，这些插件和驱动程序使用由开源社区或供应商解决方案提供的不同软件和硬件技术。Neutron 插件有两种类型：
- en: '**Core plugin** : Enables L2 connectivity functions and orchestration of network
    elements, including virtual networks, subnets, and ports'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心插件**：启用 L2 连接功能和网络元素的编排，包括虚拟网络、子网和端口。'
- en: '**Service plugin** : Enables additional network capabilities, including routing,
    private networks, firewalls, and load balancing'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务插件**：启用额外的网络功能，包括路由、私有网络、防火墙和负载均衡。'
- en: Important note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The code for plugins is run on the Neutron server, which can be configured as
    part of the cloud controller node(s).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 插件的代码在 Neutron 服务器上运行，该服务器可以配置为云控制节点的一部分。
- en: The following section will dive into the most adopted core plugins.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将深入探讨最常用的核心插件。
- en: Core plugin – Modular Layer 2
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核心插件 – 模块化二层
- en: The most widely adopted core plugin in Neutron is **Modular Layer 2** ( **ML2**
    ). The introduction of ML2 to the OpenStack networking service has increased the
    flexibility of designing network architectures. The secret sauce of ML2 is that
    it supports the use of a variety of technologies from different vendors simultaneously.
    Thus, cloud architects and operators aren’t limited to a specific set of features
    and can easily extend or change their network stacks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 中最广泛采用的核心插件是 **模块化二层**（**ML2**）。ML2 引入 OpenStack 网络服务，增加了设计网络架构的灵活性。ML2
    的“秘诀”在于它支持同时使用来自不同供应商的多种技术。因此，云架构师和操作员不会局限于特定的功能集，可以轻松扩展或更改他们的网络架构。
- en: 'This plugin has been developed due to the historical limitations prior to the
    **Havana** release, where operators had to stick to only one core plugin, referred
    to as a monolithic plugin-based component. The two most commonly used core plugins
    were **Linux Bridge** and **OVS** , which have been replaced in the latest releases
    of OpenStack by ML2. Replacing does not mean directly removing any of them but
    bundling them as **mechanism drivers** in the core ML2 plugin. ML2 is not limited
    to Linux Bridge and OVS and also supports other vendor drivers, such as VMware,
    Cisco, and OpenDayLight. An amazing filtered list can be found on the OpenStack
    community website: [https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking)](https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking))
    .'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在 **Havana** 发布之前的历史限制，开发了这个插件。在那个时候，操作员只能使用一个核心插件，称为单体插件组件。最常用的两个核心插件是 **Linux
    Bridge** 和 **OVS**，在 OpenStack 的最新版本中，它们已经被 ML2 替代。替代并不意味着直接删除它们，而是将它们作为 **机制驱动程序**
    打包到核心 ML2 插件中。ML2 不仅限于 Linux Bridge 和 OVS，还支持其他厂商的驱动程序，如 VMware、Cisco 和 OpenDayLight。在
    OpenStack 社区网站上可以找到一个很棒的过滤列表：[https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking)](https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking))。
- en: 'An overview of the modular plugins and drivers supported in Neutron is illustrated
    here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 中支持的模块化插件和驱动程序的概览如下所示：
- en: '![Figure 6.2 – An overview of the OpenStack Neutron core and service plugins
    architecture](img/B21716_06_02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – OpenStack Neutron 核心和服务插件架构概览](img/B21716_06_02.jpg)'
- en: Figure 6.2 – An overview of the OpenStack Neutron core and service plugins architecture
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – OpenStack Neutron 核心和服务插件架构概览
- en: 'As shown in the preceding diagram, Neutron relies on the mechanisms of plugins
    that provide a specific set of networking services. An API request will be forwarded
    by the Neutron server to the associated plugin configured in the controller node.
    The ML2 plugin combines two main elements into the same framework, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，Neutron 依赖于插件机制，这些插件提供一组特定的网络服务。API 请求将由 Neutron 服务器转发给控制节点中配置的相关插件。ML2
    插件将两个主要元素结合到同一个框架中，如下所示：
- en: '**Type drivers** : These expose L2 functionalities to create more segmented
    networks, including the following:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型驱动程序**：这些暴露了 L2 功能，用于创建更多分段的网络，包括以下内容：'
- en: '**VLAN** : Segregates network traffic using **802.1Q** tagging. Virtual machines
    that belong to the same VLAN are part of the same broadcast L2 domain'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VLAN**：使用 **802.1Q** 标签隔离网络流量。属于同一 VLAN 的虚拟机是同一广播 L2 域的一部分'
- en: '**VXLAN** : Uses a **virtual network identifier** ( **VNI** ) to separate and
    differentiate traffic between different networks'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VXLAN**：使用 **虚拟网络标识符**（**VNI**）来分隔和区分不同网络之间的流量'
- en: '**Flat** : VLAN tagging and network segregation are not supported where instances
    are connected in the same network'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flat**：不支持 VLAN 标签和网络隔离，其中实例连接在同一网络中'
- en: '**GRE** : Encapsulates traffic using the GRE tunneling protocol'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRE**：使用 GRE 隧道协议封装流量'
- en: '**GENEVE** : Resembles the VXLAN overlay technology but with more optimal encapsulation
    methods'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GENEVE**：类似于 VXLAN 覆盖技术，但具有更优化的封装方法'
- en: '**Local** : Connects instances only within the same network hosted in the same
    compute nodes and not with instances in different nodes'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Local**：仅连接同一网络中托管在相同计算节点中的实例，而不连接不同节点中的实例'
- en: '**Mechanism drivers** : Implements the type driver technologies through both
    network software methods such as OVS, Linux Bridge, and OVN and hardware-based
    ones'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机制驱动程序**：通过网络软件方法（如 OVS、Linux Bridge 和 OVN）以及基于硬件的方法实现类型驱动技术'
- en: An up-to-date ML2 driver support matrix list can be found at [https://docs.openstack.org/neutron/latest/admin/config-ml2.html](https://docs.openstack.org/neutron/latest/admin/config-ml2.html)
    . Each mechanism driver supports a set of network types (type drivers). For example,
    OVS supports most network types, including VLAN, VXLAN, GRE, flat, and local.
    If a need has been raised to support the GENEVE type driver while in production,
    the new mechanism driver can be integrated by just installing the driver and reconfiguring
    the Neutron driver list.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 ML2 驱动支持矩阵列表可以在 [https://docs.openstack.org/neutron/latest/admin/config-ml2.html](https://docs.openstack.org/neutron/latest/admin/config-ml2.html)
    找到。每个机制驱动程序都支持一组网络类型（类型驱动程序）。例如，OVS 支持大多数网络类型，包括 VLAN、VXLAN、GRE、flat 和 local。如果在生产环境中提出了支持
    GENEVE 类型驱动的需求，可以通过仅安装驱动并重新配置 Neutron 驱动程序列表来集成新的机制驱动程序。
- en: Building virtual switching
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建虚拟交换
- en: In this section, two mechanism drivers will be discussed to establish connectivity
    between virtual network ports and physical networks. OVS is one of the most featured
    mechanism drivers, with advanced networking capabilities such as **OpenFlow**
    . The second driver we will cover is **OVN** , an SDN implementation that enables
    programming networks by controlling the flow of packets. Linux Bridge and **L2
    Population** are also well-tested and mature mechanisms. With ML2 being an agnostic
    plugin for the supported drivers, introducing a new mechanism driver will not
    introduce a lot of complexity in an existing running OpenStack environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论两种机制驱动程序，用于在虚拟网络端口与物理网络之间建立连接。OVS 是功能最丰富的机制驱动程序之一，具有高级网络功能，如 **OpenFlow**。我们将讨论的第二个驱动程序是
    **OVN**，这是一种 SDN 实现，通过控制数据包流动来编程网络。Linux Bridge 和 **L2 Population** 也是经过充分测试和成熟的机制。由于
    ML2 是一个支持驱动程序的无关插件，介绍新的机制驱动程序不会对现有的 OpenStack 环境带来太多复杂性。
- en: Opening vSwitch in OpenStack
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 OpenStack 中启用 vSwitch
- en: 'Most OpenStack networking implementations would require a minimum of overlay
    networking technologies that provide tunnel-based virtual networking, with an
    immense level of network segmentation, such as VXLAN and GRE. Tunnel-based networks
    can provide up to 16 million networks. OVS supports most used drivers in large
    and complex OpenStack networking environments, such as VLAN, VXLAN, GRE, and flat.
    As a cloud operator, it is crucial to have a basic understanding of how OVS operates
    when installed in an OpenStack environment. OVS comes with a complete core architecture,
    with different components that operate as software switches in a host kernel space.
    The OVS implementation comes with different services when installed in an OpenStack
    environment, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 OpenStack 网络实现至少需要使用基于隧道的虚拟网络技术，提供极高程度的网络分段，如 VXLAN 和 GRE。基于隧道的网络能够支持多达
    1600 万个网络。OVS 支持大多数在大型复杂 OpenStack 网络环境中使用的驱动程序，如 VLAN、VXLAN、GRE 和扁平网络。作为云操作员，了解
    OVS 在 OpenStack 环境中如何运作是至关重要的。OVS 提供了完整的核心架构，并通过不同组件在主机内核空间中作为软件交换机运行。OVS 实现中包含多个服务，当它安装在
    OpenStack 环境中时，具体如下：
- en: '**openvswitch** : A kernel module that handles the data plane that processes
    the network packets.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**openvswitch**：一个内核模块，处理数据平面，负责处理网络数据包。'
- en: '**ovs-switchd** : A Linux process that runs on a physical host to control and
    manage virtual switches.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ovs-switchd**：一个运行在物理主机上的 Linux 进程，用于控制和管理虚拟交换机。'
- en: '**ovsdb-server** : A local database to store the virtual switches running locally.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ovsdb-server**：一个本地数据库，用于存储本地运行的虚拟交换机。'
- en: '**neutron-openvswitch-agent** : Configured in compute nodes that use the OVS
    mechanism driver.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**neutron-openvswitch-agent**：配置在使用 OVS 机制驱动的计算节点中。'
- en: '**neutron-server** : Handles API requests, with the ML2 plugin loading the
    OVS mechanism driver. The neutron-server process passes the network request to
    the OVS driver via an RPC cast message to **neutron-openvswitch-agent** . The
    latter configures the OVS switch on the compute node and sets the required resources
    of the local instance.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**neutron-server**：处理 API 请求，ML2 插件加载 OVS 机制驱动程序。neutron-server 进程通过 RPC 广播消息将网络请求传递给
    OVS 驱动程序，后者通过 **neutron-openvswitch-agent** 配置计算节点上的 OVS 交换机，并设置本地实例所需的资源。'
- en: 'Once OVS is implemented, a set of virtual network devices is installed, which
    a cloud operator should keep in mind during deployment or troubleshooting tasks.
    An Ethernet frame would travel from an instance through the following interfaces:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 OVS 被实现，一套虚拟网络设备将被安装，云操作员在部署或故障排除任务中应当牢记这一点。一个以太网帧将通过以下接口从实例传输：
- en: '**tapXXXX** : A tap interface where **XXXX** is the assigned tap interface
    ID.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tapXXXX**：一个 tap 接口，其中 **XXXX** 是分配的 tap 接口 ID。'
- en: '**br-int** : A bridge interface, known as the integration bridge, where it
    consolidates all the virtual devices, such as virtual machines, routers, and firewalls.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-int**：一个桥接接口，称为集成桥接，它将所有虚拟设备（如虚拟机、路由器、防火墙）整合在一起。'
- en: '**int-br-ethX** : A virtual patch port connecting different interfaces to the
    OVS integration bridge interface ( **br-int** ).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**int-br-ethX**：一个虚拟补丁端口，将不同接口连接到 OVS 集成桥接接口（**br-int**）。'
- en: '**phy-br-ethX** : A virtual patch port connecting different interfaces to the
    OVS provider bridge ( **br-ethX** ).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**phy-br-ethX**：一个虚拟补丁端口，将不同接口连接到 OVS 提供者桥接（**br-ethX**）。'
- en: '**br-ethX** : A bridge interface where *X* is the assigned bridge interface
    ID connecting to the physical network. It is also known as the provider bridge
    and connects to the **br-int** integration bridge through a virtual patch cable,
    provided by patch ports ( **int-br-ethX** and **phy-br-ethX** patch ports for
    **int-br** and **br-ethX** , respectively).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-ethX**：一个桥接接口，其中*X*是分配的桥接接口ID，用于连接物理网络。它也被称为提供程序桥接，通过虚拟补丁电缆与**br-int**集成桥接连接，由补丁端口（**int-br-ethX**和**phy-br-ethX**补丁端口分别用于**int-br**和**br-ethX**）提供。'
- en: '**qbrXXXX** : A Linux bridge interface where **XXXX** is the assigned bridge
    interface ID dedicated to IP tables only.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**qbrXXXX**：一个Linux桥接接口，其中**XXXX**是分配的桥接接口ID，专门用于IP表。'
- en: '**br-tun** : A bridge tunnel interface to handle packet encapsulation and de-encapsulation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-tun**：一个桥接隧道接口，用于处理数据包的封装和解封装。'
- en: '**br-ex** : A bridge interface providing connectivity to external networks.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-ex**：一个提供与外部网络连接的桥接接口。'
- en: Each spawned instance is connected via its tap interface, denoted as **tapXXXX**
    , created in the hypervisor host. The associated Linux bridge, denoted as **qbrXXXX**
    , is connected to the OVS **br-int** integration bridge, where the traffic will
    be routed based on the programmed OpenFlow rules and then forwarded through the
    virtual switch.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个启动的实例通过其tap接口连接，表示为**tapXXXX**，该接口在虚拟化主机中创建。相关的Linux桥接，表示为**qbrXXXX**，连接到OVS的**br-int**集成桥接，流量将根据编程的OpenFlow规则进行路由，然后通过虚拟交换机转发。
- en: 'Connections between the integration and provider bridges are handled by patch
    cables. Packets exit the physical network via the OVS provider bridge ( **br-ethX**
    ), connected to the physical network interface of the host. When configuring OVS
    in our existing OpenStack environment, each node will run its own integration
    and provider bridges. Connectivity between the OpenStack nodes, including the
    cloud controller, network, and compute nodes, is established via the physical
    network, as illustrated in the following diagram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 集成桥接和提供程序桥接之间的连接通过补丁电缆处理。数据包通过OVS提供程序桥接（**br-ethX**）离开物理网络，连接到主机的物理网络接口。在我们的现有OpenStack环境中配置OVS时，每个节点将运行自己的集成桥接和提供程序桥接。OpenStack节点之间的连接，包括云控制器、网络节点和计算节点，通过物理网络建立，正如下图所示：
- en: '![Figure 6.3 – Traffic flow through different interfaces using OVS](img/B21716_06_03.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 使用OVS通过不同接口的流量流动](img/B21716_06_03.jpg)'
- en: Figure 6.3 – Traffic flow through different interfaces using OVS
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 使用OVS通过不同接口的流量流动
- en: Now that we understand the basic core components of OVS within an OpenStack
    setup, we can move on to configuring our infrastructure code to deploy OVS as
    the main mechanism driver in Neutron.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了OpenStack设置中OVS的基本核心组件，可以继续配置我们的基础设施代码，将OVS部署为Neutron中的主要机制驱动程序。
- en: Deploying with OVS
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用OVS进行部署
- en: 'The OVS mechanism driver implements L2 isolation, which offers VXLAN-, GRE-,
    and VLAN-based networks. The next configuration will implement the ML2 plugin
    to use VXLAN-based networks. This will allow instances to connect via VLAN segmentation
    and VXLAN tunneling. Virtual networks for tenants will not be exposed outside
    of the compute or network nodes. **kolla-ansible** comes with the ML2 plugin enabled
    by default. A few prerequisites must be fulfilled before deploying an **OVS**
    configuration. To reach the external networks via routers and provider networks,
    adjust the **neutron_external_interface** setting in the **/etc/kolla/globals.yml**
    file to the network interface dedicated to this matter. The **eth2** interface
    has been assigned an interface to connect with the external world, as designed
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* . You can adjust the settings as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OVS机制驱动程序实现L2隔离，提供基于VXLAN、GRE和VLAN的网络。接下来的配置将实现ML2插件，以使用基于VXLAN的网络。这将允许实例通过VLAN分段和VXLAN隧道连接。租户的虚拟网络将不会暴露到计算或网络节点之外。**kolla-ansible**默认启用ML2插件。在部署**OVS**配置之前，必须满足一些先决条件。为了通过路由器和提供程序网络访问外部网络，请在**/etc/kolla/globals.yml**文件中调整**neutron_external_interface**设置，指向专门用于此目的的网络接口。**eth2**接口已被分配用于连接外部网络，正如在[*第3章*](B21716_03.xhtml#_idTextAnchor108)中设计的那样，*OpenStack控制平面
    – 共享服务*。您可以按如下方式调整设置：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: 'It is important to consider the usage of a physical interface with the OVS
    provider bridge ( **br-ethX** ) if you are planning to use more than one provider
    bridge. In this case, **neutron_external_interface** can be assigned a comma-separated
    list – for example, **neutron_external_interface: "eth2,eth3"** .'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The other check is to verify whether **kolla-ansible** is configured with an
    OVS driver mechanism that comes with the ML2 plugin. Check and adjust the following
    configuration setting in the **globals.yml** file:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next check is to verify that our Neutron services are assigned to the respective
    OpenStack nodes in the **/ansible/inventory/multi_packtpub_prod** file. As demonstrated
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* , **neutron-server** will run as part of the controller node,
    as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Neutron agents, including the DHCP, L3, and metadata agents, will run in a
    dedicated network node, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: OVS will be installed in the network and compute nodes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage
    – Block, Object, and File Shares* , the **manila-share** service provides access
    to the file share by using the Neutron plugin.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'The OVS setup in the inventory file is listed as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Launch the pipeline job to install OVS across the target nodes. Note that VXLAN
    was not explicitly configured in the **kolla-ansible** code. By default, the Neutron
    tenant network uses the VXLAN type. The default VNI range is defined from 1 to
    1,000 IDs, reserved for the tenant networks when they are created. This setting
    is denoted with the **vni_ranges** line in the **ml2_config.ini** file in the
    controller node. The ML2 configuration can be extended and we can employ a configuration
    override by creating an **/** **etc/kolla/config/neutron/ml2_config.ini** file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command line lists the different network agents, including the
    OVS ones, from the cloud controller node:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The neutron agent list output](img/B21716_06_04.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – The neutron agent list output
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: As shown here, the OVS agents should be up and running in the respective network
    and compute nodes. In the following section, we will trace the network flow with
    the OVS mechanism.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Traffic flow with OVS
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The setup of the OVS driver will dictate the travel hops of an Ethernet frame,
    from an instance all the way to the physical network. Numerous interfaces will
    be used, including the virtual switch ports and integration bridge. The deployment
    of the **openvswitch** playbook in the compute node should create the OVS logical
    bridges on the host, which can be checked by running the following command line:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output will be as shown:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The OVS bridges list](img/B21716_06_05.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – The OVS bridges list
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Instances spawned in the compute node are connected via the same local VLAN.
    With the VXLAN layout, instance ports are attached to the integration bridge,
    which will be assigned a local VLAN ID. Traffic initiated from an instance will
    be forwarded by the integration bridge ( **br-int** ) to the tunnel bridge ( **br-tun**
    ). The overall layout of the connecting bridges and their respective ports can
    be seen by running the following command line:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算节点中启动的实例通过相同的本地VLAN连接。采用VXLAN布局时，实例端口连接到集成桥，该桥将分配一个本地VLAN ID。从实例发起的流量将由集成桥（**br-int**）转发到隧道桥（**br-tun**）。通过运行以下命令行，可以查看连接桥及其各自端口的整体布局：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![Figure 6.6 – An OVS interface listing](img/B21716_06_06.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – OVS接口列表](img/B21716_06_06.jpg)'
- en: Figure 6.6 – An OVS interface listing
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – OVS接口列表
- en: As depicted in the virtual switch interfaces, the integration bridge ( **br-int**
    ) is tagged with **VLAN 1.** When traffic reaches instances in different nodes,
    the packets will travel through the tunnel bridge ( **br-tun** ) encapsulated
    in VXLAN packets. Under the hood, the local VLAN ID of **1** will be swapped with
    the VXLAN tunnel ID, as shown with **vxlan-476984a0** . Otherwise, if an instance
    connects to another one residing in the same compute node, the packets will travel
    through the integration bridge ( **br-int** ) locally to the destination port,
    denoted with the **qrXXXX** and **tapYYYY** port interfaces.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如虚拟交换机接口所示，集成桥（**br-int**）标记为**VLAN 1**。当流量到达不同节点中的实例时，数据包将通过隧道桥（**br-tun**）封装在VXLAN数据包中传输。在背后，本地VLAN
    ID **1** 将与VXLAN隧道ID交换，如**vxlan-476984a0**所示。否则，如果实例连接到另一个位于同一计算节点的实例，数据包将通过集成桥（**br-int**）本地传输到目标端口，该端口通过**qrXXXX**和**tapYYYY**端口接口表示。
- en: The following section covers a more complex mechanism driver, OVN, and demonstrates
    its deployment in our OpenStack environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分介绍了更复杂的机制驱动程序OVN，并展示了其在OpenStack环境中的部署。
- en: OVN in OpenStack
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OVN在OpenStack中的应用
- en: The other major and preferred network virtualization in an OpenStack environment
    is OVN. OVN is considered a mature implementation of the SDN philosophy that provides
    greater flexibility for network flow programming. Operators can define a set of
    forwarding rules centrally to define how the flow of packets is controlled. OVN
    uses an abstraction software layer available in a network controller to program
    switches, with conditions and rules applied to the packet flow. The most distinctive
    functionality of OVN compared to OVS is its richer capabilities and advanced features,
    such as traffic routing and access control which are programmable. Unlike the
    OVS approach, OVN decouples the control function of network devices from the actual
    packet forwarding function through flow rules. OVN supports most network types,
    including VLAN, VXLAN (version 20.09 and later), flat, and Geneve.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack环境中另一个主要且首选的网络虚拟化是OVN。OVN被认为是SDN理念的成熟实现，提供了更大的网络流编程灵活性。操作员可以集中定义一组转发规则，来定义数据包流量的控制方式。OVN使用网络控制器中的抽象软件层来编程交换机，应用于数据包流的条件和规则。与OVS相比，OVN的最显著功能是其更丰富的能力和先进特性，如可编程的流量路由和访问控制。与OVS方法不同，OVN通过流规则将网络设备的控制功能与实际的数据包转发功能解耦。OVN支持大多数网络类型，包括VLAN、VXLAN（版本20.09及以上）、平面网络和Geneve。
- en: 'As depicted in the following diagram, OVN is built on top of OVS, which leverages
    its switching capabilities and adds an abstraction layer, managed by controllers
    that are stored in a set of **OVS** **databases** ( **OVSDBs** ):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，OVN建立在OVS之上，利用其交换能力并增加一个抽象层，该抽象层由控制器管理，这些控制器存储在一组**OVS** **数据库**（**OVSDBs**）中：
- en: '![Figure 6.7 – OVN integration architecture](img/B21716_06_07.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – OVN集成架构](img/B21716_06_07.jpg)'
- en: Figure 6.7 – OVN integration architecture
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – OVN集成架构
- en: 'The main constructs of the OVN architecture integration with an OpenStack environment
    are explained here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: OVN架构与OpenStack环境集成的主要构件在此进行解释：
- en: '**Southbound database** : Referenced as **ovsdb-server** ( **ovnsb.db** ),
    this stores data of the logical and physical network flows. The database hosts
    all binding information between the logical and physical networks, such as the
    port and logical networking bindings associated with the **Port_Binding** and
    **Logical_Flow** tables, respectively.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Northbound database** : Referenced as **ovsdb-server** ( **ovnnb.db** ),
    this stores data at a high level of the virtual networks that represent a **cloud
    management system** ( **CMS** ) – in this case, an OpenStack environment. The
    database reflects the networking resources of the CMS in the table datasets, such
    as the routers and switch ports associated with the **Logical_Router** and **Logical_Switch_Port**
    tables, respectively.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OVN controller** : Referenced as **ovn-controller** , this runs on hypervisor
    nodes and connects to the southbound database. It acts as the OVS controller by
    defining the OpenFlow rules on each OVS switch instance, converting logical flows
    into physical flows, and pushing configurations to the local OVSDB.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OVN northbound service** : Referenced as **ovn-northd** , this runs as part
    of the control plane on the cloud controller. The **ovn-northd** daemon connects
    the northbound to the southbound database by translating the logical configuration
    from the **ovn-nb** data to logical data path flows. The converted data will be
    populated in the **ovn-sb** database and made ready for use by the **ovn-controller**
    instance.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OVS data plane service** : Referenced as **ovs-vswitchd** , this runs as
    part of the data plane on the hypervisor nodes. **ovs-vswitchd** applies the packet-forwarding
    rules provided by **ovn-controller** .'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local OVSDB** : Referenced as **ovsdb-server** , this runs locally in each
    hypervisor node. The OVN reference architecture takes advantage of the database
    scaling benefit by using a consistent snapshot of the database, to recover from
    possible connectivity interruptions.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata agent** : Referenced as **ovn-metadata-agent** , this runs in each
    hypervisor node. The OVN metadata agent, such as the Neutron metadata agent, is
    used to proxy metadata API requests on each compute node. This way, each OVS switch
    instance running in each hypervisor node will route the requests to the local
    metadata agent and then query the Nova metadata service to run instances.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will use **kolla-ansible** to integrate the OVN
    project in an OpenStack environment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with OVN
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OVN extends the OVS implementation to provide network services to instances.
    The ML2 mechanism driver facilitates integration with OpenStack as a CMS. The
    **kolla-ansible** playbooks include most of the OVN configuration and required
    packages to install. Similar to the OVS configuration, adjust **/etc/kolla/globals.yml**
    to use the designated interface to reach external networks, by configuring the
    **neutron_external_interface** setting, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Configure the ML2 mechanism driver to use OVN, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'OVN comes with built-in support for L3 networking functions that enable instances
    to connect to external networks. There are two ways to provide such connectivity
    in OVN – through a centralized layout that does not use a distributed floating
    IP design or via **distributed virtual routing** ( **DVR** ), which requires a
    floating IP configuration. For two instances to reach external networks or each
    other using L3, traffic must flow through the network node. With the DVR method,
    traffic is more optimized by deploying an L3 agent in each compute node, allowing
    **north-south** (going to and from external networks) traffic with a floating
    IP to be routed from and to the compute node, with an extra hop to reach the network
    node. Similarly, **east-west** (traffic flow between instances) traffic is routed
    directly to the compute nodes. The latter option is recommended for more optimal
    performance. The distributed floating IP option can be enabled in the **globals.yml**
    file, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Enable the Neutron OVN agent to provide extensible capabilities for network
    monitoring and **Quality of Service** ( **QoS** ) by configuring the following
    setting:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, make sure to adjust the **/ansible/inventory/multi_packtpub_prod** inventory
    file to install the OVN services in the assigned nodes. Similar to the OVS configuration,
    make sure to have the Neutron server and respective agents running in the controller
    and network nodes, respectively, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As discussed in the *OVN in OpenStack* section, the OVS northbound and southbound
    databases, in addition to the OVN **northd** daemon, will run on the controller
    node, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The OVN controller instances and OVN metadata agents will run on the compute
    nodes, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Since Antelope and later releases, a dedicated OVN agent has been created to
    implement the lacking **ovn-controller** function.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Launching the pipeline job should deploy the OVN services and create an ML2
    file, **/etc/neutron/plugins/ml2/ml2_conf.ini** , with the following default configurations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The default network tenant type is configured with GENEVE as the tunneling protocol.
    The main difference between GENEVE and VXLAN (configured for OVS in a previous
    section) is how the networking metadata is encapsulated and encoded between both
    protocols. VXLAN encodes the VNI only in the encapsulation header. Meanwhile,
    the GENEVE protocol uses extensible TLV to carry more information about the packet
    in the encapsulated header (it has a header length of 38 bytes, while the VXLAN
    frame can carry 8 bytes), such as the **ingress** and **egress** ports. GENEVE
    offers better capabilities, such as transport security, service chaining, and
    in-band telemetry, compared to VXLAN.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The other resulting default configuration is the default northbound and southbound
    database addresses. Each OVN controller running in each compute node should be
    able to reach both databases. Such configuration can be found in the same file,
    **ml2_conf.ini** , by checking the following settings:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once OVN is configured, further network resource creation and management will
    be handled by OVN, which maps each OpenStack resource abstraction to an entry
    object in the OVN southbound and northbound databases.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Both OVS and OVN provide different capabilities to connect instances to networks.
    In the following section, we will explore how routing is performed in Neutron
    and demonstrate the different routing traffic options in OpenStack.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Configuring cloud routing
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instances within the same virtual tenant network can reach each other, but by
    default, each tenant network cannot reach other tenants or external networks.
    Deploying virtual routers is the way to enable L3 network communication so that
    tenant virtual networks can connect by associating a subnet with a router.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Routing tenant traffic
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Under the hood, a port associated with a tenant virtual network will be associated
    with the IP address of the subnet gateway. Instances across different virtual
    networks reach each other by communicating via the virtual router, using the gateway
    IP address and their private IP addresses encapsulated in the packets. This is
    called a **NAT** ( **network address translation** ) mechanism. In OpenStack networking,
    the Neutron L3 agent manages virtual routers. IP packets are forwarded by a virtual
    router to different self-service and external networks through the following router
    interfaces:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '**qr** : Contains the tenant network gateway IP address and is dedicated to
    routing traffic through all self-service traffic networks'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qg** : Contains the external network gateway IP address and is dedicated
    to routing traffic out onto the external provider network'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upon the initiation of a virtual router instance, a network namespace will be
    created in the Neutron node that defines connections to self-service or external
    provider networks through routing tables, packet forwarding, and **iptables**
    rules. As shown in the following diagram, a router namespace refers to a virtual
    router that is attached to multiple bridge ports in an OVS configuration ( **qr**
    and **qg** ). Traffic flow between instances hosted in the same or different compute
    nodes is routed through the virtual router.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Neutron router namespace connectivity based on OVS implementation](img/B21716_06_08.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Neutron router namespace connectivity based on OVS implementation
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In the previous OVS implementation, the L3 agent should be up and running to
    start creating and managing virtual routers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The router service plugin should be enabled by default once Neutron agent L3
    is deployed, using **kolla-ansible** . The service plugin can be verified in the
    **/etc/neutron/neutron.conf** file in the Neutron cloud controller node by checking
    the **service_plugin =** **router** line.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, virtual routers can be managed through Horizon. The **kolla-ansible**
    run for the Neutron deployment should enable the router module in the dashboard.
    That can be verified in the cloud controller node’s **/etc/openstack-dashboard/local_settings.py**
    file with the following settings:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the next exercise, we will create a tenant virtual network with the ML2
    plugin configured in OVS. A virtual router will be attached to the tenant network
    using the OpenStack CLI, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a tenant virtual network using the OpenStack network CLI:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a subnet with an IP range of **10.10.0.10/24** , an auto-assigned DHCP,
    and a default DNS nameserver of **8.8.8.8** :'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a router and attach it to the created tenant network:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The router attachment to the tenant network will assign a private IP address
    to the router’s internal interface. By default, if no IP address is specified
    in the attachment command line, the internal interface will be assigned the default
    gateway of the subnet. The following command line verifies the assigned IP address
    of the router’s internal interface:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It gives the following output:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Virtual router port listing](img/B21716_06_09.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Virtual router port listing
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'The router interface can be checked within the router namespace **qr-** prefixed
    interface by running the following command:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output looks like the following:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The network namespaces list](img/B21716_06_10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – The network namespaces list
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the **qrouter** ID from the previous output, and then run the following
    command line to show the created interface of the router and the assigned IP address
    from the internal network:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will get the following output:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11 – A virtual router namespace internal interface listing](img/B21716_06_11.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – A virtual router namespace internal interface listing
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable instances to reach the external network, a second interface in the
    virtual router should be created that will be attached to the provider’s external
    network. A common networking setup, such as an external network device or a device
    integrated with a firewall appliance, should be placed in front of the OpenStack
    endpoint (such as the load balancer). Enabling internet access can be configured
    using the OpenStack CLI, as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an external network provider. As configured in the ML2 plugin for OVS,
    we can create the network as a VLAN type, with a defined segmentation ID and **physnet1**
    for the physical network attribute:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Create the subnet part of the external network with a network range of **10.20.0.0/24**
    and the default gateway as **10.20.0.1** , disable DHCP, and set an allocation
    pool of **10.20.0.10-10.20.0.100** :'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Attach the router to an external provider network by running the following
    command line:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The attachment operation will assign an external IP from the external IP pool
    to the router, which can be checked by running the following command line:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is the output:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12 – A created external port of the virtual router](img/B21716_06_12.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – A created external port of the virtual router
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'The last attachment creates a second interface, prefixed with **qg-** in the
    router namespace, which can be verified by the following command line:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.13 – A virtual router namespace external interface listing](img/B21716_06_13.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – A virtual router namespace external interface listing
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of our connectivity demonstration involves creating security
    groups and rules to allow ingress and egress traffic at the network port level.
    To reach the internet and access the instances, we will create a new security
    group and add **ICMP** and **SSH** access:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the OpenStack CLI, create a new security group to be applied to your
    instances:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create the rules associated with the created security group for SSH and ICMP,
    respectively:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Using the OpenStack CLI, create a new test instance with a **tiny** flavor
    and a **cirros** image that is connected to the private tenant network:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Important note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to adjust your available Glance image name and list of flavors, based
    on your existing resources. The **openstack server create** command line will
    fail if any of the assigned arguments do not exist. Cirros is a minimal Linux
    distribution, useful for quick testing and proof of concepts. The default session
    username is **cirros** and the password is **gocubsgo** .
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the connectivity from the instance to reach the internet, make sure
    that the instance state is **ACTIVE** , as follows:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is the output:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.14 – The instance listing](img/B21716_06_14.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – The instance listing
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Access to the created instance can be made in different ways, using the **virsh
    console** command line from the compute node or simply via SSH from the router
    namespace. Make sure to use the default **cirros** image credentials – that is,
    the username **cirros** and the password **gocubsgo** – and run a simple ping
    to reach **8.8.8.8** :'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here is the output:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Testing external connectivity](img/B21716_06_15.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Testing external connectivity
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The instance uses the internal virtual router IP address as the default gateway
    to route traffic reaching the external network. The internet is reachable through
    SNAT, performed by the router. A quick run of the **ip route** command in the
    instance shows the default gateway associated with the network route table:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Listing the default gateway](img/B21716_06_16.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Listing the default gateway
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of our walk-through is a demonstration of how resources hosted
    in external networks can reach instances in an OpenStack environment. By default,
    spawned instances will be assigned an IP address that is not visible outside of
    the tenant network. Neutron provides float IP addresses that implement **DNAT**
    ( **destination NAT** ). The router simply forwards incoming packets reaching
    its external interface to the destination instance by checking its configured
    DNAT rule. The traffic response from an instance to external resources uses the
    source IP addresses translated to the floating IP, demonstrated as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the port ID of the created instance, and create a floating IP address
    to associate with it:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here is the output:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Listing the instance port](img/B21716_06_17.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Listing the instance port
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the port ID, and run the following command line by pasting the external
    port ID after the **--** **port** option:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here is the output:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Assigning the floating IP address to the external port](img/B21716_06_18.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Assigning the floating IP address to the external port
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, the router namespace configures a secondary address associated
    with the external interface, prefixed with **''qg''** :'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is the output:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Associating the IP address with the virtual router external
    interface](img/B21716_06_19.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Associating the IP address with the virtual router external interface
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Traffic routed through the external network provider can reach the instance
    via the assigned floating IP.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have explored a standard routing implementation in OpenStack. The
    next section will uncover another way of performing routing, via dynamic routing,
    in Neutron.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Neutron dynamic routing
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dynamic routing in OpenStack networking is based on **BGP** ( **Border Gateway
    Protocol** ), enabling tenant networks to advertise their network prefixes with
    physical or virtual routers and network devices that support BGP. The new addition
    of BGP in Neutron eliminates the usage of floating IPs for tenant networks that
    do not rely on network administrators to advertise their network upstream. The
    term *dynamic routing* in Neutron was introduced with the **Mitaka** release.
    The adoption of the BGP routing mechanism varies from one cloud environment to
    another, depending on the networking setup, mostly due to the requirement for
    direct connectivity between the network node and the physical network gateway
    device to peer with (such as a LAN or WAN peer). To avoid IP overlapping when
    advertising IP prefixes, dynamic routing relies on **address scopes** and **subnet
    pools** , Neutron mechanisms that control the allocation of subnet addresses and
    prevent the usage of addresses that overlap.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of the BGP implementation, Neutron introduced the **BGP speaker**
    , which enables peering between tenant networks and external router devices. The
    BGP speaker advertises the tenant network to the tenant router, initially as a
    first hop. The BGP speaker in Neutron is not a router instance, nor does it manipulate
    BGP routes. It mainly orchestrates the BGP peer’s information between the tenant
    routers and external ones. The speaker requires a network or cloud operator to
    configure the peering endpoints. As shown in the following diagram, for a successful
    BGP dynamic routing in OpenStack, the Neutron virtual router must be attached
    to both the tenant subnet and the external provider device interface.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Neutron BGP peering and router connectivity for dynamic routing](img/B21716_06_20.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Neutron BGP peering and router connectivity for dynamic routing
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Both the BGP speaker and the external provider device must be peered (connected
    to the Neutron virtual router). Finally, both the tenant and the external networks
    must be in the same address scope. Adding BGP dynamic routing using **kolla-ansible**
    is straightforward. We will configure the dynamic routing based on the OVS implementation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Since the Antelope release, Neutron has supported dynamic routing with the OVN
    mechanism driver.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'As Neutron provides a BGP agent with default configuration, we will just need
    to enable the agent installation in the network node by adding the following line
    to the **/ansible/inventory/multi_packtpub_prod** inventory file:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Enable the agent installation in the **globals** **.yml** file, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Launch the pipeline to roll out the BGP agent installation in the network node.
    In the network node, the installed BGP agent can be checked by running the following
    command line:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here is the output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Listing the BGP network agent](img/B21716_06_21.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Listing the BGP network agent
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: The Neutron BGP CLI to manage BGP speakers can be found at [https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html](https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html)
    .
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Virtual routers are part of the building blocks of OpenStack networking, providing
    a variety of connectivity options to tenants. It is important to note that running
    with a standalone router presents a single point of failure. [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , will discuss Neutron
    implementation for highly available routers. Dynamic routing via BGP in Neutron
    is considered an amazing routing addition in OpenStack. Neutron has been enriched
    and modified across OpenStack releases. One of these major modifications is the
    development of new networking services in OpenStack, which will be discussed in
    the next section.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Joining more networking services
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the overwhelming routing and switching capabilities in Neutron that
    come with the latest OpenStack releases, Neutron enables cloud operators to offer
    additional networking services. Services such as load balancing, firewalls, and
    private networks support tenant networks to build well-architected application
    stacks for a variety of use cases. In the next section, we will cover the most
    used and stable additional Neutron services and deploy them, using **kolla-ansible**
    .
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer as a service
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the **Liberty** release, the Neutron **Load Balancer as a Service** (
    **LBaaS** ) plugin has moved from version 1 to version 2, codenamed **Octavia**
    . Cloud users who seek to balance the load of their workloads using a managed
    load balancer will appreciate the Octavia offering. Octavia is designed to scale
    horizontally, as it creates and manages virtual machines acting as load-balancer
    instances, referred to as **amphorae** .
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Octavia implements the same load-balancing terms as the popular **HAProxy**
    , which can be summarized as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual IP (VIP)** : A layer 4 object that exposes a service to be accessed,
    externally or internally, that is associated with a Neutron port. A load balancer
    is assigned a VIP, and requests that reach the VIP are distributed among backend
    servers or pool members.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pool** : A group of instances serving the same content or service, such as
    web servers.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pool members** : An instance of a pool represented by an IP address and listening
    port that exposes the service.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Listeners** : A port associated with the VIP that listens for incoming requests.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health monitor** : Orchestrates the management of pool members based on health
    checks carried out on each member. Failed health checks will discard the members
    from the pool, and traffic will be served by the healthy ones.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load-balancing algorithms** : **Round robin** , **least connections** , and
    **source IPs** are supported algorithms in Octavia that can be assigned when creating
    a pool.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Session persistence** : A mechanism that forces client requests to be served
    by the same backend server. Since the depreciation of LBaaS v1, the load-balancing
    service in Neutron can be configured through drivers. The most common ones are
    HAProxy and Octavia. Other third-party vendor drivers can also be integrated,
    such as F5 and Citrix, and Neutron will manage the orchestration of the API calls
    in the OpenStack environment. A glossary of different components in Octavia is
    as follows:'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amphora** : A virtual machine acting as a load balancer, running on compute
    nodes and preconfigured with load-balancing parameters, such as pools, backend
    members, and health monitors.'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller worker** : Updates the load-balancer instance (Amphora instance)
    with configuration settings.'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API controller** : Interacts with the controller worker for load-balancer
    (Amphora instances configurations and operations for deployment, deletion, and
    monitoring.'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health manager** : Watches the state of each load balancer (Amphora instance)
    and triggers failover events during the unexpected failure of a load balancer.'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Housekeeping manager** : Removes stale database records and handles the spare
    pool.'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The deployment of Octavia using **kolla-ansible** would require more steps
    than just installing an agent. To install the load-balancing service in OpenStack
    using the Octavia driver, start by assigning the core Octavia components to run
    as part of the cloud controller:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Enable the Octavia service in the **globals** **.yml** file, as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Make sure that in the **globals** **.yml** file, the **enable_neutron_provider_networks**
    setting is set to **true** . This is because we need the Octavia nodes to communicate
    through the management network. Assign the network interface of the **eth0** management
    network of the cloud controller node:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**kolla-ansible** automates most of the Octavia deployment and registration
    of the associated resources in the OpenStack environment. Octavia interacts with
    several OpenStack services, including Nova and Neutron. Associated resources can
    be customized in the **globals** **.yml** file, such as the amphorae Nova flavor,
    security groups, and the Octavia network and subnets, which can be set by updating
    **octavia_amp_flavor** , **octavia_amp_security_groups** , and the **Octavia management
    network** , respectively. By default, **kolla-ansible** is configured to register
    all dependent Octavia resources automatically. If you want to customize the Octavia
    registration service, make sure you set all required values for each Octavia setting
    in the **globals** **.yml** file. That can be done after changing the following
    setting from **no** to **yes** :'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Run the deployment pipeline to roll out the Octavia service. The deployment
    run should create the necessary Octavia resources, including certificates, the
    Octavia network, the flavor, the security group, and the SSH key. Make sure that
    **kolla-ansible post-deploy** runs successfully by generating an **octavia-openrc.sh**
    file, loaded as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Octavia uses images to launch amphora instances. Run the following command
    lines to download a recent amphora image for the Bobcat release, and then add
    it to Glance:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following steps assume that two web server instances named **instance1**
    and **instance2** have been deployed and are connected to the tenant network used
    in a previous demonstration, **10.10.0.0/24** :'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Note two of the created instances:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here is the output:'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Listing the created instances](img/B21716_06_22.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Listing the created instances
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly mimic a load-balancing exercise, run the **SimpleHTTPServer** Python
    module in each of the instances. Create a simple **index.html** file to trace
    the load-balancer member pool usage. On **instance1** , run the following:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Run the same command lines by specifying the second instance in the **index.html**
    file:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Make sure to allow ingress port **80** on the instances by creating and adding
    the following security group and rule, if not already attached:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Create a load balancer attached to the private subnet, **priv_subnet** . Note
    that the load balancer will be assigned a VIP that will be used to expose the
    backend service and allow clients to access it:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is the output:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Creating the load balancer](img/B21716_06_23.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Creating the load balancer
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the load balancer updates its **PROVISIONING_STATUS** state from **PENDING_CREATE**
    to **ACTIVE** and **OPERATING_STATUS** to **ONLINE** , create a TCP listener port
    named **listener80** on port **80** :'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Create a load-balancing pool named **poolweb** , and specify the load-balancing
    algorithm with **ROUND_ROBIN** :'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Add a health monitor to the created **poolweb** with the backend servers and
    probe the HTTP service port:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Join the backend instances ( **instance1** and **instance2** ) to **poolweb**
    :'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Optionally, check the status of the added backend members and created pool:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here is the output:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Listing the instance members of the load-balancer pool](img/B21716_06_24.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Listing the instance members of the load-balancer pool
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Test the load balancer by connecting to its VIP. Note that requests will be
    served consecutively by both instances, as the algorithm used for load balancing
    is round robin:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here is the output:'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Testing the load-balancer pool backend](img/B21716_06_25.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Testing the load-balancer pool backend
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the pool web backend from the internet can be done in different ways.
    One way is to attach the load balancer to a public network directly without the
    need to run behind a virtual router, and the load balancer VIP will be assigned
    a public routable IP. The other way is by creating a floating IP, and then associating
    it with the VIP of the load balancer that exists within a subnet behind the virtual
    router. A floating IP cannot be routable through the internet without a router
    that applies IP translation, using the assigned floating IP, to a public one to
    reach external networks.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'A good practice when creating load balancers in Neutron is to think about security
    first. The latest LBaaS version in OpenStack supports TLS certificate deployment.
    Even better, OpenStack has a dedicated project that handles keying and certificates,
    codenamed *Barbican* . To learn more about Barbican, refer to the official OpenStack
    docs: [https://docs.openstack.org/barbican/latest/](https://docs.openstack.org/barbican/latest/)
    . A load balancer can work in tandem with the Barbican service to handle its TLS
    certificates most securely.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack networking provides an overwhelming list of features that enable cloud
    operators and architects to implement advanced network topologies and offer cloud
    users and tenants more managed network services. As seen in this chapter, the
    ML2 plugin extends Neutron’s capabilities by unlocking some pertinent features
    that were lacking in older OpenStack releases. OVS was explored as a mechanism
    driver that brings more traffic control based on flow rules. For extended networking
    that requires common VLAN and VXLAN topologies, the chapter demonstrated the support
    of OVS and OVN drivers in Neutron within the latest OpenStack releases, in addition
    to their deployments using **kolla-ansible** . It is worth noting that the direction
    of future networking based on mechanism drivers is almost certainly moving toward
    adopting the OVN mechanism driver, due to its performance in larger production-grade
    and hybrid cloud environments. Another big deal, routing in the OpenStack networking
    world, was discussed. This chapter also revisited a routing implementation using
    OVS and covered BGP dynamic routing. OVN is becoming a popular way to implement
    BGP , and new releases might show more stable integration in that direction. However,
    the chapter did not cover all other extra Neutron services and agents, such as
    **VPN as a service** , **firewall as a service** , and **Designate** for DNS management.
    LBaaS in Neutron has seen more updates with the latest OpenStack releases. With
    the new LBaaS v2, cloud operators can configure their load-balancing service by
    using open source drivers, such as HAProxy or Octavia. Finally, the chapter demonstrated
    the deployment of a load-balancer setup using Octavia, which is becoming a more
    widely adopted LBaaS solution that is considered suitable for large-scale production
    environments.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will enhance our deployed OpenStack environment by tackling
    resiliency and high availability for the OpenStack control and data planes.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Operating the OpenStack Cloud Environment'
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will spot the operational mission of cloud operators to bring the
    cloud to the next stage. Essential operational excellence pillars will be covered,
    including high availability, monitoring, and logging, to ensure business continuity
    and keep an eye on common best practices on cloud health. This part will conclude
    with an exclusive addition on setting up a continuous infrastructure evaluation
    and resource optimization layouts.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly Available
    Cloud – Meeting the SLA*'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21716_08.xhtml#_idTextAnchor188) , *Monitoring and Logging –
    Remediating Proactively*'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21716_09.xhtml#_idTextAnchor204) , *Benchmarking the Infrastructure
    – Evaluating Resource Capacity and Optimization*'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
