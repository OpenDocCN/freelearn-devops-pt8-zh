- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking – Connectivity and Managed Service Options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “It always seems impossible until it’s done.”
  prefs: []
  type: TYPE_NORMAL
- en: – Nelson Mandela
  prefs: []
  type: TYPE_NORMAL
- en: Neutron, the OpenStack networking service, has evolved throughout different
    OpenStack releases, offering more capabilities and features. Cloud architects
    can design their OpenStack networking stack with multiple topologies, based on
    their existing networking resources and requirements. Unlike the former Nova networking
    service, which was bound only to basic networking offerings, OpenStack tenants
    can expect more granular networking control and flexibility with Neutron. Cloud
    and network administrators can offer more advanced networking features to tenants,
    such as routing, firewalls, load balancing, and private networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will walk through the rich updates that have been made
    to OpenStack networking by covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The Neutron architecture and its core service interactions in OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutron plugins and supported drivers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing Neutron agents and required network types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing and implementing an OpenStack virtual switching layout using **Open
    vSwitch** ( **OVS** ) and **Open Virtual Network** ( **OVN** ) mechanisms as a
    **Software-Defined Networking** ( **SDN** ) implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about and implementing routing, using virtual routers to interconnect
    tenant networks and provide external connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating the load balancer as a service feature using the emerging Octavia
    project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Neutron’s core components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenStack networking has evolved through different releases to build an advanced
    and functioning cloud networking stack. Like many other OpenStack services, Neutron
    is composed of several services that can be stretched across multiple hosts, as
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The OpenStack Neutron core architecture](img/B21716_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The OpenStack Neutron core architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'These components can be briefly broken down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Neutron server** : This acts as an API portal that receives API requests
    generated by services or end users and forwards them to the next process – in
    this case, the Neutron agents through the messaging queue service. The other part
    of the server interaction within an OpenStack ecosystem is access to the database
    to update the network objects for each API request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neutron agents** : Neutron’s architecture relies heavily on different types
    of agents that will be installed in other hosts to handle different networking
    features. The plugin agent, denoted as **neutron-*-agent** , is the process that
    handles virtual switching capabilities in each compute node. This type can be
    described as a **layer 2** ( **L2** ) agent. A very common L2 agent is OVS, which
    provides L2 connectivity via the ML2 mechanism driver. The second type of Neutron
    agent is the **layer 3** ( **L3** ) agent, denoted as **neutron-l3-agent** , which
    is installed in the network node and deals with instances of network access for
    L3, such as NAT, firewall, and **virtual private network** ( **VPN** ) capabilities.
    The DHCP agent, denoted as **neutron-dhcp-agent** , manages instances of DHCP
    configuration for each tenant network (a **dnsmasq** configuration).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the nature of the network function request, the Neutron service
    will reach the agents deployed on the network and/or compute nodes. Cloud operators
    should list which network functions the infrastructure supports, which will decide
    the choice of plugins and agents that can be installed.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying Neutron agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neutron relies on agent services that interact with **neutron-server** through
    the queuing message service to implement virtual networking for L2 and L3 connectivity,
    DHCP, and routing services. Different agents will be deployed in the network and
    compute nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L2 agent** : Deployed on network and compute nodes to implement L2 networking
    connectivity for instances and virtual networks, such as routers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L3 agent** : Deployed on the network node and, optionally, on compute nodes
    to perform L3 routing between different types of networks, such as tenant and
    external networks. L3 can also implement more advanced networking capabilities,
    such as firewalls and VPNs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DHCP agent** : Deployed on the network node to run the DHCP service for tenant
    networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plugin agent** : Deployed on the network node to process data packets on
    virtual networks that depend on the implemented Neutron plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata agent** : Deployed on the network node. The metadata service provided
    by Nova enables the retrieval of instance information, such as hostname and IP
    addresses, upon a metadata HTTP request ( **169.254.169.254** ). The Neutron metadata
    agent facilitates the forwarding of metadata to the Nova metadata service through
    its internal proxy component with additional information, such as instance and
    tenant IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proper function of the deployed Neutron agent will vary, based on the types
    of the designed networks in the OpenStack environment. The next section will briefly
    cover the different categories of networks that should exist in our initial design
    draft.
  prefs: []
  type: TYPE_NORMAL
- en: Network categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is essential to go through the network types to enable traffic flow, based
    on their usage. The following glossary covers the network categories that are
    used mainly with the latest releases of OpenStack:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Provider networks** : Created and managed by cloud operators who define a
    set of network attributes, such as the network’s type – for example, **VXLAN**
    , **Generic Routing Encapsulation** ( **GRE** ), or **flat** . Cloud operators
    provide and configure the underlying infrastructure, such as the physical network
    interface designated for the traffic flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-service networks** : Referred to as **tenant networks** , these are
    created by cloud users. Self-service networks are self-contained and fully isolated
    from other networks in a multi-tenancy environment managed by Neutron. Tenants
    are restricted to creating virtual networks with what was predefined by the cloud
    operators in terms of network types. For example, a cloud user cannot implement
    GRE networks if the cloud operator does not provide the option. Cloud users do
    not have access to the underlying Neutron configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External provider networks** : This refers to a provider network for specific
    external network connectivity. Cloud operators configure an external routing device
    to access the internet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section will cover the most interesting capabilities of Neutron by
    exploring the concept of plugins and their latest mode of usage.
  prefs: []
  type: TYPE_NORMAL
- en: The core of networking – Neutron plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neutron supports the use of plugins and drivers, which use different software
    and hardware technologies provided by open source communities or vendor solutions.
    There are two types of Neutron plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core plugin** : Enables L2 connectivity functions and orchestration of network
    elements, including virtual networks, subnets, and ports'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service plugin** : Enables additional network capabilities, including routing,
    private networks, firewalls, and load balancing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The code for plugins is run on the Neutron server, which can be configured as
    part of the cloud controller node(s).
  prefs: []
  type: TYPE_NORMAL
- en: The following section will dive into the most adopted core plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Core plugin – Modular Layer 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most widely adopted core plugin in Neutron is **Modular Layer 2** ( **ML2**
    ). The introduction of ML2 to the OpenStack networking service has increased the
    flexibility of designing network architectures. The secret sauce of ML2 is that
    it supports the use of a variety of technologies from different vendors simultaneously.
    Thus, cloud architects and operators aren’t limited to a specific set of features
    and can easily extend or change their network stacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This plugin has been developed due to the historical limitations prior to the
    **Havana** release, where operators had to stick to only one core plugin, referred
    to as a monolithic plugin-based component. The two most commonly used core plugins
    were **Linux Bridge** and **OVS** , which have been replaced in the latest releases
    of OpenStack by ML2. Replacing does not mean directly removing any of them but
    bundling them as **mechanism drivers** in the core ML2 plugin. ML2 is not limited
    to Linux Bridge and OVS and also supports other vendor drivers, such as VMware,
    Cisco, and OpenDayLight. An amazing filtered list can be found on the OpenStack
    community website: [https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking)](https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking))
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'An overview of the modular plugins and drivers supported in Neutron is illustrated
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – An overview of the OpenStack Neutron core and service plugins
    architecture](img/B21716_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – An overview of the OpenStack Neutron core and service plugins architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, Neutron relies on the mechanisms of plugins
    that provide a specific set of networking services. An API request will be forwarded
    by the Neutron server to the associated plugin configured in the controller node.
    The ML2 plugin combines two main elements into the same framework, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Type drivers** : These expose L2 functionalities to create more segmented
    networks, including the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VLAN** : Segregates network traffic using **802.1Q** tagging. Virtual machines
    that belong to the same VLAN are part of the same broadcast L2 domain'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VXLAN** : Uses a **virtual network identifier** ( **VNI** ) to separate and
    differentiate traffic between different networks'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flat** : VLAN tagging and network segregation are not supported where instances
    are connected in the same network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GRE** : Encapsulates traffic using the GRE tunneling protocol'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GENEVE** : Resembles the VXLAN overlay technology but with more optimal encapsulation
    methods'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local** : Connects instances only within the same network hosted in the same
    compute nodes and not with instances in different nodes'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mechanism drivers** : Implements the type driver technologies through both
    network software methods such as OVS, Linux Bridge, and OVN and hardware-based
    ones'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An up-to-date ML2 driver support matrix list can be found at [https://docs.openstack.org/neutron/latest/admin/config-ml2.html](https://docs.openstack.org/neutron/latest/admin/config-ml2.html)
    . Each mechanism driver supports a set of network types (type drivers). For example,
    OVS supports most network types, including VLAN, VXLAN, GRE, flat, and local.
    If a need has been raised to support the GENEVE type driver while in production,
    the new mechanism driver can be integrated by just installing the driver and reconfiguring
    the Neutron driver list.
  prefs: []
  type: TYPE_NORMAL
- en: Building virtual switching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, two mechanism drivers will be discussed to establish connectivity
    between virtual network ports and physical networks. OVS is one of the most featured
    mechanism drivers, with advanced networking capabilities such as **OpenFlow**
    . The second driver we will cover is **OVN** , an SDN implementation that enables
    programming networks by controlling the flow of packets. Linux Bridge and **L2
    Population** are also well-tested and mature mechanisms. With ML2 being an agnostic
    plugin for the supported drivers, introducing a new mechanism driver will not
    introduce a lot of complexity in an existing running OpenStack environment.
  prefs: []
  type: TYPE_NORMAL
- en: Opening vSwitch in OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most OpenStack networking implementations would require a minimum of overlay
    networking technologies that provide tunnel-based virtual networking, with an
    immense level of network segmentation, such as VXLAN and GRE. Tunnel-based networks
    can provide up to 16 million networks. OVS supports most used drivers in large
    and complex OpenStack networking environments, such as VLAN, VXLAN, GRE, and flat.
    As a cloud operator, it is crucial to have a basic understanding of how OVS operates
    when installed in an OpenStack environment. OVS comes with a complete core architecture,
    with different components that operate as software switches in a host kernel space.
    The OVS implementation comes with different services when installed in an OpenStack
    environment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**openvswitch** : A kernel module that handles the data plane that processes
    the network packets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ovs-switchd** : A Linux process that runs on a physical host to control and
    manage virtual switches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ovsdb-server** : A local database to store the virtual switches running locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**neutron-openvswitch-agent** : Configured in compute nodes that use the OVS
    mechanism driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**neutron-server** : Handles API requests, with the ML2 plugin loading the
    OVS mechanism driver. The neutron-server process passes the network request to
    the OVS driver via an RPC cast message to **neutron-openvswitch-agent** . The
    latter configures the OVS switch on the compute node and sets the required resources
    of the local instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once OVS is implemented, a set of virtual network devices is installed, which
    a cloud operator should keep in mind during deployment or troubleshooting tasks.
    An Ethernet frame would travel from an instance through the following interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tapXXXX** : A tap interface where **XXXX** is the assigned tap interface
    ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**br-int** : A bridge interface, known as the integration bridge, where it
    consolidates all the virtual devices, such as virtual machines, routers, and firewalls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**int-br-ethX** : A virtual patch port connecting different interfaces to the
    OVS integration bridge interface ( **br-int** ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**phy-br-ethX** : A virtual patch port connecting different interfaces to the
    OVS provider bridge ( **br-ethX** ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**br-ethX** : A bridge interface where *X* is the assigned bridge interface
    ID connecting to the physical network. It is also known as the provider bridge
    and connects to the **br-int** integration bridge through a virtual patch cable,
    provided by patch ports ( **int-br-ethX** and **phy-br-ethX** patch ports for
    **int-br** and **br-ethX** , respectively).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qbrXXXX** : A Linux bridge interface where **XXXX** is the assigned bridge
    interface ID dedicated to IP tables only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**br-tun** : A bridge tunnel interface to handle packet encapsulation and de-encapsulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**br-ex** : A bridge interface providing connectivity to external networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each spawned instance is connected via its tap interface, denoted as **tapXXXX**
    , created in the hypervisor host. The associated Linux bridge, denoted as **qbrXXXX**
    , is connected to the OVS **br-int** integration bridge, where the traffic will
    be routed based on the programmed OpenFlow rules and then forwarded through the
    virtual switch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connections between the integration and provider bridges are handled by patch
    cables. Packets exit the physical network via the OVS provider bridge ( **br-ethX**
    ), connected to the physical network interface of the host. When configuring OVS
    in our existing OpenStack environment, each node will run its own integration
    and provider bridges. Connectivity between the OpenStack nodes, including the
    cloud controller, network, and compute nodes, is established via the physical
    network, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Traffic flow through different interfaces using OVS](img/B21716_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Traffic flow through different interfaces using OVS
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the basic core components of OVS within an OpenStack
    setup, we can move on to configuring our infrastructure code to deploy OVS as
    the main mechanism driver in Neutron.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with OVS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The OVS mechanism driver implements L2 isolation, which offers VXLAN-, GRE-,
    and VLAN-based networks. The next configuration will implement the ML2 plugin
    to use VXLAN-based networks. This will allow instances to connect via VLAN segmentation
    and VXLAN tunneling. Virtual networks for tenants will not be exposed outside
    of the compute or network nodes. **kolla-ansible** comes with the ML2 plugin enabled
    by default. A few prerequisites must be fulfilled before deploying an **OVS**
    configuration. To reach the external networks via routers and provider networks,
    adjust the **neutron_external_interface** setting in the **/etc/kolla/globals.yml**
    file to the network interface dedicated to this matter. The **eth2** interface
    has been assigned an interface to connect with the external world, as designed
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* . You can adjust the settings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to consider the usage of a physical interface with the OVS
    provider bridge ( **br-ethX** ) if you are planning to use more than one provider
    bridge. In this case, **neutron_external_interface** can be assigned a comma-separated
    list – for example, **neutron_external_interface: "eth2,eth3"** .'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other check is to verify whether **kolla-ansible** is configured with an
    OVS driver mechanism that comes with the ML2 plugin. Check and adjust the following
    configuration setting in the **globals.yml** file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The next check is to verify that our Neutron services are assigned to the respective
    OpenStack nodes in the **/ansible/inventory/multi_packtpub_prod** file. As demonstrated
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* , **neutron-server** will run as part of the controller node,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Neutron agents, including the DHCP, L3, and metadata agents, will run in a
    dedicated network node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: OVS will be installed in the network and compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage
    – Block, Object, and File Shares* , the **manila-share** service provides access
    to the file share by using the Neutron plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OVS setup in the inventory file is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Launch the pipeline job to install OVS across the target nodes. Note that VXLAN
    was not explicitly configured in the **kolla-ansible** code. By default, the Neutron
    tenant network uses the VXLAN type. The default VNI range is defined from 1 to
    1,000 IDs, reserved for the tenant networks when they are created. This setting
    is denoted with the **vni_ranges** line in the **ml2_config.ini** file in the
    controller node. The ML2 configuration can be extended and we can employ a configuration
    override by creating an **/** **etc/kolla/config/neutron/ml2_config.ini** file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command line lists the different network agents, including the
    OVS ones, from the cloud controller node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The neutron agent list output](img/B21716_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – The neutron agent list output
  prefs: []
  type: TYPE_NORMAL
- en: As shown here, the OVS agents should be up and running in the respective network
    and compute nodes. In the following section, we will trace the network flow with
    the OVS mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Traffic flow with OVS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The setup of the OVS driver will dictate the travel hops of an Ethernet frame,
    from an instance all the way to the physical network. Numerous interfaces will
    be used, including the virtual switch ports and integration bridge. The deployment
    of the **openvswitch** playbook in the compute node should create the OVS logical
    bridges on the host, which can be checked by running the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The OVS bridges list](img/B21716_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – The OVS bridges list
  prefs: []
  type: TYPE_NORMAL
- en: 'Instances spawned in the compute node are connected via the same local VLAN.
    With the VXLAN layout, instance ports are attached to the integration bridge,
    which will be assigned a local VLAN ID. Traffic initiated from an instance will
    be forwarded by the integration bridge ( **br-int** ) to the tunnel bridge ( **br-tun**
    ). The overall layout of the connecting bridges and their respective ports can
    be seen by running the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – An OVS interface listing](img/B21716_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – An OVS interface listing
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in the virtual switch interfaces, the integration bridge ( **br-int**
    ) is tagged with **VLAN 1.** When traffic reaches instances in different nodes,
    the packets will travel through the tunnel bridge ( **br-tun** ) encapsulated
    in VXLAN packets. Under the hood, the local VLAN ID of **1** will be swapped with
    the VXLAN tunnel ID, as shown with **vxlan-476984a0** . Otherwise, if an instance
    connects to another one residing in the same compute node, the packets will travel
    through the integration bridge ( **br-int** ) locally to the destination port,
    denoted with the **qrXXXX** and **tapYYYY** port interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: The following section covers a more complex mechanism driver, OVN, and demonstrates
    its deployment in our OpenStack environment.
  prefs: []
  type: TYPE_NORMAL
- en: OVN in OpenStack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other major and preferred network virtualization in an OpenStack environment
    is OVN. OVN is considered a mature implementation of the SDN philosophy that provides
    greater flexibility for network flow programming. Operators can define a set of
    forwarding rules centrally to define how the flow of packets is controlled. OVN
    uses an abstraction software layer available in a network controller to program
    switches, with conditions and rules applied to the packet flow. The most distinctive
    functionality of OVN compared to OVS is its richer capabilities and advanced features,
    such as traffic routing and access control which are programmable. Unlike the
    OVS approach, OVN decouples the control function of network devices from the actual
    packet forwarding function through flow rules. OVN supports most network types,
    including VLAN, VXLAN (version 20.09 and later), flat, and Geneve.
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in the following diagram, OVN is built on top of OVS, which leverages
    its switching capabilities and adds an abstraction layer, managed by controllers
    that are stored in a set of **OVS** **databases** ( **OVSDBs** ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – OVN integration architecture](img/B21716_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – OVN integration architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The main constructs of the OVN architecture integration with an OpenStack environment
    are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Southbound database** : Referenced as **ovsdb-server** ( **ovnsb.db** ),
    this stores data of the logical and physical network flows. The database hosts
    all binding information between the logical and physical networks, such as the
    port and logical networking bindings associated with the **Port_Binding** and
    **Logical_Flow** tables, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Northbound database** : Referenced as **ovsdb-server** ( **ovnnb.db** ),
    this stores data at a high level of the virtual networks that represent a **cloud
    management system** ( **CMS** ) – in this case, an OpenStack environment. The
    database reflects the networking resources of the CMS in the table datasets, such
    as the routers and switch ports associated with the **Logical_Router** and **Logical_Switch_Port**
    tables, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OVN controller** : Referenced as **ovn-controller** , this runs on hypervisor
    nodes and connects to the southbound database. It acts as the OVS controller by
    defining the OpenFlow rules on each OVS switch instance, converting logical flows
    into physical flows, and pushing configurations to the local OVSDB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OVN northbound service** : Referenced as **ovn-northd** , this runs as part
    of the control plane on the cloud controller. The **ovn-northd** daemon connects
    the northbound to the southbound database by translating the logical configuration
    from the **ovn-nb** data to logical data path flows. The converted data will be
    populated in the **ovn-sb** database and made ready for use by the **ovn-controller**
    instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OVS data plane service** : Referenced as **ovs-vswitchd** , this runs as
    part of the data plane on the hypervisor nodes. **ovs-vswitchd** applies the packet-forwarding
    rules provided by **ovn-controller** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local OVSDB** : Referenced as **ovsdb-server** , this runs locally in each
    hypervisor node. The OVN reference architecture takes advantage of the database
    scaling benefit by using a consistent snapshot of the database, to recover from
    possible connectivity interruptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata agent** : Referenced as **ovn-metadata-agent** , this runs in each
    hypervisor node. The OVN metadata agent, such as the Neutron metadata agent, is
    used to proxy metadata API requests on each compute node. This way, each OVS switch
    instance running in each hypervisor node will route the requests to the local
    metadata agent and then query the Nova metadata service to run instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will use **kolla-ansible** to integrate the OVN
    project in an OpenStack environment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with OVN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OVN extends the OVS implementation to provide network services to instances.
    The ML2 mechanism driver facilitates integration with OpenStack as a CMS. The
    **kolla-ansible** playbooks include most of the OVN configuration and required
    packages to install. Similar to the OVS configuration, adjust **/etc/kolla/globals.yml**
    to use the designated interface to reach external networks, by configuring the
    **neutron_external_interface** setting, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the ML2 mechanism driver to use OVN, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'OVN comes with built-in support for L3 networking functions that enable instances
    to connect to external networks. There are two ways to provide such connectivity
    in OVN – through a centralized layout that does not use a distributed floating
    IP design or via **distributed virtual routing** ( **DVR** ), which requires a
    floating IP configuration. For two instances to reach external networks or each
    other using L3, traffic must flow through the network node. With the DVR method,
    traffic is more optimized by deploying an L3 agent in each compute node, allowing
    **north-south** (going to and from external networks) traffic with a floating
    IP to be routed from and to the compute node, with an extra hop to reach the network
    node. Similarly, **east-west** (traffic flow between instances) traffic is routed
    directly to the compute nodes. The latter option is recommended for more optimal
    performance. The distributed floating IP option can be enabled in the **globals.yml**
    file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable the Neutron OVN agent to provide extensible capabilities for network
    monitoring and **Quality of Service** ( **QoS** ) by configuring the following
    setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, make sure to adjust the **/ansible/inventory/multi_packtpub_prod** inventory
    file to install the OVN services in the assigned nodes. Similar to the OVS configuration,
    make sure to have the Neutron server and respective agents running in the controller
    and network nodes, respectively, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed in the *OVN in OpenStack* section, the OVS northbound and southbound
    databases, in addition to the OVN **northd** daemon, will run on the controller
    node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The OVN controller instances and OVN metadata agents will run on the compute
    nodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Since Antelope and later releases, a dedicated OVN agent has been created to
    implement the lacking **ovn-controller** function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launching the pipeline job should deploy the OVN services and create an ML2
    file, **/etc/neutron/plugins/ml2/ml2_conf.ini** , with the following default configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The default network tenant type is configured with GENEVE as the tunneling protocol.
    The main difference between GENEVE and VXLAN (configured for OVS in a previous
    section) is how the networking metadata is encapsulated and encoded between both
    protocols. VXLAN encodes the VNI only in the encapsulation header. Meanwhile,
    the GENEVE protocol uses extensible TLV to carry more information about the packet
    in the encapsulated header (it has a header length of 38 bytes, while the VXLAN
    frame can carry 8 bytes), such as the **ingress** and **egress** ports. GENEVE
    offers better capabilities, such as transport security, service chaining, and
    in-band telemetry, compared to VXLAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other resulting default configuration is the default northbound and southbound
    database addresses. Each OVN controller running in each compute node should be
    able to reach both databases. Such configuration can be found in the same file,
    **ml2_conf.ini** , by checking the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once OVN is configured, further network resource creation and management will
    be handled by OVN, which maps each OpenStack resource abstraction to an entry
    object in the OVN southbound and northbound databases.
  prefs: []
  type: TYPE_NORMAL
- en: Both OVS and OVN provide different capabilities to connect instances to networks.
    In the following section, we will explore how routing is performed in Neutron
    and demonstrate the different routing traffic options in OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring cloud routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instances within the same virtual tenant network can reach each other, but by
    default, each tenant network cannot reach other tenants or external networks.
    Deploying virtual routers is the way to enable L3 network communication so that
    tenant virtual networks can connect by associating a subnet with a router.
  prefs: []
  type: TYPE_NORMAL
- en: Routing tenant traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Under the hood, a port associated with a tenant virtual network will be associated
    with the IP address of the subnet gateway. Instances across different virtual
    networks reach each other by communicating via the virtual router, using the gateway
    IP address and their private IP addresses encapsulated in the packets. This is
    called a **NAT** ( **network address translation** ) mechanism. In OpenStack networking,
    the Neutron L3 agent manages virtual routers. IP packets are forwarded by a virtual
    router to different self-service and external networks through the following router
    interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**qr** : Contains the tenant network gateway IP address and is dedicated to
    routing traffic through all self-service traffic networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**qg** : Contains the external network gateway IP address and is dedicated
    to routing traffic out onto the external provider network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upon the initiation of a virtual router instance, a network namespace will be
    created in the Neutron node that defines connections to self-service or external
    provider networks through routing tables, packet forwarding, and **iptables**
    rules. As shown in the following diagram, a router namespace refers to a virtual
    router that is attached to multiple bridge ports in an OVS configuration ( **qr**
    and **qg** ). Traffic flow between instances hosted in the same or different compute
    nodes is routed through the virtual router.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Neutron router namespace connectivity based on OVS implementation](img/B21716_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Neutron router namespace connectivity based on OVS implementation
  prefs: []
  type: TYPE_NORMAL
- en: In the previous OVS implementation, the L3 agent should be up and running to
    start creating and managing virtual routers.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The router service plugin should be enabled by default once Neutron agent L3
    is deployed, using **kolla-ansible** . The service plugin can be verified in the
    **/etc/neutron/neutron.conf** file in the Neutron cloud controller node by checking
    the **service_plugin =** **router** line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, virtual routers can be managed through Horizon. The **kolla-ansible**
    run for the Neutron deployment should enable the router module in the dashboard.
    That can be verified in the cloud controller node’s **/etc/openstack-dashboard/local_settings.py**
    file with the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next exercise, we will create a tenant virtual network with the ML2
    plugin configured in OVS. A virtual router will be attached to the tenant network
    using the OpenStack CLI, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a tenant virtual network using the OpenStack network CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a subnet with an IP range of **10.10.0.10/24** , an auto-assigned DHCP,
    and a default DNS nameserver of **8.8.8.8** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a router and attach it to the created tenant network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The router attachment to the tenant network will assign a private IP address
    to the router’s internal interface. By default, if no IP address is specified
    in the attachment command line, the internal interface will be assigned the default
    gateway of the subnet. The following command line verifies the assigned IP address
    of the router’s internal interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It gives the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Virtual router port listing](img/B21716_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Virtual router port listing
  prefs: []
  type: TYPE_NORMAL
- en: 'The router interface can be checked within the router namespace **qr-** prefixed
    interface by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output looks like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The network namespaces list](img/B21716_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – The network namespaces list
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the **qrouter** ID from the previous output, and then run the following
    command line to show the created interface of the router and the assigned IP address
    from the internal network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11 – A virtual router namespace internal interface listing](img/B21716_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – A virtual router namespace internal interface listing
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable instances to reach the external network, a second interface in the
    virtual router should be created that will be attached to the provider’s external
    network. A common networking setup, such as an external network device or a device
    integrated with a firewall appliance, should be placed in front of the OpenStack
    endpoint (such as the load balancer). Enabling internet access can be configured
    using the OpenStack CLI, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an external network provider. As configured in the ML2 plugin for OVS,
    we can create the network as a VLAN type, with a defined segmentation ID and **physnet1**
    for the physical network attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the subnet part of the external network with a network range of **10.20.0.0/24**
    and the default gateway as **10.20.0.1** , disable DHCP, and set an allocation
    pool of **10.20.0.10-10.20.0.100** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Attach the router to an external provider network by running the following
    command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The attachment operation will assign an external IP from the external IP pool
    to the router, which can be checked by running the following command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12 – A created external port of the virtual router](img/B21716_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – A created external port of the virtual router
  prefs: []
  type: TYPE_NORMAL
- en: 'The last attachment creates a second interface, prefixed with **qg-** in the
    router namespace, which can be verified by the following command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.13 – A virtual router namespace external interface listing](img/B21716_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – A virtual router namespace external interface listing
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of our connectivity demonstration involves creating security
    groups and rules to allow ingress and egress traffic at the network port level.
    To reach the internet and access the instances, we will create a new security
    group and add **ICMP** and **SSH** access:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the OpenStack CLI, create a new security group to be applied to your
    instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the rules associated with the created security group for SSH and ICMP,
    respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the OpenStack CLI, create a new test instance with a **tiny** flavor
    and a **cirros** image that is connected to the private tenant network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to adjust your available Glance image name and list of flavors, based
    on your existing resources. The **openstack server create** command line will
    fail if any of the assigned arguments do not exist. Cirros is a minimal Linux
    distribution, useful for quick testing and proof of concepts. The default session
    username is **cirros** and the password is **gocubsgo** .
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the connectivity from the instance to reach the internet, make sure
    that the instance state is **ACTIVE** , as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.14 – The instance listing](img/B21716_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – The instance listing
  prefs: []
  type: TYPE_NORMAL
- en: 'Access to the created instance can be made in different ways, using the **virsh
    console** command line from the compute node or simply via SSH from the router
    namespace. Make sure to use the default **cirros** image credentials – that is,
    the username **cirros** and the password **gocubsgo** – and run a simple ping
    to reach **8.8.8.8** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Testing external connectivity](img/B21716_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Testing external connectivity
  prefs: []
  type: TYPE_NORMAL
- en: 'The instance uses the internal virtual router IP address as the default gateway
    to route traffic reaching the external network. The internet is reachable through
    SNAT, performed by the router. A quick run of the **ip route** command in the
    instance shows the default gateway associated with the network route table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Listing the default gateway](img/B21716_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Listing the default gateway
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of our walk-through is a demonstration of how resources hosted
    in external networks can reach instances in an OpenStack environment. By default,
    spawned instances will be assigned an IP address that is not visible outside of
    the tenant network. Neutron provides float IP addresses that implement **DNAT**
    ( **destination NAT** ). The router simply forwards incoming packets reaching
    its external interface to the destination instance by checking its configured
    DNAT rule. The traffic response from an instance to external resources uses the
    source IP addresses translated to the floating IP, demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the port ID of the created instance, and create a floating IP address
    to associate with it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Listing the instance port](img/B21716_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Listing the instance port
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the port ID, and run the following command line by pasting the external
    port ID after the **--** **port** option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Assigning the floating IP address to the external port](img/B21716_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Assigning the floating IP address to the external port
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, the router namespace configures a secondary address associated
    with the external interface, prefixed with **''qg''** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Associating the IP address with the virtual router external
    interface](img/B21716_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Associating the IP address with the virtual router external interface
  prefs: []
  type: TYPE_NORMAL
- en: Traffic routed through the external network provider can reach the instance
    via the assigned floating IP.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have explored a standard routing implementation in OpenStack. The
    next section will uncover another way of performing routing, via dynamic routing,
    in Neutron.
  prefs: []
  type: TYPE_NORMAL
- en: Neutron dynamic routing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dynamic routing in OpenStack networking is based on **BGP** ( **Border Gateway
    Protocol** ), enabling tenant networks to advertise their network prefixes with
    physical or virtual routers and network devices that support BGP. The new addition
    of BGP in Neutron eliminates the usage of floating IPs for tenant networks that
    do not rely on network administrators to advertise their network upstream. The
    term *dynamic routing* in Neutron was introduced with the **Mitaka** release.
    The adoption of the BGP routing mechanism varies from one cloud environment to
    another, depending on the networking setup, mostly due to the requirement for
    direct connectivity between the network node and the physical network gateway
    device to peer with (such as a LAN or WAN peer). To avoid IP overlapping when
    advertising IP prefixes, dynamic routing relies on **address scopes** and **subnet
    pools** , Neutron mechanisms that control the allocation of subnet addresses and
    prevent the usage of addresses that overlap.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of the BGP implementation, Neutron introduced the **BGP speaker**
    , which enables peering between tenant networks and external router devices. The
    BGP speaker advertises the tenant network to the tenant router, initially as a
    first hop. The BGP speaker in Neutron is not a router instance, nor does it manipulate
    BGP routes. It mainly orchestrates the BGP peer’s information between the tenant
    routers and external ones. The speaker requires a network or cloud operator to
    configure the peering endpoints. As shown in the following diagram, for a successful
    BGP dynamic routing in OpenStack, the Neutron virtual router must be attached
    to both the tenant subnet and the external provider device interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Neutron BGP peering and router connectivity for dynamic routing](img/B21716_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Neutron BGP peering and router connectivity for dynamic routing
  prefs: []
  type: TYPE_NORMAL
- en: Both the BGP speaker and the external provider device must be peered (connected
    to the Neutron virtual router). Finally, both the tenant and the external networks
    must be in the same address scope. Adding BGP dynamic routing using **kolla-ansible**
    is straightforward. We will configure the dynamic routing based on the OVS implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Since the Antelope release, Neutron has supported dynamic routing with the OVN
    mechanism driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'As Neutron provides a BGP agent with default configuration, we will just need
    to enable the agent installation in the network node by adding the following line
    to the **/ansible/inventory/multi_packtpub_prod** inventory file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable the agent installation in the **globals** **.yml** file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the pipeline to roll out the BGP agent installation in the network node.
    In the network node, the installed BGP agent can be checked by running the following
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Listing the BGP network agent](img/B21716_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Listing the BGP network agent
  prefs: []
  type: TYPE_NORMAL
- en: The Neutron BGP CLI to manage BGP speakers can be found at [https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html](https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Virtual routers are part of the building blocks of OpenStack networking, providing
    a variety of connectivity options to tenants. It is important to note that running
    with a standalone router presents a single point of failure. [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , will discuss Neutron
    implementation for highly available routers. Dynamic routing via BGP in Neutron
    is considered an amazing routing addition in OpenStack. Neutron has been enriched
    and modified across OpenStack releases. One of these major modifications is the
    development of new networking services in OpenStack, which will be discussed in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Joining more networking services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the overwhelming routing and switching capabilities in Neutron that
    come with the latest OpenStack releases, Neutron enables cloud operators to offer
    additional networking services. Services such as load balancing, firewalls, and
    private networks support tenant networks to build well-architected application
    stacks for a variety of use cases. In the next section, we will cover the most
    used and stable additional Neutron services and deploy them, using **kolla-ansible**
    .
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer as a service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the **Liberty** release, the Neutron **Load Balancer as a Service** (
    **LBaaS** ) plugin has moved from version 1 to version 2, codenamed **Octavia**
    . Cloud users who seek to balance the load of their workloads using a managed
    load balancer will appreciate the Octavia offering. Octavia is designed to scale
    horizontally, as it creates and manages virtual machines acting as load-balancer
    instances, referred to as **amphorae** .
  prefs: []
  type: TYPE_NORMAL
- en: 'Octavia implements the same load-balancing terms as the popular **HAProxy**
    , which can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual IP (VIP)** : A layer 4 object that exposes a service to be accessed,
    externally or internally, that is associated with a Neutron port. A load balancer
    is assigned a VIP, and requests that reach the VIP are distributed among backend
    servers or pool members.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pool** : A group of instances serving the same content or service, such as
    web servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pool members** : An instance of a pool represented by an IP address and listening
    port that exposes the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Listeners** : A port associated with the VIP that listens for incoming requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health monitor** : Orchestrates the management of pool members based on health
    checks carried out on each member. Failed health checks will discard the members
    from the pool, and traffic will be served by the healthy ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load-balancing algorithms** : **Round robin** , **least connections** , and
    **source IPs** are supported algorithms in Octavia that can be assigned when creating
    a pool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Session persistence** : A mechanism that forces client requests to be served
    by the same backend server. Since the depreciation of LBaaS v1, the load-balancing
    service in Neutron can be configured through drivers. The most common ones are
    HAProxy and Octavia. Other third-party vendor drivers can also be integrated,
    such as F5 and Citrix, and Neutron will manage the orchestration of the API calls
    in the OpenStack environment. A glossary of different components in Octavia is
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amphora** : A virtual machine acting as a load balancer, running on compute
    nodes and preconfigured with load-balancing parameters, such as pools, backend
    members, and health monitors.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller worker** : Updates the load-balancer instance (Amphora instance)
    with configuration settings.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API controller** : Interacts with the controller worker for load-balancer
    (Amphora instances configurations and operations for deployment, deletion, and
    monitoring.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health manager** : Watches the state of each load balancer (Amphora instance)
    and triggers failover events during the unexpected failure of a load balancer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Housekeeping manager** : Removes stale database records and handles the spare
    pool.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The deployment of Octavia using **kolla-ansible** would require more steps
    than just installing an agent. To install the load-balancing service in OpenStack
    using the Octavia driver, start by assigning the core Octavia components to run
    as part of the cloud controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable the Octavia service in the **globals** **.yml** file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that in the **globals** **.yml** file, the **enable_neutron_provider_networks**
    setting is set to **true** . This is because we need the Octavia nodes to communicate
    through the management network. Assign the network interface of the **eth0** management
    network of the cloud controller node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '**kolla-ansible** automates most of the Octavia deployment and registration
    of the associated resources in the OpenStack environment. Octavia interacts with
    several OpenStack services, including Nova and Neutron. Associated resources can
    be customized in the **globals** **.yml** file, such as the amphorae Nova flavor,
    security groups, and the Octavia network and subnets, which can be set by updating
    **octavia_amp_flavor** , **octavia_amp_security_groups** , and the **Octavia management
    network** , respectively. By default, **kolla-ansible** is configured to register
    all dependent Octavia resources automatically. If you want to customize the Octavia
    registration service, make sure you set all required values for each Octavia setting
    in the **globals** **.yml** file. That can be done after changing the following
    setting from **no** to **yes** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the deployment pipeline to roll out the Octavia service. The deployment
    run should create the necessary Octavia resources, including certificates, the
    Octavia network, the flavor, the security group, and the SSH key. Make sure that
    **kolla-ansible post-deploy** runs successfully by generating an **octavia-openrc.sh**
    file, loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Octavia uses images to launch amphora instances. Run the following command
    lines to download a recent amphora image for the Bobcat release, and then add
    it to Glance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following steps assume that two web server instances named **instance1**
    and **instance2** have been deployed and are connected to the tenant network used
    in a previous demonstration, **10.10.0.0/24** :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note two of the created instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Listing the created instances](img/B21716_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Listing the created instances
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly mimic a load-balancing exercise, run the **SimpleHTTPServer** Python
    module in each of the instances. Create a simple **index.html** file to trace
    the load-balancer member pool usage. On **instance1** , run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the same command lines by specifying the second instance in the **index.html**
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure to allow ingress port **80** on the instances by creating and adding
    the following security group and rule, if not already attached:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a load balancer attached to the private subnet, **priv_subnet** . Note
    that the load balancer will be assigned a VIP that will be used to expose the
    backend service and allow clients to access it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Creating the load balancer](img/B21716_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Creating the load balancer
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the load balancer updates its **PROVISIONING_STATUS** state from **PENDING_CREATE**
    to **ACTIVE** and **OPERATING_STATUS** to **ONLINE** , create a TCP listener port
    named **listener80** on port **80** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a load-balancing pool named **poolweb** , and specify the load-balancing
    algorithm with **ROUND_ROBIN** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a health monitor to the created **poolweb** with the backend servers and
    probe the HTTP service port:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Join the backend instances ( **instance1** and **instance2** ) to **poolweb**
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Optionally, check the status of the added backend members and created pool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Listing the instance members of the load-balancer pool](img/B21716_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Listing the instance members of the load-balancer pool
  prefs: []
  type: TYPE_NORMAL
- en: 'Test the load balancer by connecting to its VIP. Note that requests will be
    served consecutively by both instances, as the algorithm used for load balancing
    is round robin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Testing the load-balancer pool backend](img/B21716_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Testing the load-balancer pool backend
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the pool web backend from the internet can be done in different ways.
    One way is to attach the load balancer to a public network directly without the
    need to run behind a virtual router, and the load balancer VIP will be assigned
    a public routable IP. The other way is by creating a floating IP, and then associating
    it with the VIP of the load balancer that exists within a subnet behind the virtual
    router. A floating IP cannot be routable through the internet without a router
    that applies IP translation, using the assigned floating IP, to a public one to
    reach external networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good practice when creating load balancers in Neutron is to think about security
    first. The latest LBaaS version in OpenStack supports TLS certificate deployment.
    Even better, OpenStack has a dedicated project that handles keying and certificates,
    codenamed *Barbican* . To learn more about Barbican, refer to the official OpenStack
    docs: [https://docs.openstack.org/barbican/latest/](https://docs.openstack.org/barbican/latest/)
    . A load balancer can work in tandem with the Barbican service to handle its TLS
    certificates most securely.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack networking provides an overwhelming list of features that enable cloud
    operators and architects to implement advanced network topologies and offer cloud
    users and tenants more managed network services. As seen in this chapter, the
    ML2 plugin extends Neutron’s capabilities by unlocking some pertinent features
    that were lacking in older OpenStack releases. OVS was explored as a mechanism
    driver that brings more traffic control based on flow rules. For extended networking
    that requires common VLAN and VXLAN topologies, the chapter demonstrated the support
    of OVS and OVN drivers in Neutron within the latest OpenStack releases, in addition
    to their deployments using **kolla-ansible** . It is worth noting that the direction
    of future networking based on mechanism drivers is almost certainly moving toward
    adopting the OVN mechanism driver, due to its performance in larger production-grade
    and hybrid cloud environments. Another big deal, routing in the OpenStack networking
    world, was discussed. This chapter also revisited a routing implementation using
    OVS and covered BGP dynamic routing. OVN is becoming a popular way to implement
    BGP , and new releases might show more stable integration in that direction. However,
    the chapter did not cover all other extra Neutron services and agents, such as
    **VPN as a service** , **firewall as a service** , and **Designate** for DNS management.
    LBaaS in Neutron has seen more updates with the latest OpenStack releases. With
    the new LBaaS v2, cloud operators can configure their load-balancing service by
    using open source drivers, such as HAProxy or Octavia. Finally, the chapter demonstrated
    the deployment of a load-balancer setup using Octavia, which is becoming a more
    widely adopted LBaaS solution that is considered suitable for large-scale production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will enhance our deployed OpenStack environment by tackling
    resiliency and high availability for the OpenStack control and data planes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Operating the OpenStack Cloud Environment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will spot the operational mission of cloud operators to bring the
    cloud to the next stage. Essential operational excellence pillars will be covered,
    including high availability, monitoring, and logging, to ensure business continuity
    and keep an eye on common best practices on cloud health. This part will conclude
    with an exclusive addition on setting up a continuous infrastructure evaluation
    and resource optimization layouts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly Available
    Cloud – Meeting the SLA*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21716_08.xhtml#_idTextAnchor188) , *Monitoring and Logging –
    Remediating Proactively*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21716_09.xhtml#_idTextAnchor204) , *Benchmarking the Infrastructure
    – Evaluating Resource Capacity and Optimization*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
