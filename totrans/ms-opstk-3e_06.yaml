- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: OpenStack Networking – Connectivity and Managed Service Options
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack网络 – 连接性和托管服务选项
- en: “It always seems impossible until it’s done.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “事情总是看起来不可能，直到它完成。”
- en: – Nelson Mandela
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 纳尔逊·曼德拉
- en: Neutron, the OpenStack networking service, has evolved throughout different
    OpenStack releases, offering more capabilities and features. Cloud architects
    can design their OpenStack networking stack with multiple topologies, based on
    their existing networking resources and requirements. Unlike the former Nova networking
    service, which was bound only to basic networking offerings, OpenStack tenants
    can expect more granular networking control and flexibility with Neutron. Cloud
    and network administrators can offer more advanced networking features to tenants,
    such as routing, firewalls, load balancing, and private networking.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron是OpenStack的网络服务，随着不同版本的发布不断发展，提供了更多的功能和特性。云架构师可以根据现有的网络资源和需求，设计多种拓扑结构的OpenStack网络堆栈。与之前仅提供基础网络服务的Nova网络服务不同，OpenStack租户可以在Neutron中获得更细粒度的网络控制和灵活性。云和网络管理员可以为租户提供更高级的网络功能，如路由、防火墙、负载均衡和私有网络。
- en: 'In this chapter, we will walk through the rich updates that have been made
    to OpenStack networking by covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍OpenStack网络的丰富更新，涵盖以下主题：
- en: The Neutron architecture and its core service interactions in OpenStack
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack中Neutron架构及其核心服务交互
- en: Neutron plugins and supported drivers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neutron插件和支持的驱动程序
- en: Existing Neutron agents and required network types
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的Neutron代理和所需的网络类型
- en: Designing and implementing an OpenStack virtual switching layout using **Open
    vSwitch** ( **OVS** ) and **Open Virtual Network** ( **OVN** ) mechanisms as a
    **Software-Defined Networking** ( **SDN** ) implementation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Open vSwitch**（**OVS**）和**Open Virtual Network**（**OVN**）机制设计并实现OpenStack虚拟交换布局，作为**软件定义网络**（**SDN**）的实现
- en: Learning about and implementing routing, using virtual routers to interconnect
    tenant networks and provide external connectivity
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解并实现路由，使用虚拟路由器互联租户网络并提供外部连接
- en: Integrating the load balancer as a service feature using the emerging Octavia
    project
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新兴的Octavia项目将负载均衡作为服务功能进行集成
- en: Exploring Neutron’s core components
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Neutron的核心组件
- en: 'OpenStack networking has evolved through different releases to build an advanced
    and functioning cloud networking stack. Like many other OpenStack services, Neutron
    is composed of several services that can be stretched across multiple hosts, as
    shown in the following diagram:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack网络通过不同版本的发布不断发展，构建了一个先进且功能完善的云网络堆栈。像许多其他OpenStack服务一样，Neutron由多个服务组成，可以跨多个主机进行扩展，正如以下图所示：
- en: '![Figure 6.1 – The OpenStack Neutron core architecture](img/B21716_06_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – OpenStack Neutron核心架构](img/B21716_06_01.jpg)'
- en: Figure 6.1 – The OpenStack Neutron core architecture
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – OpenStack Neutron核心架构
- en: 'These components can be briefly broken down as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件可以简要地分解如下：
- en: '**Neutron server** : This acts as an API portal that receives API requests
    generated by services or end users and forwards them to the next process – in
    this case, the Neutron agents through the messaging queue service. The other part
    of the server interaction within an OpenStack ecosystem is access to the database
    to update the network objects for each API request.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neutron服务器**：它充当一个API门户，接收由服务或最终用户生成的API请求，并将其转发到下一个处理环节——在这种情况下，通过消息队列服务将请求传递给Neutron代理。OpenStack生态系统中服务器交互的另一个部分是访问数据库，以便更新每个API请求的网络对象。'
- en: '**Neutron agents** : Neutron’s architecture relies heavily on different types
    of agents that will be installed in other hosts to handle different networking
    features. The plugin agent, denoted as **neutron-*-agent** , is the process that
    handles virtual switching capabilities in each compute node. This type can be
    described as a **layer 2** ( **L2** ) agent. A very common L2 agent is OVS, which
    provides L2 connectivity via the ML2 mechanism driver. The second type of Neutron
    agent is the **layer 3** ( **L3** ) agent, denoted as **neutron-l3-agent** , which
    is installed in the network node and deals with instances of network access for
    L3, such as NAT, firewall, and **virtual private network** ( **VPN** ) capabilities.
    The DHCP agent, denoted as **neutron-dhcp-agent** , manages instances of DHCP
    configuration for each tenant network (a **dnsmasq** configuration).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neutron 代理**：Neutron 架构在很大程度上依赖于安装在其他主机上的不同类型的代理，以处理不同的网络功能。插件代理，标记为 **neutron-*-agent**，是处理每个计算节点中虚拟交换功能的进程。这种类型可以被描述为
    **第二层**（**L2**）代理。一种非常常见的 L2 代理是 OVS，它通过 ML2 机制驱动程序提供 L2 连通性。第二种 Neutron 代理是 **第三层**（**L3**）代理，标记为
    **neutron-l3-agent**，安装在网络节点上，处理 L3 网络访问的实例，如 NAT、防火墙和 **虚拟私人网络**（**VPN**）功能。DHCP
    代理，标记为 **neutron-dhcp-agent**，管理每个租户网络的 DHCP 配置实例（**dnsmasq** 配置）。'
- en: Depending on the nature of the network function request, the Neutron service
    will reach the agents deployed on the network and/or compute nodes. Cloud operators
    should list which network functions the infrastructure supports, which will decide
    the choice of plugins and agents that can be installed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据网络功能请求的性质，Neutron 服务将与部署在网络节点和/或计算节点上的代理进行交互。云运营商应列出基础设施支持的网络功能，这将决定可以安装的插件和代理的选择。
- en: Demystifying Neutron agents
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭示 Neutron 代理的秘密
- en: 'Neutron relies on agent services that interact with **neutron-server** through
    the queuing message service to implement virtual networking for L2 and L3 connectivity,
    DHCP, and routing services. Different agents will be deployed in the network and
    compute nodes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 依赖与 **neutron-server** 通过排队消息服务交互的代理服务，以实现 L2 和 L3 连通性、DHCP 和路由服务的虚拟网络。不同的代理将在网络和计算节点上部署：
- en: '**L2 agent** : Deployed on network and compute nodes to implement L2 networking
    connectivity for instances and virtual networks, such as routers.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 代理**：部署在网络节点和计算节点上，为实例和虚拟网络（如路由器）实现 L2 网络连通性。'
- en: '**L3 agent** : Deployed on the network node and, optionally, on compute nodes
    to perform L3 routing between different types of networks, such as tenant and
    external networks. L3 can also implement more advanced networking capabilities,
    such as firewalls and VPNs.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L3 代理**：部署在网络节点上，并可选地部署在计算节点上，用于在不同类型的网络之间执行 L3 路由，例如租户网络和外部网络。L3 还可以实现更高级的网络功能，如防火墙和
    VPN。'
- en: '**DHCP agent** : Deployed on the network node to run the DHCP service for tenant
    networks.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DHCP 代理**：部署在网络节点上，为租户网络运行 DHCP 服务。'
- en: '**Plugin agent** : Deployed on the network node to process data packets on
    virtual networks that depend on the implemented Neutron plugin.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件代理**：部署在网络节点上，处理依赖于已实现 Neutron 插件的虚拟网络中的数据包。'
- en: '**Metadata agent** : Deployed on the network node. The metadata service provided
    by Nova enables the retrieval of instance information, such as hostname and IP
    addresses, upon a metadata HTTP request ( **169.254.169.254** ). The Neutron metadata
    agent facilitates the forwarding of metadata to the Nova metadata service through
    its internal proxy component with additional information, such as instance and
    tenant IDs.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据代理**：部署在网络节点上。Nova 提供的元数据服务使得可以通过元数据 HTTP 请求（**169.254.169.254**）检索实例信息，如主机名和
    IP 地址。Neutron 元数据代理通过其内部代理组件将元数据转发到 Nova 元数据服务，并附加额外的信息，如实例和租户 ID。'
- en: The proper function of the deployed Neutron agent will vary, based on the types
    of the designed networks in the OpenStack environment. The next section will briefly
    cover the different categories of networks that should exist in our initial design
    draft.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的 Neutron 代理的正常功能将根据 OpenStack 环境中设计的网络类型而有所不同。下一节将简要介绍我们初步设计草稿中应存在的不同网络类别。
- en: Network categories
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络类别
- en: 'It is essential to go through the network types to enable traffic flow, based
    on their usage. The following glossary covers the network categories that are
    used mainly with the latest releases of OpenStack:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 必须了解不同的网络类型以启用流量流动，具体取决于它们的使用方式。以下术语表涵盖了主要与 OpenStack 最新版本一起使用的网络类别：
- en: '**Provider networks** : Created and managed by cloud operators who define a
    set of network attributes, such as the network’s type – for example, **VXLAN**
    , **Generic Routing Encapsulation** ( **GRE** ), or **flat** . Cloud operators
    provide and configure the underlying infrastructure, such as the physical network
    interface designated for the traffic flow.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供商网络**：由云操作员创建和管理，操作员定义一组网络属性，例如网络类型——例如，**VXLAN**、**通用路由封装**（**GRE**）或**平面**。云操作员提供并配置底层基础设施，例如指定用于流量流动的物理网络接口。'
- en: '**Self-service networks** : Referred to as **tenant networks** , these are
    created by cloud users. Self-service networks are self-contained and fully isolated
    from other networks in a multi-tenancy environment managed by Neutron. Tenants
    are restricted to creating virtual networks with what was predefined by the cloud
    operators in terms of network types. For example, a cloud user cannot implement
    GRE networks if the cloud operator does not provide the option. Cloud users do
    not have access to the underlying Neutron configuration.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自服务网络**：也称为 **租户网络**，这些网络由云用户创建。自服务网络是独立的，完全与由 Neutron 管理的多租户环境中的其他网络隔离。租户只能根据云操作员预定义的网络类型来创建虚拟网络。例如，如果云操作员未提供该选项，则云用户无法实现
    GRE 网络。云用户无法访问底层的 Neutron 配置。'
- en: '**External provider networks** : This refers to a provider network for specific
    external network connectivity. Cloud operators configure an external routing device
    to access the internet.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部提供商网络**：指特定外部网络连接的提供商网络。云操作员配置外部路由设备以访问互联网。'
- en: The next section will cover the most interesting capabilities of Neutron by
    exploring the concept of plugins and their latest mode of usage.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将通过探讨插件的概念及其最新使用模式，涵盖 Neutron 最具吸引力的功能。
- en: The core of networking – Neutron plugins
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络核心 – Neutron 插件
- en: 'Neutron supports the use of plugins and drivers, which use different software
    and hardware technologies provided by open source communities or vendor solutions.
    There are two types of Neutron plugins:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 支持使用插件和驱动程序，这些插件和驱动程序使用由开源社区或供应商解决方案提供的不同软件和硬件技术。Neutron 插件有两种类型：
- en: '**Core plugin** : Enables L2 connectivity functions and orchestration of network
    elements, including virtual networks, subnets, and ports'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心插件**：启用 L2 连接功能和网络元素的编排，包括虚拟网络、子网和端口。'
- en: '**Service plugin** : Enables additional network capabilities, including routing,
    private networks, firewalls, and load balancing'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务插件**：启用额外的网络功能，包括路由、私有网络、防火墙和负载均衡。'
- en: Important note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The code for plugins is run on the Neutron server, which can be configured as
    part of the cloud controller node(s).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 插件的代码在 Neutron 服务器上运行，该服务器可以配置为云控制节点的一部分。
- en: The following section will dive into the most adopted core plugins.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将深入探讨最常用的核心插件。
- en: Core plugin – Modular Layer 2
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核心插件 – 模块化二层
- en: The most widely adopted core plugin in Neutron is **Modular Layer 2** ( **ML2**
    ). The introduction of ML2 to the OpenStack networking service has increased the
    flexibility of designing network architectures. The secret sauce of ML2 is that
    it supports the use of a variety of technologies from different vendors simultaneously.
    Thus, cloud architects and operators aren’t limited to a specific set of features
    and can easily extend or change their network stacks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 中最广泛采用的核心插件是 **模块化二层**（**ML2**）。ML2 引入 OpenStack 网络服务，增加了设计网络架构的灵活性。ML2
    的“秘诀”在于它支持同时使用来自不同供应商的多种技术。因此，云架构师和操作员不会局限于特定的功能集，可以轻松扩展或更改他们的网络架构。
- en: 'This plugin has been developed due to the historical limitations prior to the
    **Havana** release, where operators had to stick to only one core plugin, referred
    to as a monolithic plugin-based component. The two most commonly used core plugins
    were **Linux Bridge** and **OVS** , which have been replaced in the latest releases
    of OpenStack by ML2. Replacing does not mean directly removing any of them but
    bundling them as **mechanism drivers** in the core ML2 plugin. ML2 is not limited
    to Linux Bridge and OVS and also supports other vendor drivers, such as VMware,
    Cisco, and OpenDayLight. An amazing filtered list can be found on the OpenStack
    community website: [https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking)](https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking))
    .'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在 **Havana** 发布之前的历史限制，开发了这个插件。在那个时候，操作员只能使用一个核心插件，称为单体插件组件。最常用的两个核心插件是 **Linux
    Bridge** 和 **OVS**，在 OpenStack 的最新版本中，它们已经被 ML2 替代。替代并不意味着直接删除它们，而是将它们作为 **机制驱动程序**
    打包到核心 ML2 插件中。ML2 不仅限于 Linux Bridge 和 OVS，还支持其他厂商的驱动程序，如 VMware、Cisco 和 OpenDayLight。在
    OpenStack 社区网站上可以找到一个很棒的过滤列表：[https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking)](https://www.openstack.org/marketplace/drivers/#project=neutron%20(networking))。
- en: 'An overview of the modular plugins and drivers supported in Neutron is illustrated
    here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 中支持的模块化插件和驱动程序的概览如下所示：
- en: '![Figure 6.2 – An overview of the OpenStack Neutron core and service plugins
    architecture](img/B21716_06_02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – OpenStack Neutron 核心和服务插件架构概览](img/B21716_06_02.jpg)'
- en: Figure 6.2 – An overview of the OpenStack Neutron core and service plugins architecture
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – OpenStack Neutron 核心和服务插件架构概览
- en: 'As shown in the preceding diagram, Neutron relies on the mechanisms of plugins
    that provide a specific set of networking services. An API request will be forwarded
    by the Neutron server to the associated plugin configured in the controller node.
    The ML2 plugin combines two main elements into the same framework, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，Neutron 依赖于插件机制，这些插件提供一组特定的网络服务。API 请求将由 Neutron 服务器转发给控制节点中配置的相关插件。ML2
    插件将两个主要元素结合到同一个框架中，如下所示：
- en: '**Type drivers** : These expose L2 functionalities to create more segmented
    networks, including the following:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型驱动程序**：这些暴露了 L2 功能，用于创建更多分段的网络，包括以下内容：'
- en: '**VLAN** : Segregates network traffic using **802.1Q** tagging. Virtual machines
    that belong to the same VLAN are part of the same broadcast L2 domain'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VLAN**：使用 **802.1Q** 标签隔离网络流量。属于同一 VLAN 的虚拟机是同一广播 L2 域的一部分'
- en: '**VXLAN** : Uses a **virtual network identifier** ( **VNI** ) to separate and
    differentiate traffic between different networks'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VXLAN**：使用 **虚拟网络标识符**（**VNI**）来分隔和区分不同网络之间的流量'
- en: '**Flat** : VLAN tagging and network segregation are not supported where instances
    are connected in the same network'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flat**：不支持 VLAN 标签和网络隔离，其中实例连接在同一网络中'
- en: '**GRE** : Encapsulates traffic using the GRE tunneling protocol'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRE**：使用 GRE 隧道协议封装流量'
- en: '**GENEVE** : Resembles the VXLAN overlay technology but with more optimal encapsulation
    methods'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GENEVE**：类似于 VXLAN 覆盖技术，但具有更优化的封装方法'
- en: '**Local** : Connects instances only within the same network hosted in the same
    compute nodes and not with instances in different nodes'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Local**：仅连接同一网络中托管在相同计算节点中的实例，而不连接不同节点中的实例'
- en: '**Mechanism drivers** : Implements the type driver technologies through both
    network software methods such as OVS, Linux Bridge, and OVN and hardware-based
    ones'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机制驱动程序**：通过网络软件方法（如 OVS、Linux Bridge 和 OVN）以及基于硬件的方法实现类型驱动技术'
- en: An up-to-date ML2 driver support matrix list can be found at [https://docs.openstack.org/neutron/latest/admin/config-ml2.html](https://docs.openstack.org/neutron/latest/admin/config-ml2.html)
    . Each mechanism driver supports a set of network types (type drivers). For example,
    OVS supports most network types, including VLAN, VXLAN, GRE, flat, and local.
    If a need has been raised to support the GENEVE type driver while in production,
    the new mechanism driver can be integrated by just installing the driver and reconfiguring
    the Neutron driver list.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 ML2 驱动支持矩阵列表可以在 [https://docs.openstack.org/neutron/latest/admin/config-ml2.html](https://docs.openstack.org/neutron/latest/admin/config-ml2.html)
    找到。每个机制驱动程序都支持一组网络类型（类型驱动程序）。例如，OVS 支持大多数网络类型，包括 VLAN、VXLAN、GRE、flat 和 local。如果在生产环境中提出了支持
    GENEVE 类型驱动的需求，可以通过仅安装驱动并重新配置 Neutron 驱动程序列表来集成新的机制驱动程序。
- en: Building virtual switching
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建虚拟交换
- en: In this section, two mechanism drivers will be discussed to establish connectivity
    between virtual network ports and physical networks. OVS is one of the most featured
    mechanism drivers, with advanced networking capabilities such as **OpenFlow**
    . The second driver we will cover is **OVN** , an SDN implementation that enables
    programming networks by controlling the flow of packets. Linux Bridge and **L2
    Population** are also well-tested and mature mechanisms. With ML2 being an agnostic
    plugin for the supported drivers, introducing a new mechanism driver will not
    introduce a lot of complexity in an existing running OpenStack environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论两种机制驱动程序，用于在虚拟网络端口与物理网络之间建立连接。OVS 是功能最丰富的机制驱动程序之一，具有高级网络功能，如 **OpenFlow**。我们将讨论的第二个驱动程序是
    **OVN**，这是一种 SDN 实现，通过控制数据包流动来编程网络。Linux Bridge 和 **L2 Population** 也是经过充分测试和成熟的机制。由于
    ML2 是一个支持驱动程序的无关插件，介绍新的机制驱动程序不会对现有的 OpenStack 环境带来太多复杂性。
- en: Opening vSwitch in OpenStack
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 OpenStack 中启用 vSwitch
- en: 'Most OpenStack networking implementations would require a minimum of overlay
    networking technologies that provide tunnel-based virtual networking, with an
    immense level of network segmentation, such as VXLAN and GRE. Tunnel-based networks
    can provide up to 16 million networks. OVS supports most used drivers in large
    and complex OpenStack networking environments, such as VLAN, VXLAN, GRE, and flat.
    As a cloud operator, it is crucial to have a basic understanding of how OVS operates
    when installed in an OpenStack environment. OVS comes with a complete core architecture,
    with different components that operate as software switches in a host kernel space.
    The OVS implementation comes with different services when installed in an OpenStack
    environment, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 OpenStack 网络实现至少需要使用基于隧道的虚拟网络技术，提供极高程度的网络分段，如 VXLAN 和 GRE。基于隧道的网络能够支持多达
    1600 万个网络。OVS 支持大多数在大型复杂 OpenStack 网络环境中使用的驱动程序，如 VLAN、VXLAN、GRE 和扁平网络。作为云操作员，了解
    OVS 在 OpenStack 环境中如何运作是至关重要的。OVS 提供了完整的核心架构，并通过不同组件在主机内核空间中作为软件交换机运行。OVS 实现中包含多个服务，当它安装在
    OpenStack 环境中时，具体如下：
- en: '**openvswitch** : A kernel module that handles the data plane that processes
    the network packets.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**openvswitch**：一个内核模块，处理数据平面，负责处理网络数据包。'
- en: '**ovs-switchd** : A Linux process that runs on a physical host to control and
    manage virtual switches.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ovs-switchd**：一个运行在物理主机上的 Linux 进程，用于控制和管理虚拟交换机。'
- en: '**ovsdb-server** : A local database to store the virtual switches running locally.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ovsdb-server**：一个本地数据库，用于存储本地运行的虚拟交换机。'
- en: '**neutron-openvswitch-agent** : Configured in compute nodes that use the OVS
    mechanism driver.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**neutron-openvswitch-agent**：配置在使用 OVS 机制驱动的计算节点中。'
- en: '**neutron-server** : Handles API requests, with the ML2 plugin loading the
    OVS mechanism driver. The neutron-server process passes the network request to
    the OVS driver via an RPC cast message to **neutron-openvswitch-agent** . The
    latter configures the OVS switch on the compute node and sets the required resources
    of the local instance.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**neutron-server**：处理 API 请求，ML2 插件加载 OVS 机制驱动程序。neutron-server 进程通过 RPC 广播消息将网络请求传递给
    OVS 驱动程序，后者通过 **neutron-openvswitch-agent** 配置计算节点上的 OVS 交换机，并设置本地实例所需的资源。'
- en: 'Once OVS is implemented, a set of virtual network devices is installed, which
    a cloud operator should keep in mind during deployment or troubleshooting tasks.
    An Ethernet frame would travel from an instance through the following interfaces:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 OVS 被实现，一套虚拟网络设备将被安装，云操作员在部署或故障排除任务中应当牢记这一点。一个以太网帧将通过以下接口从实例传输：
- en: '**tapXXXX** : A tap interface where **XXXX** is the assigned tap interface
    ID.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tapXXXX**：一个 tap 接口，其中 **XXXX** 是分配的 tap 接口 ID。'
- en: '**br-int** : A bridge interface, known as the integration bridge, where it
    consolidates all the virtual devices, such as virtual machines, routers, and firewalls.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-int**：一个桥接接口，称为集成桥接，它将所有虚拟设备（如虚拟机、路由器、防火墙）整合在一起。'
- en: '**int-br-ethX** : A virtual patch port connecting different interfaces to the
    OVS integration bridge interface ( **br-int** ).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**int-br-ethX**：一个虚拟补丁端口，将不同接口连接到 OVS 集成桥接接口（**br-int**）。'
- en: '**phy-br-ethX** : A virtual patch port connecting different interfaces to the
    OVS provider bridge ( **br-ethX** ).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**phy-br-ethX**：一个虚拟补丁端口，将不同接口连接到 OVS 提供者桥接（**br-ethX**）。'
- en: '**br-ethX** : A bridge interface where *X* is the assigned bridge interface
    ID connecting to the physical network. It is also known as the provider bridge
    and connects to the **br-int** integration bridge through a virtual patch cable,
    provided by patch ports ( **int-br-ethX** and **phy-br-ethX** patch ports for
    **int-br** and **br-ethX** , respectively).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-ethX**：一个桥接接口，其中*X*是分配的桥接接口ID，用于连接物理网络。它也被称为提供程序桥接，通过虚拟补丁电缆与**br-int**集成桥接连接，由补丁端口（**int-br-ethX**和**phy-br-ethX**补丁端口分别用于**int-br**和**br-ethX**）提供。'
- en: '**qbrXXXX** : A Linux bridge interface where **XXXX** is the assigned bridge
    interface ID dedicated to IP tables only.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**qbrXXXX**：一个Linux桥接接口，其中**XXXX**是分配的桥接接口ID，专门用于IP表。'
- en: '**br-tun** : A bridge tunnel interface to handle packet encapsulation and de-encapsulation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-tun**：一个桥接隧道接口，用于处理数据包的封装和解封装。'
- en: '**br-ex** : A bridge interface providing connectivity to external networks.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**br-ex**：一个提供与外部网络连接的桥接接口。'
- en: Each spawned instance is connected via its tap interface, denoted as **tapXXXX**
    , created in the hypervisor host. The associated Linux bridge, denoted as **qbrXXXX**
    , is connected to the OVS **br-int** integration bridge, where the traffic will
    be routed based on the programmed OpenFlow rules and then forwarded through the
    virtual switch.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个启动的实例通过其tap接口连接，表示为**tapXXXX**，该接口在虚拟化主机中创建。相关的Linux桥接，表示为**qbrXXXX**，连接到OVS的**br-int**集成桥接，流量将根据编程的OpenFlow规则进行路由，然后通过虚拟交换机转发。
- en: 'Connections between the integration and provider bridges are handled by patch
    cables. Packets exit the physical network via the OVS provider bridge ( **br-ethX**
    ), connected to the physical network interface of the host. When configuring OVS
    in our existing OpenStack environment, each node will run its own integration
    and provider bridges. Connectivity between the OpenStack nodes, including the
    cloud controller, network, and compute nodes, is established via the physical
    network, as illustrated in the following diagram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 集成桥接和提供程序桥接之间的连接通过补丁电缆处理。数据包通过OVS提供程序桥接（**br-ethX**）离开物理网络，连接到主机的物理网络接口。在我们的现有OpenStack环境中配置OVS时，每个节点将运行自己的集成桥接和提供程序桥接。OpenStack节点之间的连接，包括云控制器、网络节点和计算节点，通过物理网络建立，正如下图所示：
- en: '![Figure 6.3 – Traffic flow through different interfaces using OVS](img/B21716_06_03.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 使用OVS通过不同接口的流量流动](img/B21716_06_03.jpg)'
- en: Figure 6.3 – Traffic flow through different interfaces using OVS
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 使用OVS通过不同接口的流量流动
- en: Now that we understand the basic core components of OVS within an OpenStack
    setup, we can move on to configuring our infrastructure code to deploy OVS as
    the main mechanism driver in Neutron.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了OpenStack设置中OVS的基本核心组件，可以继续配置我们的基础设施代码，将OVS部署为Neutron中的主要机制驱动程序。
- en: Deploying with OVS
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用OVS进行部署
- en: 'The OVS mechanism driver implements L2 isolation, which offers VXLAN-, GRE-,
    and VLAN-based networks. The next configuration will implement the ML2 plugin
    to use VXLAN-based networks. This will allow instances to connect via VLAN segmentation
    and VXLAN tunneling. Virtual networks for tenants will not be exposed outside
    of the compute or network nodes. **kolla-ansible** comes with the ML2 plugin enabled
    by default. A few prerequisites must be fulfilled before deploying an **OVS**
    configuration. To reach the external networks via routers and provider networks,
    adjust the **neutron_external_interface** setting in the **/etc/kolla/globals.yml**
    file to the network interface dedicated to this matter. The **eth2** interface
    has been assigned an interface to connect with the external world, as designed
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* . You can adjust the settings as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OVS机制驱动程序实现L2隔离，提供基于VXLAN、GRE和VLAN的网络。接下来的配置将实现ML2插件，以使用基于VXLAN的网络。这将允许实例通过VLAN分段和VXLAN隧道连接。租户的虚拟网络将不会暴露到计算或网络节点之外。**kolla-ansible**默认启用ML2插件。在部署**OVS**配置之前，必须满足一些先决条件。为了通过路由器和提供程序网络访问外部网络，请在**/etc/kolla/globals.yml**文件中调整**neutron_external_interface**设置，指向专门用于此目的的网络接口。**eth2**接口已被分配用于连接外部网络，正如在[*第3章*](B21716_03.xhtml#_idTextAnchor108)中设计的那样，*OpenStack控制平面
    – 共享服务*。您可以按如下方式调整设置：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: 'It is important to consider the usage of a physical interface with the OVS
    provider bridge ( **br-ethX** ) if you are planning to use more than one provider
    bridge. In this case, **neutron_external_interface** can be assigned a comma-separated
    list – for example, **neutron_external_interface: "eth2,eth3"** .'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '如果计划使用多个提供者桥接，则需要考虑使用带有OVS提供者桥接（**br-ethX**）的物理接口。在这种情况下，**neutron_external_interface**可以分配一个以逗号分隔的列表——例如，**neutron_external_interface:
    "eth2,eth3"**。'
- en: 'The other check is to verify whether **kolla-ansible** is configured with an
    OVS driver mechanism that comes with the ML2 plugin. Check and adjust the following
    configuration setting in the **globals.yml** file:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个检查是验证**kolla-ansible**是否配置了带有ML2插件的OVS驱动机制。请检查并调整**globals.yml**文件中的以下配置设置：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next check is to verify that our Neutron services are assigned to the respective
    OpenStack nodes in the **/ansible/inventory/multi_packtpub_prod** file. As demonstrated
    in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane
    – Shared Services* , **neutron-server** will run as part of the controller node,
    as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个检查是验证我们的Neutron服务是否分配给**/ansible/inventory/multi_packtpub_prod**文件中的相应OpenStack节点。如[*第3章*](B21716_03.xhtml#_idTextAnchor108)所示，*OpenStack控制平面——共享服务*，**neutron-server**将在控制节点中作为一部分运行，如下所示：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Neutron agents, including the DHCP, L3, and metadata agents, will run in a
    dedicated network node, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron代理，包括DHCP、L3和元数据代理，将在专用网络节点上运行，如下所示：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: OVS will be installed in the network and compute nodes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: OVS将在网络和计算节点中安装。
- en: Important note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As shown in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage
    – Block, Object, and File Shares* , the **manila-share** service provides access
    to the file share by using the Neutron plugin.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第5章*](B21716_05.xhtml#_idTextAnchor146)所示，*OpenStack存储——块存储、对象存储和文件共享*，**manila-share**服务通过使用Neutron插件提供文件共享访问。
- en: 'The OVS setup in the inventory file is listed as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在清单文件中列出的OVS设置如下：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Launch the pipeline job to install OVS across the target nodes. Note that VXLAN
    was not explicitly configured in the **kolla-ansible** code. By default, the Neutron
    tenant network uses the VXLAN type. The default VNI range is defined from 1 to
    1,000 IDs, reserved for the tenant networks when they are created. This setting
    is denoted with the **vni_ranges** line in the **ml2_config.ini** file in the
    controller node. The ML2 configuration can be extended and we can employ a configuration
    override by creating an **/** **etc/kolla/config/neutron/ml2_config.ini** file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 启动管道任务以在目标节点上安装OVS。请注意，VXLAN在**kolla-ansible**代码中未显式配置。默认情况下，Neutron租户网络使用VXLAN类型。默认的VNI范围定义为从1到1000个ID，当租户网络创建时将为其保留。此设置在控制节点的**ml2_config.ini**文件中的**vni_ranges**行中表示。可以扩展ML2配置，并且我们可以通过创建**/**
    **etc/kolla/config/neutron/ml2_config.ini**文件来采用配置重写。
- en: 'The following command line lists the different network agents, including the
    OVS ones, from the cloud controller node:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令行列出了来自云控制器节点的不同网络代理，包括OVS代理：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![Figure 6.4 – The neutron agent list output](img/B21716_06_04.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – Neutron代理列表输出](img/B21716_06_04.jpg)'
- en: Figure 6.4 – The neutron agent list output
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – Neutron代理列表输出
- en: As shown here, the OVS agents should be up and running in the respective network
    and compute nodes. In the following section, we will trace the network flow with
    the OVS mechanism.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，OVS代理应该在各自的网络和计算节点中正常运行。在接下来的部分中，我们将跟踪使用OVS机制的网络流。
- en: Traffic flow with OVS
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OVS的流量流向
- en: 'The setup of the OVS driver will dictate the travel hops of an Ethernet frame,
    from an instance all the way to the physical network. Numerous interfaces will
    be used, including the virtual switch ports and integration bridge. The deployment
    of the **openvswitch** playbook in the compute node should create the OVS logical
    bridges on the host, which can be checked by running the following command line:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: OVS驱动程序的设置将决定以太网帧的传输跳数，从实例一直到物理网络。将使用多个接口，包括虚拟交换机端口和集成桥接。**openvswitch**剧本在计算节点的部署应该在主机上创建OVS逻辑桥接，可以通过运行以下命令行来检查：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output will be as shown:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 6.5 – The OVS bridges list](img/B21716_06_05.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – OVS桥接列表](img/B21716_06_05.jpg)'
- en: Figure 6.5 – The OVS bridges list
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – OVS桥接列表
- en: 'Instances spawned in the compute node are connected via the same local VLAN.
    With the VXLAN layout, instance ports are attached to the integration bridge,
    which will be assigned a local VLAN ID. Traffic initiated from an instance will
    be forwarded by the integration bridge ( **br-int** ) to the tunnel bridge ( **br-tun**
    ). The overall layout of the connecting bridges and their respective ports can
    be seen by running the following command line:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算节点中启动的实例通过相同的本地VLAN连接。采用VXLAN布局时，实例端口连接到集成桥，该桥将分配一个本地VLAN ID。从实例发起的流量将由集成桥（**br-int**）转发到隧道桥（**br-tun**）。通过运行以下命令行，可以查看连接桥及其各自端口的整体布局：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![Figure 6.6 – An OVS interface listing](img/B21716_06_06.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – OVS接口列表](img/B21716_06_06.jpg)'
- en: Figure 6.6 – An OVS interface listing
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – OVS接口列表
- en: As depicted in the virtual switch interfaces, the integration bridge ( **br-int**
    ) is tagged with **VLAN 1.** When traffic reaches instances in different nodes,
    the packets will travel through the tunnel bridge ( **br-tun** ) encapsulated
    in VXLAN packets. Under the hood, the local VLAN ID of **1** will be swapped with
    the VXLAN tunnel ID, as shown with **vxlan-476984a0** . Otherwise, if an instance
    connects to another one residing in the same compute node, the packets will travel
    through the integration bridge ( **br-int** ) locally to the destination port,
    denoted with the **qrXXXX** and **tapYYYY** port interfaces.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如虚拟交换机接口所示，集成桥（**br-int**）标记为**VLAN 1**。当流量到达不同节点中的实例时，数据包将通过隧道桥（**br-tun**）封装在VXLAN数据包中传输。在背后，本地VLAN
    ID **1** 将与VXLAN隧道ID交换，如**vxlan-476984a0**所示。否则，如果实例连接到另一个位于同一计算节点的实例，数据包将通过集成桥（**br-int**）本地传输到目标端口，该端口通过**qrXXXX**和**tapYYYY**端口接口表示。
- en: The following section covers a more complex mechanism driver, OVN, and demonstrates
    its deployment in our OpenStack environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分介绍了更复杂的机制驱动程序OVN，并展示了其在OpenStack环境中的部署。
- en: OVN in OpenStack
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OVN在OpenStack中的应用
- en: The other major and preferred network virtualization in an OpenStack environment
    is OVN. OVN is considered a mature implementation of the SDN philosophy that provides
    greater flexibility for network flow programming. Operators can define a set of
    forwarding rules centrally to define how the flow of packets is controlled. OVN
    uses an abstraction software layer available in a network controller to program
    switches, with conditions and rules applied to the packet flow. The most distinctive
    functionality of OVN compared to OVS is its richer capabilities and advanced features,
    such as traffic routing and access control which are programmable. Unlike the
    OVS approach, OVN decouples the control function of network devices from the actual
    packet forwarding function through flow rules. OVN supports most network types,
    including VLAN, VXLAN (version 20.09 and later), flat, and Geneve.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack环境中另一个主要且首选的网络虚拟化是OVN。OVN被认为是SDN理念的成熟实现，提供了更大的网络流编程灵活性。操作员可以集中定义一组转发规则，来定义数据包流量的控制方式。OVN使用网络控制器中的抽象软件层来编程交换机，应用于数据包流的条件和规则。与OVS相比，OVN的最显著功能是其更丰富的能力和先进特性，如可编程的流量路由和访问控制。与OVS方法不同，OVN通过流规则将网络设备的控制功能与实际的数据包转发功能解耦。OVN支持大多数网络类型，包括VLAN、VXLAN（版本20.09及以上）、平面网络和Geneve。
- en: 'As depicted in the following diagram, OVN is built on top of OVS, which leverages
    its switching capabilities and adds an abstraction layer, managed by controllers
    that are stored in a set of **OVS** **databases** ( **OVSDBs** ):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，OVN建立在OVS之上，利用其交换能力并增加一个抽象层，该抽象层由控制器管理，这些控制器存储在一组**OVS** **数据库**（**OVSDBs**）中：
- en: '![Figure 6.7 – OVN integration architecture](img/B21716_06_07.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – OVN集成架构](img/B21716_06_07.jpg)'
- en: Figure 6.7 – OVN integration architecture
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – OVN集成架构
- en: 'The main constructs of the OVN architecture integration with an OpenStack environment
    are explained here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: OVN架构与OpenStack环境集成的主要构件在此进行解释：
- en: '**Southbound database** : Referenced as **ovsdb-server** ( **ovnsb.db** ),
    this stores data of the logical and physical network flows. The database hosts
    all binding information between the logical and physical networks, such as the
    port and logical networking bindings associated with the **Port_Binding** and
    **Logical_Flow** tables, respectively.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**南向数据库**：引用为 **ovsdb-server**（**ovnsb.db**），它存储逻辑和物理网络流的数据。该数据库托管逻辑与物理网络之间的所有绑定信息，如与
    **Port_Binding** 和 **Logical_Flow** 表分别关联的端口和逻辑网络绑定。'
- en: '**Northbound database** : Referenced as **ovsdb-server** ( **ovnnb.db** ),
    this stores data at a high level of the virtual networks that represent a **cloud
    management system** ( **CMS** ) – in this case, an OpenStack environment. The
    database reflects the networking resources of the CMS in the table datasets, such
    as the routers and switch ports associated with the **Logical_Router** and **Logical_Switch_Port**
    tables, respectively.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**北向数据库**：引用为 **ovsdb-server**（**ovnnb.db**），它在虚拟网络的高层存储数据，这些虚拟网络代表一个 **云管理系统**（**CMS**），在本例中即
    OpenStack 环境。该数据库通过表格数据集反映 CMS 的网络资源，如与 **Logical_Router** 和 **Logical_Switch_Port**
    表分别关联的路由器和交换机端口。'
- en: '**OVN controller** : Referenced as **ovn-controller** , this runs on hypervisor
    nodes and connects to the southbound database. It acts as the OVS controller by
    defining the OpenFlow rules on each OVS switch instance, converting logical flows
    into physical flows, and pushing configurations to the local OVSDB.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OVN 控制器**：引用为 **ovn-controller**，它在 Hypervisor 节点上运行并连接到南向数据库。它充当 OVS 控制器，通过在每个
    OVS 交换机实例上定义 OpenFlow 规则，将逻辑流转换为物理流，并将配置推送到本地 OVSDB。'
- en: '**OVN northbound service** : Referenced as **ovn-northd** , this runs as part
    of the control plane on the cloud controller. The **ovn-northd** daemon connects
    the northbound to the southbound database by translating the logical configuration
    from the **ovn-nb** data to logical data path flows. The converted data will be
    populated in the **ovn-sb** database and made ready for use by the **ovn-controller**
    instance.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OVN 北向服务**：引用为 **ovn-northd**，它作为控制平面的一部分在云控制器上运行。**ovn-northd** 守护进程通过将来自
    **ovn-nb** 数据的逻辑配置转换为逻辑数据路径流，将北向数据库连接到南向数据库。转换后的数据将被填充到 **ovn-sb** 数据库中，并为 **ovn-controller**
    实例准备好使用。'
- en: '**OVS data plane service** : Referenced as **ovs-vswitchd** , this runs as
    part of the data plane on the hypervisor nodes. **ovs-vswitchd** applies the packet-forwarding
    rules provided by **ovn-controller** .'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OVS 数据平面服务**：引用为 **ovs-vswitchd**，它作为数据平面的一部分在 Hypervisor 节点上运行。**ovs-vswitchd**
    应用由 **ovn-controller** 提供的包转发规则。'
- en: '**Local OVSDB** : Referenced as **ovsdb-server** , this runs locally in each
    hypervisor node. The OVN reference architecture takes advantage of the database
    scaling benefit by using a consistent snapshot of the database, to recover from
    possible connectivity interruptions.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地 OVSDB**：引用为 **ovsdb-server**，它在每个 Hypervisor 节点上本地运行。OVN 参考架构通过使用数据库的一致快照来利用数据库扩展优势，以从可能的连接中断中恢复。'
- en: '**Metadata agent** : Referenced as **ovn-metadata-agent** , this runs in each
    hypervisor node. The OVN metadata agent, such as the Neutron metadata agent, is
    used to proxy metadata API requests on each compute node. This way, each OVS switch
    instance running in each hypervisor node will route the requests to the local
    metadata agent and then query the Nova metadata service to run instances.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据代理**：引用为 **ovn-metadata-agent**，它在每个 Hypervisor 节点上运行。OVN 元数据代理，例如 Neutron
    元数据代理，用于在每个计算节点上代理元数据 API 请求。这样，每个 Hypervisor 节点上运行的 OVS 交换机实例将把请求路由到本地元数据代理，然后查询
    Nova 元数据服务以运行实例。'
- en: In the following section, we will use **kolla-ansible** to integrate the OVN
    project in an OpenStack environment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分，我们将使用 **kolla-ansible** 将 OVN 项目集成到 OpenStack 环境中。
- en: Deploying with OVN
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 OVN 部署
- en: 'OVN extends the OVS implementation to provide network services to instances.
    The ML2 mechanism driver facilitates integration with OpenStack as a CMS. The
    **kolla-ansible** playbooks include most of the OVN configuration and required
    packages to install. Similar to the OVS configuration, adjust **/etc/kolla/globals.yml**
    to use the designated interface to reach external networks, by configuring the
    **neutron_external_interface** setting, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: OVN 扩展了 OVS 实现，以为实例提供网络服务。ML2 机制驱动程序帮助与 OpenStack 作为 CMS 集成。**kolla-ansible**
    剧本包含了大部分 OVN 配置和所需的安装包。与 OVS 配置类似，通过配置 **neutron_external_interface** 设置，调整 **/etc/kolla/globals.yml**，以使用指定的接口连接外部网络，具体如下：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Configure the ML2 mechanism driver to use OVN, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 ML2 机制驱动程序以使用 OVN，如下所示：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'OVN comes with built-in support for L3 networking functions that enable instances
    to connect to external networks. There are two ways to provide such connectivity
    in OVN – through a centralized layout that does not use a distributed floating
    IP design or via **distributed virtual routing** ( **DVR** ), which requires a
    floating IP configuration. For two instances to reach external networks or each
    other using L3, traffic must flow through the network node. With the DVR method,
    traffic is more optimized by deploying an L3 agent in each compute node, allowing
    **north-south** (going to and from external networks) traffic with a floating
    IP to be routed from and to the compute node, with an extra hop to reach the network
    node. Similarly, **east-west** (traffic flow between instances) traffic is routed
    directly to the compute nodes. The latter option is recommended for more optimal
    performance. The distributed floating IP option can be enabled in the **globals.yml**
    file, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: OVN 内置支持 L3 网络功能，使实例能够连接到外部网络。在 OVN 中提供此类连接性有两种方式——通过不使用分布式浮动 IP 设计的集中式布局，或者通过**分布式虚拟路由**（**DVR**），这需要浮动
    IP 配置。为了让两个实例通过 L3 到达外部网络或彼此之间的通信，流量必须通过网络节点。使用 DVR 方法时，通过在每个计算节点上部署 L3 代理，流量将得到优化，使得**北南向**（进出外部网络）的浮动
    IP 流量可以从计算节点路由并返回，并额外跳转到达网络节点。类似地，**东西向**（实例间的流量）流量直接路由到计算节点。后者选项对于获得更优化的性能更为推荐。分布式浮动
    IP 选项可以在**globals.yml**文件中启用，如下所示：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Enable the Neutron OVN agent to provide extensible capabilities for network
    monitoring and **Quality of Service** ( **QoS** ) by configuring the following
    setting:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 Neutron OVN 代理，通过配置以下设置，提供可扩展的网络监控和**服务质量**（**QoS**）功能：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, make sure to adjust the **/ansible/inventory/multi_packtpub_prod** inventory
    file to install the OVN services in the assigned nodes. Similar to the OVS configuration,
    make sure to have the Neutron server and respective agents running in the controller
    and network nodes, respectively, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，确保调整**/ansible/inventory/multi_packtpub_prod**库存文件，在指定的节点上安装 OVN 服务。类似于
    OVS 配置，确保在控制节点和网络节点上分别运行 Neutron 服务器及相应代理，具体如下：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As discussed in the *OVN in OpenStack* section, the OVS northbound and southbound
    databases, in addition to the OVN **northd** daemon, will run on the controller
    node, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如*OVN 在 OpenStack 中*部分所讨论，OVS 北向和南向数据库，以及 OVN **northd** 守护进程，将在控制节点上运行，具体如下：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The OVN controller instances and OVN metadata agents will run on the compute
    nodes, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: OVN 控制器实例和 OVN 元数据代理将在计算节点上运行，具体如下：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Since Antelope and later releases, a dedicated OVN agent has been created to
    implement the lacking **ovn-controller** function.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Antelope 版本及以后的版本开始，已经创建了一个专用的 OVN 代理来实现缺失的**ovn-controller**功能。
- en: 'Launching the pipeline job should deploy the OVN services and create an ML2
    file, **/etc/neutron/plugins/ml2/ml2_conf.ini** , with the following default configurations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 启动管道作业应部署 OVN 服务并创建一个 ML2 文件，**/etc/neutron/plugins/ml2/ml2_conf.ini**，其中包含以下默认配置：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The default network tenant type is configured with GENEVE as the tunneling protocol.
    The main difference between GENEVE and VXLAN (configured for OVS in a previous
    section) is how the networking metadata is encapsulated and encoded between both
    protocols. VXLAN encodes the VNI only in the encapsulation header. Meanwhile,
    the GENEVE protocol uses extensible TLV to carry more information about the packet
    in the encapsulated header (it has a header length of 38 bytes, while the VXLAN
    frame can carry 8 bytes), such as the **ingress** and **egress** ports. GENEVE
    offers better capabilities, such as transport security, service chaining, and
    in-band telemetry, compared to VXLAN.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的网络租户类型配置为使用 GENEVE 作为隧道协议。GENEVE 与 VXLAN（在前述部分中为 OVS 配置的协议）之间的主要区别在于两种协议之间如何封装和编码网络元数据。VXLAN
    仅在封装头中编码 VNI。而 GENEVE 协议使用可扩展的 TLV 来承载有关封装头中的数据包的更多信息（其头部长度为 38 字节，而 VXLAN 帧可以承载
    8 字节），例如**入口**和**出口**端口。与 VXLAN 相比，GENEVE 提供了更强大的功能，例如传输安全、服务链和带内遥测。
- en: 'The other resulting default configuration is the default northbound and southbound
    database addresses. Each OVN controller running in each compute node should be
    able to reach both databases. Such configuration can be found in the same file,
    **ml2_conf.ini** , by checking the following settings:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个结果是默认的北向和南向数据库地址。每个计算节点中运行的 OVN 控制器应该能够访问这两个数据库。此配置可以在同一个文件 **ml2_conf.ini**
    中找到，通过检查以下设置：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once OVN is configured, further network resource creation and management will
    be handled by OVN, which maps each OpenStack resource abstraction to an entry
    object in the OVN southbound and northbound databases.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了 OVN，进一步的网络资源创建和管理将由 OVN 处理，它将每个 OpenStack 资源抽象映射到 OVN 南向和北向数据库中的条目对象。
- en: Both OVS and OVN provide different capabilities to connect instances to networks.
    In the following section, we will explore how routing is performed in Neutron
    and demonstrate the different routing traffic options in OpenStack.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: OVS 和 OVN 提供不同的能力来将实例连接到网络。在接下来的章节中，我们将探讨 Neutron 中的路由是如何执行的，并展示 OpenStack 中不同的路由流量选项。
- en: Configuring cloud routing
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置云路由
- en: Instances within the same virtual tenant network can reach each other, but by
    default, each tenant network cannot reach other tenants or external networks.
    Deploying virtual routers is the way to enable L3 network communication so that
    tenant virtual networks can connect by associating a subnet with a router.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 同一虚拟租户网络中的实例可以互相访问，但默认情况下，每个租户网络不能访问其他租户或外部网络。部署虚拟路由器是启用 L3 网络通信的方式，从而通过将子网与路由器关联，连接租户虚拟网络。
- en: Routing tenant traffic
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路由租户流量
- en: 'Under the hood, a port associated with a tenant virtual network will be associated
    with the IP address of the subnet gateway. Instances across different virtual
    networks reach each other by communicating via the virtual router, using the gateway
    IP address and their private IP addresses encapsulated in the packets. This is
    called a **NAT** ( **network address translation** ) mechanism. In OpenStack networking,
    the Neutron L3 agent manages virtual routers. IP packets are forwarded by a virtual
    router to different self-service and external networks through the following router
    interfaces:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，与租户虚拟网络相关联的端口将与子网网关的 IP 地址相关联。不同虚拟网络中的实例通过虚拟路由器进行通信，使用网关 IP 地址和它们的私有 IP
    地址封装在数据包中，从而互相访问。这称为 **NAT** （**网络地址转换**）机制。在 OpenStack 网络中，Neutron L3 代理管理虚拟路由器。IP
    数据包通过虚拟路由器转发到不同的自服务和外部网络，经过以下路由器接口：
- en: '**qr** : Contains the tenant network gateway IP address and is dedicated to
    routing traffic through all self-service traffic networks'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**qr** : 包含租户网络网关 IP 地址，专门用于将流量路由到所有自服务流量网络'
- en: '**qg** : Contains the external network gateway IP address and is dedicated
    to routing traffic out onto the external provider network'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**qg** : 包含外部网络网关 IP 地址，专门用于将流量路由到外部提供商网络'
- en: Upon the initiation of a virtual router instance, a network namespace will be
    created in the Neutron node that defines connections to self-service or external
    provider networks through routing tables, packet forwarding, and **iptables**
    rules. As shown in the following diagram, a router namespace refers to a virtual
    router that is attached to multiple bridge ports in an OVS configuration ( **qr**
    and **qg** ). Traffic flow between instances hosted in the same or different compute
    nodes is routed through the virtual router.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟路由器实例启动时，将在 Neutron 节点中创建一个网络命名空间，该命名空间通过路由表、数据包转发和 **iptables** 规则定义与自服务网络或外部提供商网络的连接。如以下图所示，路由器命名空间指的是附加到多个桥接端口的虚拟路由器（**qr**
    和 **qg**）。托管在同一或不同计算节点中的实例之间的流量将通过虚拟路由器路由。
- en: '![Figure 6.8 – Neutron router namespace connectivity based on OVS implementation](img/B21716_06_08.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 基于 OVS 实现的 Neutron 路由器命名空间连接](img/B21716_06_08.jpg)'
- en: Figure 6.8 – Neutron router namespace connectivity based on OVS implementation
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 基于 OVS 实现的 Neutron 路由器命名空间连接
- en: In the previous OVS implementation, the L3 agent should be up and running to
    start creating and managing virtual routers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的 OVS 实现中，L3 代理应该已经启动并运行，以开始创建和管理虚拟路由器。
- en: Important note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The router service plugin should be enabled by default once Neutron agent L3
    is deployed, using **kolla-ansible** . The service plugin can be verified in the
    **/etc/neutron/neutron.conf** file in the Neutron cloud controller node by checking
    the **service_plugin =** **router** line.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署了 Neutron 代理 L3，路由器服务插件应默认启用，使用**kolla-ansible**。可以通过检查 Neutron 云控制器节点的**/etc/neutron/neutron.conf**文件中的**service_plugin
    = router**行来验证该服务插件。
- en: 'Optionally, virtual routers can be managed through Horizon. The **kolla-ansible**
    run for the Neutron deployment should enable the router module in the dashboard.
    That can be verified in the cloud controller node’s **/etc/openstack-dashboard/local_settings.py**
    file with the following settings:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，虚拟路由器可以通过 Horizon 进行管理。Neutron 部署的**kolla-ansible**运行应在仪表板中启用路由器模块。可以通过在云控制器节点的**/etc/openstack-dashboard/local_settings.py**文件中检查以下设置来验证：
- en: '[PRE17]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the next exercise, we will create a tenant virtual network with the ML2
    plugin configured in OVS. A virtual router will be attached to the tenant network
    using the OpenStack CLI, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将使用在 OVS 中配置的 ML2 插件创建一个租户虚拟网络。将使用 OpenStack CLI 将虚拟路由器附加到租户网络，如下所示：
- en: 'Create a tenant virtual network using the OpenStack network CLI:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 OpenStack 网络 CLI 创建一个租户虚拟网络：
- en: '[PRE18]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a subnet with an IP range of **10.10.0.10/24** , an auto-assigned DHCP,
    and a default DNS nameserver of **8.8.8.8** :'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 IP 范围为**10.10.0.10/24**的子网，自动分配 DHCP，默认 DNS 名称服务器为**8.8.8.8**：
- en: '[PRE19]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a router and attach it to the created tenant network:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个路由器并将其附加到创建的租户网络：
- en: '[PRE20]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The router attachment to the tenant network will assign a private IP address
    to the router’s internal interface. By default, if no IP address is specified
    in the attachment command line, the internal interface will be assigned the default
    gateway of the subnet. The following command line verifies the assigned IP address
    of the router’s internal interface:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 路由器附加到租户网络将为路由器的内部接口分配一个私有 IP 地址。默认情况下，如果在附加命令行中没有指定 IP 地址，内部接口将分配子网的默认网关。以下命令行验证路由器内部接口分配的
    IP 地址：
- en: '[PRE21]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It gives the following output:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它给出的输出如下：
- en: '![Figure 6.9 – Virtual router port listing](img/B21716_06_09.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 虚拟路由器端口列表](img/B21716_06_09.jpg)'
- en: Figure 6.9 – Virtual router port listing
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 虚拟路由器端口列表
- en: 'The router interface can be checked within the router namespace **qr-** prefixed
    interface by running the following command:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过运行以下命令检查带有**qr-**前缀的路由器命名空间中的路由器接口：
- en: '[PRE22]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output looks like the following:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 6.10 – The network namespaces list](img/B21716_06_10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 网络命名空间列表](img/B21716_06_10.jpg)'
- en: Figure 6.10 – The network namespaces list
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 网络命名空间列表
- en: 'Copy the **qrouter** ID from the previous output, and then run the following
    command line to show the created interface of the router and the assigned IP address
    from the internal network:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从先前的输出中复制**qrouter** ID，然后运行以下命令行显示路由器创建的接口以及从内部网络分配的 IP 地址：
- en: '[PRE23]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will get the following output:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将获得如下输出：
- en: '![Figure 6.11 – A virtual router namespace internal interface listing](img/B21716_06_11.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 虚拟路由器命名空间内部接口列表](img/B21716_06_11.jpg)'
- en: Figure 6.11 – A virtual router namespace internal interface listing
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 虚拟路由器命名空间内部接口列表
- en: 'To enable instances to reach the external network, a second interface in the
    virtual router should be created that will be attached to the provider’s external
    network. A common networking setup, such as an external network device or a device
    integrated with a firewall appliance, should be placed in front of the OpenStack
    endpoint (such as the load balancer). Enabling internet access can be configured
    using the OpenStack CLI, as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为使实例能够访问外部网络，应创建虚拟路由器的第二个接口，并将其附加到提供商的外部网络。应将一个常见的网络设置，如外部网络设备或与防火墙设备集成的设备，放置在
    OpenStack 端点（如负载均衡器）之前。启用互联网访问可以使用 OpenStack CLI 配置，如下所示：
- en: 'Create an external network provider. As configured in the ML2 plugin for OVS,
    we can create the network as a VLAN type, with a defined segmentation ID and **physnet1**
    for the physical network attribute:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个外部网络提供者。如在 OVS 的 ML2 插件中配置的那样，我们可以将网络创建为 VLAN 类型，并定义分段 ID 和**physnet1**作为物理网络属性：
- en: '[PRE24]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Create the subnet part of the external network with a network range of **10.20.0.0/24**
    and the default gateway as **10.20.0.1** , disable DHCP, and set an allocation
    pool of **10.20.0.10-10.20.0.100** :'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建外部网络的子网部分，网络范围为**10.20.0.0/24**，默认网关为**10.20.0.1**，禁用DHCP，并设置分配池为**10.20.0.10-10.20.0.100**：
- en: '[PRE25]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Attach the router to an external provider network by running the following
    command line:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令行，将路由器附加到外部提供商网络：
- en: '[PRE26]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The attachment operation will assign an external IP from the external IP pool
    to the router, which can be checked by running the following command line:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 附加操作将从外部IP池中分配一个外部IP到路由器，可以通过运行以下命令行进行检查：
- en: '[PRE27]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is the output:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.12 – A created external port of the virtual router](img/B21716_06_12.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 创建的虚拟路由器外部端口](img/B21716_06_12.jpg)'
- en: Figure 6.12 – A created external port of the virtual router
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 创建的虚拟路由器外部端口
- en: 'The last attachment creates a second interface, prefixed with **qg-** in the
    router namespace, which can be verified by the following command line:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的附加操作在路由器命名空间中创建一个以**qg-**为前缀的第二个接口，可以通过以下命令行进行验证：
- en: '[PRE28]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.13 – A virtual router namespace external interface listing](img/B21716_06_13.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 虚拟路由器命名空间外部接口列表](img/B21716_06_13.jpg)'
- en: Figure 6.13 – A virtual router namespace external interface listing
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 虚拟路由器命名空间外部接口列表
- en: 'The next part of our connectivity demonstration involves creating security
    groups and rules to allow ingress and egress traffic at the network port level.
    To reach the internet and access the instances, we will create a new security
    group and add **ICMP** and **SSH** access:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的连接性演示的下一部分涉及创建安全组和规则，以允许在网络端口级别的入站和出站流量。为了连接到互联网并访问实例，我们将创建一个新的安全组，并添加**ICMP**和**SSH**访问：
- en: 'Using the OpenStack CLI, create a new security group to be applied to your
    instances:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用OpenStack CLI，创建一个新的安全组并将其应用到你的实例：
- en: '[PRE29]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create the rules associated with the created security group for SSH and ICMP,
    respectively:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建与已创建安全组相关的规则，用于SSH和ICMP，分别为：
- en: '[PRE30]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Using the OpenStack CLI, create a new test instance with a **tiny** flavor
    and a **cirros** image that is connected to the private tenant network:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用OpenStack CLI，创建一个新的测试实例，选择**tiny**规格和**cirros**镜像，并连接到私有租户网络：
- en: '[PRE31]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Important note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Make sure to adjust your available Glance image name and list of flavors, based
    on your existing resources. The **openstack server create** command line will
    fail if any of the assigned arguments do not exist. Cirros is a minimal Linux
    distribution, useful for quick testing and proof of concepts. The default session
    username is **cirros** and the password is **gocubsgo** .
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 确保根据现有资源调整你的可用Glance镜像名称和规格列表。如果任何指定的参数不存在，**openstack server create**命令将会失败。Cirros是一个最小化的Linux发行版，适用于快速测试和概念验证。默认的会话用户名是**cirros**，密码是**gocubsgo**。
- en: 'To test the connectivity from the instance to reach the internet, make sure
    that the instance state is **ACTIVE** , as follows:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试实例是否能连接到互联网，请确保实例的状态是**ACTIVE**，如下所示：
- en: '[PRE32]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is the output:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.14 – The instance listing](img/B21716_06_14.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 实例列表](img/B21716_06_14.jpg)'
- en: Figure 6.14 – The instance listing
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 实例列表
- en: 'Access to the created instance can be made in different ways, using the **virsh
    console** command line from the compute node or simply via SSH from the router
    namespace. Make sure to use the default **cirros** image credentials – that is,
    the username **cirros** and the password **gocubsgo** – and run a simple ping
    to reach **8.8.8.8** :'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过不同的方式访问创建的实例，使用计算节点上的**virsh console**命令行，或直接通过路由器命名空间的SSH进行访问。确保使用默认的**cirros**镜像凭证——即用户名**cirros**和密码**gocubsgo**——并运行一个简单的ping命令以连接到**8.8.8.8**：
- en: '[PRE33]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here is the output:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.15 – Testing external connectivity](img/B21716_06_15.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 测试外部连接性](img/B21716_06_15.jpg)'
- en: Figure 6.15 – Testing external connectivity
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 测试外部连接性
- en: 'The instance uses the internal virtual router IP address as the default gateway
    to route traffic reaching the external network. The internet is reachable through
    SNAT, performed by the router. A quick run of the **ip route** command in the
    instance shows the default gateway associated with the network route table:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例使用内部虚拟路由器IP地址作为默认网关来路由流量到达外部网络。通过路由器执行SNAT，互联网可以访问。通过在实例中快速运行**ip route**命令，可以查看与网络路由表相关联的默认网关：
- en: '![Figure 6.16 – Listing the default gateway](img/B21716_06_16.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – 列出默认网关](img/B21716_06_16.jpg)'
- en: Figure 6.16 – Listing the default gateway
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – 列出默认网关
- en: 'The next part of our walk-through is a demonstration of how resources hosted
    in external networks can reach instances in an OpenStack environment. By default,
    spawned instances will be assigned an IP address that is not visible outside of
    the tenant network. Neutron provides float IP addresses that implement **DNAT**
    ( **destination NAT** ). The router simply forwards incoming packets reaching
    its external interface to the destination instance by checking its configured
    DNAT rule. The traffic response from an instance to external resources uses the
    source IP addresses translated to the floating IP, demonstrated as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的演示将展示托管在外部网络中的资源如何访问OpenStack环境中的实例。默认情况下，启动的实例将被分配一个在租户网络外不可见的IP地址。Neutron提供了浮动IP地址来实现**DNAT**（**目标NAT**）。路由器通过检查其配置的DNAT规则，将到达其外部接口的传入数据包转发到目标实例。实例向外部资源发送的流量响应使用已翻译为浮动IP的源IP地址，演示如下：
- en: 'Extract the port ID of the created instance, and create a floating IP address
    to associate with it:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取创建的实例的端口ID，并创建一个浮动IP地址与其关联：
- en: '[PRE34]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here is the output:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![Figure 6.17 – Listing the instance port](img/B21716_06_17.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 列出实例端口](img/B21716_06_17.jpg)'
- en: Figure 6.17 – Listing the instance port
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 列出实例端口
- en: 'Copy the port ID, and run the following command line by pasting the external
    port ID after the **--** **port** option:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制端口ID，并在**--** **port**选项后粘贴外部端口ID，然后运行以下命令：
- en: '[PRE35]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here is the output:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![Figure 6.18 – Assigning the floating IP address to the external port](img/B21716_06_18.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – 将浮动IP地址分配给外部端口](img/B21716_06_18.jpg)'
- en: Figure 6.18 – Assigning the floating IP address to the external port
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – 将浮动IP地址分配给外部端口
- en: 'Under the hood, the router namespace configures a secondary address associated
    with the external interface, prefixed with **''qg''** :'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在背后，路由器命名空间配置了一个与外部接口关联的辅助地址，前缀为**'qg'**：
- en: '[PRE36]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is the output:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![Figure 6.19 – Associating the IP address with the virtual router external
    interface](img/B21716_06_19.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – 将IP地址与虚拟路由器外部接口关联](img/B21716_06_19.jpg)'
- en: Figure 6.19 – Associating the IP address with the virtual router external interface
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – 将IP地址与虚拟路由器外部接口关联
- en: Traffic routed through the external network provider can reach the instance
    via the assigned floating IP.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过外部网络提供商路由的流量可以通过分配的浮动IP到达实例。
- en: So far, we have explored a standard routing implementation in OpenStack. The
    next section will uncover another way of performing routing, via dynamic routing,
    in Neutron.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了在OpenStack中实现标准路由的方式。接下来的部分将介绍通过动态路由在Neutron中执行路由的另一种方式。
- en: Neutron dynamic routing
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Neutron动态路由
- en: Dynamic routing in OpenStack networking is based on **BGP** ( **Border Gateway
    Protocol** ), enabling tenant networks to advertise their network prefixes with
    physical or virtual routers and network devices that support BGP. The new addition
    of BGP in Neutron eliminates the usage of floating IPs for tenant networks that
    do not rely on network administrators to advertise their network upstream. The
    term *dynamic routing* in Neutron was introduced with the **Mitaka** release.
    The adoption of the BGP routing mechanism varies from one cloud environment to
    another, depending on the networking setup, mostly due to the requirement for
    direct connectivity between the network node and the physical network gateway
    device to peer with (such as a LAN or WAN peer). To avoid IP overlapping when
    advertising IP prefixes, dynamic routing relies on **address scopes** and **subnet
    pools** , Neutron mechanisms that control the allocation of subnet addresses and
    prevent the usage of addresses that overlap.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack网络中的动态路由基于**BGP**（**边界网关协议**），使租户网络能够与支持BGP的物理或虚拟路由器和网络设备广告其网络前缀。Neutron中新增加的BGP功能消除了对于不依赖网络管理员来将网络前缀向上游广告的租户网络而言，使用浮动IP的需求。Neutron中的*动态路由*概念是在**Mitaka**版本中引入的。BGP路由机制的采纳因云环境的不同而有所变化，主要取决于网络设置，尤其是要求网络节点与物理网络网关设备之间具有直接连接（如LAN或WAN对等连接）。为了避免广告IP前缀时的IP重叠，动态路由依赖于**地址范围**和**子网池**，这是Neutron机制，用来控制子网地址的分配并防止重叠地址的使用。
- en: At the heart of the BGP implementation, Neutron introduced the **BGP speaker**
    , which enables peering between tenant networks and external router devices. The
    BGP speaker advertises the tenant network to the tenant router, initially as a
    first hop. The BGP speaker in Neutron is not a router instance, nor does it manipulate
    BGP routes. It mainly orchestrates the BGP peer’s information between the tenant
    routers and external ones. The speaker requires a network or cloud operator to
    configure the peering endpoints. As shown in the following diagram, for a successful
    BGP dynamic routing in OpenStack, the Neutron virtual router must be attached
    to both the tenant subnet and the external provider device interface.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BGP 实现的核心部分，Neutron 引入了 **BGP speaker**，它使租户网络与外部路由器设备之间能够建立对等连接。BGP speaker
    将租户网络广告到租户路由器，最初作为第一跳。Neutron 中的 BGP speaker 不是路由器实例，也不会操作 BGP 路由。它主要负责协调租户路由器与外部路由器之间的
    BGP 对等信息。BGP speaker 需要网络或云操作员配置对等端点。如以下图所示，要在 OpenStack 中成功实现 BGP 动态路由，Neutron
    虚拟路由器必须同时连接到租户子网和外部提供商设备接口。
- en: '![Figure 6.20 – Neutron BGP peering and router connectivity for dynamic routing](img/B21716_06_20.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.20 – Neutron BGP 对等连接与路由器的动态路由连接](img/B21716_06_20.jpg)'
- en: Figure 6.20 – Neutron BGP peering and router connectivity for dynamic routing
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – Neutron BGP 对等连接与路由器的动态路由连接
- en: Both the BGP speaker and the external provider device must be peered (connected
    to the Neutron virtual router). Finally, both the tenant and the external networks
    must be in the same address scope. Adding BGP dynamic routing using **kolla-ansible**
    is straightforward. We will configure the dynamic routing based on the OVS implementation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: BGP speaker 和外部提供商设备必须建立对等连接（连接到 Neutron 虚拟路由器）。最后，租户网络和外部网络必须在相同的地址范围内。使用 **kolla-ansible**
    添加 BGP 动态路由非常简单。我们将基于 OVS 实现配置动态路由。
- en: Important note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Since the Antelope release, Neutron has supported dynamic routing with the OVN
    mechanism driver.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Antelope 版本以来，Neutron 支持使用 OVN 机制驱动程序进行动态路由。
- en: 'As Neutron provides a BGP agent with default configuration, we will just need
    to enable the agent installation in the network node by adding the following line
    to the **/ansible/inventory/multi_packtpub_prod** inventory file:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Neutron 提供了默认配置的 BGP 代理，我们只需要通过将以下行添加到 **/ansible/inventory/multi_packtpub_prod**
    清单文件中，启用网络节点中的代理安装：
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Enable the agent installation in the **globals** **.yml** file, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **globals** **.yml** 文件中启用代理安装，如下所示：
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Launch the pipeline to roll out the BGP agent installation in the network node.
    In the network node, the installed BGP agent can be checked by running the following
    command line:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 启动管道以在网络节点中部署 BGP 代理安装。在网络节点中，可以通过运行以下命令检查已安装的 BGP 代理：
- en: '[PRE39]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here is the output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![Figure 6.21 – Listing the BGP network agent](img/B21716_06_21.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.21 – 列出 BGP 网络代理](img/B21716_06_21.jpg)'
- en: Figure 6.21 – Listing the BGP network agent
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – 列出 BGP 网络代理
- en: The Neutron BGP CLI to manage BGP speakers can be found at [https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html](https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html)
    .
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下链接找到用于管理 BGP speakers 的 Neutron BGP CLI：[https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html](https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/neutron.html)。
- en: Virtual routers are part of the building blocks of OpenStack networking, providing
    a variety of connectivity options to tenants. It is important to note that running
    with a standalone router presents a single point of failure. [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , will discuss Neutron
    implementation for highly available routers. Dynamic routing via BGP in Neutron
    is considered an amazing routing addition in OpenStack. Neutron has been enriched
    and modified across OpenStack releases. One of these major modifications is the
    development of new networking services in OpenStack, which will be discussed in
    the next section.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟路由器是 OpenStack 网络构建模块的一部分，为租户提供多种连接选项。需要注意的是，使用独立路由器运行会存在单点故障的问题。[*第 7 章*](B21716_07.xhtml#_idTextAnchor174)，*运行高可用云
    – 满足 SLA* 将讨论 Neutron 高可用路由器的实现。通过 BGP 在 Neutron 中的动态路由被认为是 OpenStack 中的一项出色的路由补充。Neutron
    在 OpenStack 版本中不断得到丰富和修改。其中一项重大修改是 OpenStack 中新网络服务的开发，下一节将讨论这些服务。
- en: Joining more networking services
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入更多网络服务
- en: Besides the overwhelming routing and switching capabilities in Neutron that
    come with the latest OpenStack releases, Neutron enables cloud operators to offer
    additional networking services. Services such as load balancing, firewalls, and
    private networks support tenant networks to build well-architected application
    stacks for a variety of use cases. In the next section, we will cover the most
    used and stable additional Neutron services and deploy them, using **kolla-ansible**
    .
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Neutron在最新OpenStack版本中提供的强大路由和交换能力外，Neutron还使云操作员能够提供额外的网络服务。诸如负载均衡、防火墙和私有网络等服务，支持租户网络构建适用于多种用例的良好架构的应用堆栈。在接下来的章节中，我们将介绍最常用和最稳定的额外Neutron服务，并使用**kolla-ansible**进行部署。
- en: Load balancer as a service
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡即服务
- en: Since the **Liberty** release, the Neutron **Load Balancer as a Service** (
    **LBaaS** ) plugin has moved from version 1 to version 2, codenamed **Octavia**
    . Cloud users who seek to balance the load of their workloads using a managed
    load balancer will appreciate the Octavia offering. Octavia is designed to scale
    horizontally, as it creates and manages virtual machines acting as load-balancer
    instances, referred to as **amphorae** .
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 自**Liberty**版本发布以来，Neutron **负载均衡即服务**（**LBaaS**）插件已从版本1升级至版本2，代号为**Octavia**。希望通过托管负载均衡器平衡工作负载的云用户，将会欣赏Octavia的功能。Octavia的设计旨在水平扩展，因为它创建并管理作为负载均衡器实例的虚拟机，这些虚拟机被称为**amphorae**。
- en: 'Octavia implements the same load-balancing terms as the popular **HAProxy**
    , which can be summarized as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Octavia实现了与流行的**HAProxy**相同的负载均衡术语，具体如下：
- en: '**Virtual IP (VIP)** : A layer 4 object that exposes a service to be accessed,
    externally or internally, that is associated with a Neutron port. A load balancer
    is assigned a VIP, and requests that reach the VIP are distributed among backend
    servers or pool members.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟IP（VIP）**：一个四层对象，用于暴露服务，供外部或内部访问，并与Neutron端口关联。负载均衡器被分配一个VIP，所有到达VIP的请求都会在后端服务器或池成员之间分发。'
- en: '**Pool** : A group of instances serving the same content or service, such as
    web servers.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池**：提供相同内容或服务的一组实例，例如Web服务器。'
- en: '**Pool members** : An instance of a pool represented by an IP address and listening
    port that exposes the service.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池成员**：通过IP地址和监听端口表示的池实例，暴露服务。'
- en: '**Listeners** : A port associated with the VIP that listens for incoming requests.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监听器**：与VIP相关的端口，用于监听传入的请求。'
- en: '**Health monitor** : Orchestrates the management of pool members based on health
    checks carried out on each member. Failed health checks will discard the members
    from the pool, and traffic will be served by the healthy ones.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健康监控器**：基于对每个成员进行健康检查来协调池成员的管理。健康检查失败时，将从池中丢弃相应成员，流量将由健康成员处理。'
- en: '**Load-balancing algorithms** : **Round robin** , **least connections** , and
    **source IPs** are supported algorithms in Octavia that can be assigned when creating
    a pool.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡算法**：**轮询**、**最少连接数**和**源IP**是Octavia中支持的算法，可以在创建池时分配。'
- en: '**Session persistence** : A mechanism that forces client requests to be served
    by the same backend server. Since the depreciation of LBaaS v1, the load-balancing
    service in Neutron can be configured through drivers. The most common ones are
    HAProxy and Octavia. Other third-party vendor drivers can also be integrated,
    such as F5 and Citrix, and Neutron will manage the orchestration of the API calls
    in the OpenStack environment. A glossary of different components in Octavia is
    as follows:'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**会话持久性**：强制客户端请求由同一后端服务器处理的机制。自LBaaS v1版本废弃以来，Neutron中的负载均衡服务可以通过驱动程序进行配置。最常见的驱动程序是HAProxy和Octavia。还可以集成其他第三方供应商的驱动程序，如F5和Citrix，Neutron将管理OpenStack环境中的API调用编排。Octavia中不同组件的术语表如下：'
- en: '**Amphora** : A virtual machine acting as a load balancer, running on compute
    nodes and preconfigured with load-balancing parameters, such as pools, backend
    members, and health monitors.'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amphora**：作为负载均衡器的虚拟机，运行在计算节点上，并预配置负载均衡参数，如池、后端成员和健康监控器。'
- en: '**Controller worker** : Updates the load-balancer instance (Amphora instance)
    with configuration settings.'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器工作节点**：更新负载均衡实例（Amphora实例）并配置其设置。'
- en: '**API controller** : Interacts with the controller worker for load-balancer
    (Amphora instances configurations and operations for deployment, deletion, and
    monitoring.'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API控制器**：与控制器工作节点交互，用于负载均衡器（Amphora实例）的配置、操作和部署、删除及监控。'
- en: '**Health manager** : Watches the state of each load balancer (Amphora instance)
    and triggers failover events during the unexpected failure of a load balancer.'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健康管理器**：监控每个负载均衡器（Amphora实例）的状态，并在负载均衡器意外故障时触发故障转移事件。'
- en: '**Housekeeping manager** : Removes stale database records and handles the spare
    pool.'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理管理器**：删除过时的数据库记录并处理备用池。'
- en: 'The deployment of Octavia using **kolla-ansible** would require more steps
    than just installing an agent. To install the load-balancing service in OpenStack
    using the Octavia driver, start by assigning the core Octavia components to run
    as part of the cloud controller:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**kolla-ansible**部署Octavia需要的步骤比仅安装代理多。要使用Octavia驱动程序在OpenStack中安装负载均衡服务，首先将核心Octavia组件分配为云控制器的一部分：
- en: '[PRE40]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Enable the Octavia service in the **globals** **.yml** file, as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在**globals** **.yml**文件中启用Octavia服务，如下所示：
- en: '[PRE41]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Make sure that in the **globals** **.yml** file, the **enable_neutron_provider_networks**
    setting is set to **true** . This is because we need the Octavia nodes to communicate
    through the management network. Assign the network interface of the **eth0** management
    network of the cloud controller node:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在**globals** **.yml**文件中，**enable_neutron_provider_networks**设置为**true**。这是因为我们需要Octavia节点通过管理网络进行通信。分配云控制器节点的**eth0**管理网络接口：
- en: '[PRE42]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**kolla-ansible** automates most of the Octavia deployment and registration
    of the associated resources in the OpenStack environment. Octavia interacts with
    several OpenStack services, including Nova and Neutron. Associated resources can
    be customized in the **globals** **.yml** file, such as the amphorae Nova flavor,
    security groups, and the Octavia network and subnets, which can be set by updating
    **octavia_amp_flavor** , **octavia_amp_security_groups** , and the **Octavia management
    network** , respectively. By default, **kolla-ansible** is configured to register
    all dependent Octavia resources automatically. If you want to customize the Octavia
    registration service, make sure you set all required values for each Octavia setting
    in the **globals** **.yml** file. That can be done after changing the following
    setting from **no** to **yes** :'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible**自动化了Octavia的绝大部分部署过程，并在OpenStack环境中注册相关资源。Octavia与多个OpenStack服务交互，包括Nova和Neutron。相关资源可以在**globals**
    **.yml**文件中自定义，例如amphorae的Nova flavor、安全组、Octavia网络和子网，可以通过更新**octavia_amp_flavor**、**octavia_amp_security_groups**和**Octavia管理网络**来设置。默认情况下，**kolla-ansible**被配置为自动注册所有依赖的Octavia资源。如果你想自定义Octavia注册服务，确保在**globals**
    **.yml**文件中为每个Octavia设置提供所有必需的值。可以在将以下设置从**no**更改为**yes**后进行此操作：'
- en: '[PRE43]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Run the deployment pipeline to roll out the Octavia service. The deployment
    run should create the necessary Octavia resources, including certificates, the
    Octavia network, the flavor, the security group, and the SSH key. Make sure that
    **kolla-ansible post-deploy** runs successfully by generating an **octavia-openrc.sh**
    file, loaded as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 运行部署管道以发布Octavia服务。部署过程应该创建必要的Octavia资源，包括证书、Octavia网络、flavor、安全组和SSH密钥。确保**kolla-ansible
    post-deploy**运行成功，通过生成一个**octavia-openrc.sh**文件，并按以下方式加载：
- en: '[PRE44]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Octavia uses images to launch amphora instances. Run the following command
    lines to download a recent amphora image for the Bobcat release, and then add
    it to Glance:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Octavia使用镜像来启动amphora实例。运行以下命令下载Bobcat发布的最新amphora镜像，然后将其添加到Glance中：
- en: '[PRE45]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following steps assume that two web server instances named **instance1**
    and **instance2** have been deployed and are connected to the tenant network used
    in a previous demonstration, **10.10.0.0/24** :'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤假设已部署名为**instance1**和**instance2**的两个Web服务器实例，并且它们已连接到之前演示中使用的租户网络**10.10.0.0/24**：
- en: 'Note two of the created instances:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意创建的两个实例：
- en: '[PRE46]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here is the output:'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![Figure 6.22 – Listing the created instances](img/B21716_06_22.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22 – 列出创建的实例](img/B21716_06_22.jpg)'
- en: Figure 6.22 – Listing the created instances
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – 列出创建的实例
- en: 'To quickly mimic a load-balancing exercise, run the **SimpleHTTPServer** Python
    module in each of the instances. Create a simple **index.html** file to trace
    the load-balancer member pool usage. On **instance1** , run the following:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了快速模拟负载均衡练习，在每个实例中运行**SimpleHTTPServer** Python模块。创建一个简单的**index.html**文件来跟踪负载均衡器成员池的使用情况。在**instance1**上运行以下命令：
- en: '[PRE47]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Run the same command lines by specifying the second instance in the **index.html**
    file:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在**index.html**文件中指定第二个实例，运行相同的命令：
- en: '[PRE48]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Make sure to allow ingress port **80** on the instances by creating and adding
    the following security group and rule, if not already attached:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保通过创建并添加以下安全组和规则来允许实例的入口端口**80**，如果尚未附加的话：
- en: '[PRE49]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Create a load balancer attached to the private subnet, **priv_subnet** . Note
    that the load balancer will be assigned a VIP that will be used to expose the
    backend service and allow clients to access it:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个附加到私有子网**priv_subnet**的负载均衡器。请注意，负载均衡器将被分配一个 VIP，用于暴露后端服务并允许客户端访问：
- en: '[PRE50]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is the output:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.23 – Creating the load balancer](img/B21716_06_23.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.23 – 创建负载均衡器](img/B21716_06_23.jpg)'
- en: Figure 6.23 – Creating the load balancer
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 – 创建负载均衡器
- en: 'Once the load balancer updates its **PROVISIONING_STATUS** state from **PENDING_CREATE**
    to **ACTIVE** and **OPERATING_STATUS** to **ONLINE** , create a TCP listener port
    named **listener80** on port **80** :'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦负载均衡器将其**PROVISIONING_STATUS**状态从**PENDING_CREATE**更新为**ACTIVE**，并且**OPERATING_STATUS**更新为**ONLINE**，则在端口**80**上创建一个名为**listener80**的
    TCP 监听器端口：
- en: '[PRE51]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Create a load-balancing pool named **poolweb** , and specify the load-balancing
    algorithm with **ROUND_ROBIN** :'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为**poolweb**的负载均衡池，并使用**ROUND_ROBIN**指定负载均衡算法：
- en: '[PRE52]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Add a health monitor to the created **poolweb** with the backend servers and
    probe the HTTP service port:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为创建的**poolweb**添加健康监视器，并探测 HTTP 服务端口：
- en: '[PRE53]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Join the backend instances ( **instance1** and **instance2** ) to **poolweb**
    :'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将后端实例（**instance1** 和 **instance2**）加入到**poolweb**：
- en: '[PRE54]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Optionally, check the status of the added backend members and created pool:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，检查已添加的后端成员和创建的池的状态：
- en: '[PRE55]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here is the output:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.24 – Listing the instance members of the load-balancer pool](img/B21716_06_24.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.24 – 列出负载均衡池的实例成员](img/B21716_06_24.jpg)'
- en: Figure 6.24 – Listing the instance members of the load-balancer pool
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24 – 列出负载均衡池的实例成员
- en: 'Test the load balancer by connecting to its VIP. Note that requests will be
    served consecutively by both instances, as the algorithm used for load balancing
    is round robin:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过连接到负载均衡器的 VIP 来测试负载均衡器。请注意，负载均衡使用的算法是轮询法（round robin），请求将被连续服务于两个实例：
- en: '[PRE56]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here is the output:'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 6.25 – Testing the load-balancer pool backend](img/B21716_06_25.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.25 – 测试负载均衡池的后端](img/B21716_06_25.jpg)'
- en: Figure 6.25 – Testing the load-balancer pool backend
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.25 – 测试负载均衡池的后端
- en: Important note
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Accessing the pool web backend from the internet can be done in different ways.
    One way is to attach the load balancer to a public network directly without the
    need to run behind a virtual router, and the load balancer VIP will be assigned
    a public routable IP. The other way is by creating a floating IP, and then associating
    it with the VIP of the load balancer that exists within a subnet behind the virtual
    router. A floating IP cannot be routable through the internet without a router
    that applies IP translation, using the assigned floating IP, to a public one to
    reach external networks.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 从互联网访问池的 Web 后端有多种方式。一种方式是将负载均衡器直接连接到公共网络，而无需运行在虚拟路由器后面，负载均衡器 VIP 将被分配一个公共可路由
    IP。另一种方式是创建一个浮动 IP，然后将其与负载均衡器的 VIP 关联，该 VIP 位于虚拟路由器后面的子网中。没有路由器应用 IP 转换并使用分配的浮动
    IP 映射到公共 IP，浮动 IP 是无法通过互联网路由的，无法访问外部网络。
- en: 'A good practice when creating load balancers in Neutron is to think about security
    first. The latest LBaaS version in OpenStack supports TLS certificate deployment.
    Even better, OpenStack has a dedicated project that handles keying and certificates,
    codenamed *Barbican* . To learn more about Barbican, refer to the official OpenStack
    docs: [https://docs.openstack.org/barbican/latest/](https://docs.openstack.org/barbican/latest/)
    . A load balancer can work in tandem with the Barbican service to handle its TLS
    certificates most securely.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Neutron 中创建负载均衡器时，一个良好的做法是首先考虑安全性。OpenStack 中最新的 LBaaS 版本支持 TLS 证书部署。更好的是，OpenStack
    拥有一个专门的项目来处理密钥和证书，代号为*Barbican*。要了解有关 Barbican 的更多信息，请参考 OpenStack 官方文档：[https://docs.openstack.org/barbican/latest/](https://docs.openstack.org/barbican/latest/)。负载均衡器可以与
    Barbican 服务配合使用，以最安全的方式处理其 TLS 证书。
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: OpenStack networking provides an overwhelming list of features that enable cloud
    operators and architects to implement advanced network topologies and offer cloud
    users and tenants more managed network services. As seen in this chapter, the
    ML2 plugin extends Neutron’s capabilities by unlocking some pertinent features
    that were lacking in older OpenStack releases. OVS was explored as a mechanism
    driver that brings more traffic control based on flow rules. For extended networking
    that requires common VLAN and VXLAN topologies, the chapter demonstrated the support
    of OVS and OVN drivers in Neutron within the latest OpenStack releases, in addition
    to their deployments using **kolla-ansible** . It is worth noting that the direction
    of future networking based on mechanism drivers is almost certainly moving toward
    adopting the OVN mechanism driver, due to its performance in larger production-grade
    and hybrid cloud environments. Another big deal, routing in the OpenStack networking
    world, was discussed. This chapter also revisited a routing implementation using
    OVS and covered BGP dynamic routing. OVN is becoming a popular way to implement
    BGP , and new releases might show more stable integration in that direction. However,
    the chapter did not cover all other extra Neutron services and agents, such as
    **VPN as a service** , **firewall as a service** , and **Designate** for DNS management.
    LBaaS in Neutron has seen more updates with the latest OpenStack releases. With
    the new LBaaS v2, cloud operators can configure their load-balancing service by
    using open source drivers, such as HAProxy or Octavia. Finally, the chapter demonstrated
    the deployment of a load-balancer setup using Octavia, which is becoming a more
    widely adopted LBaaS solution that is considered suitable for large-scale production
    environments.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack网络提供了一个丰富的功能列表，使云运营商和架构师能够实现先进的网络拓扑，并为云用户和租户提供更多的托管网络服务。如本章所述，ML2插件通过解锁一些在旧版本OpenStack中缺失的相关功能，扩展了Neutron的能力。OVS作为机制驱动程序，基于流规则带来了更多的流量控制。对于需要常见VLAN和VXLAN拓扑的扩展网络，本章展示了在最新OpenStack版本中，Neutron对OVS和OVN驱动程序的支持，以及它们使用**kolla-ansible**的部署。值得注意的是，基于机制驱动程序的未来网络方向几乎肯定会朝着采用OVN机制驱动程序发展，因为它在更大规模的生产级别和混合云环境中的性能表现突出。另一个大话题是OpenStack网络中的路由问题，本章还探讨了使用OVS的路由实现，并覆盖了BGP动态路由。OVN正成为实现BGP的流行方式，未来的版本可能会在这一方向上提供更稳定的集成。然而，本章并未涉及所有其他额外的Neutron服务和代理，如**VPN即服务**、**防火墙即服务**和**Designate**（DNS管理）。Neutron中的LBaaS在最新的OpenStack版本中有了更多的更新。通过新的LBaaS
    v2，云运营商可以使用开源驱动程序，如HAProxy或Octavia，配置他们的负载均衡服务。最后，本章展示了使用Octavia部署负载均衡器设置的过程，Octavia已成为一种广泛采用的LBaaS解决方案，适用于大规模生产环境。
- en: In the next chapter, we will enhance our deployed OpenStack environment by tackling
    resiliency and high availability for the OpenStack control and data planes.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过解决OpenStack控制平面和数据平面的弹性与高可用性问题，来增强我们部署的OpenStack环境。
- en: 'Part 2: Operating the OpenStack Cloud Environment'
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：运营OpenStack云环境
- en: This part will spot the operational mission of cloud operators to bring the
    cloud to the next stage. Essential operational excellence pillars will be covered,
    including high availability, monitoring, and logging, to ensure business continuity
    and keep an eye on common best practices on cloud health. This part will conclude
    with an exclusive addition on setting up a continuous infrastructure evaluation
    and resource optimization layouts.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将重点关注云运营商将云环境提升到下一个阶段的运营任务。将涵盖基本的运营卓越支柱，包括高可用性、监控与日志记录，以确保业务连续性，并关注云健康的常见最佳实践。本部分将以关于建立持续的基础设施评估和资源优化布局的独家内容作为结束。
- en: 'This part has the following chapters:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly Available
    Cloud – Meeting the SLA*'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B21716_07.xhtml#_idTextAnchor174)，*运行高可用的云——满足SLA要求*'
- en: '[*Chapter 8*](B21716_08.xhtml#_idTextAnchor188) , *Monitoring and Logging –
    Remediating Proactively*'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B21716_08.xhtml#_idTextAnchor188)，*监控与日志记录——主动修复*'
- en: '[*Chapter 9*](B21716_09.xhtml#_idTextAnchor204) , *Benchmarking the Infrastructure
    – Evaluating Resource Capacity and Optimization*'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B21716_09.xhtml#_idTextAnchor204)，*基础设施基准测试——评估资源容量与优化*'
