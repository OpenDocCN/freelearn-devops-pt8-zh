- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Publishing Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发布应用程序
- en: Running applications on Kubernetes adds resilience to all of an application’s
    components by running its processes as containers. This helps us to provide stability
    and update these components without impacting our users. Although Kubernetes provides
    a lot of resources to simplify the cluster-wide management of the applications,
    we do need to understand how using these resources will affect the way our applications
    are reached by our users.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上运行应用程序通过将其进程作为容器运行，为应用程序的所有组件增加了弹性。这有助于我们提供稳定性，并在不影响用户的情况下更新这些组件。尽管
    Kubernetes 提供了大量资源来简化应用程序的集群管理，但我们确实需要了解使用这些资源会如何影响我们的应用程序被用户访问的方式。
- en: In this chapter, we will learn how to publish our applications to make them
    accessible to our users. This will involve publishing certain Pods or containers
    to provide services, but sometimes, we may also need to debug our applications
    to fix issues that arise.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何发布我们的应用程序，使其可供用户访问。这将涉及将某些 Pods 或容器发布以提供服务，但有时我们也可能需要调试应用程序以解决出现的问题。
- en: By the end of this chapter, we will have learned how **NetworkPolicy resources**
    help us to isolate the workloads deployed in our cluster, and we will have reviewed
    the use of **service mesh** solutions to improve the overall security between
    our applications’ components.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将了解 **NetworkPolicy 资源** 如何帮助我们隔离部署在集群中的工作负载，并回顾使用 **服务网格** 解决方案来提高应用程序组件之间的整体安全性。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding Kubernetes features for publishing applications cluster-wide
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 发布应用程序的集群范围特性
- en: Proxying and forwarding applications for debugging
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为调试代理和转发应用程序
- en: Using the host network namespace for publishing applications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主机网络命名空间发布应用程序
- en: Publishing applications with Kubernetes’ NodePort feature
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 的 NodePort 特性发布应用程序
- en: Providing access to your Services with LoadBalancer Services
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 LoadBalancer 服务提供对服务的访问
- en: Understanding Ingress Controllers
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Ingress 控制器
- en: Improving our applications’ security
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高我们应用程序的安全性
- en: We will start this chapter by reviewing the different options we have with Kubernetes
    out of the box for delivering our applications to our users.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过回顾 Kubernetes 开箱即用的不同选项，开始讲解如何将应用程序交付给用户。
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the labs for this chapter at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter11](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter11),
    where you will find some extended explanations, omitted in the chapter’s content
    to make it easier to follow. The *Code In Action* video for this chapter can be
    found at [https://packt.link/JdOIY](https://packt.link/JdOIY).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter11](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter11)
    找到本章的实验，其中包含一些扩展的解释，这些内容在章节内容中被省略，以便更容易跟随。本章的 *Code In Action* 视频可以在 [https://packt.link/JdOIY](https://packt.link/JdOIY)
    找到。
- en: Understanding Kubernetes features for publishing applications cluster-wide
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 发布应用程序的集群范围特性
- en: Kubernetes is a container orchestrator that allows users to run their applications’
    workloads cluster-wide. We reviewed in [*Chapter 9*](B19845_09.xhtml#_idTextAnchor202),
    *Implementing Architecture Patterns*, the different patterns we can use to deploy
    our applications using different Kubernetes resources. Pods are the minimum deployment
    unit for our applications and have dynamic IP addresses, thus we can’t use them
    for publishing our applications. Dynamism affects the exposure of all the components
    internally and externally – while Kubernetes successfully makes the creation and
    removal of containers simple, the IP addresses used will continuously change.
    Therefore, we need an intermediate component, the Service resource, to manage
    the interaction of any kind of client with the Pods (running on the backend) associated
    with an application component. We can also have Service resources pointing to
    external resources (for example, the `ExternalName` Service type).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个容器编排器，允许用户在整个集群中运行其应用程序的工作负载。 我们在[*第9章*](B19845_09.xhtml#_idTextAnchor202)，*实施架构模式*中回顾了使用不同
    Kubernetes 资源部署应用程序时可以使用的不同模式。 Pod 是我们应用程序的最小部署单元，并且具有动态 IP 地址，因此我们无法将它们用于发布我们的应用程序。
    动态性影响了所有组件的内部和外部暴露 - 虽然 Kubernetes 成功地使容器的创建和删除变得简单，但使用的 IP 地址将持续变化。 因此，我们需要一个中间组件，即服务资源，来管理任何类型客户端与应用程序组件相关的后端运行的
    Pod 的交互。 我们也可以让服务资源指向外部资源（例如，`ExternalName` 服务类型）。
- en: Important note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is crucial to understand that not all an application’s components need to
    be accessible outside of the cluster or even namespace scopes. In this chapter,
    we are going to learn different options and mechanisms for publishing applications
    for access both inside and outside of the Kubernetes cluster. You as a developer
    must know and understand which of the application’s components will act as frontends
    for your application and thus must be accessible and which should act as backends
    and be reachable, and employ the appropriate mechanisms in each case.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '理解一个应用程序的组件并不是所有的组件都需要在集群外或者甚至命名空间范围内可访问是至关重要的。 在本章中，我们将学习发布应用程序以便在 Kubernetes
    集群内外访问的不同选项和机制。 作为开发者，您必须知道并理解应用程序的哪些组件将作为应用程序的前端，因此必须可访问，哪些应该作为后端并且可以访问，并在每种情况下使用适当的机制。 '
- en: We will use Kubernetes Service resources to publish the application’s Pods internally
    or externally as required. We will never connect to Pods’ published ports directly.
    Pods’ ports will be associated with a Service resource using labels. An intermediate
    resource is created to associate Services with Pods, EndpointSlices, and Endpoint
    resources. These resources are created automatically for you when you create a
    Service and the associated Pods are located. The EndpointSlices point to Endpoint
    resources, which are updated when the backend Pods (or external Services) change.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Kubernetes 服务资源根据需要在应用的 Pod 内部或外部发布。我们永远不会直接连接到 Pod 的发布端口。 Pod 的端口将使用标签与服务资源关联。
    创建中间资源以关联服务与 Pod、EndpointSlices 和 Endpoint 资源。 创建这些资源是在创建服务并定位相关的 Pod 时自动进行的。
    EndpointSlices 指向 Endpoint 资源，后者在后端 Pod（或外部服务）更改时进行更新。
- en: 'Let’s see how this works with an example. We will create a Service resource
    before its actual Pods. The following code snippet shows an example of a Service
    resource manifest:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例看看这是如何工作的。 我们将在实际创建其 Pod 之前创建一个服务资源。 以下代码片段显示了一个服务资源清单的示例：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we create the Service resource using the preceding YAML manifest and retrieve
    the created endpoints, we will see which Pods (with their IP addresses) are associated
    as backends. Let’s see the currently associated endpoints:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用上述 YAML 清单创建服务资源并检索创建的端点，我们将看到哪些 Pod（及其 IP 地址）与后端相关联。 让我们查看当前关联的端点：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The list of endpoints is empty because we don’t have any associated backend
    Pod (with the `myapp=test` label). Let’s create a simple Pod with this label using
    `kubectl run`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 端点列表为空，因为我们没有任何关联的后端 Pod（带有 `myapp=test` 标签）。 让我们使用 `kubectl run` 创建一个带有此标签的简单
    Pod：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now review the associated Pods again:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次查看关联的 Pod：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that we didn’t specify any port for the Pod, hence the association may
    be wrong (in fact, the `docker.io/nginx:alpine` image defines port `80` for the
    process). Kubernetes does not verify this information; it just creates the required
    links between resources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有为Pod指定任何端口，因此关联可能是错误的（实际上，`docker.io/nginx:alpine`镜像为进程定义了端口`80`）。Kubernetes不会验证这些信息；它只是创建资源之间所需的链接。
- en: EndpointSlice resources are managed by Kubernetes and are dynamically updated
    whenever a new Pod is created or an old one fails (in fact, the backend Endpoint
    resources change and the update is propagated). If you are having problems with
    a Service not responding but your Pods are running, this is something you may
    need to check.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: EndpointSlice资源由Kubernetes管理，并且会在创建新的Pod或旧的Pod失败时动态更新（实际上，后端Endpoint资源会发生变化，并且更新会传播）。如果你遇到Service没有响应，但Pods却在运行，这可能是你需要检查的内容。
- en: 'This is just an example of creating a simple **ClusterIP** Service, which is
    the default option. We already learned the different Service resource types in
    [*Chapter 9*](B19845_09.xhtml#_idTextAnchor202), *Implementing Architecture Patterns*,
    but it may be important to quickly review those types that allow us to publish
    applications:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是创建一个简单的**ClusterIP** Service的示例，这是默认选项。我们已经在[*第9章*](B19845_09.xhtml#_idTextAnchor202)《实现架构模式》中学习了不同的Service资源类型，但快速回顾一下那些允许我们发布应用程序的类型可能仍然很重要。
- en: '**ClusterIP**: This is the default type, used to publish a Service internally.
    An FQDN is created in Kubernetes’ internal DNS (CoreDNS component) associated
    with the IP address of the Service resource (assigned by the internal IPAM from
    the Services’ pool).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterIP**：这是默认类型，用于在内部发布Service。Kubernetes的内部DNS（CoreDNS组件）会为该Service资源的IP地址创建一个FQDN（由Service池的内部IPAM分配）。'
- en: '**Headless**: These Services don’t have an associated IP address, although
    they also have an FQDN. In this case, all IP addresses of the Pods associated
    with the Endpoint resource will be resolved.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Headless**：这些Services没有关联的IP地址，尽管它们也有FQDN。在这种情况下，所有与Endpoint资源关联的Pods的IP地址都会被解析。'
- en: '`30000`-`32767` port range. It is important to understand that NodePort Services
    have a ClusterIP address, associated via the internal Service’s FQDN.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`30000`-`32767`端口范围。理解NodePort Services具有ClusterIP地址，并通过内部Service的FQDN进行关联，这一点很重要。'
- en: '**LoadBalancer**: This type of Service resource integrates with external cloud
    or on-premise software or hardware load balancers (or it creates them in your
    cloud infrastructure) from the underlying infrastructure to route user traffic
    to an application’s Pods. In this case, a NodePort is created (along with its
    associated ClusterIP) to route the traffic from the external load balancer to
    the backend Endpoint resources. Kubernetes will use its own integration with the
    cloud infrastructure to create the required load balancers or apply specific configurations
    pointing to the associated NodePorts whenever a LoadBalancer Service resource
    is created.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoadBalancer**：这种类型的Service资源与外部云或本地软件或硬件负载均衡器（或它在你的云基础设施中创建负载均衡器）集成，用于将用户流量路由到应用程序的Pods。在这种情况下，会创建一个NodePort（以及其关联的ClusterIP），以将流量从外部负载均衡器路由到后端的Endpoint资源。当创建LoadBalancer
    Service资源时，Kubernetes将使用其与云基础设施的集成来创建所需的负载均衡器，或应用指向关联NodePort的特定配置。'
- en: Important note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: We employ ClusterIP and Headless Services for internal use and NodePort and
    LoadBalancer Services whenever we are going to expose our applications. But this
    is not strictly true, as we can also use **Ingress Controllers** to publish applications
    without using either NodePort or LoadBalancer resources. This helps you to abstract
    your applications from the underlying infrastructure.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用ClusterIP和Headless Services进行内部使用，而在需要公开应用程序时使用NodePort和LoadBalancer Services。但这并不完全准确，因为我们也可以使用**Ingress
    Controllers**来发布应用程序，而不需要使用NodePort或LoadBalancer资源。这有助于将应用程序与底层基础设施进行抽象。
- en: Let’s continue exploring the different options provided by the Kubernetes platform
    for publishing applications by introducing the **Ingress Controller** concept.
    An Ingress Controller is a Kubernetes controller that we can add to our cluster
    to implement reverse proxy functionalities. This will allow us to use ClusterIP
    Service resources to expose our applications because the traffic coming from our
    users will be routed entirely internally from this proxy component to the Service
    and then reach the associated Pods. This proxy is configured dynamically by using
    **Ingress** resources. These resources allow us to define our applications’ host
    headers and link them to our Service resources. Your work as a developer involves
    creating appropriate Ingress resources for your frontend application’s components.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探索 Kubernetes 平台提供的不同选项，通过引入 **Ingress Controller** 概念来发布应用程序。Ingress Controller
    是一个 Kubernetes 控制器，我们可以将其添加到集群中以实现反向代理功能。这将允许我们使用 ClusterIP 服务资源来暴露我们的应用程序，因为来自用户的流量将完全通过这个代理组件内部路由到服务，再到达相关的
    Pods。这个代理通过使用 **Ingress** 资源动态配置。这些资源允许我们定义应用程序的主机头并将其与服务资源链接起来。作为开发人员，你的工作是为前端应用程序的组件创建适当的
    Ingress 资源。
- en: Finally, let’s introduce the Kubernetes `/``api/v1/namespaces/default/pods`
    path.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们介绍 Kubernetes `/api/v1/namespaces/default/pods` 路径。
- en: For debugging purposes, we can also use `kubectl port-forward`, which proxies
    specific Services to our desktop computer client. Note that neither method, `proxy`
    or `port-forward`, should be permitted in production because they directly expose
    important resources, bypassing our Kubernetes and load balancer infrastructure
    security.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 出于调试目的，我们还可以使用 `kubectl port-forward`，它将特定的服务代理到我们的桌面计算机客户端。请注意，在生产环境中不应允许使用这两种方法，`proxy`
    或 `port-forward`，因为它们直接暴露重要资源，绕过了我们的 Kubernetes 和负载均衡器基础设施安全。
- en: In the next section, we will use the `kubectl proxy` feature to access a Service
    resource and reach our application.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用 `kubectl proxy` 功能来访问服务资源并访问我们的应用程序。
- en: Proxying and forwarding applications for debugging
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试时的代理和转发应用程序
- en: In this section, we will learn how to publish the Kubernetes API directly on
    our desktop computer and reach any Service created in the cluster (if we have
    the appropriate permissions), and how to forward a Service directly to our client
    computer using the `port-forward` feature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何直接在桌面计算机上发布 Kubernetes API，并访问集群中创建的任何服务（如果我们有适当的权限），以及如何使用 `port-forward`
    功能将服务直接转发到我们的客户端计算机。
- en: Kubernetes client proxy feature
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 客户端代理功能
- en: 'We use `kubectl proxy` to enable the Kubernetes proxy feature. Some important
    options help us manage how and where the Kubernetes API will be accessible. We
    use the following options to define where the Kubernetes API will be published:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `kubectl proxy` 来启用 Kubernetes 代理功能。一些重要选项帮助我们管理 Kubernetes API 的访问方式和位置。我们使用以下选项来定义
    Kubernetes API 的发布位置：
- en: '`--address`: This option allows us to define the IP address of our client host
    used for publishing the Kubernetes API. By default, `127.0.0.1` is used.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--address`：此选项允许我们定义用于发布 Kubernetes API 的客户端主机 IP 地址。默认情况下使用 `127.0.0.1`。'
- en: '`--port` or `-p`: This option is used to set the specific port where the Kubernetes
    API will be available. The default value is `8001`, and although we can let Kubernetes
    use a random port by using `-p=0`, it is recommended to always define a specific
    port.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--port` 或 `-p`：此选项用于设置 Kubernetes API 可用的具体端口。默认值是 `8001`，虽然我们可以通过使用 `-p=0`
    让 Kubernetes 使用随机端口，但建议始终定义一个特定端口。'
- en: '`--unix-socket` or `-u`: This option is used to define a Unix socket instead
    of a TCP port, which is more secure if you limit access to the socket at the filesystem
    level.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--unix-socket` 或 `-u`：此选项用于定义 Unix 套接字而非 TCP 端口，这在限制文件系统级别的套接字访问时更为安全。'
- en: 'The following options are used to secure the Kubernetes API access:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下选项用于保护 Kubernetes API 访问：
- en: '`--accept-hosts` and `--accept-paths`: These options allow us to ensure that
    only specific host headers and API paths will be allowed. For example, we can
    ensure local access only using the following regex pattern, `''^localhost$,^127\.0\.0\.1$,^\[::1\]$''`,
    with the `--``accept-hosts` argument.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--accept-hosts` 和 `--accept-paths`：这些选项允许我们确保只有特定的主机头和 API 路径会被允许。例如，我们可以使用以下正则表达式模式
    `''^localhost$,^127\.0\.0\.1$,^\[::1\]$''` 来确保仅允许本地访问，并配合 `--accept-hosts` 参数使用。'
- en: '`--reject-methods`: We can block specific API methods by rejecting them. For
    example, we can disable the patching of any Kubernetes resource by using `kubectl`
    `proxy --reject-methods=''PATCH''`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--reject-methods`：我们可以通过拒绝特定的API方法来阻止它们。例如，我们可以通过使用`kubectl` `proxy --reject-methods=''PATCH''`来禁用对任何Kubernetes资源的修补。'
- en: '`--reject-paths`: We can specify certain paths to be denied by using this option.
    We can, for example, disable the attachment of a new process to a Pod resource
    (`kubectl exec` equivalent) by using `–-reject-paths=''^/api/.*/pods/.*/exec,''`.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--reject-paths`：我们可以使用此选项指定某些路径被拒绝。例如，我们可以通过使用`–-reject-paths=''^/api/.*/pods/.*/exec,''`来禁用向Pod资源附加新进程（相当于`kubectl
    exec`）。'
- en: It is important to understand that, although we have seen some options for ensuring
    security, the Kubernetes proxy feature shouldn’t be used in production environments
    because it may be possible to bypass the RBAC system if someone gets access to
    the API via the proxied port. The user authentication used for creating the proxy
    will allow access to anyone via the exposed API.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的是，尽管我们已经看到了一些确保安全的选项，但Kubernetes代理功能不应在生产环境中使用，因为如果有人通过代理端口访问API，可能绕过RBAC系统。用于创建代理的用户身份验证将允许任何人通过暴露的API访问。
- en: This method should only be used for debugging in either your own Docker Desktop,
    Rancher Desktop, or Minikube for exposing a Kubernetes remote development environment.
    Your Kubernetes administrators must enable this method for you if you are not
    using your own Kubernetes environment. You must ensure that your operating system
    allows access to the specified port by reviewing your firewall settings if you
    still aren’t able to reach the proxied Kubernetes API.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法仅应在自己的Docker Desktop、Rancher Desktop或Minikube中用于调试，暴露Kubernetes远程开发环境。如果你不是在使用自己的Kubernetes环境，你的Kubernetes管理员必须为你启用此方法。如果你仍然无法访问代理的Kubernetes
    API，必须通过查看防火墙设置来确保操作系统允许访问指定端口。
- en: 'Now we have reviewed how we can employ this method to publish Kubernetes APIs,
    let’s use it to access a created Service resource with a quick example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了如何使用此方法发布Kubernetes API，接下来让我们通过一个快速示例来访问已创建的服务资源：
- en: '![Figure 11.1 – Creating a simple webserver Service with NGINX and exposing
    the Kubernetes API](img/B19845_11_1.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 使用NGINX创建简单的webserver服务并暴露Kubernetes API](img/B19845_11_1.jpg)'
- en: Figure 11.1 – Creating a simple webserver Service with NGINX and exposing the
    Kubernetes API
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 使用NGINX创建简单的webserver服务并暴露Kubernetes API
- en: Important note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Notice that we executed `kubectl proxy` in the background using `&`. We did
    this to be able to continue in the current terminal. The `kubectl proxy` action
    runs in the foreground, and it will keep running until we issue *Ctrl* + *C* to
    terminate the process. To end the background execution, we can use the following
    steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`&`在后台执行了`kubectl proxy`。这样做是为了能够继续在当前终端中操作。`kubectl proxy`操作会在前台运行，并且会一直运行，直到我们按下*Ctrl*
    + *C*终止该进程。要结束后台执行，我们可以使用以下步骤：
- en: '`$` `jobs`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`$` `jobs`'
- en: '`[1]+ Running kubectl` `proxy &`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`[1]+ 运行 kubectl` `proxy &`'
- en: '`$` `kill %1`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`$` `kill %1`'
- en: 'Now that we have access to Kubernetes API, we can access the ClusterIP Service
    resource directly using the proxied port, but first, let’s review the Service
    resource:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以访问Kubernetes API，我们可以直接通过代理端口访问ClusterIP服务资源，但首先让我们回顾一下服务资源：
- en: '![Figure 11.2 – Accessing the webserver Service resource using the Kubernetes
    proxy](img/B19845_11_2.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 使用Kubernetes代理访问webserver服务资源](img/B19845_11_2.jpg)'
- en: Figure 11.2 – Accessing the webserver Service resource using the Kubernetes
    proxy
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 使用Kubernetes代理访问webserver服务资源
- en: 'We configured port `8080` for the `webserver` Service resource. The Kubernetes
    proxy will publish the Service resources using the following URI format (Kubernetes
    API):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`webserver`服务资源配置了端口`8080`。Kubernetes代理将使用以下URI格式发布服务资源（Kubernetes API）：
- en: '`/``api/v1/namespaces/<NAMESPACE>/services/<SERVICE_NAME>:<SERVICE_PORT>/proxy/`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`/``api/v1/namespaces/<NAMESPACE>/services/<SERVICE_NAME>:<SERVICE_PORT>/proxy/`'
- en: 'Therefore, the `webserver` Service is accessible in `/api/v1/namespaces/default/services/webserver:8080/proxy/`,
    and we can reach NGINX’s default `index.xhtml` page, as we can see in the following
    screenshot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`webserver`服务可以通过`/api/v1/namespaces/default/services/webserver:8080/proxy/`访问，并且我们可以访问NGINX的默认`index.xhtml`页面，如下图所示：
- en: '![Figure 11.3 – Accessing the webserver Service using the kubectl proxy feature](img/B19845_11_3.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 使用kubectl代理功能访问webserver服务](img/B19845_11_3.jpg)'
- en: Figure 11.3 – Accessing the webserver Service using the kubectl proxy feature
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 使用 kubectl proxy 功能访问 webserver 服务
- en: The Service is accessible and we reached the `webserver` Service’s default page.
    Let’s now review how we can forward the Service’s port to our desktop computer
    without having to implement a complex routing infrastructure.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务是可以访问的，我们已经成功到达了`webserver`服务的默认页面。现在，让我们来看看如何将服务的端口转发到我们的桌面计算机，而不需要实现复杂的路由基础设施。
- en: Kubernetes client port-forward feature
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 客户端端口转发功能
- en: Instead of accessing the full Kubernetes API, we can use `kubectl port-forward`
    to forward ports from a Service, Deployment, ReplicaSet, StatefulSet, or even
    a Pod resource directly. In this case, a transparent NAT is used to forward a
    backend port to a port defined on our desktop computer by executing the `kubectl`
    command-line client.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `kubectl port-forward` 来将服务、部署、ReplicaSet、StatefulSet，甚至是 Pod 资源的端口直接转发，而无需访问整个
    Kubernetes API。在这种情况下，使用透明的 NAT 将后端端口转发到我们桌面计算机上定义的端口，通过执行 `kubectl` 命令行客户端来实现。
- en: 'Let’s see how this works, using the `webserver` Service defined in the previous
    section as an example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下这如何工作，使用上一节中定义的 `webserver` 服务作为示例：
- en: '![Figure 11.4 – Using port-forward to publish the webserver Service resource
    example](img/B19845_11_4.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – 使用端口转发发布 webserver 服务资源示例](img/B19845_11_4.jpg)'
- en: Figure 11.4 – Using port-forward to publish the webserver Service resource example
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 使用端口转发发布 webserver 服务资源示例
- en: As you can see in this example, it is quite simple to forward any application’s
    Kubernetes resource listening on a defined port. We can specify the port attached
    to our application in our local client by using `[LOCAL_PORT:]RESOURCE_PORT`.
    Note that it is important to choose the local IP address when working on a multihomed
    host with multiple IP addresses using the `--address` argument. This will improve
    the overall security by attaching an interface if we define the appropriate firewall
    rules to only allow our host. By default, `localhost` is used, which means that
    it will remain secure as long as we are the only user with access to our desktop
    computer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所示，转发任何监听指定端口的 Kubernetes 资源是相当简单的。我们可以通过使用 `[LOCAL_PORT:]RESOURCE_PORT`
    来指定应用程序附加的本地端口。请注意，当在具有多个 IP 地址的多网卡主机上工作时，使用 `--address` 参数选择本地 IP 地址非常重要。这将通过附加接口并定义适当的防火墙规则，仅允许我们的主机来提高整体安全性。默认情况下，使用
    `localhost`，这意味着只要我们是唯一能够访问桌面计算机的用户，它将保持安全。
- en: In the next section, we will discuss the direct use of the host’s kernel network
    namespace for publishing Pod resources.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何直接使用主机的内核网络命名空间来发布 Pod 资源。
- en: Using the host network namespace for publishing applications
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主机网络命名空间发布应用程序
- en: So far, we have seen different methods for accessing ClusterIP Service resources
    or Pods (created using different workload types) by either proxying or forwarding
    their ports to our desktop computers. Sometimes, however, the applications require
    a direct connection to the host’s interfaces, without the bridge interface created
    by the container runtime. In this case, the containers in the Pod will use the
    network namespace of the host, which allows the processes inside to control the
    host because they will have access to all the host’s interfaces and network traffic.
    This can be dangerous and must only be used to manage and monitor the host’s interfaces.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了通过代理或将端口转发到桌面计算机的不同方法，来访问 ClusterIP 服务资源或 Pods（使用不同工作负载类型创建）。然而，有时应用程序需要直接连接到主机的接口，而不是通过容器运行时创建的桥接接口。在这种情况下，Pod
    中的容器将使用主机的网络命名空间，这使得容器内部的进程能够控制主机，因为它们可以访问主机的所有接口和网络流量。这可能是危险的，必须仅用于管理和监控主机的接口。
- en: Using the hostNetwork key
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 hostNetwork 键
- en: 'To use the host’s network namespace, we set the `hostNetwork` key to `true`.
    The Pod will now get all the IP addresses associated with the host, including
    those of all the virtual interfaces associated with the containers running in
    that host. But what is particularly important in terms of publishing our applications
    is that they will be accessible through any of the host’s IP addresses, waiting
    for requests on the ports defined by the `ports` keys in the Pod `spec` section.
    Let’s see how this works by executing an NGINX Pod with the aforementioned `hostNetwork`
    key. We will use `cat` (redirected to `kubectl`) to create a Pod resource on the
    fly using the `nginx/nginx-unprivileged:stable-alpine3.18` image (which uses the
    unprivileged port `8080`):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用主机的网络命名空间，我们将`hostNetwork`键设置为`true`。Pod现在将获得与主机相关的所有IP地址，包括与该主机中运行的容器相关的所有虚拟接口的IP地址。但在发布应用程序时，特别重要的一点是，它们将可以通过主机的任何IP地址进行访问，并在Pod的`spec`部分定义的`ports`键所定义的端口上等待请求。让我们通过执行一个带有前述`hostNetwork`键的NGINX
    Pod来看一下这如何运作。我们将使用`cat`（重定向到`kubectl`）来快速创建一个Pod资源，使用`nginx/nginx-unprivileged:stable-alpine3.18`镜像（该镜像使用无特权端口`8080`）：
- en: '![Figure 11.5 – Exposing a Pod using hostNetwork](img/B19845_11_5.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 使用hostNetwork暴露Pod](img/B19845_11_5.jpg)'
- en: Figure 11.5 – Exposing a Pod using hostNetwork
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 使用hostNetwork暴露Pod
- en: 'This way, your NGINX web server will be accessible in the host’s IP address
    where it is running (in this example, on the IP address `192.168.65.4`, which
    is the address of our Docker Desktop worker and master host). The following code
    snippet shows the creation of the `webserver` application using the host’s interfaces,
    and how we get the content of the NGINX process:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，您的NGINX web服务器将在其运行的主机的IP地址上可访问（在这个例子中是IP地址`192.168.65.4`，这是我们的Docker
    Desktop工作节点和主节点的地址）。以下代码片段展示了如何使用主机的接口创建`webserver`应用程序，以及如何获取NGINX进程的内容：
- en: '![Figure 11.6 – Accessing the webserver application using the host’s IP address](img/B19845_11_6.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 使用主机的IP地址访问webserver应用程序](img/B19845_11_6.jpg)'
- en: Figure 11.6 – Accessing the webserver application using the host’s IP address
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 使用主机的IP地址访问webserver应用程序
- en: Notice that we executed the `curl` binary inside the `webserver` Pod. In this
    example, we are using Docker Desktop with `frjaraur/nettools` image (developed
    and maintained by me), to verify that the application is accessible.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在`webserver` Pod内部执行了`curl`二进制文件。在这个示例中，我们使用的是Docker Desktop与`frjaraur/nettools`镜像（由我开发和维护），来验证应用程序是否可访问。
- en: In this case, we are using just one port on our Pod; in fact, we didn’t even
    declare the ports on our Pod’s container, hence all the ports defined in the container
    image will be used. Using `hostNetwork`, all the ports defined in the image will
    be exposed, which may be a problem if you don’t want to expose some specific Pods
    externally (for example, if your application has an internal API or administration
    interface that you will not be able to access). If you manage the platform yourself,
    you can manage access by modifying the host’s firewall, but this can be tricky.
    In such situations, we can use the `hostPort` key at container level, instead
    of using `hostNetwork` at the Pod resource level. Let’s explore this in the next
    section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们只使用了Pod中的一个端口；事实上，我们甚至没有在Pod的容器中声明端口，因此容器镜像中定义的所有端口都会被使用。使用`hostNetwork`时，镜像中定义的所有端口都会被暴露，如果你不想将某些特定的Pod暴露到外部（例如，如果你的应用程序有一个内部API或管理界面，而你不希望能够外部访问），这可能会成为问题。如果你自己管理平台，可以通过修改主机的防火墙来管理访问，但这可能会比较棘手。在这种情况下，我们可以在容器级别使用`hostPort`键，而不是在Pod资源级别使用`hostNetwork`。我们将在下一节中探讨这个问题。
- en: Using hostPort
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`hostPort`
- en: 'The `hostPort` key is used inside the `containers` section of the Pod, where
    we define the ports to be exposed either internally or externally. With `hostPort`,
    we can expose only those ports that are required, while the remainder can stay
    internal. Let’s see an example involving defining two containers within the `webserver`
    Pod:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostPort`键用于Pod的`containers`部分，在这里我们定义要暴露的端口，可以是内部端口或外部端口。通过`hostPort`，我们只能暴露需要的端口，其余端口可以保持内部访问。让我们看一个示例，定义`webserver`
    Pod中的两个容器：'
- en: '![Figure 11.7 – Example with two containers but only one exposed at the host
    level](img/B19845_11_7.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – 一个示例，包含两个容器，但仅有一个在主机级别暴露](img/B19845_11_7.jpg)'
- en: Figure 11.7 – Example with two containers but only one exposed at the host level
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 一个示例，包含两个容器，但仅有一个在主机级别暴露
- en: 'In the preceding screenshot, we have two containers. Let’s verify whether they
    are reachable using the `frjaraur/nettools` image again, trying to access both
    ports via `curl`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们有两个容器。让我们再次使用 `frjaraur/nettools` 镜像，尝试通过 `curl` 访问这两个端口，来验证它们是否可达：
- en: '![Figure 11.8 – Access to the webserver Service’s ports 8080 and 80](img/B19845_11_8.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – 访问 web 服务器服务的端口 8080 和 80](img/B19845_11_8.jpg)'
- en: Figure 11.8 – Access to the webserver Service’s ports 8080 and 80
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 访问 web 服务器服务的端口 8080 和 80
- en: In the preceding screenshot, we can see that only port `8080` is accessible
    on the host’s IP address. Port `80` is accessible locally within the Kubernetes
    cluster.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到，只有端口 `8080` 可以通过主机的 IP 地址访问。端口 `80` 仅在 Kubernetes 集群内本地可达。
- en: Neither `hostNetwork` nor `hostPort` should be used without a Kubernetes administrator’s
    supervision. Both represent a security breach and should be avoided unless strictly
    necessary for our application. They are commonly used for monitoring or administrative
    workloads when we need to manage or monitor the hosts’ IP addresses.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostNetwork` 和 `hostPort` 都不应在没有 Kubernetes 管理员监督的情况下使用。两者都代表了安全漏洞，除非应用程序严格需要，否则应避免使用。它们通常用于监控或管理工作负载，当我们需要管理或监控主机的
    IP 地址时使用。'
- en: Now that we have learned the different options we have at the host level, let’s
    continue reviewing the NodePort mechanism associated with Service resources.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了在主机级别的不同选项，让我们继续回顾与 Service 资源相关的 NodePort 机制。
- en: Publishing applications with Kubernetes’ NodePort feature
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 的 NodePort 功能发布应用程序
- en: 'As we mentioned at the beginning of this chapter, in the *Understanding the
    Kubernetes features for publishing applications cluster-wide* section, every NodePort
    Service resource has an associated ClusterIP IP address. This IP address is used
    to internally load balance all the client requests (from the Kubernetes cluster,
    internal, and external clients). Kubernetes provides this internal load to all
    available Pod replicas. All replicas will have the same weight, hence they will
    receive the same amount of requests. The ClusterIP IP address makes the applications
    running within Pods accessible internally. To make them available externally,
    the NodePort Service type attaches the defined port on all cluster nodes using
    NAT. The following schema represents the route taken by a request to an application
    running inside a Kubernetes cluster:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头提到的，在*理解 Kubernetes 特性以发布集群范围的应用程序*部分中，每个 NodePort 服务资源都有一个相关联的 ClusterIP
    地址。这个 IP 地址用于内部负载均衡所有客户端请求（来自 Kubernetes 集群的内部和外部客户端）。Kubernetes 将这个内部负载分配给所有可用的
    Pod 副本。所有副本具有相同的权重，因此它们将接收相同数量的请求。ClusterIP 地址使得运行在 Pod 内的应用程序能够在内部访问。为了让它们在外部可用，NodePort
    服务类型通过 NAT 在所有集群节点上附加定义的端口。以下架构表示请求到达 Kubernetes 集群内运行的应用程序的路径：
- en: '![Figure 11.9 – NodePort simplified communications schema](img/B19845_11_9.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – NodePort 简化的通信架构](img/B19845_11_9.jpg)'
- en: Figure 11.9 – NodePort simplified communications schema
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – NodePort 简化的通信架构
- en: 'The Endpoint resource is used to map the Pods’ backends with the Service’s
    ClusterIP. This resource is dynamically configured using label selectors in the
    Service’s YAML manifest. Here is a simple example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoint 资源用于将 Pod 的后端与服务的 ClusterIP 进行映射。该资源使用 Service YAML 清单中的标签选择器动态配置。以下是一个简单示例：
- en: '![Figure 11.10 – Simple Pod and NodePort YAML manifests](img/B19845_11_10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – 简单的 Pod 和 NodePort YAML 清单](img/B19845_11_10.jpg)'
- en: Figure 11.10 – Simple Pod and NodePort YAML manifests
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – 简单的 Pod 和 NodePort YAML 清单
- en: The preceding screenshot shows the most common Service resource usage. With
    this manifest, an EndpointSlice resource will be created, associating the application’s
    Pods with the Service by using the labels defined in the selector section. Notice
    that using these label selectors will create these EndpointSlice resources pointing
    to backend Pod resources running in the same namespace. But we can create Service
    resources without dynamic Pods attachment. This scenario could be useful, for
    example, to link external Services running outside of Kubernetes with an internal
    Service resource (this is how the `ExternalName` Service resource type works),
    or to access a Service from another namespace as if it were deployed on your current
    namespace. The internal Pods are made accessible thanks to the kube-proxy component,
    which will inject the traffic to the Pod’s containers. This only happens in those
    nodes where the actual Pods are running, although the Service is accessible cluster-wide.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图展示了最常见的服务资源使用方式。通过这个清单，会创建一个EndpointSlice资源，利用选择器部分定义的标签，将应用程序的Pod与服务关联。注意，使用这些标签选择器会创建指向在同一命名空间中运行的后端Pod资源的EndpointSlice资源。但是，我们也可以创建没有动态Pod附加的服务资源。例如，这种场景可能有用，例如将运行在Kubernetes外部的外部服务与内部服务资源连接起来（这就是`ExternalName`服务资源类型的工作方式），或者从另一个命名空间访问服务，就好像它在当前命名空间上部署一样。由于kube-proxy组件，内部Pod得以被访问，它会将流量注入到Pod的容器中。尽管服务可以在集群范围内访问，但这只会发生在实际Pod运行的节点上。
- en: EndpointSlice resources using label selectors will create Endpoint resources,
    and thus, their status updates are propagated. Failed Pod resources will be deprecated
    from the actual Service and requests will not be routed to those backends, hence
    Kubernetes will only route to healthy Pods. This is the most popular and recommended
    method for using Service resources because this way, your resources are infrastructure
    agnostic.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标签选择器的EndpointSlice资源将创建Endpoint资源，因此，它们的状态更新会传播。失败的Pod资源将从实际服务中弃用，且请求不会被路由到这些后端，因此Kubernetes只会将流量路由到健康的Pod。这是最流行和推荐的使用服务资源的方式，因为通过这种方式，您的资源与基础设施无关。
- en: 'Let’s see a quick example of how Endpoint resource creation works by creating
    a `webserver` Pod and publishing the web process in NodePort mode by using `kubectl
    expose`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建一个`webserver` Pod并使用`kubectl expose`以NodePort模式发布Web服务，快速查看Endpoint资源创建的工作原理：
- en: '![Figure 11.11 – Exposing a Pod using the imperative format](img/B19845_11_11.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – 使用命令式格式暴露Pod](img/B19845_11_11.jpg)'
- en: Figure 11.11 – Exposing a Pod using the imperative format
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – 使用命令式格式暴露Pod
- en: In the preceding example, we created a Pod and then exposed it by using `30000`-`32767`
    range to the port of the Service resource. We also retrieved a list of endpoints
    and the dynamic configurations created. We used the `kubectl expose <WORKLOAD_TYPE>
    <WORKLOAD_NAME>` format syntax to create the Service. This uses label selectors
    for the creation of the Service resource, taking the labels from the actual workload,
    hence an EndpointSlice resource was created to attach the available Pods to the
    Service.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们创建了一个Pod，然后通过使用`30000`到`32767`范围将其暴露给服务资源的端口。我们还检索了一个端点列表和创建的动态配置。我们使用`kubectl
    expose <WORKLOAD_TYPE> <WORKLOAD_NAME>`格式语法创建了服务。这使用标签选择器来创建服务资源，从实际工作负载中获取标签，因此创建了一个EndpointSlice资源，将可用的Pod附加到服务。
- en: In this example, the `webserver` application will be accessible using the `docker-desktop`
    node IP address, which may require additional configuration if you use WSL2 for
    execution. This is because in this infrastructure we will need to declare an NAT
    IP address to forward to your desktop computer. This will not be required if Hyper-V
    or Minikube are used as a Kubernetes environment on your PC. In a remote Kubernetes
    cluster, you must ensure that the IP addresses of the hosts and the ports are
    reachable from your computer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`webserver`应用程序将通过`docker-desktop`节点的IP地址进行访问，如果你使用WSL2进行执行，可能需要额外的配置。这是因为在这个基础架构中，我们需要声明一个NAT
    IP地址来转发到你的桌面计算机。如果使用Hyper-V或Minikube作为PC上的Kubernetes环境，则不需要这种配置。在远程Kubernetes集群中，您必须确保主机的IP地址和端口可以从您的计算机访问。
- en: Important note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Because the NodePort Services use the host’s ports, these ports must be allowed
    in each node’s firewall. Your Kubernetes administrator may have configured multiple
    interfaces on your Kubernetes platform nodes and should inform you about which
    IP addresses to use to make your applications accessible.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 NodePort 服务使用主机的端口，这些端口必须在每个节点的防火墙中允许。你的 Kubernetes 管理员可能已在 Kubernetes 平台节点上配置了多个接口，并应告知你使用哪些
    IP 地址使应用程序可访问。
- en: If your workloads run on a cloud infrastructure, additional steps may be required
    to allow access to your Service resources, and thus this is often not a good option
    for publishing your applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的工作负载运行在云基础设施上，可能需要额外的步骤来允许访问你的服务资源，因此这通常不是发布应用程序的最佳选择。
- en: In the next section, we will review the LoadBalancer Service type, which was
    created specifically for cloud environments but is now also available for on-premises
    infrastructure thanks to software load balancers such as MetalLB.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾 LoadBalancer 服务类型，该类型是专门为云环境创建的，但现在也可以在本地基础设施中使用，这得益于像 MetalLB 这样的软件负载均衡器。
- en: Providing access to your Services with LoadBalancer Services
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LoadBalancer 服务提供对你的服务的访问
- en: A LoadBalancer-type Service requires an external device to integrate your application.
    This type of Service resource includes a NodePort and its ClusterIP IP address.
    The external device provides a LoadBalancer IP address that will be load-balanced
    to the IP addresses of the cluster nodes and associated NodePorts. This configuration
    is completely managed for you by Kubernetes, but you must define an appropriate
    `spec` section for your infrastructure. This type of resource depends on the actual
    infrastructure because it will use the APIs from software-defined networking infrastructure
    to route and publish the applications’ Services. Loadbalancer Service resources
    were prepared primarily for Kubernetes cloud platforms but are now more commonly
    encountered in modern local data centers with software-defined networks and API-managed
    devices, although they require a good knowledge of the underlying platform to
    work. As mentioned before, each LoadBalancer Service resource is assigned an IP
    address dynamically, which may require additional management on your cloud infrastructure
    and even additional costs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer 类型的服务需要外部设备来集成你的应用程序。这种类型的服务资源包括一个 NodePort 和其 ClusterIP IP 地址。外部设备提供一个
    LoadBalancer IP 地址，该地址将被负载均衡到集群节点的 IP 地址和关联的 NodePort。这个配置完全由 Kubernetes 为你管理，但你必须为你的基础设施定义一个合适的
    `spec` 部分。这种类型的资源依赖于实际的基础设施，因为它将使用来自软件定义网络基础设施的 API 来路由和发布应用程序的服务。LoadBalancer
    服务资源最初是为 Kubernetes 云平台准备的，但现在在现代本地数据中心中更常见，尤其是那些具有软件定义网络和 API 管理设备的环境，尽管它们需要对底层平台有较好的了解才能使用。如前所述，每个
    LoadBalancer 服务资源都会动态分配一个 IP 地址，这可能需要在你的云基础设施上进行额外的管理，甚至可能会产生额外的费用。
- en: The cloud provider decides how the Service is to be load-balanced. Depending
    on the cloud platform used, the NodePort part can sometimes be omitted as direct
    routing may be available if defined by the platform vendor.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 云提供商决定如何对服务进行负载均衡。根据使用的云平台，NodePort 部分有时可以省略，因为如果平台供应商定义了，可能会提供直接路由。
- en: On-premises virtual cloud infrastructures such as OpenStack can be integrated
    into our Kubernetes platforms to manage this type of Service resource because
    they are also part of the Kubernetes core. But if you are not using OpenStack
    or any other on-premise virtual cloud infrastructure, there are solutions such
    as MetalLB ([https://metallb.org/](https://metallb.org/)) that make it possible
    to run a Kubernetes-compatible and dynamically configurable load balancer on any
    bare-metal infrastructure.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本地虚拟云基础设施，如 OpenStack，可以集成到我们的 Kubernetes 平台中，以管理这种类型的服务资源，因为它们也是 Kubernetes
    核心的一部分。但如果你不使用 OpenStack 或其他本地虚拟云基础设施，可以通过像 MetalLB ([https://metallb.org/](https://metallb.org/))
    这样的解决方案，在任何裸金属基础设施上运行一个 Kubernetes 兼容且动态可配置的负载均衡器。
- en: This type of Service resource is not recommended if you are looking for maximum
    compatibility and want to avoid vendor-specific resources. It really has a lot
    of dependencies on the underlying infrastructure and may require additional configurations
    to be done on the platform.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望获得最大的兼容性并避免使用特定厂商的资源，则不推荐使用这种类型的服务资源。它确实依赖于底层基础设施，并可能需要在平台上进行额外的配置。
- en: If you as a developer have to implement a Service of type LoadBalancer (or you’re
    simply curious about their definition), you can use Minikube as it implements
    this functionality on your desktop computer without any external requirements
    to negotiate. Docker Desktop will report the LoadBalancer IP address as `localhost`,
    hence you will be able to connect to the given Services directly using the `127.0.0.1`
    IP address.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你作为开发人员需要实现一个类型为 LoadBalancer 的服务（或者你只是对它们的定义感到好奇），可以使用 Minikube，因为它能够在桌面计算机上实现此功能，而无需任何外部要求来进行协商。Docker
    Desktop 会将 LoadBalancer 的 IP 地址报告为 `localhost`，因此你可以直接使用 `127.0.0.1` IP 地址连接到给定的服务。
- en: 'Let’s see how this works with a simple example. We will first start a new Minikube
    cluster environment (ensure your Docker Desktop or Rancher Desktop instances are
    stopped before starting Minikube), and then we will create a `minikube start`
    and `minikube` `tunnel` commands:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来看这个是如何工作的。我们将首先启动一个新的 Minikube 集群环境（确保在启动 Minikube 之前停止 Docker
    Desktop 或 Rancher Desktop 实例），然后我们将执行 `minikube start` 和 `minikube tunnel` 命令：
- en: '![Figure 11.12 – Execution of a Minikube cluster from an administrator console](img/B19845_11_12.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.12 – 从管理员控制台执行 Minikube 集群](img/B19845_11_12.jpg)'
- en: Figure 11.12 – Execution of a Minikube cluster from an administrator console
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 – 从管理员控制台执行 Minikube 集群
- en: 'We will open another console, but this time we will connect to the Kubernetes
    cluster, so we don’t need to execute the commands as an administrator. We create
    a Pod and then expose it using imperative mode with the LoadBalancer type:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将打开另一个控制台，但这次我们将连接到 Kubernetes 集群，因此不需要以管理员身份执行命令。我们创建一个 Pod，然后使用命令式模式暴露它，类型为
    LoadBalancer。
- en: '![Figure 11.13 – Creating a LoadBalancer Service in Minikube](img/B19845_11_13.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.13 – 在 Minikube 中创建 LoadBalancer 服务](img/B19845_11_13.jpg)'
- en: Figure 11.13 – Creating a LoadBalancer Service in Minikube
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 – 在 Minikube 中创建 LoadBalancer 服务
- en: Notice that we have a new column with the external IP addresses. In this case,
    it is an emulation of a real external load balancer device that provides a specific
    IP address for the new Service manifest. Minikube, in fact, creates a tunnel from
    the Kubernetes node to your desktop computer, making the Pod accessible over the
    assigned load-balanced IP address `(EXTERNAL-IP)` and the Service’s port. In this
    case, we will reach the NGINX web server at `http://10.98.19.87:8080`. We then
    test the accessibility of the application with `curl` (which is an alias for Windows
    PowerShell’s `Invoke-WebRequest` command).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有一个新的列显示外部 IP 地址。在这种情况下，它是一个真实外部负载均衡器设备的仿真，为新的服务清单提供了特定的 IP 地址。实际上，Minikube
    会从 Kubernetes 节点创建一个通道到你的桌面计算机，使得 Pod 可以通过分配的负载均衡 IP 地址 `(EXTERNAL-IP)` 和服务端口进行访问。在这种情况下，我们可以通过
    `http://10.98.19.87:8080` 访问 NGINX Web 服务器。然后，我们使用 `curl` 测试应用程序的可访问性（`curl` 是
    Windows PowerShell 的 `Invoke-WebRequest` 命令的别名）。
- en: The dependency of the LoadBalancer Service type on the platform infrastructure
    makes this type too specific for day-to-day usage and may not be available in
    all Kubernetes clusters. Therefore, the best solution for compatibility is to
    use Ingress Controllers, as we will learn in the following section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer 服务类型对平台基础设施的依赖使得这种类型过于特定，难以在日常使用中广泛应用，且可能在并非所有 Kubernetes 集群中都可用。因此，兼容性最好的解决方案是使用
    Ingress 控制器，正如我们在接下来的章节中将学习的那样。
- en: Understanding Ingress Controllers
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Ingress 控制器
- en: An **Ingress Controller** is a piece of software that provides load balancing,
    SSL termination, and host-based virtual routing. It is a reverse proxy that runs
    in the Kubernetes cluster, which manages a reverse proxy network component that
    can run inside the Kubernetes cluster or externally, just like any other network
    infrastructure device. An Ingress Controller acts just like any other controller
    deployed in a Kubernetes cluster, although it is not managed by the cluster itself.
    We must deploy this controller manually as it is not part of the Kubernetes core.
    If required, we can deploy multiple Ingress Controllers in a cluster and define
    which one is to be used by default if none is specified.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ingress 控制器**是一种软件，提供负载均衡、SSL 终止和基于主机的虚拟路由。它是一个反向代理，运行在 Kubernetes 集群中，管理一个反向代理网络组件，可以在
    Kubernetes 集群内部或外部运行，类似于任何其他网络基础设施设备。Ingress 控制器的作用就像部署在 Kubernetes 集群中的其他控制器，尽管它并不由集群本身管理。我们必须手动部署这个控制器，因为它不是
    Kubernetes 核心的一部分。如果需要，我们可以在集群中部署多个 Ingress 控制器，并定义默认使用哪个控制器（如果没有指定）。'
- en: Ingress Controllers work very well with HTTP/HTTPS applications (OSI Layer 7,
    the application layer), but we can publish TCP and UDP applications too (OSI Layer
    4, the transport layer), although this does require more configuration and may
    not be the best option. In such cases, it may be better to use an external load
    balancer and route traffic to NodePort Service resources because TCP and UDP Ingress
    resources will need additional ports to distribute incoming traffic.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器在 HTTP/HTTPS 应用程序（OSI 第 7 层，应用层）中表现非常好，但我们也可以发布 TCP 和 UDP 应用程序（OSI
    第 4 层，传输层），尽管这需要更多的配置，并且可能不是最佳选项。在这种情况下，最好使用外部负载均衡器并将流量路由到 NodePort 服务资源，因为 TCP
    和 UDP Ingress 资源将需要额外的端口来分配传入流量。
- en: Kubernetes administrators use **IngressClass resources** to declare the different
    Ingress Controllers available on a platform. Each Ingress Controller is associated
    with an IngressClass resource. You as the developer must create Ingress resources,
    which are the definitions required for reverse-proxying your application’s workloads.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 管理员使用 **IngressClass 资源** 来声明平台上可用的不同 Ingress 控制器。每个 Ingress 控制器都与一个
    IngressClass 资源相关联。作为开发人员，你必须创建 Ingress 资源，这是反向代理你的应用工作负载所需的定义。
- en: 'There are multiple options for deploying an Ingress Controller: cloud providers
    and many software vendors have developed their own solutions, and you can include
    any of them in your own Kubernetes setup, but you must understand their specific
    features and particularities. You can review the available solutions in the Kubernetes
    documentation at [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)
    and ask your Kubernetes administrator about the Ingress Controllers available
    on your platform before preparing your applications. Small tweaks may be necessary
    on your side in your Ingress resources. In the following section, we will examine
    the most frequently encountered option, the **Kubernetes NGINX Ingress Controller**,
    included by default in some Kubernetes solutions.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 Ingress 控制器有多种选择：云服务提供商和许多软件供应商已经开发了他们自己的解决方案，你可以将它们中的任何一种包含到你的 Kubernetes
    设置中，但你必须理解它们的特定功能和特点。你可以在 Kubernetes 文档中查看可用的解决方案，链接为：[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)，并在准备应用程序之前向你的
    Kubernetes 管理员询问你平台上可用的 Ingress 控制器。在你的 Ingress 资源中可能需要做一些小调整。接下来的部分，我们将讨论最常见的选项，默认包含在一些
    Kubernetes 解决方案中的 **Kubernetes NGINX Ingress Controller**。
- en: Deploying an Ingress Controller
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 Ingress 控制器
- en: 'To deploy an Ingress Controller, we simply follow the specific instructions
    for the chosen solution. There may be different approaches for installing the
    given software in your cluster, but we will follow the easiest one: deploying
    a YAML file containing all the required resources in one file ([https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml)).
    Make sure to check the latest available release before using the URL just provided
    and consult the specific instructions provided for it. At the time of writing
    this book, NGINX Controller release `1.8.1` was the latest release available.
    In this example, we use the cloud YAML file, although you can use the bare-metal
    option if you have a fully functional Kubernetes environment installed (this version
    uses NodePort instead of the LoadBalancer type). Let’s work through our simple
    example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 Ingress 控制器时，我们只需按照所选解决方案的具体指示操作。安装给定软件到你的集群中可能有不同的方法，但我们将遵循最简单的方式：部署一个包含所有必需资源的
    YAML 文件（[https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml)）。在使用该
    URL 之前，请确保检查最新的可用版本，并参考特定的安装说明。撰写本书时，NGINX 控制器版本 `1.8.1` 是最新的版本。在这个例子中，我们使用了云端
    YAML 文件，尽管如果你已经安装了一个完全功能的 Kubernetes 环境，你也可以选择裸金属选项（该版本使用 NodePort 而不是 LoadBalancer
    类型）。让我们通过一个简单的例子来操作：
- en: 'We start by running `kubectl apply` on a Docker Desktop environment:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从在 Docker Desktop 环境中运行 `kubectl apply` 开始：
- en: '![Figure 11.14 – Deployment of the popular Kubernetes NGINX Ingress Controller](img/B19845_11_14.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.14 – 流行的 Kubernetes NGINX Ingress Controller 部署](img/B19845_11_14.jpg)'
- en: Figure 11.14 – Deployment of the popular Kubernetes NGINX Ingress Controller
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14 – 部署流行的 Kubernetes NGINX Ingress Controller
- en: 'As you can see in the previous screenshot, many resources are created for the
    Ingress Controller to work. A new namespace was created, `ingress-nginx`, and
    some Pods are now running there:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的截图所示，Ingress Controller 工作所需的许多资源已经创建。一个新的命名空间 `ingress-nginx` 已创建，且一些 Pod
    已经在其中运行：
- en: '![Figure 11.15 – Deployment, Pods, and IngressClass resources created](img/B19845_11_15.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.15 – 部署、Pod 和 IngressClass 资源已创建](img/B19845_11_15.jpg)'
- en: Figure 11.15 – Deployment, Pods, and IngressClass resources created
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15 – 部署、Pod 和 IngressClass 资源已创建
- en: In the preceding screenshot, we can see an `IngressClass` resource created.
    We may need to configure it as default.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到创建了一个 `IngressClass` 资源。我们可能需要将其配置为默认。
- en: 'Let’s check the deployed Ingress Controller. We first check the Service resource
    created to reach the Deployment resource using `kubectl` `get svc`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查已部署的 Ingress Controller。我们首先检查为访问 Deployment 资源而创建的 Service 资源，使用 `kubectl`
    `get svc`：
- en: '![Figure 11.16 – Verification of the deployed Ingress Controller](img/B19845_11_16.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.16 – 验证已部署的 Ingress Controller](img/B19845_11_16.jpg)'
- en: Figure 11.16 – Verification of the deployed Ingress Controller
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16 – 验证已部署的 Ingress Controller
- en: Note in the preceding screenshot how the Service resource was created as a `LoadBalancer`
    type. It acquired the `localhost` IP address (we are using Docker Desktop in this
    example), which means that we should be able to reach the NGINX Ingress Controller
    backend directly with `curl` using `localhost`. The Service is listening on ports
    `80` and `443`, and we were able to reach both (we passed the `-k` argument to
    `curl` to avoid having to verify the associated auto-signed and untrusted SSL
    certificate).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意前面截图中，Service 资源被创建为 `LoadBalancer` 类型。它获得了 `localhost` IP 地址（我们在此示例中使用的是
    Docker Desktop），这意味着我们应该能够直接使用 `curl` 通过 `localhost` 访问 NGINX Ingress Controller
    后端。该 Service 正在监听端口 `80` 和 `443`，我们能够访问这两个端口（我们通过给 `curl` 添加 `-k` 参数，以避免验证相关的自动签名和不受信任的
    SSL 证书）。
- en: The use of Ingress Controllers improves security when we add SSL certificates
    to implement SSL tunnels between our applications’ exposed components and our
    users, or even between different components that use the Ingress URL associated
    with the Service resource.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ingress Controllers 可以提高安全性，当我们添加 SSL 证书以实现应用程序暴露组件与用户之间，或甚至不同组件之间使用与 Service
    资源关联的 Ingress URL 的 SSL 隧道时。
- en: Let’s go ahead now and learn how to manage the behavior of our applications
    using Ingress resources.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续学习如何使用 Ingress 资源管理应用程序的行为。
- en: Ingress resources
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress 资源
- en: 'As with any other resource, we need to define `apiVersion`, `kind`, `metadata`,
    and `spec` keys and sections. The most important section is `.spec.rules`, which
    defines a list of host rules that dynamically configure the reverse proxy deployed
    by the Ingress Controller. Let’s see a basic example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他资源一样，我们需要定义 `apiVersion`、`kind`、`metadata` 和 `spec` 键及其对应的部分。最重要的部分是 `.spec.rules`，它定义了一组主机规则，用来动态配置由
    Ingress Controller 部署的反向代理。让我们看一个基本的示例：
- en: '![Figure 11.17 – Ingress resource example](img/B19845_11_17.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.17 – Ingress 资源示例](img/B19845_11_17.jpg)'
- en: Figure 11.17 – Ingress resource example
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17 – Ingress 资源示例
- en: In the preceding screenshot, we can see the `ingressClassName` key, which indicates
    the Ingress Controller to be used. The `rules` section defines a list of host
    headers and the paths associated with the different backends. In our example,
    the [www.webserver.com](http://www.webserver.com) host header is required; if
    requests do not include it, they will be redirected to the default backend (if
    defined) or be shown a `404` error (page not found). The `backend` section describes
    the Service resource that will receive the application’s requests.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到 `ingressClassName` 键，它指示将使用的 Ingress Controller。`rules` 部分定义了一组主机头和与不同后端关联的路径。在我们的示例中，要求包含
    [www.webserver.com](http://www.webserver.com) 主机头；如果请求未包含该头，它们将被重定向到默认后端（如果已定义），或显示
    `404` 错误（页面未找到）。`backend` 部分描述了将接收应用程序请求的 Service 资源。
- en: 'Let’s run a quick example using the `webserver` Service resource created in
    the previous section. It will listen on port `8080`, hence we create an Ingress
    resource with a fake hostname and validate its accessibility with `curl -H "host:
    <FAKE_HOST>" http://localhost` (we use `localhost` because its IP address is the
    one associated with the `LoadBalancer` Service):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们使用上一节创建的 `webserver` 服务资源快速运行一个示例。它将在端口 `8080` 上监听，因此我们创建一个带有虚假主机名的 Ingress
    资源，并使用 `curl -H "host: <FAKE_HOST>" http://localhost` 来验证其可访问性（我们使用 `localhost`，因为它的
    IP 地址是与 `LoadBalancer` 服务关联的）。'
- en: '![Figure 11.18 – Ingress webserver resource for the webserver Service example](img/B19845_11_18.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.18 – 用于 Web 服务器服务示例的 Ingress Web 服务器资源](img/B19845_11_18.jpg)'
- en: Figure 11.18 – Ingress webserver resource for the webserver Service example
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 – 用于 Web 服务器服务示例的 Ingress Web 服务器资源
- en: Security features are implemented in the `.spec.tls` section, where we link
    the hosts with their keys and certificates, integrated into a Secret resource.
    This Secret must be included in the namespace in which you defined the Ingress
    resource, and it is of the `tls` type. The `data` sections in these Secrets must
    include the key for the generation of the certificate along with the generated
    certificate itself. We will learn how to create this via an example in the *Labs*
    section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 安全功能在 `.spec.tls` 部分实现，在这里我们将主机与其密钥和证书链接，并集成到一个 Secret 资源中。这个 Secret 必须包含在你定义
    Ingress 资源的命名空间中，并且是 `tls` 类型。这些 Secrets 中的 `data` 部分必须包含生成证书的密钥和生成的证书本身。我们将在
    *实验* 部分通过一个示例学习如何创建它。
- en: We can have an Ingress resource with rules for multiple hosts and each host
    with multiple paths, although it is more common to separate each host on a different
    Ingress resource for easier management and include multiple paths for reaching
    different backend Service resources. This combination represents a typical microservice
    architecture where each application functionality is provided by different backend
    Services.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以拥有一个包含多个主机规则的 Ingress 资源，每个主机有多个路径，尽管将每个主机分开到不同的 Ingress 资源中以便于管理，并为不同的后端服务资源设置多个路径是更常见的做法。这个组合代表了一个典型的微服务架构，其中每个应用功能由不同的后端服务提供。
- en: 'The `annotations` section can be used to instruct the Ingress Controller regarding
    special configurations. Here is a list of some of the most important configurations
    we can manage via annotations for the Kubernetes NGINX Ingress Controller:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`annotations` 部分可以用来指示 Ingress 控制器进行特殊配置。以下是我们可以通过注解为 Kubernetes NGINX Ingress
    控制器管理的一些最重要的配置：'
- en: '`nginx.ingress.kubernetes.io/rewrite-target`: It is usual to integrate some
    rewrite rules in our Ingress resource for rewriting the application’s URI paths.
    There are also options for redirecting URLs.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nginx.ingress.kubernetes.io/rewrite-target`：通常我们会在 Ingress 资源中集成一些重写规则，用于重写应用程序的
    URI 路径。这里也有一些选项用于重定向 URL。'
- en: '`nginx.ingress.kubernetes.io/auth-type` and `nginx.ingress.kubernetes.io/auth-secret`:
    These will allow us to use basic authentication at the Ingress level for our applications.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nginx.ingress.kubernetes.io/auth-type` 和 `nginx.ingress.kubernetes.io/auth-secret`：这将允许我们在
    Ingress 层面为我们的应用程序使用基本认证。'
- en: '`nginx.ingress.kubernetes.io/proxy-ssl-verify`: If our Service resource backends
    use TLS, there are many annotations available to manage how NGINX connects to
    them.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nginx.ingress.kubernetes.io/proxy-ssl-verify`：如果我们的服务资源后端使用 TLS，那么有许多注解可以管理
    NGINX 如何与它们连接。'
- en: '`nginx.ingress.kubernetes.io/enable-cors`: We may need to enable **Cross-Origin
    Resource Sharing** (**CORS**) in our application to allow some external routes
    and URLs. There are also other interesting options here for managing and securing
    CORS behavior.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nginx.ingress.kubernetes.io/enable-cors`：我们可能需要在应用程序中启用**跨域资源共享**（**CORS**），以允许一些外部路由和
    URL。这里还有其他一些有趣的选项，用于管理和保护 CORS 行为。'
- en: '`nginx.ingress.kubernetes.io/client-body-buffer-size`: It’s quite common to
    limit the size of client requests to avoid overall performance issues, but your
    application may require larger responses.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nginx.ingress.kubernetes.io/client-body-buffer-size`：限制客户端请求的大小以避免整体性能问题是很常见的，但你的应用程序可能需要更大的响应。'
- en: 'There are many options available beyond these, and you may need to ask your
    Kubernetes and infrastructure administrators for advice. The range on offer includes
    integrating external authentication backends, limiting the rate of requests to
    avoid **distributed denial-of-service** (**DDoS**) attacks, redirecting and rewriting
    URLs, enabling SSL passthrough, and even managing canary application deployments,
    routing some requests to a newer release of your workload backends. Some of the
    options can be defined at the Ingress Controller level, which will affect all
    Ingress resources at once. For a full list of available annotations, please refer
    to the following page: [https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些选项外，还有许多其他选项可供选择，您可能需要向您的 Kubernetes 和基础设施管理员寻求建议。可供选择的范围包括集成外部身份验证后端、限制请求速率以防止**分布式拒绝服务**（**DDoS**）攻击、重定向和重写
    URL、启用 SSL 通透，甚至管理金丝雀应用程序部署，将部分请求路由到工作负载后端的更新版本。有些选项可以在 Ingress 控制器级别进行定义，这将影响所有的
    Ingress 资源。有关可用注解的完整列表，请参阅以下页面：[https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/)。
- en: It is very important to understand that the options mentioned here may differ
    from those available for other Ingress Controllers (at least, they will use other
    annotation keys, for sure). Some Ingress Controllers such as Kong also implement
    API management for your backend Services, which can be very useful if they are
    involved in many interactions. Ask your Kubernetes administrators about the Ingress
    Controllers deployed on your platform.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这里提到的选项非常重要，因为它们可能与其他 Ingress 控制器提供的选项有所不同（至少，它们肯定会使用其他注解键）。一些 Ingress 控制器，例如
    Kong，还为后端服务实现了 API 管理，如果这些服务涉及许多交互，这将非常有用。请向你的 Kubernetes 管理员咨询有关平台上部署的 Ingress
    控制器。
- en: 'We covered the basics here, but as always, note that your Ingress resource
    may need some small tweaks to fully implement your platform requirements. In OpenShift,
    for example, Ingress Controllers can be enabled, but by default, Kubernetes will
    use OpenShift Route, which is the Red Hat implementation of a L7 reverse proxy
    for publishing applications in Kubernetes. Ingress Controllers and OpenShift Route
    are quite similar (even their resources look alike) but you should review further
    specific information about it if your application needs to run on an OpenShift
    cluster. The following link may help you decide which one to use if both implementations
    are available for your application: [https://cloud.redhat.com/blog/kubernetes-ingress-vs-openshift-route](https://cloud.redhat.com/blog/kubernetes-ingress-vs-openshift-route).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里涵盖了基础知识，但像往常一样，请注意，您的 Ingress 资源可能需要一些小调整才能完全实现您的平台要求。例如，在 OpenShift 中，可以启用
    Ingress 控制器，但默认情况下，Kubernetes 将使用 OpenShift Route，这是 Red Hat 实现的用于在 Kubernetes
    中发布应用的 L7 反向代理。Ingress 控制器和 OpenShift Route 非常相似（即使它们的资源看起来也很相似），但如果您的应用需要在 OpenShift
    集群上运行，您应该进一步查看相关的特定信息。如果在您的应用上可以同时使用这两种实现，以下链接可能有助于您决定使用哪个：[https://cloud.redhat.com/blog/kubernetes-ingress-vs-openshift-route](https://cloud.redhat.com/blog/kubernetes-ingress-vs-openshift-route)。
- en: By default, Kubernetes implements a flat network, without any access boundaries
    between applications. This applies no restrictions to any lateral movement (East-West
    traffic), a configuration that could cause critical security issues. In the next
    section, we will review some security improvements to help us publish our applications
    securely.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kubernetes 实现了一个扁平网络，应用之间没有任何访问边界。这对横向流量（东-西流量）没有任何限制，这种配置可能导致严重的安全问题。在接下来的章节中，我们将回顾一些安全改进，以帮助我们安全地发布应用。
- en: Improving our applications’ security
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进我们应用程序的安全性
- en: In Kubernetes, application traffic flows freely by default. A flat network is
    deployed to cover Pod-to-Pod and Service-to-Pod communications – remember that
    containers within a Pod have a common, shared IP address. Pods running within
    a Kubernetes cluster will see each other, and it will require some extra work
    to protect one application from another, even if they run on different nodes and
    in different namespaces.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，应用流量默认是自由流动的。部署了一个扁平网络来覆盖 Pod 到 Pod 以及 Service 到 Pod 的通信——请记住，Pod
    内的容器共享一个公共 IP 地址。运行在 Kubernetes 集群中的 Pod 会彼此看到，即使它们运行在不同的节点和命名空间中，保护一个应用免受另一个应用的影响也需要额外的工作。
- en: It may be strange to hear, but applications running in different namespaces
    can see each other. In fact, if they have an associated Service resource, it would
    be easy to use the internal DNS to resolve its associated IP address and access
    its processes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来可能有些奇怪，但运行在不同命名空间中的应用程序可以彼此看到。实际上，如果它们有相关联的 Service 资源，就可以轻松通过内部 DNS 解析其关联的
    IP 地址并访问其进程。
- en: In the next section, we will learn how NetworkPolicy resources can be used to
    define our applications’ communications and have Kubernetes block any unwanted
    connectivity for us.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将学习如何使用 NetworkPolicy 资源定义我们的应用程序通信，并让 Kubernetes 为我们阻止任何不必要的连接。
- en: Network policies
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络策略
- en: NetworkPolicy resources (also referred to as `netpol`) allow us to manage OSI
    Layer 3 and 4 communications (IP and port access, respectively). Kubernetes provides
    the NetworkPolicy resource as part of its core, but its implementation depends
    on the CNI deployed in your cluster. Therefore, it is vital to use a CNI (such
    as Calico, Canal, or Cilium, among others) that implements this feature.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkPolicy 资源（也称为 `netpol`）允许我们管理 OSI 第 3 层和第 4 层的通信（分别为 IP 和端口访问）。Kubernetes
    将 NetworkPolicy 资源作为核心的一部分提供，但其实现依赖于集群中部署的 CNI。因此，使用实现此功能的 CNI（如 Calico、Canal
    或 Cilium 等）至关重要。
- en: 'NetworkPolicy resources define all aspects of Pod communications: egress (output
    traffic) and ingress (input traffic). As we will see in a few moments, NetworkPolicies
    are applied to specific sets of Pods by using the `.spec.podSelector` section.
    NetworkPolicy resources are namespaced, hence `podSelector` allows us to decide
    which Pods are to be affected by our rule definitions. Multiple rules can be applied
    to a Pod, and your Kubernetes administrators may have included some `GlobalNetworkPolicy`
    resources that affect the entire cluster, thus you should inquire whether any
    cluster default rules require allowing some egress or ingress traffic. It is quite
    common to allow only DNS traffic by default, disallowing all additional egress
    traffic. If this is the case in your cluster, you will need to declare all egress
    (as well as ingress) communications in your applications’ manifests. Let’s see
    a quick example of a NetworkPolicy in which we declare both ingress and egress
    communications:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkPolicy 资源定义了 Pod 通信的所有方面：出口（输出流量）和入口（输入流量）。正如我们稍后将看到的，NetworkPolicy 是通过使用
    `.spec.podSelector` 部分应用于特定 Pod 集合的。NetworkPolicy 资源是命名空间范围的，因此 `podSelector`
    允许我们决定哪些 Pods 会受到规则定义的影响。可以对 Pod 应用多个规则，您的 Kubernetes 管理员可能已包括一些影响整个集群的 `GlobalNetworkPolicy`
    资源，因此您应该询问是否有任何集群默认规则需要允许某些出口或入口流量。在您的集群中，通常默认仅允许 DNS 流量，禁止所有其他出口流量。如果您的集群是这种情况，您将需要在应用程序的清单中声明所有的出口（以及入口）通信。我们来看一个简单的例子，展示如何在
    NetworkPolicy 中声明出口和入口通信：
- en: '![Figure 11.19 – NetworkPolicy resource manifest with both egress and ingress
    rules](img/B19845_11_19.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.19 – 包含出口和入口规则的 NetworkPolicy 资源清单](img/B19845_11_19.jpg)'
- en: Figure 11.19 – NetworkPolicy resource manifest with both egress and ingress
    rules
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 – 包含出口和入口规则的 NetworkPolicy 资源清单
- en: 'Let’s review some of the most important keys and sections available in the
    preceding code snippet. The `.spec.podSelector` section declares which Pods in
    the current namespace (if none are declared under the `metadata` section) will
    be affected by this policy. Under the `policyTypes` key, we can see a list of
    policy types defined. We should clarify here that egress communications are those
    initiated from a Pod, while ingress communications are those that go into the
    Pod. If you declare both types, egress and ingress, and then only declare a section
    for one of them (either an `egress` or `ingress` section, as in the preceding
    example), the omitted one is declared as empty, meaning that that type of communication
    will not be allowed *at all*. The `egress` section in this example is a list of
    rules to be applied. Let’s have a closer look at this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下前面代码片段中一些最重要的键和值。`.spec.podSelector` 部分声明了当前命名空间中（如果 `metadata` 部分没有声明）哪些
    Pods 会受到此策略的影响。在 `policyTypes` 键下，我们可以看到定义的策略类型列表。在这里需要澄清的是，出口通信是由 Pod 发起的，而入口通信是指进入
    Pod 的通信。如果同时声明了出口和入口类型，而仅为其中之一声明了部分（如前面的例子中的 `egress` 或 `ingress` 部分），则未声明的部分将被视为空，意味着该类型的通信将**完全不被允许**。本例中的
    `egress` 部分是要应用的一系列规则。我们再仔细看一下：
- en: The first rule allows egress communications from selected Pods (those with the
    `app=myapp` label) to port `5978` on any host in the `10.0.0.0/24` subnet
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一条规则允许从选定的 Pods（带有 `app=myapp` 标签）到 `10.0.0.0/24` 子网中任何主机的端口 `5978` 的外发通信。
- en: The second rule allows egress communications to UDP port `53` on any host (Kubernetes
    internal and external DNS)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二条规则允许向任何主机的 UDP 端口 `53` 进行外发通信（Kubernetes 内部和外部 DNS）。
- en: 'In the `ingress` section, two rules are also declared:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `ingress` 部分，也声明了两条规则：
- en: The first rule allows access to port `6379` on the selected Pods (those containing
    the `app=myapp` label) for any communication coming from the `172.17.0.0/16` subnet
    (except those hosted on the `172.17.1.0/24` subnet), from Pods running in namespaces
    with the `project=myproject` label, and from Pods in the current namespace with
    the `role=frontend` label. We can say that *Kubernetes is all* *about labels*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一条规则允许从 `172.17.0.0/16` 子网（但不包括 `172.17.1.0/24` 子网）中的主机、带有 `project=myproject`
    标签的命名空间中的 Pods 以及当前命名空间中带有 `role=frontend` 标签的 Pods 访问选定 Pods（带有 `app=myapp` 标签）的端口
    `6379`。我们可以说，*Kubernetes 完全是* *关于标签*。
- en: The second ingress rule allows access to selected Pods on port `80` from hosts
    on the `192.168.200.0/24` subnet.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二条入站规则允许来自 `192.168.200.0/24` 子网中的主机访问选定 Pods 的端口 `80`。
- en: These rules may seem complex but are quite easy to implement in practice.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则看起来可能很复杂，但实际上实施起来非常简单。
- en: 'If you are planning to deploy all your application’s components in a specific
    namespace, it could be worthwhile to allow all egress and ingress communications
    between your Pods. This isn’t a good idea for production because only attackers’
    lateral movements to other namespaces will be blocked, but a security issue in
    one of your Pods could affect others in the same namespace. While preparing your
    NetworkPolicy resources or debugging your application, allowing all namespace
    East-West traffic may also be necessary. The following YAML manifests allow all
    internal communication and expose only the frontend component externally:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划将应用程序的所有组件部署在特定命名空间中，可能值得允许所有 Pods 之间的所有外发和入站通信。但在生产环境中这样做并不是一个好主意，因为这样只有攻击者横向移动到其他命名空间时才会被阻止，但如果你某个
    Pod 中出现安全问题，可能会影响同一命名空间中的其他 Pods。在准备 NetworkPolicy 资源或调试应用程序时，允许所有命名空间内部的东西-West
    流量也可能是必要的。以下的 YAML 清单允许所有内部通信，并且仅将前端组件暴露给外部：
- en: '![Figure 11.20 – Example manifests for allowing all namespaced communications
    as well as access to port 80 on a specific Pod](img/B19845_11_20.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.20 – 示例清单，允许所有命名空间通信并允许对特定 Pod 的端口 80 进行访问](img/B19845_11_20.jpg)'
- en: Figure 11.20 – Example manifests for allowing all namespaced communications
    as well as access to port 80 on a specific Pod
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.20 – 示例清单，允许所有命名空间通信并允许对特定 Pod 的端口 80 进行访问。
- en: 'The preceding screenshot shows two manifests:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图显示了两个清单：
- en: The one on the left declares a NetworkPolicy resource that allows all communications
    between all Pods deployed in the current namespace. The rule applies to all Pods
    in the namespace because `podSelector` is empty.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧的清单声明了一个 NetworkPolicy 资源，允许当前命名空间中所有 Pods 之间的所有通信。由于 `podSelector` 为空，因此规则适用于命名空间中的所有
    Pods。
- en: The manifest on the right allows access to the Pod with the `appcomponent=frontend`
    label (`podSelector` applies on this Pod) on port `80`, but only from hosts on
    the `192.168.200.0/24` subnet.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧的清单允许从 `192.168.200.0/24` 子网中的主机访问带有 `appcomponent=frontend` 标签的 Pod（`podSelector`
    适用于该 Pod）上的端口 `80`。
- en: Important note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: NetworkPolicy resources apply at the connection level and don’t leave any connectivity
    traces by default, which may be inconvenient when trying to fix some connectivity
    issues between components. Some CNI plugins such as Calico enable you to log connections
    between Pods. This requires additional permissions on your Kubernetes environment.
    Ask your Kubernetes administrators if they can provide some connectivity traces
    if required for debugging your applications. In some cases, it is best to start
    with a NetworkPolicy that allows and logs all the connections made in the namespace.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkPolicy 资源在连接级别应用，并且默认情况下不会留下任何连接痕迹，这在尝试修复组件之间的某些连接问题时可能会很不方便。一些 CNI 插件，如
    Calico，允许记录 Pods 之间的连接。这需要在 Kubernetes 环境中额外的权限。如果需要调试应用程序，可以向 Kubernetes 管理员询问是否可以提供一些连接痕迹。在某些情况下，最好从一个允许并记录命名空间中所有连接的
    NetworkPolicy 开始。
- en: You as a developer are responsible for creating and maintaining your application’s
    resource manifests and thus, the NetworkPolicy resources required by your application.
    It is up to you how to organize them, but it’s recommended to use descriptive
    names and group multiple rules in a manifest per each application component. This
    way, you will be able to fine-tune each component’s configuration. In the *Labs*
    section, we have prepared for you a specific exercise where you will protect an
    application by allowing only trusted access.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，你需要负责创建和维护应用程序的资源清单，因此，也包括应用程序所需的 NetworkPolicy 资源。如何组织这些资源由你决定，但建议使用具有描述性的名称，并将多个规则按每个应用程序组件分组在一个清单中。通过这种方式，你可以精细调整每个组件的配置。在*实验*部分，我们为你准备了一个具体的练习，帮助你通过只允许受信任的访问来保护应用程序。
- en: NetworkPolicies allow you to thoroughly isolate all your application’s components,
    and although they may be hard to implement, this solution does provide great granularity
    and does not depend on the underlying infrastructure. You only require a Kubernetes
    CNI that supports this feature.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkPolicies 允许你彻底隔离所有应用程序的组件，尽管它们可能很难实现，但这个解决方案确实提供了极高的粒度，并且不依赖于底层基础设施。你只需要一个支持此功能的
    Kubernetes CNI。
- en: In the next section, we will review how service mesh solutions can provide more
    complex security functionality by injecting small, lightweight proxies on all
    your application’s Pods.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将回顾服务网格解决方案如何通过在所有应用程序的 Pod 上注入小型、轻量级的代理来提供更复杂的安全功能。
- en: Service mesh
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务网格
- en: By implementing NetworkPolicies, we enforce some firewall-like connectivity
    rules between our applications’ workloads, but this may not be enough. A service
    mesh is considered an infrastructure layer that interconnects services and manages
    how they will interact with each other. A service mesh is used to manage East-West
    and North-South traffic to background Services, in some cases even substituting
    the Ingress Controller if the service mesh solution is deployed in Kubernetes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施 NetworkPolicies，我们强制执行一些类似防火墙的连接规则，限制应用程序工作负载之间的互联，但这可能还不够。服务网格被视为一个基础设施层，它连接服务并管理它们如何相互交互。服务网格用于管理东西向和南北向流量到后台服务，在某些情况下，甚至可以替代
    Ingress 控制器，前提是服务网格解决方案已在 Kubernetes 中部署。
- en: The most popular service mesh solution is **Istio** (an open source solution
    that is part of the **Cloud Native Computing Foundation** (**CNCF**)), although
    other options worth mentioning include Linkerd, Consul Connect, and Traefik Mesh.
    If you are running a cloud Kubernetes platform, you may have your own cloud provider
    solution available.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的服务网格解决方案是**Istio**（一个开源解决方案，属于**云原生计算基金会**（**CNCF**）），尽管还有一些其他值得提及的选项，如
    Linkerd、Consul Connect 和 Traefik Mesh。如果你正在使用云 Kubernetes 平台，可能会有自己的云提供商解决方案可用。
- en: Service mesh solutions are capable of adding TLS communications, traffic management,
    and observability to your applications without having to modify their code. If
    you are looking for a transparent security and management layer, using a service
    mesh solution may be perfect for you, but it also adds a high level of complexity
    and some platform overhead.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格解决方案能够为你的应用程序添加 TLS 通信、流量管理和可观察性，而无需修改其代码。如果你在寻找一个透明的安全和管理层，使用服务网格解决方案可能是完美的选择，但它也增加了高度的复杂性和一定的平台开销。
- en: Service mesh solutions deploy a small proxy on all your application workloads.
    These proxies intercept all your application’s network traffic and apply rules
    to allow or disallow your application processes’ communications.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格解决方案在你所有的应用程序工作负载上部署一个小型代理。这些代理会拦截你应用程序的所有网络流量，并应用规则来允许或禁止你的应用程序进程之间的通信。
- en: The implementation and use of service meshes are out of the scope of this book
    but it is worth investigating whether your Kubernetes administrators have deployed
    a service mesh solution on your platform that may necessitate the implementation
    of service mesh-specific resources.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的范围不包括服务网格的实现和使用，但值得调查一下你的 Kubernetes 管理员是否在平台上部署了服务网格解决方案，这可能需要实现特定于服务网格的资源。
- en: As mentioned earlier in this section, NetworkPolicy resources isolate your application’s
    workloads by disabling unauthorized communications, which may provide sufficient
    security for a production environment. These resources are highly configurable,
    and you are responsible for defining the required communications between your
    application’s components and preparing the required YAML manifests to fully implement
    all your application communications. In the following *Labs* section, we will
    see some of the content learned in this chapter in action as we try publishing
    the `simplestlab` application used in previous chapters.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节前面提到的，NetworkPolicy 资源通过禁用未经授权的通信来隔离应用的工作负载，这可能为生产环境提供足够的安全性。这些资源高度可配置，你需要负责定义应用各个组件之间所需的通信，并准备所需的
    YAML 清单文件，以完整实现所有应用通信。在接下来的 *实验* 部分，我们将看到本章中学到的一些内容在实际操作中的应用，尝试发布前几章使用的 `simplestlab`
    应用。
- en: Labs
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: In this section, we show you how to work through the implementation of the Ingress
    resources for the `simplestlab` Tier-3 application, prepared for Kubernetes in
    [*Chapter 9*](B19845_09.xhtml#_idTextAnchor202), *Implementing Architecture Patterns*,
    and improved upon in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231), *Leveraging
    Application Data Management in Kubernetes*. Manifests for all the resources have
    been prepared for you in this book’s GitHub repository at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git)
    and can be found in the `Chapter11` folder. Ensure you have the latest revision
    available by simply executing `git clone` to download all its content, or use
    `git pull` if you have already downloaded the repository before. All the manifests
    and steps required for running the `simplestlab` application are located inside
    the `Containers-for-Developers-Handbook/Chapter11/simplestlab` directory, while
    all the manifests for Ingress and NetworkPolicy resources can be found directly
    in the `Chapter11` folder.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何实现 `simplestlab` Tier-3 应用的 Ingress 资源，该应用为 Kubernetes 环境准备，相关内容在
    [*第 9 章*](B19845_09.xhtml#_idTextAnchor202)《*实现架构模式*》中进行了介绍，并在 [*第 10 章*](B19845_10.xhtml#_idTextAnchor231)《*在
    Kubernetes 中利用应用数据管理*》中得到了改进。所有资源的清单文件已经为你准备好，存放在本书的 GitHub 仓库中，地址为 [https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git](https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git)，可以在
    `Chapter11` 文件夹中找到。通过执行 `git clone` 命令下载所有内容，确保你获取到最新版本，或者如果你之前已经下载过该仓库，可以使用 `git
    pull` 更新。所有运行 `simplestlab` 应用所需的清单文件和步骤都位于 `Containers-for-Developers-Handbook/Chapter11/simplestlab`
    目录下，而 Ingress 和 NetworkPolicy 资源的清单文件则直接位于 `Chapter11` 文件夹中。
- en: These labs will help you learn and understand how Ingress and NetworkPolicy
    resources work in Kubernetes. You will deploy an Ingress Controller, publish the
    `simplestlab` example application using the HTTP and HTTPS protocols, and create
    some NetworkPolicy resources to allow only appropriate connectivity. The Ingress
    Controller lab will work on Docker Desktop, Minikube, and Rancher, but for the
    NetworkPolicy resources part, you will need to use an appropriate Kubernetes CNI
    with support for such resources, such as Calico. Each Kubernetes desktop or platform
    implementation manages and presents its own networking infrastructure to users
    in a different way.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验将帮助你学习和理解 Kubernetes 中 Ingress 和 NetworkPolicy 资源的工作原理。你将部署一个 Ingress 控制器，使用
    HTTP 和 HTTPS 协议发布 `simplestlab` 示例应用，并创建一些 NetworkPolicy 资源来仅允许适当的连接。Ingress 控制器实验可以在
    Docker Desktop、Minikube 和 Rancher 上运行，但对于 NetworkPolicy 资源部分，你需要使用支持此类资源的 Kubernetes
    CNI，例如 Calico。每个 Kubernetes 桌面或平台的实现方式不同，它们以不同的方式管理并展示各自的网络基础设施。
- en: 'These are the tasks you will find in this chapter’s GitHub repository:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你在本章 GitHub 仓库中可以找到的任务：
- en: We will first deploy the Kubernetes NGINX Ingress Controller (if you don’t have
    your own Ingress Controller in your labs platform).
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将部署 Kubernetes NGINX Ingress 控制器（如果你在实验平台中没有自己的 Ingress 控制器）。
- en: We will deploy all the manifests prepared for the `simplestlab` application,
    located inside the `simplestlab` folder. We will use `kubectl create -``f simplestlab`.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将部署为 `simplestlab` 应用准备的所有清单文件，这些文件位于 `simplestlab` 文件夹内。我们将使用 `kubectl create
    -f simplestlab` 命令。
- en: Once all the components are ready, we will create an Ingress resource using
    the manifest prepared for this task.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有组件准备就绪，我们将使用为此任务准备的清单文件创建一个 Ingress 资源。
- en: In the GitHub repository, you will find instructions for deploying a more advanced
    Ingress manifest with a self-signed certificate and encrypting the client communications.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 GitHub 仓库中，你将找到有关部署更高级的 Ingress 清单的说明，包含自签名证书以及加密客户端通信的内容。
- en: There is also a NetworkPolicy lab in the GitHub repository that will help you
    understand how to secure your applications using this feature with a compatible
    CNI (Calico).
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 GitHub 仓库中还有一个 NetworkPolicy 实验，帮助你了解如何通过兼容的 CNI（如 Calico）使用此功能来保护应用程序。
- en: In the first task, we will deploy our own Ingress Controller.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个任务中，我们将部署我们自己的 Ingress 控制器。
- en: Improving application access by deploying your own Ingress Controller
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过部署你自己的 Ingress 控制器来改善应用程序访问
- en: 'For this task, we will use Docker Desktop, which provides a good LoadBalancer
    service implementation. These Service resources will attach the localhost IP address,
    which will make it easy to connect to the published services. We will use the
    cloud deployment of Kubernetes NGINX Ingress Controller ([https://kubernetes.github.io](https://kubernetes.github.io))
    based on the LoadBalancer Service type, described in the following manifest: [https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml).
    If you are using a completely bare-metal infrastructure, you can use the bare-metal
    YAML ([https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml))
    and follow the additional instructions at [https://kubernetes.github.io/ingress-nginx/deploy/baremetal/](https://kubernetes.github.io/ingress-nginx/deploy/baremetal/)
    for NodePort routing.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此任务，我们将使用 Docker Desktop，它提供了良好的负载均衡器服务实现。这些 Service 资源将附加本地 IP 地址，便于连接已发布的服务。我们将使用基于
    LoadBalancer 服务类型的 Kubernetes NGINX Ingress 控制器的云部署（[https://kubernetes.github.io](https://kubernetes.github.io)），其清单描述如下：[https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml)。如果你使用的是完全裸金属基础设施，可以使用裸金属
    YAML（[https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml)）并按照
    [https://kubernetes.github.io/ingress-nginx/deploy/baremetal/](https://kubernetes.github.io/ingress-nginx/deploy/baremetal/)
    中的附加说明进行 NodePort 路由配置。
- en: Important note
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Local copies of both YAML files are provided in the repository as `kubernetes-nginx-ingress-controller-full-install-cloud.yaml`
    and `kubernetes-nginx-ingress-controller-full-install-baremetal.yaml`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中提供了两个 YAML 文件的本地副本，分别为 `kubernetes-nginx-ingress-controller-full-install-cloud.yaml`
    和 `kubernetes-nginx-ingress-controller-full-install-baremetal.yaml`。
- en: 'Once this is done, follow these steps:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此步骤后，按照以下步骤进行操作：
- en: 'We will just deploy the cloud version, provided in the YAML as a series of
    concatenated manifests. We just use `kubectl apply` to deploy the controller:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只会部署云版本，在 YAML 文件中以一系列连接的清单提供。我们将使用 `kubectl apply` 来部署控制器：
- en: '[PRE4]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can review the workload resources created:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以查看已创建的工作负载资源：
- en: '[PRE5]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Chapter11$ curl http://localhost
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Chapter11$ curl http://localhost
- en: <html>
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <html>
- en: <head><title>404 Not Found</title></head>
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <head><title>404 未找到</title></head>
- en: <body>
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <body>
- en: <center><h1>404 Not Found</h1></center>
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <center><h1>404 未找到</h1></center>
- en: <hr><center>nginx</center>
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <hr><center>nginx</center>
- en: </body>
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: </body>
- en: '–k argument to avoid certificate validation):'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: –k 参数用于避免证书验证：
- en: '[PRE6]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The Ingress Controller is now deployed and listening, and the `404` error indicates
    that there isn’t an associated Ingress resource with the `localhost` host (in
    fact there isn’t even a default one configured, but the Ingress Controller responds
    correctly).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器现在已部署并在监听，`404` 错误表明 `localhost` 主机没有关联的 Ingress 资源（实际上甚至没有配置默认资源，但
    Ingress 控制器响应正确）。
- en: Publishing the simplestlab application on Kubernetes using an Ingress Controller
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ingress 控制器在 Kubernetes 上发布 simplestlab 应用程序
- en: 'In this lab, we will deploy `simplestlab`, a very simplified tier-3 application,
    located in the `simplestlab` directory, and we’ll publish its frontend, the `lb`
    component, without TLS encryption. You can follow these steps:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将部署 `simplestlab`，一个非常简化的三级应用，位于 `simplestlab` 目录中，并且我们将发布其前端组件 `lb`，无需
    TLS 加密。你可以按照以下步骤进行：
- en: 'The manifests for the application are already written for you; we will just
    have to use `kubectl` to create an appropriate namespace for the application and
    then deploy all its resources:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Chapter11$ kubectl get all -n simplestlab
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                       READY   STATUS    RESTARTS   AGE
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pod/app-5f9797d755-5t4nz   1/1     Running   0          81s
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pod/app-5f9797d755-9rzlh   1/1     Running   0          81s
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pod/app-5f9797d755-nv58j   1/1     Running   0          81s
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pod/db-0                   1/1     Running   0          80s
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: pod/lb-5wl7c               1/1     Running   0          80s
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: service/app   ClusterIP   10.99.29.167    <none>        3000/TCP   81s
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: service/db    ClusterIP   None            <none>        5432/TCP   81s
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: service/lb    ClusterIP   10.105.219.69   <none>        80/TCP     80s
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE
    SELECTOR   AGE
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: daemonset.apps/lb   1         1         1       1            1            <none>          80s
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: deployment.apps/app   3/3     3            3           81s
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                             DESIRED   CURRENT   READY   AGE
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: replicaset.apps/app-5f9797d755   3         3         3       81s
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NAME                  READY   AGE
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: lb component isn’t exposed. It is listening on port 80, but ClusterIP is used,
    hence the Service is only available internally, cluster-wide.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will now create an Ingress resource. There are two manifests in the `ingress`
    directory. We will use `simplestlab.ingress.yaml`, which will be deployed without
    custom TLS encryption:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will just deploy the previously created manifest:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Chapter11$ curl -H "host: simplestlab.local.lab" http://localhost/'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <!DOCTYPE html>
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <html>
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <head>
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: …
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: </head>
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <body>
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: …
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: </body>
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: </html>
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: </body>
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: simplestlab application is now available and accessible.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can change our `/etc/hosts` file (or equivalent MS Windows `c:\system32\drivers\etc\hosts`
    file). Add the following line and open the web browser to access the `simplestlab`
    application:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This requires root or Administrator access, hence it may be more interesting
    to use `curl` with the `-H` or `--header` arguments to check the application.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: You can use an extension on your web browser that allows you to modify the headers
    of your requests or an FQDN including [nip.io](http://nip.io), which will be used
    in [*Chapter 13*](B19845_13.xhtml#_idTextAnchor287), *Managing the Application
    Life Cycle*. For example, you can simply add the `simple-modify-headers` extension
    if you are using MS Edge (you will find equivalent ones for other web browsers
    and operating systems). Additional information for configuring this extension
    is discussed in the GitHub `Readme.md` file for this chapter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'The application will be available at [http://localhost](http://localhost) (notice
    that we defined the URL pattern as `http://locahost/*` in the `simple-modify-headers`
    extension configuration):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序将在[http://localhost](http://localhost)可用（请注意，我们在`simple-modify-headers`扩展配置中定义了URL模式为`http://locahost/*`）：
- en: '![ Figure 11.21 – SIMPLE MODIFY HEADERS Edge extension configuration](img/B19845_11_21.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![ 图11.21 – 简单修改标题 Edge 扩展配置](img/B19845_11_21.jpg)'
- en: Figure 11.21 – SIMPLE MODIFY HEADERS Edge extension configuration
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21 – 简单修改标题 Edge 扩展配置
- en: 'Once the extension is configured, we can reach the `simplestlab` application
    using [http://localhost](http://localhost):'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展配置完成后，我们可以使用[http://localhost](http://localhost)访问`simplestlab`应用程序：
- en: '![Figure 11.22 – The simplestlab application is accessible thanks to the Ingress
    Controller](img/B19845_11_22.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图11.22 – 由于Ingress控制器，simplestlab应用程序可访问](img/B19845_11_22.jpg)'
- en: Figure 11.22 – The simplestlab application is accessible thanks to the Ingress
    Controller
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.22 – 由于Ingress控制器，simplestlab应用程序可访问
- en: In the GitHub repository, you will find instructions to add TLS to the Ingress
    resource to improve our application security and how to implement NetworkPolicy
    resources using Calico as a CNI with Minikube.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub存储库中，您将找到有关如何向Ingress资源添加TLS以提高应用程序安全性以及如何使用Calico作为CNI与Minikube实现NetworkPolicy资源的说明。
- en: These labs have helped you understand how to improve the security of your applications
    by isolating their components and exposing and publishing only those required
    by the users and other applications’ components.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验室帮助您了解如何通过隔离应用程序的组件并仅暴露和发布用户和其他应用程序组件所需的内容来提高应用程序的安全性。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to publish our applications in Kubernetes for
    our users and for other components deployed either internally in the same cluster
    or externally. Different mechanisms for this were examined, but ultimately, it
    is up to you to determine which of your applications’ components should be exposed
    and accessible.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何为我们的用户和其他部署在同一集群内部或外部的组件发布我们的应用程序。我们考察了不同的机制，但最终，决定哪些应用程序组件应该被暴露和访问是由您决定的。
- en: Throughout this chapter, we reviewed some quick solutions for debugging and
    publishing Service resources directly on our desktop computers with the `kubectl`
    client. We also examined different Service types that could be useful for locally
    accessing our remote applications on remote Kubernetes development clusters. We
    discussed how LoadBalancer Services are part of the Kubernetes core and were prepared
    for cloud platforms, due to which they may be difficult to implement on-premises,
    and this is why the recommended option for delivering applications is to create
    your own Ingress resource manifest. Ingress Controllers will help you to publish
    applications on any Kubernetes platform. You will use Ingress resources to define
    how applications will be published, and you may need to tweak their syntax according
    to the Ingress Controller deployed in your Kubernetes platform.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了一些快速解决方案，用于在我们的桌面计算机上直接调试和发布服务资源，使用`kubectl`客户端。我们还研究了不同的服务类型，这些类型对于在远程Kubernetes开发集群上本地访问远程应用程序可能很有用。我们讨论了LoadBalancer服务是Kubernetes核心的一部分，并且为云平台做好了准备，因此在本地部署可能会很困难，这就是为什么推荐的交付应用程序的选项是创建自己的Ingress资源清单。Ingress控制器将帮助您在任何Kubernetes平台上发布应用程序。您将使用Ingress资源定义应用程序的发布方式，并根据部署在您的Kubernetes平台上的Ingress控制器调整其语法。
- en: Toward the end of the chapter, we introduced the NetworkPolicy resource and
    the service mesh concept, which offers the means to improve the security of our
    applications by dropping any untrusted and undefined communications. This was
    followed by some labs to test what we learned.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们介绍了NetworkPolicy资源和服务网格概念，通过删除任何不受信任和未定义的通信方式，提供了改善应用程序安全性的手段。接着是一些实验室来测试我们所学到的内容。
- en: In the next chapter, we will review some useful mechanisms and tools for monitoring
    and gathering performance data from our applications.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾一些有用的机制和工具，用于监控和收集应用程序的性能数据。
