<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer029">
<h1 class="chapter-number" id="_idParaDest-16"><a id="_idTextAnchor015"/>1</h1>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Modern Infrastructure and Applications with Docker</h1>
<p>Software engineering and development is always evolving and introducing new technologies in its architectures and workflows. Software containers appeared more than a decade ago, becoming particularly popular over the last five years thanks to Docker, which made the concept mainstream. Currently, every enterprise manages its container-based application infrastructure in production in both the cloud and on-premises distributed infrastructures. This book will teach you how to increase your development productivity using software containers so that you can create, test, share, and run your applications. You will use a container-based workflow and your final application artifact will be a Docker image-based deployment, ready to run in <span class="No-Break">production environments.</span></p>
<p>This chapter will introduce software containers in the context of the current software development culture, which needs faster software supply chains made of moving, distributed pieces. We will review how containers work and how they fit into modern application architectures based on distributed components with very specific functionalities (microservices). This allows developers to choose the best language for each application component and distribute the total application load. We will learn about the kernel features that make software containers possible and learn how to create, share, and run application components as software containers. At the end of this chapter, we will learn about the different tools that can help us work with software containers and provide specific use cases for your laptop, desktop computer, <span class="No-Break">and servers.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Evolution of application architecture, from monoliths to distributed <span class="No-Break">microservice architectures</span></li>
<li>Developing <span class="No-Break">microservice-based applications</span></li>
<li>How containers fit in the <span class="No-Break">microservices model</span></li>
<li>Understanding the main concepts, features, and components of <span class="No-Break">software containers</span></li>
<li>Comparing virtualization <span class="No-Break">and containers</span></li>
<li>Building, sharing, and <span class="No-Break">running containers</span></li>
<li>Explaining <span class="No-Break">Windows containers</span></li>
<li>Improving security using <span class="No-Break">software containers</span></li>
</ul>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Technical requirements</h1>
<p>This book will teach you how to use software containers to improve your application development. We will use open source tools for building, sharing, and running containers, along with a few commercial ones that don’t require licensing for non-professional use. Also included in this book are some labs to help you practically understand the content that we’ll work through. These labs can be found at <a href="https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter1">https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter1</a><span class="hidden"><a id="_idTextAnchor018"/></span>. The <em class="italic">Code In Action</em> video for this chapter can be found <span class="No-Break">at </span><a href="https://packt.link/JdOIY"><span class="No-Break">https://packt.link/JdOIY</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor019"/>From monoliths to distributed microservice architectures</h1>
<p>Application architectures<a id="_idIndexMarker000"/> are continuously evolving due to technological improvements. Throughout the history of computation, every time a technical gap is resolved in hardware and software engineering, software architects rethink how applications can be improved to take advantage of the new developments. For example, network speed increases made distributing application components across different servers possible, and nowadays, it’s not even a problem to distribute these components across data centers in <span class="No-Break">multiple countries.</span></p>
<p>To take a quick look at how computers were adopted by enterprises, we must go back in time to the old mainframe days (before the 1990s). This can be considered the base for what we <a id="_idIndexMarker001"/>call <strong class="bold">unitary architecture</strong> – one big computer with all the processing functionality, accessed by users through terminals. Following this, the <strong class="bold">client-server</strong> model <a id="_idIndexMarker002"/>became very popular as technology also advanced on the user side. Server technologies improved while clients gained more and more functionality, freeing up the server load for publishing applications. We consider both<a id="_idIndexMarker003"/> models as <strong class="bold">monoliths</strong> as all application components run on one server; even if the databases are decoupled from the rest of the components, running all important components in a dedicated server is still considered monolithic. Both of these models were very difficult to upgrade when performance started to drop. In these cases, newer hardware with higher specifications was always required. These models also suffer from availability issues, meaning that any maintenance tasks required <a id="_idIndexMarker004"/>on either the server or application layer will probably lead to service outages, which affects the normal <span class="No-Break">system uptime.</span></p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor020"/>Exploring monolithic applications</h2>
<p><strong class="bold">Monolithic applications</strong> are <a id="_idIndexMarker005"/>those in which all functionalities are provided by just one component, or a set of them so tightly integrated that they cannot be decoupled from one another. This makes them hard to maintain. They weren’t designed with reusability or modularity in mind, meaning that every time developers need to fix an issue, add some new functionality, or change an application’s behavior, the entire application is affected due to, for example, having to recompile the whole <span class="No-Break">application’s code.</span></p>
<p>Providing high availability to monolithic applications required duplicated hardware, quorum resources, and continuous visibility between application nodes. This may not have changed too much today but we have many other resources for providing high availability. As applications grew in complexity and gained responsibility for many tasks and functionalities, we started to decouple them into a few smaller components (with specific functions such as the web server, database, and more), although core components were kept immutable. Running all application components together on the same server was better than distributing them into smaller pieces because network communication speeds weren’t high enough. Local filesystems were usually used for sharing information between application processes. These applications were difficult to scale (more hardware resources were required, usually leading to acquiring newer servers) and difficult to upgrade (testing, staging, and certification environments before production require the same hardware or at least compatible ones). In fact, some applications could run only on specific hardware and operating system versions, and developers needed workstations or servers with the same hardware or operating system to be able to develop fixes or new functionality for <span class="No-Break">these applications.</span></p>
<p>Now that we know<a id="_idIndexMarker006"/> how applications were designed in the early days, let’s introduce virtualization in <span class="No-Break">data centers.</span></p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor021"/>Virtual machines</h2>
<p>The concept of <strong class="bold">virtualization</strong> – providing a set of physical hardware resources for specific <a id="_idIndexMarker007"/>purposes – was<a id="_idIndexMarker008"/> already present in the mainframe days before the 1990s, but in those days, it was closer to the <a id="_idIndexMarker009"/>definition of <strong class="bold">time-sharing</strong> at the compute level. The concept we commonly associate with virtualization comes from the introduction <a id="_idIndexMarker010"/>of the <strong class="bold">hypervisor</strong> and the new technology introduced in the late 1990s that allowed for the creation of complete virtual servers running their own virtualized operating systems. This hypervisor software component was able to virtualize and share host resources in virtualized guest operating systems. In the 1990s, the adoption of Microsoft Windows and the emergence of Linux as a server operating system in the enterprise world established x86 servers as the industry standard, and virtualization helped the growth of both of these in our data centers, improving hardware usage and server upgrades. The virtualization layer simplified virtual hardware upgrades when applications required more memory or CPU and also improved the process of providing services with high availability. Data centers became smaller as newer servers could run dozens of virtual servers, and as physical servers’ hardware capabilities increased, the number of virtualized servers per <span class="No-Break">node increased.</span></p>
<p>In the late 1990s, the servers became services. This means that companies started to think about the services they provided instead of the way they did it. Cloud providers arrived to provide services to small businesses that didn’t want to acquire and maintain their own data centers. Thus, a new architecture model was created, which became pretty <a id="_idIndexMarker011"/>popular: the <strong class="bold">cloud computing infrastructure</strong> model. Amazon <a id="_idIndexMarker012"/>launched <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), providing storage, computation, databases, and other infrastructure resources. And pretty soon after that, Elastic Compute Cloud entered the arena of virtualization, allowing you to run your own servers with a few clicks. Cloud providers also allowed users to use their well-documented <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) for<a id="_idIndexMarker013"/> automation, and the<a id="_idIndexMarker014"/> concept of <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>) was introduced. We were able to create our virtualization instances using programmatic and reusable code. This model also changed the service/hardware relationship and what started as a good idea at first – using cloud platforms for every enterprise service – became a problem for big enterprises, which saw increased costs pretty quickly based on network bandwidth usage and as a result of not sufficiently controlling their use of cloud resources. Controlling cloud service costs soon became a priority for many enterprises, and many open source projects started with the premise of providing cloud-like<a id="_idIndexMarker015"/> infrastructures. <strong class="bold">Infrastructure elasticity</strong> and <strong class="bold">easy provisioning</strong> are the<a id="_idIndexMarker016"/> keys to these projects. OpenStack was the first one, distributed in smaller projects, each one focused on different functionalities (storage, networking, compute, provisioning, and so on). The idea of having on-premises cloud infrastructure led software and infrastructure vendors into new alliances with each other, in the end providing new technologies for data centers with the required flexibility and resource distribution. They also provided APIs for quickly deploying and managing provisioned infrastructure, and nowadays, we can provision either <a id="_idIndexMarker017"/>cloud infrastructure resources or resources on our data centers using the same code with <span class="No-Break">few changes.</span></p>
<p>Now that we have a good idea of how server infrastructures work today, let’s go back <span class="No-Break">to applications.</span></p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor022"/>Three-tier architecture</h2>
<p>Even with these <a id="_idIndexMarker018"/>decoupled infrastructures, applications can still be monoliths if we don’t prepare them for separation into different components. Elastic infrastructures allow us to distribute resources and it would be nice to have distributed components. Network communications are essential and technological evolution has increased speeds, allowing us to consume network-provided services as if they were local and facilitating the use of <span class="No-Break">distributed components.</span></p>
<p><strong class="bold">Three-tier architecture</strong> is a software application architecture where the application is decoupled into three to five logical and physical computing layers. We have the <strong class="bold">presentation tier</strong>, or<a id="_idIndexMarker019"/> user<a id="_idIndexMarker020"/> interface; the <strong class="bold">application tier</strong>, or backend, where data is processed; and <a id="_idIndexMarker021"/>the <strong class="bold">data tier</strong>, where the data for use in the application is stored and managed, such as in a database. This model was used even before virtualization arrived on the scene, but you can imagine the improvement of being able to distribute application components across different virtual servers instead of increasing the number of servers in your <span class="No-Break">data center.</span></p>
<p>Just to recap before continuing our journey: the evolution of infrastructure and network communications has allowed us to run component-distributed applications, but we just have a few components per application in the three-tier model. Note that in this model, different roles are involved in application maintenance as different software technologies are usually employed. For example, we need database administrators, middleware administrators, and infrastructure administrators for systems and network communications. In this model, although we are still forced to use servers (virtual or physical), application component maintenance, scalability, and availability are significantly improved. We can manage each component in isolation, executing different maintenance tasks and fixes and adding new functionalities decoupled from the application core. In this model, developers can focus on either frontend or backend components. Some coding languages are specialized for each layer – for example, JavaScript was the language of choice for frontend developers (although it evolved for backend <span class="No-Break">services too).</span></p>
<p>As Linux systems grew in popularity in the late 1990s, applications were distributed into different components, and eventually different applications working together and running in different operating systems became a new requirement. Shared files, provided by network filesystems<a id="_idIndexMarker022"/> using either <strong class="bold">network-attached storage</strong> (<strong class="bold">NAS</strong>) or more <a id="_idIndexMarker023"/>complex <strong class="bold">storage area network</strong> (<strong class="bold">SAN</strong>) storage backends were used at first, but <strong class="bold">Simple Object Access Protocol</strong> (<strong class="bold">SOAP</strong>) and<a id="_idIndexMarker024"/> other queueing message technologies helped applications to distribute data between components and manage their information without filesystem<a id="_idIndexMarker025"/> interactions. This helped decouple applications into more and more distributed components running on top of different <span class="No-Break">operating systems.</span></p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor023"/>Microservices architecture</h2>
<p>The <strong class="bold">microservices architecture</strong> model <a id="_idIndexMarker026"/>goes a step further, decoupling applications into smaller pieces with enough functionality to be considered components. This model allows us to manage a completely independent component life cycle, freeing us to choose whatever coding language fits best with the functionality in question. Application components are kept light in terms of functionality and content, which should lead to them using fewer host resources and responding faster to start and stop commands. Faster restarts are key to resilience and help us maintain our applications while up, with fewer outages. Application health should not depend on component-external infrastructure; we should improve components’ logic and resilience so that they can start and stop as fast as possible. This means that we can ensure that changes to an application are applied quickly, and in the case of failure, the required processes will be up and running in seconds. This also helps in managing the application components’ life cycle as we can upgrade components very fast and prepare circuit breakers to manage <span class="No-Break">stopped dependencies.</span></p>
<p>Microservices<a id="_idIndexMarker027"/> use the <strong class="bold">stateless</strong> paradigm; therefore, application components should be stateless. This means that a microservice’s state must be abstracted from its logic or execution. This is key to being able to run multiple replicas of an application component, allowing us to run them distributed on different nodes from <span class="No-Break">a pool.</span></p>
<p>This model also introduced the concept of <em class="italic">run everywhere</em>, where an application should be able to run its components on either cloud or on-premise infrastructures, or even a mix of both (for example, the presentation layer for components could run on cloud infrastructure<a id="_idIndexMarker028"/> while the data resides in our <span class="No-Break">data center).</span></p>
<p>Microservices architecture provides the following <span class="No-Break">helpful features:</span></p>
<ul>
<li>Applications are decoupled<a id="_idIndexMarker029"/> into different smaller pieces that provide different features or functionalities; thus, we can change any of them at any time without impacting the <span class="No-Break">whole application.</span></li>
<li>Decoupling applications into smaller pieces lets developers focus on specific functionalities and allows them to use the most appropriate programming language for <span class="No-Break">each component.</span></li>
<li>Interaction between application components is usually provided via <strong class="bold">Representational State Transfer</strong> (<strong class="bold">REST</strong>) API <a id="_idIndexMarker030"/>calls using HTTP. RESTful systems aim for fast performance and reliability and can scale without <span class="No-Break">any problem.</span></li>
<li>Developers describe which methods, actions, and data they provide in their microservice, which are then consumed by other developers or users. Software architects must standardize how application components talk with each other and how microservices <span class="No-Break">are consumed.</span></li>
<li>Distributing application components across different nodes allows us to group microservices into nodes for the best performance, closer to data sources and with better security. We<a id="_idIndexMarker031"/> can create nodes with different features to provide the best fit for our <span class="No-Break">application components.</span></li>
</ul>
<p>Now that we’ve learned what microservices architecture is, let’s take a look at its impact on the <span class="No-Break">development process.</span></p>
<h1 id="_idParaDest-24"><a id="_idTextAnchor024"/>Developing distributed applications</h1>
<p>Monolith<a id="_idIndexMarker032"/> applications, as we saw in the previous section, are applications in which all functionalities run together. Most of these applications were created for specific hardware, operating systems, libraries, binary versions, and so on. To run these applications in production, you need a least one dedicated server with the right hardware, operating system, libraries, and so on, and developers require a similar node architecture and resources even just for fixing possible application issues. Adding to this, the pre-production environments for tasks such as certification and testing will multiply the number of servers significantly. Even if your enterprise had the budget for all these servers, any maintenance task as a result of any upgrade in any operating system-related component in production should always be replicated on all other environments. Automation helps in replicating changes between environments, but this is not easy. You have to replicate environments and maintain them. On the other hand, new node provisioning could have taken months in the old days (preparing the specifications for a new node, drawing up the budget, submitting it to your company’s approvals workflow, looking for a hardware provider, and so on). Virtualization helped system administrators provision new nodes for developers faster, and automation (provided by tools such as Chef, Puppet, and, my favorite, Ansible) allowed for the alignment of changes between all environments. Therefore, developers were able to obtain their development environments quickly and ensure they were using an aligned version of system resources, improving the process of <span class="No-Break">application maintenance.</span></p>
<p>Virtualization also worked very well with the three-tier application architecture. It was easy to run application components for developers in need of a database server to connect to while coding new changes. The problem with virtualization comes from the concept of replicating a complete operating system with server application components when we only need the software part. A lot of hardware resources are consumed for the operating system alone, and restarting these nodes takes some time as they are a complete operating system running on top of a hypervisor, itself running on a physical server with its own <span class="No-Break">operating system.</span></p>
<p>Anyhow, developers were hampered by outdated operating system releases and packages, making it difficult for them to enable the evolution of their applications. System administrators started to manage hundreds of virtual hosts and even with automation, they weren’t able to maintain operating systems and application life cycles in alignment. Provisioning virtual machines on cloud providers using their <strong class="bold">Infrastructure-as-a-Service</strong> (<strong class="bold">IaaS</strong>) platforms <a id="_idIndexMarker033"/>or using their <strong class="bold">Platform-as-a-Service</strong> (<strong class="bold">PaaS</strong>) environments <a id="_idIndexMarker034"/>and scripting the infrastructure using their APIs (IaC) helped but the problem wasn’t fully resolved due to the quickly growing number of applications and required changes. The application life cycle changed from one or two updates per year to dozens <span class="No-Break">per day.</span></p>
<p>Developers started <a id="_idIndexMarker035"/>to use cloud-provided services and using scripts and applications quickly became more important than the infrastructure on which they were running, which today seems completely normal and logical. Faster network communications and distributed reliability made it easier to start deploying our applications anywhere, and data centers became smaller. We can say that developers started this movement and it became so popular that we finished decoupling application components from the underlying <span class="No-Break">operating systems.</span></p>
<p>Software containers are the evolution of process isolation features that were learned throughout the development of computer history. Mainframe computers allowed us to share CPU time and memory resources many years ago. Chroot and jail environments were common ways of sharing operating system resources with users, who were able to use all the binaries and libraries prepared for them by system administrators in BSD operating systems. On Solaris systems, we <a id="_idIndexMarker036"/>had <strong class="bold">zones</strong> as resource containers, which acted as completely isolated virtual servers within a single operating <span class="No-Break">system instance.</span></p>
<p>So, why don’t we just isolate processes instead of full operating systems? This is the main idea behind containers. Containers use kernel features to provide process isolation at the operating system level, and all processes run on the same host but are isolated from each other. So, every process has its own set of resources sharing the same <span class="No-Break">host kernel.</span></p>
<p>Linux kernels have featured this design of process grouping since the late 2000s in the form of <strong class="bold">control groups</strong> (<strong class="bold">cgroups</strong>). This<a id="_idIndexMarker037"/> feature allows the Linux kernel to manage, restrict, and audit groups <span class="No-Break">of processes.</span></p>
<p>Another very important Linux kernel feature that’s used with containers is <strong class="bold">kernel namespaces</strong>, which <a id="_idIndexMarker038"/>allow Linux to run processes wrapped with their process hierarchy, along with their own network interfaces, users, filesystem mounts, and inter-process communication. Using kernel namespaces and control groups, we can completely isolate a process within an operating system. It will run as if it were on its own, using its own operating system and limited CPU and memory (we can even limit its <span class="No-Break">disk I/O).</span></p>
<p>The <strong class="bold">Linux Containers</strong> (<strong class="bold">LXC</strong>) project <a id="_idIndexMarker039"/>took this idea further and created the first working implementation of it. This project is still available, is still in progress, and was the key to what we now <a id="_idIndexMarker040"/>know as <strong class="bold">Docker containers</strong>. LXC introduced <a id="_idIndexMarker041"/>terms such as <strong class="bold">templates</strong> to describe the creation of encapsulated processes using <span class="No-Break">kernel namespaces.</span></p>
<p>Docker containers took all these concepts and created Docker Inc., an open source project that made it easy to run software containers on our systems. Containers ushered in a great revolution, just as virtualization did more than 20 <span class="No-Break">years ago.</span></p>
<p>Going back to microservices architecture, the ideal application decoupling would mean running defined <a id="_idIndexMarker042"/>and specific application functionalities as completely standalone and isolated processes. This led to the idea of running microservice applications’ components within containers, with minimum operating <span class="No-Break">system overhead.</span></p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor025"/>What are containers?</h1>
<p>We can define a <a id="_idIndexMarker043"/>container as a process with all its requirements isolated using cgroups and namespace kernel features. A <strong class="bold">process</strong> is the<a id="_idIndexMarker044"/> way we execute a task within the operating system. If we <a id="_idIndexMarker045"/>define a <strong class="bold">program</strong> as the set of instructions developed using a programming language, included in an executable format on disk, we can say that a process is a program <span class="No-Break">in action.</span></p>
<p>The execution of a process involves the use of some system resources, such as CPU and memory, and although it runs on its own environment, it can use the same information as other processes sharing the same <span class="No-Break">host system.</span></p>
<p>Operating systems provide tools for manipulating the behavior of processes during execution, allowing system administrators to prioritize the critical ones. Each process running on a system is uniquely<a id="_idIndexMarker046"/> identified by a <strong class="bold">Process Identifier</strong> (<strong class="bold">PID</strong>). A parent-child relationship between processes is developed when one process executes a new process (or creates a new thread) during its execution. The new process (or sub-process) that’s created will have as its parent the previous one, and so on. The operating system stores information about process relations using PIDs and parent PIDs. Processes may inherit a parent hierarchy from the user who runs them, so users own and manage their own processes. Only administrators and privileged users can interact with other users’ processes. This behavior also applies to child processes created by <span class="No-Break">our executions.</span></p>
<p>Each process runs on its own environment and we can manipulate its behavior using operating system features. Processes can access files as needed and use pointers to descriptors during execution to manage these <span class="No-Break">filesystem resources.</span></p>
<p>The operating system kernel manages all processes, scheduling them on its physical or virtualized CPUs, giving them appropriate CPU time, and providing them with memory or network resources (<span class="No-Break">among others).</span></p>
<p>These definitions are<a id="_idIndexMarker047"/> common to all modern operating systems and are key for understanding software containers, which we will discuss in detail in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor026"/>Understanding the main concepts of containers</h2>
<p>We have learned that<a id="_idIndexMarker048"/> as opposed to virtualization, containers are processes running in isolation and sharing the host operating system kernel. In this section, we will review the components that make <span class="No-Break">containers possible.</span></p>
<h3>Kernel process isolation</h3>
<p>We already introduced <a id="_idIndexMarker049"/>kernel process namespace<a id="_idIndexMarker050"/> isolation as a key feature for running software containers. Operating system kernels provide <a id="_idIndexMarker051"/>namespace-based <strong class="bold">isolation</strong>. This feature has been present in Linux kernels since 2006 and provides different layers of isolation associated with the properties or attributes a process has when it runs on a host. When we apply these namespaces to processes, they will run their own set of properties and will not see the other processes running alongside them. Hence, kernel resources are partitioned such that each set of processes sees different sets of resources. Resources may exist in multiple spaces and processes may <span class="No-Break">share them.</span></p>
<p>Containers, as they are host processes, run with their own associated set of kernel namespaces, such as <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Processes</strong>: The container’s main process is the parent of others within the container. All these processes share the same <span class="No-Break">process namespace.</span></li>
<li><strong class="bold">Network</strong>: Each container receives a network stack with unique interfaces and IP addresses. Processes (or containers) sharing the same network namespace will get the same IP address. Communications between containers pass through host <span class="No-Break">bridge interfaces.</span></li>
<li><strong class="bold">Users</strong>: Users within containers are unique; therefore, each container gets its own set of users, but these users are mapped to real host <span class="No-Break">user identifiers.</span></li>
<li><strong class="bold">Inter-process communication</strong> (<strong class="bold">IPC</strong>): Each container receives its own set of shared <a id="_idIndexMarker052"/>memory, semaphores, and message queues so that it doesn’t conflict with other processes on <span class="No-Break">the host.</span></li>
<li><strong class="bold">Mounts</strong>: Each container mounts a root filesystem; we can also attach remote and host <span class="No-Break">local mounts.</span></li>
<li> <strong class="bold">Unix time-sharing</strong> (<strong class="bold">UTS</strong>): Each container is assigned a hostname and the time is <a id="_idIndexMarker053"/>synced with the <span class="No-Break">underlying host.</span></li>
</ul>
<p>Processes running inside a container sharing the same process kernel namespace will receive PIDs as if they were running alone inside their own kernel. The container’s main process is assigned PID 1 and other sub-processes or threads will get subsequent IDs, inheriting the main process hierarchy. The container will die if the main process dies (or <span class="No-Break">is stopped).</span></p>
<p>The following diagram shows how our system manages container PIDs inside the container’s PID<a id="_idIndexMarker054"/> namespace (represented by the gray box) and <a id="_idIndexMarker055"/>outside, at the <span class="No-Break">host level:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<img alt="Figure 1.1 – Schema showing a hierarchy of PIDs when you execute an NGINX web server with four worker processes" height="371" src="image/B19845_01_01.jpg" width="652"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Schema showing a hierarchy of PIDs when you execute an NGINX web server with four worker processes</p>
<p>In the preceding figure, the main process running inside a container is assigned PID 1, while the other<a id="_idIndexMarker056"/> processes are its children. The host runs its <a id="_idIndexMarker057"/>own PID 1 process and all other processes run in association with this <span class="No-Break">initial process.</span></p>
<h3>Control groups</h3>
<p>A <strong class="bold">cgroup</strong> is a feature <a id="_idIndexMarker058"/>provided by the Linux kernel that enables <a id="_idIndexMarker059"/>us to limit and isolate the host resources associated with processes (such as CPU, memory, and disk I/O). This provides the <span class="No-Break">following features:</span></p>
<ul>
<li><strong class="bold">Resource limits</strong>: Host resources<a id="_idIndexMarker060"/> are limited by using a cgroup and thus, the number of resources that a process can use, including CPU <span class="No-Break">or memory</span></li>
<li><strong class="bold">Prioritization</strong>: If resource contention is observed, the amount of host resources (CPU, disk, or network) that a process can use compared to processes in another cgroup can <span class="No-Break">be controlled</span></li>
<li><strong class="bold">Accounting</strong>: Cgroups monitor and report resource limits usage at the <span class="No-Break">cgroup level</span></li>
<li><strong class="bold">Control</strong>: We can manage the status of all processes in <span class="No-Break">a cgroup</span></li>
</ul>
<p>The isolation of cgroups will not allow containers to bring down a host by exhausting its resources. An interesting fact is that you can use cgroups without software containers just by mounting a cgroup (cgroup type system), adjusting the CPU limits of this group, and finally adding a set of PIDs to this group. This procedure will apply to either cgroups-V1 or the <span class="No-Break">newer cgroups-V2.</span></p>
<h3>Container runtime</h3>
<p>A <strong class="bold">container runtime</strong>, or <strong class="bold">container engine</strong>, is a <a id="_idIndexMarker061"/>piece <a id="_idIndexMarker062"/>of <a id="_idIndexMarker063"/>software that runs containers on a host. It is responsible for downloading container images from a registry to create containers, monitoring the resources available in the host to run the images, and managing the isolation layers provided by the operating system. The container runtime also reviews the current status of containers and manages their life cycle, starting again when their main process dies (if we declare them to be available whenever <span class="No-Break">this happens).</span></p>
<p>We generally group <a id="_idIndexMarker064"/>container <a id="_idIndexMarker065"/>runtimes into <strong class="bold">low-level runtimes</strong> and <span class="No-Break"><strong class="bold">high-level runtimes</strong></span><span class="No-Break">.</span></p>
<p>Low-level runtimes are those simple runtimes focused only on software container execution. We can<a id="_idIndexMarker066"/> consider <strong class="bold">runC</strong> and <strong class="bold">crun</strong> in this<a id="_idIndexMarker067"/> group. Created by Docker and the <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OCI</strong>), runC is <a id="_idIndexMarker068"/>still the de facto standard. Red Hat created crun, which is faster than runC with a lower memory footprint. These low-level runtimes do not require container images to run – we can use a configuration file and a folder with our application and all its required files (which is the content of a Docker image, but without any metadata information). This folder usually contains a file structure resembling a Linux root filesystem, which, as we mentioned before, is everything required by an application (or component) to work. Imagine that we execute the <strong class="source-inline">ldd</strong> command on our binaries and libraries and iterate this process with all its dependencies, and so on. We will get a complete list of all the files strictly required for the process and this would become the smallest image for <span class="No-Break">the application.</span></p>
<p>High-level container runtimes usually implement the <strong class="bold">Container Runtime Interface</strong> (<strong class="bold">CRI</strong>) specification <a id="_idIndexMarker069"/>of the OCI. This was created to make container orchestration more runtime-agnostic. In this group, we have Docker, CRI-O, and <span class="No-Break">Windows/Hyper-V containers.</span></p>
<p>The CRI interface defines the rules so that we can integrate our container runtimes into container orchestrators, such as Kubernetes. Container runtimes should have the <span class="No-Break">following characteristics:</span></p>
<ul>
<li>Be capable of <span class="No-Break">starting/stopping pods</span></li>
<li>Deal with all containers (start, pause, stop, and <span class="No-Break">delete them)</span></li>
<li>Manage <span class="No-Break">container images</span></li>
<li>Provide metrics collection and access to <span class="No-Break">container logs</span></li>
</ul>
<p>The Docker <a id="_idIndexMarker070"/>container<a id="_idIndexMarker071"/> runtime became mainstream in 2016, making the execution of containers very easy for users. CRI-O was created explicitly for the Kubernetes orchestrator by Red Hat to allow the execution of containers using any OCI-compliant low-level runtime. High-level runtimes provide tools for interacting with them, and that’s why most people <span class="No-Break">choose them.</span></p>
<p>A middle ground between low-level and high-level container runtimes is provided by Containerd, which is an industry-standard container runtime. It runs on Linux and Windows and can manage the complete container <span class="No-Break">life cycle.</span></p>
<p>The technology behind runtimes is evolving very fast; we can even improve the interaction between containers and hosts using sandboxes (<strong class="bold">gVisor</strong> from <a id="_idIndexMarker072"/>Google) and virtualized <a id="_idIndexMarker073"/>runtimes (<strong class="bold">Kata Containers</strong>). The former increases containers’ isolation by not sharing the host’s kernel with them. A specific kernel (the small <strong class="bold">unikernel</strong> with <a id="_idIndexMarker074"/>restricted capabilities) is provided to containers as a proxy to the real kernel. Virtualized runtimes, on the other hand, use virtualization technology to isolate a container within a very small virtual machine. Although both cases add some load to the underlying operating system, security is increased as containers don’t interact directly with the <span class="No-Break">host’s kernel.</span></p>
<p>Container runtimes<a id="_idIndexMarker075"/> only review the main process <a id="_idIndexMarker076"/>execution. If any other process running inside a container dies and the main process isn’t affected, the container will <span class="No-Break">continue running.</span></p>
<h3>Kernel capabilities</h3>
<p>Starting with Linux<a id="_idIndexMarker077"/> kernel release 2.2, the operating system <a id="_idIndexMarker078"/>divides process privileges into distinct units, known as <strong class="bold">capabilities</strong>. These <a id="_idIndexMarker079"/>capabilities can be enabled or disabled by operating system and <span class="No-Break">system administrators.</span></p>
<p>Previously, we learned that containers run processes in isolation using the host’s kernel. However, it is important to know that only a restricted set of these kernel capabilities are allowed inside containers unless they are explicitly declared. Therefore, containers improve their processes’ security at the host level because those processes can’t do anything they want. The capabilities that are currently available inside a container running on top of the Docker container runtime are <strong class="source-inline">SETPCAP</strong>, <strong class="source-inline">MKNOD</strong>, <strong class="source-inline">AUDIT_WRITE</strong>, <strong class="source-inline">CHOWN</strong>, <strong class="source-inline">NET_RAW</strong>, <strong class="source-inline">DAC_OVERRIDE</strong>, <strong class="source-inline">FOWNER</strong>, <strong class="source-inline">FSETID</strong>, <strong class="source-inline">KILL</strong>, <strong class="source-inline">SETGID</strong>, <strong class="source-inline">SETUID</strong>, <strong class="source-inline">NET_BIND_SERVICE</strong>, <strong class="source-inline">SYS_CHROOT</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">SETFCAP</strong></span><span class="No-Break">.</span></p>
<p>This set of capabilities allows, for example, processes inside a container to attach and listen on ports below <strong class="source-inline">1024</strong> (the <strong class="source-inline">NET_BIND_SERVICE</strong> capability) or use ICMP (the <span class="No-Break"><strong class="source-inline">NET_RAW</strong></span><span class="No-Break"> capability).</span></p>
<p>If our process inside a container requires us to, for example, create a new network interface (perhaps to run a containerized OpenVPN server), the <strong class="source-inline">NET_ADMIN</strong> capability should <span class="No-Break">be included.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Container runtimes allow containers to run with full privileges using special parameters. The processes within these <a id="_idIndexMarker080"/>containers will run with all kernel capabilities and it could be very dangerous. You should avoid using privileged containers – it is best to take some time to verify which capabilities are needed by an application to <span class="No-Break">work correctly.</span></p>
<h3>Container orchestrators</h3>
<p>Now that we know <a id="_idIndexMarker081"/>that we need a runtime to execute <a id="_idIndexMarker082"/>containers, we must also understand that this will work in a standalone environment, without hardware high availability. This means that server maintenance, operating system upgrades, and any other problem at the software, operating system, or hardware levels may affect <span class="No-Break">your application.</span></p>
<p>High availability requires resource duplicity and thus more servers and/or hardware. These resources will allow containers to run on multiple hosts, each one with a container runtime. However, maintaining application availability in this situation isn’t easy. We need to ensure that containers will be able to run on any of these nodes; in the <em class="italic">Overlay filesystems</em> section, we’ll learn that synchronizing container-related resources within nodes involves more than just copying a few files. <strong class="bold">Container orchestrators</strong> manage node resources and provide them to containers. They schedule containers as needed, take care of their status, provide resources for persistence, and manage internal and external communications (in <a href="B19845_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Fundamentals of Orchestration</em>, we will learn how some orchestrators delegate some of these features to different modules to optimize <span class="No-Break">their work).</span></p>
<p>The most famous and widely used container orchestrator <a id="_idIndexMarker083"/>today is <strong class="bold">Kubernetes</strong>. It has a lot of great features to help manage clustered containers, although the learning curve can be tough. Also, <strong class="bold">Docker Swarm</strong> is<a id="_idIndexMarker084"/> quite simple and allows you to quickly execute your applications with high availability (or resilience). We will cover both in detail in <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating with Swarm</em>, and <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the Kubernetes Orchestrator</em>. There were other opponents in this race but they stayed by the wayside while Kubernetes took <span class="No-Break">the lead.</span></p>
<p>HashiCorp’s <strong class="bold">Nomad</strong> and <a id="_idIndexMarker085"/>Apache’s <strong class="bold">Mesos</strong> are still<a id="_idIndexMarker086"/> being used for very special projects but are out of scope for most enterprises and users. Kubernetes and Docker Swarm are community projects and some vendors even include them within their enterprise-ready solutions. Red <a id="_idIndexMarker087"/>Hat’s <strong class="bold">OpenShift</strong>, SUSE’s <strong class="bold">Rancher</strong>, Mirantis’ <strong class="bold">Kubernetes Engine</strong> (old Docker <a id="_idIndexMarker088"/>Enterprise platform), and<a id="_idIndexMarker089"/> VMware’s <strong class="bold">Tanzu</strong>, among others, all provide on-premises<a id="_idIndexMarker090"/> and some cloud-prepared custom Kubernetes platforms. But those who made Kubernetes the most-used platform were the well-known cloud providers – Google, Amazon, Azure, and Alibaba, among others, serve their own container orchestration tools, such<a id="_idIndexMarker091"/> as <a id="_idIndexMarker092"/>Amazon’s <strong class="bold">Elastic Container Service</strong> or <strong class="bold">Fargate</strong>, Google’s <strong class="bold">Cloud Run</strong>, and<a id="_idIndexMarker093"/> Microsoft’s <strong class="bold">Azure Container Instances</strong>, and they<a id="_idIndexMarker094"/> also package and manage their own Kubernetes infrastructures for us to use (Google’s GKE, Amazon’s EKS, Microsoft’s AKS, and so on). They provide <strong class="bold">Kubernetes-as-a-Service</strong> platforms <a id="_idIndexMarker095"/>where you only need an account to start deploying your applications. They also serve you storage, advanced networking tools, resources for publishing your applications, and even <em class="italic">follow-the-sun</em> or worldwide <span class="No-Break">distributed architectures.</span></p>
<p>There are many Kubernetes implementations. The most popular is probably OpenShift or its open source project, OKD. There are others based on a binary that launches and creates all of the Kubernetes components using automated procedures, such as Rancher RKE (or its government-prepared release, RKE2), and those featuring only the strictly necessary Kubernetes components, such as K3S or K0S, to provide the lightest platform for IoT and more modest hardware. And finally, we have some Kubernetes distributions for desktop computers, offering all the features of Kubernetes ready to develop and test applications with. In this group, we have Docker Desktop, Rancher Desktop, Minikube, and <strong class="bold">Kubernetes in Docker</strong> (<strong class="bold">KinD</strong>). We <a id="_idIndexMarker096"/>will learn how to use them in this book to develop, package, and prepare applications <span class="No-Break">for production.</span></p>
<p>We shouldn’t forget solutions for running orchestrated applications based on multiple containers on standalone servers or desktop computers, such <a id="_idIndexMarker097"/>as <strong class="bold">Docker Compose</strong>. Docker has prepared a simple Python-based orchestrator for quick application development, managing the<a id="_idIndexMarker098"/> container dependencies for us. It is very convenient for testing all of our components<a id="_idIndexMarker099"/> together on a laptop with minimum overhead, instead of running a full Kubernetes or Swarm cluster. We will cover this tool, seeing as it has evolved a lot and is now part of the common Docker client command line, in <a href="B19845_05.xhtml#_idTextAnchor118"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Creating </em><span class="No-Break"><em class="italic">Multi-Container Applications</em></span><span class="No-Break">.</span></p>
<h3>Container images</h3>
<p>Earlier in this chapter, we<a id="_idIndexMarker100"/> mentioned that containers run<a id="_idIndexMarker101"/> thanks to <strong class="bold">container images</strong>, which are used as templates for executing processes in isolation and attached to a filesystem; therefore, a container image contains all the files (binaries, libraries, configurations, and so on) required by its processes. These files can be a subset of some operating system or just a few binaries with configurations built <span class="No-Break">by yourself.</span></p>
<p>Virtual machine templates are immutable, as are container templates. This immutability means that they don’t change between executions. This feature is key because it ensures that we get the same results every time we use an image for creating a container. Container behavior can be changed using configurations or command-line arguments through the container runtime. This ensures that images created by developers will work in production as expected, and moving applications to production (or even creating upgrades between different releases) will be smooth and fast, reducing the time <span class="No-Break">to market.</span></p>
<p>Container images are a collection of files distributed in layers. We shouldn’t add anything more than the files required by the application. As images are immutable, all these layers will be presented to containerized processes as read-only sets of files. But we don’t duplicate files between layers. Only files modified on one layer will be stored in the next layer above – this way, each layer retains the changes from the original base layer (referenced as<a id="_idIndexMarker102"/> the <span class="No-Break">base image).</span></p>
<p>The following diagram shows how we create a container image using <span class="No-Break">multiple layers:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer009">
<img alt="Figure 1.2 – Schema of stacked layers representing a container image" height="415" src="image/B19845_01_02.jpg" width="1092"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Schema of stacked layers representing a container image</p>
<p>A base layer is always included, although it could be empty. The layers above this base layer may include new binaries or just include new meta-information (which does not create a layer but a <span class="No-Break">meta-information modification).</span></p>
<p>To easily share these templates between computers or even environments, these file layers are packaged into <strong class="source-inline">.tar</strong> files, which are finally what we call images. These packages contain all layered files, along with meta-information that describes the content, specifies the process <a id="_idIndexMarker103"/>to be executed, identifies the ports that will be exposed to communicate with other containerized processes, specifies the user who will own it, indicates the directories that will be kept out of container life cycle, and <span class="No-Break">so on.</span></p>
<p>We use different methods to create these images, but we aim to make the process reproducible, and thus we use Dockerfiles as recipes. In <a href="B19845_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Building Container Images</em>, we will learn about the image creation workflow while utilizing best practices and diving deep into <span class="No-Break">command-line options.</span></p>
<p>These container images are stored on registries. This application software is intended to store file layers and meta-information in a centralized location, making it easy to share common layers between different images. This means that two images using a common Debian base image (a subset of files from the complete operating system) will share these base files, thus optimizing disk space usage. This can also be employed on containers’ underlying host local filesystems, saving a lot <span class="No-Break">of space.</span></p>
<p>Another result of the use of these layers is that containers using the same template image to execute their processes will use the same set of files, and only those files that get modified will <span class="No-Break">be stored.</span></p>
<p>All these behaviors<a id="_idIndexMarker104"/> related to the optimized use of files<a id="_idIndexMarker105"/> shared between different images and containers are provided by operating systems thanks to <span class="No-Break">overlay filesystems.</span></p>
<h3>Overlay filesystems</h3>
<p>An <strong class="bold">overlay filesystem</strong> is a union <a id="_idIndexMarker106"/>mount filesystem (a way<a id="_idIndexMarker107"/> of combining multiple directories into one that appears to contain their whole combined content) that combines multiple underlying mount points. This results in a structure with a single directory that contains all underlying files and sub-directories from <span class="No-Break">all sources.</span></p>
<p>Overlay filesystems merge content from directories, combining the file objects (if any) yielded by different processes, with the <em class="italic">upper</em> filesystem taking precedence. This is the magic behind container-image layers’ reusability and disk <span class="No-Break">space saving.</span></p>
<p>Now that we understand how images are packaged and how they share content, let’s go back to learning a bit more about containers. As you may have learned in this section, containers are processes that run in isolation on top of a host operating system thanks to a container runtime. Although the kernel host is shared by multiple containers, features such as kernel namespaces and cgroups provide special containment layers that allow us to isolate them. Container processes need some files to work, which are included in the container space as immutable templates. As you may think, these processes will probably need to modify or create some new files found on container image layers, and a new read-write layer will be used to store these changes. The container runtime presents this new layer to the container to enable changes – we usually refer to this as<a id="_idIndexMarker108"/> the <span class="No-Break"><strong class="bold">container layer</strong></span><span class="No-Break">.</span></p>
<p>The following schema outlines the read-write layers coming from the container image template with the <a id="_idIndexMarker109"/>newly added container layer, where the container’s running processes store their <span class="No-Break">file modifications:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<img alt="Figure 1.3 – Container image layers will always be read-only; the container adds a new layer with read-write capabilities" height="410" src="image/B19845_01_03.jpg" width="829"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Container image layers will always be read-only; the container adds a new layer with read-write capabilities</p>
<p>The changes made by container processes are always <em class="italic">ephemeral</em> as the container layer will be lost whenever we remove the container, while image layers are immutable and will remain unchanged. With this behavior in mind, it is easy to understand that we can run multiple containers <a id="_idIndexMarker110"/>using the same <span class="No-Break">container image.</span></p>
<p>The following figure represents this situation where three different running containers were created from the <span class="No-Break">same image:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer011">
<img alt="Figure 1.4 – Three different containers run using the same container image" height="354" src="image/B19845_01_04.jpg" width="824"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Three different containers run using the same container image</p>
<p>As you may have <a id="_idIndexMarker111"/>noticed, this behavior leaves a very small footprint on our operating systems in terms of disk space. Container layers are very small (or at least they should be, and you as a developer will learn which files shouldn’t be left inside the container <span class="No-Break">life cycle).</span></p>
<p>Container runtimes manage how these overlay folders will be included inside containers and the magic behind that. The mechanism for this is based on specific operating system drivers that <a id="_idIndexMarker112"/>implement <strong class="bold">copy-on-write</strong> filesystems. Layers are arranged one on top of the other and only files modified within them are merged on the upper layer. This process is managed at speed by operating system drivers, but some small overhead is always expected, so keep in mind that all files that are modified continuously by your application (logs, for example) should never be part of <span class="No-Break">the container.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout"><em class="italic">Copy-on-write</em> uses small layered filesystems or folders. Files from any layer are accessible to read access, but <em class="italic">write</em> requires searching for the file within the underlying layers and copying this file to the upper layer to store the changes. Therefore, the I/O overhead from reading files is very small and we can keep multiple layers for better file distribution between containers. In contrast, writing requires more resources and it would be better to leave big files and those subject to many or continuous modifications out of the <span class="No-Break">container layer.</span></p>
<p>It is also important to<a id="_idIndexMarker113"/> notice that containers are not ephemeral at all. As mentioned previously, changes in the container layer are retained until the container is removed from the operating system; so, if you create a 10 GB file in the container layer, it will reside on your host’s disk. Container orchestrators manage this behavior, but be careful where you store your persistent files. Administrators should do container housekeeping and disk maintenance to avoid <span class="No-Break">disk-pressure problems.</span></p>
<p>Developers should keep this in mind and prepare their applications using containers to be logically ephemeral and store persistent data outside the container’s layers. We will learn about options <a id="_idIndexMarker114"/>for persistence in <a href="B19845_10.xhtml#_idTextAnchor231"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Leveraging Application Data Management </em><span class="No-Break"><em class="italic">in Kubernetes</em></span><span class="No-Break">.</span></p>
<p>This thinking leads us to<a id="_idIndexMarker115"/> the next section, where we will discuss the intrinsic dynamism of <span class="No-Break">container environments.</span></p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor027"/>Understanding dynamism in container-based applications</h2>
<p>We have seen how <a id="_idIndexMarker116"/>containers run using immutable<a id="_idIndexMarker117"/> storage (container images) and how the container runtime adds a new layer for managing changed files. Although we mentioned in the previous section that containers are not ephemeral in terms of disk usage, we have to include this feature in our application’s design. Containers will start and stop whenever you upgrade your application’s components. Whenever you change the base image, a completely new container will be created (remember the layers ecosystem described in the previous section). This will become even worse if you want to distribute these application components across a cluster – even using the same image will result in different containers being created on different hosts. Thus, this <strong class="bold">dynamism</strong> is inherited in <span class="No-Break">these platforms.</span></p>
<p>In the context of networking communications inside containers, we know that processes running inside a container share its network namespace, and thus they all get the same network stack and IP address. But every time a new container is created, the container runtime will provide a new IP address. Thanks to <a id="_idIndexMarker118"/>container orchestration and the <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) included, we can communicate with our containers. As IP addresses are dynamically managed by the container runtime’s <a id="_idIndexMarker119"/>internal <strong class="bold">IP Address Management</strong> (<strong class="bold">IPAM</strong>) using defined pools, every time a container dies (whether the main process is stopped, killed manually, or ended by an error), it will free its IP address and IPAM will assign it to a new container that might be part of a completely different application. Hence, we can trust the IP address assignment although we shouldn’t use container IP addresses in our application configurations (or even worse, write them in our code, which is a bad practice in every scenario). IP addresses will be dynamically managed by the IPAM container runtime component by default. We will learn about better mechanisms we can use to reference our application’s containers, such as service names, in <a href="B19845_04.xhtml#_idTextAnchor096"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Running </em><span class="No-Break"><em class="italic">Docker Containers</em></span><span class="No-Break">.</span></p>
<p>Applications use fully qualified domain names (or short names if we are using internal domain communications, as we will learn when we use Docker Compose to run multi-container applications, and also when applications run in more complicated <span class="No-Break">container orchestrations).</span></p>
<p>Because IP addresses are dynamic, special resources should be used to assign sets of IP addresses (or unique IP addresses, if we have just one process replica) to service names. In the same way, publishing application components requires some resource mappings, using <strong class="bold">network address translation</strong> (<strong class="bold">NAT</strong>) for communicating between users and external services<a id="_idIndexMarker120"/> and those running inside containers, distributed across a cluster in different servers or even different infrastructures (such as cloud-provided container orchestrators, <span class="No-Break">for example).</span></p>
<p>Since we’re<a id="_idIndexMarker121"/> reviewing <a id="_idIndexMarker122"/>the main concepts related to containers in this chapter, we can’t miss out on the tools that are used for creating, executing, and <span class="No-Break">sharing containers.</span></p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor028"/>Tools for managing containers</h2>
<p>As we learned <a id="_idIndexMarker123"/>previously, the container runtime will manage most of the actions we can achieve with containers. Most of these runtimes <a id="_idIndexMarker124"/>run as <strong class="bold">daemons</strong> and provide an interface for interacting with them. Among these tools, Docker stands out as it provides <em class="italic">all the tools in a box</em>. Docker acts as a client-server application and in newer releases, both the client and server components are packaged separately, but in any case, both are needed by users. At first, when Docker Engine was the most popular and reliable container engine, Kubernetes adopted it as its runtime. But this marriage did not last long, and Docker Engine was deprecated in Kubernetes release 1.22. This happened because Docker manages its own integration of Containerd, which is not standardized nor directly usable by the Kubernetes CRI. Despite this fact, Docker is still the most widely used option for developing container-based applications and the de facto standard for <span class="No-Break">building images.</span></p>
<p>We mentioned Docker Desktop and Rancher Desktop earlier in this section. Both act as container runtime clients that use either the <strong class="source-inline">docker</strong> or <strong class="source-inline">nerdctl</strong> command lines. We can use such clients because in both cases, <strong class="source-inline">dockerd</strong> or <strong class="source-inline">containerd</strong> act as <span class="No-Break">container runtimes.</span></p>
<p>Developers and the wider community pushed Docker to provide a solution for users who prefer to run containers without having to run a privileged system daemon, which is dockerd’s default behavior. It took some time but finally, a few years ago, Docker published its rootless runtime with user privileges. During this development phase, another container executor arrived, called Podman, created by Red Hat to solve the same problem. This solution can run without root privileges and aims to avoid the use of a daemonized container runtime. The host user can run containers without any system privilege by default; only a few tweaks are required by administrators if the containers are to be run in a security-hardened environment. This made Podman a very secure option for running containers in production (without orchestration). Docker also included rootless containers by the end of 2019, making both options secure <span class="No-Break">by default.</span></p>
<p>As you learned at the beginning of this section, containers are processes that run on top of an operating system, isolated using its kernel features. It is quite evident why containers are so popular in microservice environments (one container runs a process, which is ultimately a microservice), although we can still build microservice-based applications without containers. It is also possible to use containers to run whole application components together, although<a id="_idIndexMarker125"/> this isn’t an <span class="No-Break">ideal situation.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">In this chapter, we’ll largely focus on software containers in the context of Linux operating systems. This is because they were only introduced in Windows systems much later. However, we will also briefly discuss them in the context <span class="No-Break">of Windows.</span></p>
<p>We shouldn’t compare containers with virtual nodes. As discussed earlier in this section, containers are mainly based on cgroups and kernel namespaces while virtual nodes are based on hypervisor software. This software provides sandboxing capabilities and specific virtualized hardware resources to guest hosts. We still need to prepare operating systems to run these<a id="_idIndexMarker126"/> virtual guest hosts. Each guest node will receive a piece of virtualized hardware and we must manage servers’ interactions as if they <span class="No-Break">were physical.</span></p>
<p>We’ll compare these models side by side in the <span class="No-Break">following section.</span></p>
<h1 id="_idParaDest-29"><a id="_idTextAnchor029"/>Comparing virtualization and containers</h1>
<p>The following<a id="_idIndexMarker127"/> schema <a id="_idIndexMarker128"/>represents a couple of virtual guest nodes running on top of a <span class="No-Break">physical host:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer012">
<img alt="Figure 1.5 – Applications running on top of virtual guest nodes, running on top of a physical server" height="913" src="image/B19845_01_05.jpg" width="1612"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Applications running on top of virtual guest nodes, running on top of a physical server</p>
<p>A physical server running its own operating system executes a hypervisor software layer to provide virtualization capabilities. A specific amount of hardware resources is virtualized and provisioned to these new virtual guest nodes. We should install new operating systems for these new hosts and after that, we will be able to run applications. Physical host resources are partitioned for guest hosts and both nodes are completely isolated from each other. Each virtual machine executes its own kernel and its operating system runs on top of the host. There is complete isolation between guests’ operating systems because the underlying host’s hypervisor software keeps <span class="No-Break">them separated.</span></p>
<p>In this model, we require a lot of resources, even if we just need to run a couple of processes per virtual host. Starting and stopping virtual hosts will take time. Lots of non-required software and processes will probably run on our guest host and it will require some tuning to <span class="No-Break">remove them.</span></p>
<p>As we have learned, the microservices model is based on the idea of applications running decoupled in different processes with complete functionality. Thus, running a complete operating system within just a couple of processes doesn’t seem like a <span class="No-Break">good idea.</span></p>
<p>Although automation will help us, we need to maintain and configure those guest operating systems in terms of running the required processes and managing users, access rights, and network communications, among other things. System administrators maintain these hosts as if they were physical. Developers require their own copies to develop, test, and <a id="_idIndexMarker129"/>certify<a id="_idIndexMarker130"/> application components. Scaling up these virtual servers can be a problem because in most cases, increasing resources require a complete reboot to apply <span class="No-Break">the changes.</span></p>
<p>Modern virtualization software provides API-based management, which enhances their usage and virtual node maintenance, but it is not enough for microservice environments. Elastic environments, where components should be able to scale up or down on demand, will not fit well in <span class="No-Break">virtual machines.</span></p>
<p>Now, let’s review the following schema, which represents a set of containers running on physical and <span class="No-Break">virtual hosts:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 1.6 – A set of containers running on top of physical and virtual hosts" height="718" src="image/B19845_01_06.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – A set of containers running on top of physical and virtual hosts</p>
<p>All containers in this<a id="_idIndexMarker131"/> schema <a id="_idIndexMarker132"/>share the same host kernel as they are just processes running on top of an operating system. In this case, we don’t care whether they run on a virtual or a physical host; we expect the same behavior. Instead of hypervisor software, we have a <strong class="bold">container runtime</strong> for <a id="_idIndexMarker133"/>running containers. Only a template filesystem and a set of defined resources are required for each container. To clarify, a complete operating system filesystem is not required – we just need the specific files required by our process to work. For example, if a process runs on a Linux kernel and is going to use some network capabilities, then the <strong class="source-inline">/etc/hosts</strong> and <strong class="source-inline">/etc/nsswitch.conf</strong> files would probably be required (along with some network libraries and their dependencies). The <strong class="bold">attack surface</strong> will <a id="_idIndexMarker134"/>be completely different than having a whole operating system full of binaries, libraries, and running services, regardless of whether the application uses them <span class="No-Break">or not.</span></p>
<p>Containers are designed to run just one main process (and its threads or sub-processes) and this makes them lightweight. They can start and stop as fast as their main <span class="No-Break">process does.</span></p>
<p>All the resources consumed by a container are related to the given process, which is great in terms of the allocation of hardware resources. We can calculate our application’s resource consumption by observing the load of all <span class="No-Break">its microservices.</span></p>
<p>We define <strong class="bold">images</strong> as templates for running containers. These images contain all the files required by the container to work plus some meta-information providing its features, capabilities, and which commands or binaries will be used to start the process. Using images, we can ensure that all the containers created with one template will run the same. This eliminates infrastructure friction and helps developers prepare their applications to run in production. The configuration (and of course security information such as credentials) is the only thing that differs between the development, testing, certification, and <span class="No-Break">production environments.</span></p>
<p>Software containers also improve application security because they run by default with limited privileges and allow only a set of system calls. They run anywhere; all we need is a container runtime to be able to create, share, and <span class="No-Break">run containers.</span></p>
<p>Now that we know<a id="_idIndexMarker135"/> what<a id="_idIndexMarker136"/> containers are and the most important concepts involved, let’s try to understand how they fit into <span class="No-Break">development processes.</span></p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor030"/>Building, sharing, and running containers</h1>
<p><em class="italic">Build, ship, and run</em>: you<a id="_idIndexMarker137"/> might <a id="_idIndexMarker138"/>have heard or read<a id="_idIndexMarker139"/> this quote years ago. Docker Inc. used it to promote the ease of using containers. When creating container-based applications, we can use Docker to build container images, share these images within environments, move the content from our development workstations to testing and staging environments, execute them as containers, and finally use these packages in production. Only a few changes are required throughout, mainly at the application’s configuration level. This workflow ensures application usage and immutability between the development, testing, and staging stages. Depending on the container runtime and container orchestrator chosen for each stage, Docker could be present throughout (Docker Engine and Docker Swarm). Either way, most people still use the Docker command line to create container images due to its great, always-evolving features that allow us, for example, to build images for different processor architectures using our <span class="No-Break">desktop computers.</span></p>
<p>Adding <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>) (or <strong class="bold">continuous delivery</strong>, depending<a id="_idIndexMarker140"/> on<a id="_idIndexMarker141"/> the source) to <a id="_idIndexMarker142"/>the equation simplifies developers’ lives so they can focus on their application’s architecture <span class="No-Break">and code.</span></p>
<p>They can code on their workstations and push their code to a source code repository, and this event will trigger a CI/CD automation to build applications artifacts, compiling their code and providing the artifacts in the form of binaries or libraries. This automation can also include these artifacts inside container images. These become the new application artifacts and are stored in image registries (the backends that store container images). Different executions can be chained to test this newly compiled component together with other components in the integration phase, achieve verification via some tests in the testing phase, and so on, passing through different stages until it gets to production. All these chained workflows are based on containers, configuration, and the images used for execution. In this workflow, developers never explicitly create a release image; they only build and test development ones, but the same Dockerfile recipe is used on their workstations and in the CI/CD phases executed on servers. Reproducibility <span class="No-Break">is key.</span></p>
<p>Developers can<a id="_idIndexMarker143"/> run <a id="_idIndexMarker144"/>multiple containers on their developer<a id="_idIndexMarker145"/> workstations as if they were using the real environment. They can test their code along with other components in their environment, allowing them to evaluate and discover problems faster and fix them even before moving their components to the CI/CD pipelines. When their code is ready, they can push it to their code repository and trigger the automation. Developers can build their development images, test them locally (be it a standalone component, multiple components, or even a full application), prepare their release code, then push it, and the CI/CD orchestrator will build the release image <span class="No-Break">for them.</span></p>
<p>In these contexts, images are shared between environments via the use of image registries. <em class="italic">Shipping</em> images from server to server is easy as the host’s container runtime will download the images from the given registries – but only those layers not already present on the servers will be downloaded, hence the layer distribution within container images <span class="No-Break">is key.</span></p>
<p>The following schema outlines this <span class="No-Break">simplified workflow:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 1.7 – Simplified schema representing a CI/CD workflow example using software containers to deliver applications to production" height="572" src="image/B19845_01_07.jpg" width="1209"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Simplified schema representing a CI/CD workflow example using software containers to deliver applications to production</p>
<p>Servers running these different stages can be either standalone servers, pools of nodes from orchestrated clusters, or even more complex dedicated infrastructures, including in some cases cloud-provided hosts or whole clusters. Using container images ensures the artifact’s content and infrastructure-specific configurations will run in the custom application environment in <span class="No-Break">each case.</span></p>
<p>With this in <a id="_idIndexMarker146"/>mind, we <a id="_idIndexMarker147"/>can imagine how we could build a<a id="_idIndexMarker148"/> full development chain using containers. We talked about Linux kernel namespaces already, so let’s continue by understanding how these isolation mechanisms work on <span class="No-Break">Microsoft Windows.</span></p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor031"/>Explaining Windows containers</h1>
<p>During this chapter, we <a id="_idIndexMarker149"/>have focused on software containers within Linux operating systems. Software containers started on Linux systems, but due to their importance and advances in technology in terms of host resource usage, Microsoft introduced them in the Microsoft Windows Server 2016 operating system. Before this, Windows users and administrators were only capable of using software containers for Linux through virtualization. Thus, there was the Docker Toolbox solution, of which Docker Desktop formed a part, and installing this software on our Windows-based computer allowed us to have a terminal with the Docker command line, a fancy GUI, and a Hyper-V Linux virtual machine where containers would run. This made it easy for entry-level users to use software containers on their Windows desktops, but Microsoft <a id="_idIndexMarker150"/>eventually brought in a game-changer here, creating a new <span class="No-Break">encapsulation model.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Container runtimes are client-server applications, so we can serve the runtime to local (by default) and remote clients. When we use a remote runtime, we can use our clients to execute commands on this runtime using different clients, such as <strong class="source-inline">docker</strong> or <strong class="source-inline">nerdctl</strong>, depending on the server side. Earlier in this chapter, we mentioned that desktop solutions such as Docker Desktop or Rancher Desktop use this model, running a container runtime server where the common clients, executed from common Linux terminals or Microsoft PowerShell, can manage software containers running on the <span class="No-Break">server side.</span></p>
<p>Microsoft provided two different software <span class="No-Break">container models:</span></p>
<ul>
<li><strong class="bold">Hyper-V Linux Containers</strong>: The old <a id="_idIndexMarker151"/>model, which uses a Linux <span class="No-Break">virtual machine</span></li>
<li><strong class="bold">Windows Server Containers</strong>, also<a id="_idIndexMarker152"/> known<a id="_idIndexMarker153"/> as <strong class="bold">Windows Process Containers</strong>: This is the new model, allowing the execution of Windows <span class="No-Break">operating-system-based applications</span></li>
</ul>
<p>From the user’s perspective, the management and execution of containers running on Windows are the same, no matter which of the preceding models is in use, but only one model can be used per server, thus applying to all containers on that server. The differences here come from the isolation used in <span class="No-Break">each model.</span></p>
<p><strong class="bold">Process isolation</strong> on Windows <a id="_idIndexMarker154"/>works in the same way it does on Linux. Multiple processes run on a host, accessing the host’s kernel, and the host provides isolation using namespaces and resources control (along with other specific methods, depending on the underlying operating system). As we already know, processes get their own filesystem, network, processes identifiers, and so on, but in this case, they also get their own Windows registry and <span class="No-Break">object namespace.</span></p>
<p>Due to the very nature of the Microsoft Windows operating system, some system services and <strong class="bold">dynamic linked libraries</strong> (<strong class="bold">DLLs</strong>) are required within the containers and cannot be shared <a id="_idIndexMarker155"/>from the host. Thus, process containers need to contain a copy of these resources, which makes Windows images quite a lot bigger than Linux-based container images. You may also encounter some compatibility issues within image releases, depending on which base operating system (files tree) was used to <span class="No-Break">generate it.</span></p>
<p>The following schema <a id="_idIndexMarker156"/>represents both models side by side so that we can observe the main <span class="No-Break">stack differences:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 1.8 – A comparison of Microsoft Windows software container models" height="500" src="image/B19845_01_08.jpg" width="1500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – A comparison of Microsoft Windows software container models</p>
<p>We will use Windows Server containers when our application requires strong integration with the Microsoft operating system, for example, for integrating <strong class="bold">Group Managed Service Accounts</strong> (<strong class="bold">gMSA</strong>) or <a id="_idIndexMarker157"/>encapsulating applications that don’t run under <span class="No-Break">Linux hosts.</span></p>
<p>From my experience, Windows Server containers became very popular when they initially arrived, but as Microsoft improved the support of their applications for Linux operating systems, the fact that developers could create their applications in .NET Core for either Microsoft Windows or Linux, and the lack of many cloud providers offering this technology, made them almost disappear from <span class="No-Break">the scene.</span></p>
<p>It is also important to mention that orchestration technology evolution helped developers move to Linux-only containers. Windows Server containers were supported only on top of Docker Swarm until 2019 when Kubernetes announced their support. Due to the large increase of Kubernetes’ adoption in the developer community and even in enterprise environments, Windows Server container usage reduced to very specific and niche <span class="No-Break">use cases.</span></p>
<p>Nowadays, Kubernetes supports Microsoft Windows Server hosts running as worker roles, allowing process container execution. We will learn about Kubernetes and host roles in <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the Kubernetes Orchestrator</em>. Despite this fact, you will probably not find many Kubernetes clusters running Windows Server <span class="No-Break">container workloads.</span></p>
<p>We mentioned that <a id="_idIndexMarker158"/>containers improve application security. The next section will show you the improvements at the host and container levels that make containers <em class="italic">safer </em><span class="No-Break"><em class="italic">by default</em></span><span class="No-Break">.</span></p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor032"/>Improving security using software containers</h1>
<p>In this section, we <a id="_idIndexMarker159"/>are going to introduce some of<a id="_idIndexMarker160"/> the features found on container platforms that help improve <span class="No-Break">application security.</span></p>
<p>If we keep in mind how containers run, we know that we first need a <strong class="bold">host</strong> with a container runtime. So, having a host with just the software required is the first security measure. We should use dedicated hosts in production for running container workloads. We do not need to concern ourselves with this while developing, but system administrators should prepare production nodes with minimal attack surfaces. We should never share these hosts for use in serving other technologies or services. This feature is so important that we can even find dedicated operating systems, such as Red Hat’s CoreOS, SuSE’s RancherOS, VMware’s PhotonOS, TalOS, or Flatcar Linux, just to mention the most popular ones. These are minimal operating systems that just include a container runtime. You can even create your own by using Moby’s LinuxKit project. Some vendors’ customized Kubernetes platforms, such as Red Hat’s OpenShift, create their clusters using CoreOS, improving the whole <span class="No-Break">environment’s security.</span></p>
<p>We will never connect to any cluster host to execute containers. Container runtimes work in client-server mode. Rather, we expose this engine service and simply using a client on our laptop or desktop computers will be more than enough to execute containers on <span class="No-Break">the host.</span></p>
<p>Locally, clients connect to container runtimes<a id="_idIndexMarker161"/> using <strong class="bold">sockets</strong> (<strong class="source-inline">/var/run/docker.sock</strong> for <strong class="source-inline">dockerd</strong>, for example). Adding read-write access to this socket to specific users will allow them to use the daemon to build, pull, and push images or execute containers. Configuring the container runtime in this way may be worse if the host has a master role in an orchestrated environment. It is crucial to understand this feature and know which users will be able to run containers on each host. System<a id="_idIndexMarker162"/> administrators <a id="_idIndexMarker163"/>should keep their container runtimes’ sockets safe from untrusted users and only allow authorized access. These sockets are local and, depending on which runtime we are using, TCP or even SSH (in <strong class="source-inline">dockerd</strong>, for example) can be used to secure remote access. Always ensure <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) is used <a id="_idIndexMarker164"/>to secure <span class="No-Break">socket access.</span></p>
<p>It is important to note that container runtimes do not provide any <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>). We will <a id="_idIndexMarker165"/>need to add this layer later with other tools. Docker Swarm does not provide RBAC, but Kubernetes does. RBAC is key for managing user privileges and multiple <span class="No-Break">application isolation.</span></p>
<p>We should say here that, currently, desktop environments (Docker Desktop and Rancher Desktop) also work with this model, in which you don’t connect to the host running the container runtime. A virtualized environment is deployed on your system (using Qemu if on Linux, or Hyper-V or the newer Windows Subsystem for Linux on Windows hosts) and our client, using a terminal, will connect to this virtual container runtime (or the Kubernetes API when deploying workloads on Kubernetes, as we will learn in <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the </em><span class="No-Break"><em class="italic">Kubernetes Orchestrator</em></span><span class="No-Break">).</span></p>
<p>Here, we have to reiterate that container runtimes add only a subset of kernel capabilities by default to container processes. But this may not be enough in some cases. To improve containers’ security behavior, container runtimes also include a default <strong class="bold">Secure Computing Mode</strong> (<strong class="bold">Seccomp</strong>) profile. Seccomp is a Linux security facility that filters the system <a id="_idIndexMarker166"/>calls allowed inside containers. Specific profiles can be included and used by runtimes to add some required system calls. You, as the developer, need to notice when your application requires extra capabilities or uncommon system calls. The special features described in this section are used on host monitoring tools, for example, or if we need to add a new kernel module using system <span class="No-Break">administration containers.</span></p>
<p>Container runtimes usually run as daemons; thus, they will quite probably run as root users. This means that any container can contain the host’s files inside (we will learn how we can mount volumes and host paths within containers in <a href="B19845_04.xhtml#_idTextAnchor096"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Running Docker Containers</em>) or include the host’s namespaces (container processes may access host’s PIDs, networks, IPCs, and so on). To avoid the undesired effects of running container runtime privileges, system administrators should apply special security measures using <strong class="bold">Linux Security Modules</strong> (<strong class="bold">LSM</strong>), such <a id="_idIndexMarker167"/>as SELinux or AppArmor, <span class="No-Break">among others.</span></p>
<p>SELinux should be integrated into container runtimes and container orchestration. These integrations can be used to ensure, for example, that only certain paths are allowed inside containers. If your application requires access to the host’s files, non-default SELinux labels should be included to modify the default runtime behavior. Container runtimes’ software installation packages include these settings, among others, to ensure that common applications will run without problems. However, those with special requirements, such<a id="_idIndexMarker168"/> as<a id="_idIndexMarker169"/> those that are prepared to read hosts’ logs, will require further <span class="No-Break">security configurations.</span></p>
<p>So far in this chapter, we have provided a quick overview of the key concepts related to containers. In the following section, we’ll put this <span class="No-Break">into practice.</span></p>
<h1 id="_idParaDest-33"><a id="_idTextAnchor033"/>Labs</h1>
<p>In this first chapter, we covered a lot of content, learning what containers are and how they fit into the modern <span class="No-Break">microservices architecture.</span></p>
<p>In this lab, we will install a fully functional development environment for container-based applications. We will use Docker Desktop because it includes a container runtime, its client, and a minimal but fully functional Kubernetes <span class="No-Break">orchestration solution.</span></p>
<p>We could use Docker Engine in Linux directly (the container runtime only, following the instructions at <a href="https://docs.docker.com/">https://docs.docker.com/</a>) for most labs but we will need to install a new tool for the Kubernetes labs, which requires a minimal Kubernetes cluster installation. Thus, even for just using the command line, we will use the Docker <span class="No-Break">Desktop environment.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">We will use a Kubernetes desktop environment to minimize CPU and memory requirements. There are even lighter Kubernetes cluster alternatives such as KinD or K3S, but these may require some customization. Of course, you can also use any cloud provider’s Kubernetes environment if you feel more comfortable <span class="No-Break">doing so.</span></p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>Installing Docker Desktop</h2>
<p>This lab will guide <a id="_idIndexMarker170"/>you through the installation of <strong class="bold">Docker Desktop</strong> on your laptop or workstation and how to execute a test to verify that it <span class="No-Break">works correctly.</span></p>
<p>Docker Desktop can be installed on Microsoft Windows 10, most of the common Linux flavors, and macOS (the arm64 and amd64 architectures are both supported). This lab will show you how to install this software on Windows 10, but I will use Windows and Linux interchangeably in other labs as they mostly work the same – we will review any differences between the platforms <span class="No-Break">when required.</span></p>
<p>We will follow the simple steps documented at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>. Docker Desktop can be deployed on Windows<a id="_idIndexMarker171"/> using <strong class="bold">Hyper-V</strong> or the newer <strong class="bold">Windows Subsystem for Linux</strong> 2 (<strong class="bold">WSL 2</strong>). This<a id="_idIndexMarker172"/> second option uses less compute and memory resources and is nicely integrated into Microsoft Windows, making it the preferred installation method, but note that WSL2 is required on your host before installing Docker Desktop. Please follow the instructions from Microsoft at <a href="https://learn.microsoft.com/en-us/windows/wsl/install">https://learn.microsoft.com/en-us/windows/wsl/install</a> before installing Docker Desktop. You can install any Linux distribution because the integration will be <span class="No-Break">automatically included.</span></p>
<p>We will use the <strong class="bold">Ubuntu</strong> WSL <a id="_idIndexMarker173"/>distribution. It is available <a id="_idIndexMarker174"/>from the <strong class="bold">Microsoft Store</strong> and is simple <span class="No-Break">to install:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 1.9 – Ubuntu in the Microsoft Store" height="584" src="image/B19845_01_09.jpg" width="831"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Ubuntu in the Microsoft Store</p>
<p>During the installation, you will be prompted for <strong class="bold">username</strong> and <strong class="bold">password</strong> details for this Windows <span class="No-Break">subsystem installation:</span></p>
<p class="IMG---Figure"><img alt="Figure 1.10 – After installing Ubuntu, you will have a fully functional Linux Terminal" height="357" src="image/B19845_01_10.png" width="815"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – After installing Ubuntu, you will have a fully functional Linux Terminal</p>
<p>You can close this<a id="_idIndexMarker175"/> Ubuntu Terminal as the Docker Desktop integration will require you to open a new one once it has <span class="No-Break">been configured.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">You may need to execute some additional steps at <a href="https://docs.microsoft.com/windows/wsl/wsl2-kernel">https://docs.microsoft.com/windows/wsl/wsl2-kernel</a> to update WSL2 if your operating system hasn’t <span class="No-Break">been updated.</span></p>
<p>Now, let’s continue with the Docker <span class="No-Break">Desktop installation:</span></p>
<ol>
<li>Download the installer <span class="No-Break">from </span><a href="https://docs.docker.com/get-docker/"><span class="No-Break">https://docs.docker.com/get-docker/</span></a><span class="No-Break">:</span></li>
</ol>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 1.11 – Docker Desktop download section" height="743" src="image/B19845_01_11.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – Docker Desktop download section</p>
<ol>
<li value="2">Once <a id="_idIndexMarker176"/>downloaded, execute the <strong class="source-inline">Docker Desktop Installer.exe</strong> binary. You will be asked to choose between Hyper-V or WSL2 backend virtualization; we will <span class="No-Break">choose WSL2:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 1.12 – Choosing the WSL2 integration for better performance" height="270" src="image/B19845_01_12.jpg" width="876"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – Choosing the WSL2 integration for better performance</p>
<ol>
<li value="3">After clicking <strong class="bold">Ok</strong>, the <a id="_idIndexMarker177"/>installation process will begin decompressing the required files (libraries, binaries, default configurations, and so on). This could take some time (1 to 3 minutes), depending on your host’s disk speed and <span class="No-Break">compute resources:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 1.13 – The installation process will take a while as the application files are decompressed and installed on your system" height="538" src="image/B19845_01_13.jpg" width="872"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – The installation process will take a while as the application files are decompressed and installed on your system</p>
<ol>
<li value="4">To finish the <a id="_idIndexMarker178"/>installation, we will be asked to log out and log in again because our user was added to new system groups (Docker) to enable access to the remote Docker daemon via operating system pipes (similar to <span class="No-Break">Unix sockets):</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 1.14 – Docker Desktop has been successfully installed and we must log out" height="448" src="image/B19845_01_14.jpg" width="874"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – Docker Desktop has been successfully installed and we must log out</p>
<ol>
<li value="5">Once we log in, we<a id="_idIndexMarker179"/> can execute Docker Desktop using the newly added application icon. We can enable Docker Desktop execution on start, which could be very useful, but it may slow down your computer if you are short on resources. I recommend starting Docker Desktop only when you are going to <span class="No-Break">use it.</span><p class="list-inset">Once we’ve accepted the Docker Subscription license terms, Docker Desktop will start. This may take <span class="No-Break">a minute:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 1.15 – Docker Desktop is starting" height="450" src="image/B19845_01_15.jpg" width="1010"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – Docker Desktop is starting</p>
<p class="list-inset">You can skip the <a id="_idIndexMarker180"/>quick guide that will appear when Docker Desktop is running because we will learn more about this in the following chapters as we deep dive into building container images and <span class="No-Break">container execution.</span></p>
<ol>
<li value="6">We will get the following screen, showing us that Docker Desktop <span class="No-Break">is ready:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 1.16 – Docker Desktop main screen" height="680" src="image/B19845_01_16.jpg" width="1270"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – Docker Desktop main screen</p>
<ol>
<li value="7">We need to<a id="_idIndexMarker181"/> enable WSL2 integration with our favorite <span class="No-Break">Linux distribution:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="Figure 1.17 – Enabling our previously installed Ubuntu using WSL2" height="690" src="image/B19845_01_17.jpg" width="1270"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – Enabling our previously installed Ubuntu using WSL2</p>
<ol>
<li value="8">After this step, we are finally ready to work with Docker Desktop. Let’s open a terminal using our Ubuntu distribution, execute <strong class="source-inline">docker</strong>, and, after that, <span class="No-Break"><strong class="source-inline">docker info</strong></span><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer025">
<img alt="Figure 1.18 – Executing some Docker commands just to verify container runtime integration" height="512" src="image/B19845_01_18.jpg" width="807"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – Executing some Docker commands just to verify container runtime integration</p>
<p class="list-inset">As you can<a id="_idIndexMarker182"/> see, we have a fully functional Docker client command line associated with the Docker Desktop <span class="No-Break">WSL2 server.</span></p>
<ol>
<li value="9">We will end this lab by<a id="_idIndexMarker183"/> executing an <strong class="bold">Alpine container</strong> (a small Linux distribution), reviewing its process tree and the list of its <span class="No-Break">root filesystem.</span><p class="list-inset">We can execute <strong class="source-inline">docker run-ti alpine</strong> to download the Alpine image and execute a container <span class="No-Break">using it:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Figure 1.19 – Creating a container and executing some commands inside before exiting" height="528" src="image/B19845_01_19.jpg" width="642"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – Creating a container and executing some commands inside before exiting</p>
<ol>
<li value="10">This container <a id="_idIndexMarker184"/>execution left changes in Docker Desktop; we can review the current images present in our <span class="No-Break">container runtime:</span></li>
</ol>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="Figure 1.20 – Docker Desktop – the Images view" height="540" src="image/B19845_01_20.jpg" width="1270"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.20 – Docker Desktop – the Images view</p>
<ol>
<li value="11">We can also review the container, which is already dead because we exited by simply executing <strong class="source-inline">exit</strong> inside <span class="No-Break">its shell:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 1.21 – Docker Desktop – the Containers view" height="430" src="image/B19845_01_21.jpg" width="1270"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.21 – Docker Desktop – the Containers view</p>
<p>Now, Docker Desktop <a id="_idIndexMarker185"/>works and we are ready to work through the following labs using our WSL2 Ubuntu <span class="No-Break">Linux distribution.</span></p>
<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Summary</h1>
<p>In this chapter, we learned the basics around containers and how they fit into modern microservices applications. The content presented in this chapter has helped you understand how to implement containers in distributed architectures, using already-present host operating system isolation features and container runtimes, which are the pieces of software required for building, sharing, and <span class="No-Break">executing containers.</span></p>
<p>Software containers assist application development by providing resilience, high availability, scalability, and portability thanks to their very nature, and will help you create and manage the application <span class="No-Break">life cycle.</span></p>
<p>In the next chapter, we will deep dive into the process of creating <span class="No-Break">container images.</span></p>
</div>
</div></body></html>