<html><head></head><body>
<div id="_idContainer142">
<h1 class="chapter-number" id="_idParaDest-206"><a id="_idTextAnchor509"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-207"><a id="_idTextAnchor510"/><span class="koboSpan" id="kobo.2.1">Containerize on Azure – Building Solutions with Azure Kubernetes Service</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we built and automated our solution on Azure utilizing Azure VMs. </span><span class="koboSpan" id="kobo.3.2">We built VM images with Packer and provisioned our VMs using Terraform. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we’ll follow a similar path, but instead of working with VMs, we’ll look at hosting our application in containers within a </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">Kubernetes cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">To achieve this, we’ll need to alter our approach by ditching Packer and replacing it with Docker to create a deployable artifact for our application. </span><span class="koboSpan" id="kobo.5.2">Once again, we’ll be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.6.1">azurerm</span></strong><span class="koboSpan" id="kobo.7.1"> provider for Terraform and revisiting the </span><strong class="source-inline"><span class="koboSpan" id="kobo.8.1">kubernetes</span></strong><span class="koboSpan" id="kobo.9.1"> provider for Terraform that we saw when we took the same step while on our journey </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">with AWS.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Since an overwhelming majority of this remains the same when we move to Azure, we won’t revisit these topics at the same length in this chapter. </span><span class="koboSpan" id="kobo.11.2">However, I would encourage you to put a bookmark in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.13.1"> and reference </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">it frequently.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.17.1">Laying </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">the foundation</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Designing </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Building </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Automating </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">the deployment</span></span></li>
</ul>
<h1 id="_idParaDest-208"><a id="_idTextAnchor511"/><span class="koboSpan" id="kobo.25.1">Laying the foundation</span></h1>
<p><span class="koboSpan" id="kobo.26.1">Our story continues through</span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.27.1"> the lens of Söze Enterprises, founded by the enigmatic Turkish billionaire Keyser Söze. </span><span class="koboSpan" id="kobo.27.2">Our team has been hard at work building the next-generation autonomous vehicle orchestration platform. </span><span class="koboSpan" id="kobo.27.3">Previously, we had hoped to leapfrog the competition by leveraging Azure’s rock-solid platform, leveraging our team’s existing skills, and focusing on feature development. </span><span class="koboSpan" id="kobo.27.4">The team was just getting into their groove when a curveball came out </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">of nowhere.</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">Over the weekend, our elusive executive was influenced by a rendezvous with Scott Guthrie, the President of Microsoft’s Cloud + AI Division, in Abu Dhabi. </span><span class="koboSpan" id="kobo.29.2">The Yas Marina Circuit was buzzing with energy. </span><span class="koboSpan" id="kobo.29.3">The sun was setting, casting a golden glow over the track as fans and celebrities gathered for the season-ending Abu Dhabi Grand Prix. </span><span class="koboSpan" id="kobo.29.4">While in the exclusive Paddock Club, Keyser spotted Scott “Gu” in his iconic red polo near the hors d’oeuvres. </span><span class="koboSpan" id="kobo.29.5">Scott excitedly shared news</span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.30.1"> about some recent improvements to </span><strong class="bold"><span class="koboSpan" id="kobo.31.1">Azure Kubernetes Service</span></strong><span class="koboSpan" id="kobo.32.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.33.1">AKS</span></strong><span class="koboSpan" id="kobo.34.1">). </span><span class="koboSpan" id="kobo.34.2">Keyser was enchanted by the prospect of more efficient resource utilization, leading to improved cost optimization and faster deployment and rollback times, and he was hooked. </span><span class="koboSpan" id="kobo.34.3">His new autonomous vehicle platform needed to harness the power of the cloud, and container-based architecture was the way to do it. </span><span class="koboSpan" id="kobo.34.4">So, he decided to accelerate his plans to adopt </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">cloud-native architecture!</span></span></p>
<p><span class="koboSpan" id="kobo.36.1">The news of transitioning to a container-based architecture means reevaluating their approach, diving into new technologies, and possibly even reshuffling team dynamics. </span><span class="koboSpan" id="kobo.36.2">For the team, containers were always the long-term plan, but now, things need to be sped up, which will require a significant investment in time, resources, </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">and training.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">As the team scrambles to adjust their plans, they can’t help but feel a mix of excitement and apprehension. </span><span class="koboSpan" id="kobo.38.2">They know that they are part of something groundbreaking under Keyser’s leadership. </span><span class="koboSpan" id="kobo.38.3">His vision for the future of autonomous vehicles is bold and transformative. </span><span class="koboSpan" id="kobo.38.4">And while his methods</span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.39.1"> may be unconventional, they have learned that his instincts are often right. </span><span class="koboSpan" id="kobo.39.2">In this chapter, we’ll explore this transformation from VMs to containers using </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">Microsoft Azur</span><a id="_idTextAnchor512"/><span class="koboSpan" id="kobo.41.1">e.</span></span></p>
<h1 id="_idParaDest-209"><a id="_idTextAnchor513"/><span class="koboSpan" id="kobo.42.1">Designing the solution</span></h1>
<p><span class="koboSpan" id="kobo.43.1">As we saw in the previous chapter, where we built</span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.44.1"> our solution using VMs on Azure, we had full control over the operating system configuration through the VM images we provisioned with Packer. </span><span class="koboSpan" id="kobo.44.2">Just as we did when we went through the same process on our journey with AWS in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.45.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.46.1">, we’ll need to introduce a new tool to replace VM images with container images – </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">Docker:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<span class="koboSpan" id="kobo.48.1"><img alt="Figure 11.1 – Logical architecture for the autonomous vehicle platform" src="image/B21183_11_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.49.1">Figure 11.1 – Logical architecture for the autonomous vehicle platform</span></p>
<p><span class="koboSpan" id="kobo.50.1">Our application architecture, comprising</span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.51.1"> a frontend, a backend, and a database, will remain the same but we will need to provision different resources with Terraform and harness new tools from Docker and Kubernetes to automate the deployment of our solution to this </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">new infrastructure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<span class="koboSpan" id="kobo.53.1"><img alt="Figure 11.2 – Source control structure of our repository" src="image/B21183_11_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.54.1">Figure 11.2 – Source control structure of our repository</span></p>
<p><span class="koboSpan" id="kobo.55.1">In this solution, we’ll have seven parts. </span><span class="koboSpan" id="kobo.55.2">We still have the application code and Dockerfiles (replacing the Packer-based VM images) for both the frontend and backend. </span><span class="koboSpan" id="kobo.55.3">We still have GitHub Actions to implement our CI/CD process, but now we have two Terraform code bases – one for provisioning the underlying infrastructure to Azure and another for provisioning our application to the Kubernetes cluster hosted on AKS. </span><span class="koboSpan" id="kobo.55.4">Then, we have the two code</span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.56.1"> bases for our application’s frontend </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">and backe</span><a id="_idTextAnchor514"/><span class="koboSpan" id="kobo.58.1">nd.</span></span></p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor515"/><span class="koboSpan" id="kobo.59.1">Cloud architecture</span></h2>
<p><span class="koboSpan" id="kobo.60.1">There will be many similarities</span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.61.1"> between the work</span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.62.1"> that we did in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.63.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.64.1"> when we performed a similar transition from VMs to containers using AWS. </span><span class="koboSpan" id="kobo.64.2">We’ll try and focus only on the key differences and avoid retreading the same ground. </span><span class="koboSpan" id="kobo.64.3">To obtain a complete and multi-cloud perspective, I’d encourage</span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.65.1"> you to read </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.66.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.67.1"> (in case you skipped it) as well as the upcoming</span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.68.1"> chapter, where</span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.69.1"> we’ll tackle the same problem on </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">Google Cloud </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.71.1">Platfor</span><a id="_idTextAnchor516"/><span class="koboSpan" id="kobo.72.1">m</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.73.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.74.1">GCP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">).</span></span></p>
<h3><span class="koboSpan" id="kobo.76.1">Virtual network</span></h3>
<p><span class="koboSpan" id="kobo.77.1">In the previous chapter, we set up a virtual network</span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.78.1"> for two distinct groups of VMs, and then we connected our application to a database-managed service. </span><span class="koboSpan" id="kobo.78.2">When setting up a virtual network for a Kubernetes cluster, we’ll use a similar approach. </span><span class="koboSpan" id="kobo.78.3">However, the considerations are slightly different. </span><span class="koboSpan" id="kobo.78.4">We no longer have distinct and loose VMs where we host different components of our application. </span><span class="koboSpan" id="kobo.78.5">However, depending on the configuration of our Kubernetes cluster, we may need to consider the placement of the different node pools that we configure and other services that we want to provision within that network to allow the workloads we host on Kubernetes to </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">access them:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<span class="koboSpan" id="kobo.80.1"><img alt="Figure 11.3 – With AKS, virtual network subnets are organized along infrastructure boundaries rather than application boundaries" src="image/B21183_11_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.81.1">Figure 11.3 – With AKS, virtual network subnets are organized along infrastructure boundaries rather than application boundaries</span></p>
<p><span class="koboSpan" id="kobo.82.1">In its simplest form, a single subnet can be designated for all the node pools within an AKS cluster, but this can be very limiting as your workload needs to scale up over time. </span><span class="koboSpan" id="kobo.82.2">For more advanced scenarios, you should carefully consider the segmentation of your subnets based on your node pool design and scale considerations for each of your workloads. </span><span class="koboSpan" id="kobo.82.3">In doing so, you can provide better network isolation for the various workloads you host on </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">the cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.84.1">As we saw when working</span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.85.1"> with Amazon’s Kubernetes offering in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.86.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.87.1">, Azure’s Kubernetes offering</span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.88.1"> also supports two networking modes: </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">Kubenet</span></strong><span class="koboSpan" id="kobo.90.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.91.1">CNI</span></strong><span class="koboSpan" id="kobo.92.1">. </span><span class="koboSpan" id="kobo.92.2">For this book, we’ll be focusing</span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.93.1"> on Kubenet as it’s the most commonly </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">u</span><a id="_idTextAnchor517"/><span class="koboSpan" id="kobo.95.1">sed option.</span></span></p>
<h3><span class="koboSpan" id="kobo.96.1">Container registry</span></h3>
<p><span class="koboSpan" id="kobo.97.1">Just as we saw with</span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.98.1"> AWS, Azure has a robust</span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.99.1"> container registry service known as </span><strong class="bold"><span class="koboSpan" id="kobo.100.1">Azure Container Registry</span></strong><span class="koboSpan" id="kobo.101.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.102.1">ACR</span></strong><span class="koboSpan" id="kobo.103.1">). </span><span class="koboSpan" id="kobo.103.2">It acts as a private registry for hosting and managing your container images and Helm charts. </span><span class="koboSpan" id="kobo.103.3">As we did in the expedition along the Amazon, we’ll be using Docker to publish our container images to this repository so that we can reference them later from the Terraform code that provisions resources to our AKS cluster. </span><span class="koboSpan" id="kobo.103.4">We’ll need to grant our cluster</span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.104.1"> access using Azure managed identities and Azure </span><strong class="bold"><span class="koboSpan" id="kobo.105.1">Role-Based Access Control</span></strong><span class="koboSpan" id="kobo.106.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.107.1">RBAC</span></strong><span class="koboSpan" id="kobo.108.1">), which is similar to how we granted access to Amazon EKS using AWS’s IAM </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">ser</span><a id="_idTextAnchor518"/><span class="koboSpan" id="kobo.110.1">vice policies.</span></span></p>
<h3><span class="koboSpan" id="kobo.111.1">Load balancing</span></h3>
<p><span class="koboSpan" id="kobo.112.1">One of the biggest</span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.113.1"> advantages of hosting your container-based workloads using a Kubernetes-managed service is that much of the underlying infrastructure is automatically configured and maintained on your behalf. </span><span class="koboSpan" id="kobo.113.2">The service interprets your Kubernetes resource configuration and provisions the necessary resources within the cluster to properly configure Azure to support your workloads. </span><span class="koboSpan" id="kobo.113.3">Sometimes, this is handled transparently, and other times, there are special hooks that allow you more control over the configuration of the underlying resources </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">on Azure.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">In this manner, under the hood, AKS streamlines load balancing using either a basic Azure load balancer or a more feature-rich Azure application gateway. </span><span class="koboSpan" id="kobo.115.2">AKS manages the creation and configuration of these load balancers when services of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.116.1">LoadBalancer</span></strong><span class="koboSpan" id="kobo.117.1"> type are created within the Kubernetes cluster. </span><span class="koboSpan" id="kobo.117.2">For more control, users can also utilize</span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.118.1"> Ingress controllers such as NGINX or the Azure </span><strong class="bold"><span class="koboSpan" id="kobo.119.1">Application Gateway Ingress Controller</span></strong><span class="koboSpan" id="kobo.120.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.121.1">AGIC</span></strong><span class="koboSpan" id="kobo.122.1">) for advanced routing, SSL termination, and </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">other capabilities:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<span class="koboSpan" id="kobo.124.1"><img alt="Figure 11.4 – Network traffic flow of an AKS cluster" src="image/B21183_11_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.125.1">Figure 11.4 – Network traffic flow of an AKS cluster</span></p>
<p><span class="koboSpan" id="kobo.126.1">As we saw in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.127.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.128.1"> when working with AWS, we will be using the NGINX ingress controller but this time, we’ll be provisioning an Azure Application Gateway service to route traffic to NGINX. </span><span class="koboSpan" id="kobo.128.2">This works a bit differently than on AWS, where the NGINX ingress controller automatically configures the ALB through Kubernetes annotation. </span><span class="koboSpan" id="kobo.128.3">With Azure, we need to set up the NGINX ingress controller and then provision Azure Application Gateway</span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.129.1"> and configure it to forward t</span><a id="_idTextAnchor519"/><span class="koboSpan" id="kobo.130.1">raffic </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">to NGINX.</span></span></p>
<h3><span class="koboSpan" id="kobo.132.1">Network security</span></h3>
<p><span class="koboSpan" id="kobo.133.1">In AKS, network security</span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.134.1"> is managed in a manner akin to the practices described in </span><a href="B21183_10.xhtml#_idTextAnchor474"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.135.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.136.1"> for VMs as they are deployed within Azure virtual networks, thus allowing them to integrate seamlessly with existing Azure networking features. </span><span class="koboSpan" id="kobo.136.2">However, because Kubernetes</span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.137.1"> has an overlay network called Kubenet, which is the network on which our workloads (or pods) live, we need to use Kubernetes Network policies to control network traffic between our workloads based on Kubernetes tags or namespaces. </span><span class="koboSpan" id="kobo.137.2">There are more advanced networking security capabilities when you are working with Azure CNI and other open source solutions such as Calico, but these are beyond the s</span><a id="_idTextAnchor520"/><span class="koboSpan" id="kobo.138.1">cope</span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.139.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">this book.</span></span></p>
<h3><span class="koboSpan" id="kobo.141.1">Secrets management</span></h3>
<p><span class="koboSpan" id="kobo.142.1">Just as we saw</span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.143.1"> on our tour down the Amazon, Azure’s Kubernetes offering also integrates with other Azure</span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.144.1"> services, such as Azure’s secret management service, </span><strong class="bold"><span class="koboSpan" id="kobo.145.1">Azure Key Vault</span></strong><span class="koboSpan" id="kobo.146.1">. </span><span class="koboSpan" id="kobo.146.2">This integration is done through a combination of an AKS extension being enabled on the cluster itself and Kubernetes resources that are provisioned within the cluster, creating Kubernetes resources that our pods can use as a conduit to the secrets hosted on Azure Key Vault. </span><span class="koboSpan" id="kobo.146.3">Again, nothing is stopping us from using native Kubernetes secrets, but Azure Key Vault provides a much more streamlined and secure mechanism for granting Azure secrets. </span><span class="koboSpan" id="kobo.146.4">It allows us to keep secrets up-to-date to avoid outages when secret rotations occur, and it allows us to use managed identities to access the secrets rather than storing them on the </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">cluster itself.</span></span></p>
<p><span class="koboSpan" id="kobo.148.1">Just as we saw in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.149.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.150.1"> when building our solution with AWS EKS, we need to facilitate a bridge between Kubernetes and the cloud platform’s identity management system. </span><span class="koboSpan" id="kobo.150.2">On AWS, that was IAM; on Azure, that’s Entra ID. </span><span class="koboSpan" id="kobo.150.3">The process is largely the same but the terminology </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">is different:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<span class="koboSpan" id="kobo.152.1"><img alt="Figure 11.5 – AKS with Workload Identity" src="image/B21183_11_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.153.1">Figure 11.5 – AKS with Workload Identity</span></p>
<p><span class="koboSpan" id="kobo.154.1">First, we need to create</span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.155.1"> a managed identity that will represent the workload. </span><span class="koboSpan" id="kobo.155.2">This is an Azure resource that represents an Entra ID identity that is managed by the Azure platform. </span><span class="koboSpan" id="kobo.155.3">As we did with EKS, we need to federate between the Kubernetes cluster and Entra ID. </span><span class="koboSpan" id="kobo.155.4">On Azure, we do that by creating a federated identity credential that links the managed identity, the AKS cluster’s internal Open ID Connect provider, and Entra ID. </span><span class="koboSpan" id="kobo.155.5">Like on AWS, we plant a seed for this managed identity so that it can linked to a Kubernetes service account resource that will be provisioned later </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">within Kubernetes:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<span class="koboSpan" id="kobo.157.1"><img alt="Figure 11.6 – AKS Secrets Manager integration" src="image/B21183_11_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.158.1">Figure 11.6 – AKS Secrets Manager integration</span></p>
<p><span class="koboSpan" id="kobo.159.1">After Workload Identity has been established, we can grant access to Azure resources such as Key Vault and databases such as Azure Cosmos DB or Azure SQL Database. </span><span class="koboSpan" id="kobo.159.2">Just as we did in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.160.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.161.1"> with EKS, we’ll use the secrets store CSI driver and the Azure provider to integrate</span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.162.1"> our Kubernetes deployments </span><a id="_idTextAnchor521"/><span class="koboSpan" id="kobo.163.1">with Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">Key Vault.</span></span></p>
<h3><span class="koboSpan" id="kobo.165.1">Kubernetes cluster</span></h3>
<p><span class="koboSpan" id="kobo.166.1">Finally, creating a Kubernetes cluster</span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.167.1"> using AKS involves a few critical components. </span><span class="koboSpan" id="kobo.167.2">As we’ve established, we need a virtual network, managed identities, and sufficient RBAC to access the resources our cluster needs, such as container registries and Azure Key Vault secrets. </span><span class="koboSpan" id="kobo.167.3">However, the main components of our Kubernetes cluster are the node pools, which provide compute resources to host </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">our pods:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<span class="koboSpan" id="kobo.169.1"><img alt="Figure 11.7 – Anatomy of an AKS cluster" src="image/B21183_11_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.170.1">Figure 11.7 – Anatomy of an AKS cluster</span></p>
<p><span class="koboSpan" id="kobo.171.1">By default, every AKS cluster comes with a default node pool, which is where Kubernetes’ system services are hosted. </span><span class="koboSpan" id="kobo.171.2">However, we can add additional node pools either to isolate our application workloads or to grant access to different types of computing resources, such as different hardware</span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.172.1"> profiles to meet the specific needs o</span><a id="_idTextAnchor522"/><span class="koboSpan" id="kobo.173.1">f </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">different workloads.</span></span></p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor523"/><span class="koboSpan" id="kobo.175.1">Deployment architecture</span></h2>
<p><span class="koboSpan" id="kobo.176.1">As we saw with the cloud</span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.177.1"> architecture, there were many similarities between the work that we did in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.178.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.179.1"> with AWS. </span><span class="koboSpan" id="kobo.179.2">The deployment architecture will mirror what we saw in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.180.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.181.1"> as well. </span><span class="koboSpan" id="kobo.181.2">We looked at the differences in the Terraform provider in the previous chapter when we configured the </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">azurerm</span></strong><span class="koboSpan" id="kobo.183.1"> provider to provision our solution to </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">Azure VMs.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">Now, using container-based architecture, the only real differences from the way we deployed in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.186.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.187.1"> with AWS will be the way we authenticate with the container registry and the Kubernetes cluster. </span><span class="koboSpan" id="kobo.187.2">I encourage you to review the deployment architectural approach outlined in the corresponding section of </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.188.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.189.1">. </span><span class="koboSpan" id="kobo.189.2">In the next section, we’ll go into the details of building the same solution on Azure, but again, we’ll take care not to re</span><a id="_idTextAnchor524"/><span class="koboSpan" id="kobo.190.1">-tread the </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">same ground.</span></span></p>
<p><span class="koboSpan" id="kobo.192.1">In this section, we reviewed the key changes in our architecture as we transitioned from VM-based architecture to container-based architecture. </span><span class="koboSpan" id="kobo.192.2">We were careful not to retread the ground we covered in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.193.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.194.1">, where we went through this transformation first on AWS. </span><span class="koboSpan" id="kobo.194.2">In the next section, we’ll get tactical in building the solution, but again, we’ll be careful to build on the foundations we built in the previous chapter when we first set up our solution</span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.195.1"> on Microsoft Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">using VMs.</span></span></p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor525"/><span class="koboSpan" id="kobo.197.1">Building the solution</span></h1>
<p><span class="koboSpan" id="kobo.198.1">In this section, we’ll be taking</span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.199.1"> our theoretical knowledge and applying it to a tangible, functioning solution while harnessing the power of Docker, Terraform, and Kubernetes on the Microsoft Azure platform. </span><span class="koboSpan" id="kobo.199.2">Some parts of this process will require significant change, such as when we provision our Azure infrastructure using Terraform; other parts will have minor changes, such as the Kubernetes configuration that we use to deploy our application to our Kubernetes cluster; and some will have almost no change whatsoever, such as when we build and push our Docker images t</span><a id="_idTextAnchor526"/><span class="koboSpan" id="kobo.200.1">o our </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">container registry.</span></span></p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor527"/><span class="koboSpan" id="kobo.202.1">Docker</span></h2>
<p><span class="koboSpan" id="kobo.203.1">In this section, we’ll learn how</span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.204.1"> to implement our Dockerfile, which installs our .NET application code and runs the service in a container. </span><span class="koboSpan" id="kobo.204.2">If you skipped </span><em class="italic"><span class="koboSpan" id="kobo.205.1">Chapters 7</span></em><span class="koboSpan" id="kobo.206.1"> through </span><em class="italic"><span class="koboSpan" id="kobo.207.1">9</span></em><span class="koboSpan" id="kobo.208.1"> due to a lack of interest in AWS, I can’t hold that against you – particularly if your primary interest in reading this book is working on the Microsoft Azure cloud platform. </span><span class="koboSpan" id="kobo.208.2">However, I encourage you to review the corresponding section within </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.209.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.210.1"> to see how we use Docker to configure a container with </span><a id="_idTextAnchor528"/><span class="koboSpan" id="kobo.211.1">our .NET </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">application code.</span></span></p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor529"/><span class="koboSpan" id="kobo.213.1">Infrastructure</span></h2>
<p><span class="koboSpan" id="kobo.214.1">As we discussed</span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.215.1"> in the previous section, much of the infrastructure</span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.216.1"> is unchanged when using container-based architecture. </span><span class="koboSpan" id="kobo.216.2">Therefore, in this section, we’ll be focusing on what’s different when</span><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.217.1"> we use Azure’s Kubernetes</span><a id="_idIndexMarker884"/> <span class="No-Break"><span class="koboSpan" id="kobo.218.1">managed service.</span></span></p>
<h3><span class="koboSpan" id="kobo.219.1">Container registry</span></h3>
<p><span class="koboSpan" id="kobo.220.1">The first component</span><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.221.1"> we need to provision is our </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">container registry</span></strong><span class="koboSpan" id="kobo.223.1">. </span><span class="koboSpan" id="kobo.223.2">The container registry is often provisioned as part of a separate deployment that’s reserved for shared infrastructure that is reused across multiple applications. </span><span class="koboSpan" id="kobo.223.3">This can help when you have a common set of custom-built images that multiple teams or projects need to use in their applications or services. </span><span class="koboSpan" id="kobo.223.4">However, you should keep in mind that the container registry does act as an important security boundary, so if you want to ensure that application teams can only access images built for their applications, you should provision an isolated container registry for each </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">project team:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.225.1">
resource "azurerm_container_registry" "main" {
  name                    = replace("acr${var.application_name}${var.environment_name}", "-", "")
  resource_group_name     = azurerm_resource_group.main.name
  location                = azurerm_resource_group.main.location
  sku                     = "Premium"
  admin_enabled           = true
  zone_redundancy_enabled = true
}</span></pre> <p><span class="koboSpan" id="kobo.226.1">The preceding code provisions the Azure container registry. </span><span class="koboSpan" id="kobo.226.2">It’s important to note that this resource has very specific requirements for </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">the name:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.228.1">
resource "azurerm_role_assignment" "acr_push" {
  count = length(var.container_registry_pushers)
  scope                = azurerm_container_registry.main.id
  role_definition_name = "AcrPush"
  principal_id         = var.container_registry_pushers[count.index]
}</span></pre> <p><span class="koboSpan" id="kobo.229.1">The preceding code creates a role assignment that will allow different users to push container images to this container registry. </span><span class="koboSpan" id="kobo.229.2">This is a critical requirement that allows our GitHub Action to publish the Docker image we build to our Azure container registry. </span><span class="koboSpan" id="kobo.229.3">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.230.1">principal_id</span></strong><span class="koboSpan" id="kobo.231.1"> must be set to the identity of the service account that our GitHub Action impersonates. </span><span class="koboSpan" id="kobo.231.2">In this case, I passed in a collection of these and iterated over that collection using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.232.1">count</span></strong><span class="koboSpan" id="kobo.233.1"> meta-argument. </span><span class="koboSpan" id="kobo.233.2">In the case of role assignments, because these resources are so lightweight, it doesn’t matter much if we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">for_each</span></strong><span class="koboSpan" id="kobo.235.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.236.1">count</span></strong><span class="koboSpan" id="kobo.237.1"> because the drop-create that will occur more frequently when using </span><strong class="source-inline"><span class="koboSpan" id="kobo.238.1">count</span></strong><span class="koboSpan" id="kobo.239.1"> has little impact</span><a id="_idIndexMarker886"/><span class="koboSpan" id="kobo.240.1"> on </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">the deployment.</span></span></p>
<h3><span class="koboSpan" id="kobo.242.1">Kubernetes cluster</span></h3>
<p><span class="koboSpan" id="kobo.243.1">The next step is to provision</span><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.244.1"> a Kubernetes cluster using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">azurerm_kubernetes_cluster</span></strong><span class="koboSpan" id="kobo.246.1"> resource. </span><span class="koboSpan" id="kobo.246.2">This resource will be the central figure in our </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">AKS infrastructure:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.248.1">
resource "azurerm_kubernetes_cluster" "main" {
  name                      = "aks-${var.application_name}-${var.environment_name}"
  location                  = azurerm_resource_group.main.location
  resource_group_name       = azurerm_resource_group.main.name
  dns_prefix                = "${var.application_name}-${var.environment_name}"
  node_resource_group       = "${azurerm_resource_group.main.name}-cluster"
  sku_tier                  = "Standard"
  ...
</span><span class="koboSpan" id="kobo.248.2">}</span></pre> <p><span class="koboSpan" id="kobo.249.1">The preceding code configures some important top-level attributes that influence pricing, networking, and internally managed resource placement. </span><span class="koboSpan" id="kobo.249.2">AKS will provision resources to two resource groups. </span><span class="koboSpan" id="kobo.249.3">One is where the AKS resource exists, and the other is where AKS provisions the internal Azure resources that make up the internals of the cluster. </span><span class="koboSpan" id="kobo.249.4">This secondary resource group’s name is controlled by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">node_resource_group</span></strong><span class="koboSpan" id="kobo.251.1"> attribute. </span><span class="koboSpan" id="kobo.251.2">I would always recommend setting the </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">node_resource_group</span></strong><span class="koboSpan" id="kobo.253.1"> name to something cohesive with the naming convention of the AKS cluster </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">resource itself.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">As we learned in </span><a href="B21183_05.xhtml#_idTextAnchor278"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.256.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.257.1">, Kubernetes</span><a id="_idIndexMarker888"/><span class="koboSpan" id="kobo.258.1"> has several system services that need to be deployed and in good health for the cluster to function correctly. </span><span class="koboSpan" id="kobo.258.2">Our AKS cluster needs to have one or more node pools to host system and user workloads. </span><span class="koboSpan" id="kobo.258.3">The default node pool is a great place to host these </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">system services:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.260.1">
resource "azurerm_kubernetes_cluster" "main" {
  ...
</span><span class="koboSpan" id="kobo.260.2">  default_node_pool {
  name                        = "systempool"
  vm_size                     = var.aks_system_pool.vm_size
  enable_auto_scaling         = true
  min_count                   = var.aks_system_pool.min_node_count
  max_count                   = var.aks_system_pool.max_node_count
  vnet_subnet_id              = azurerm_subnet.kubernetes.id
  os_disk_type                = "Ephemeral"
  os_disk_size_gb             = 30
  orchestrator_version        = var.aks_orchestration_version
  temporary_name_for_rotation = "workloadpool"
  zones = [1, 2, 3]
  upgrade_settings {
    max_surge = "33%"
  }
  ...
</span><span class="koboSpan" id="kobo.260.3">}</span></pre> <p><span class="koboSpan" id="kobo.261.1">Additional node pools, such as the following</span><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.262.1"> one, can be created to allow us to isolate our custom deployments on dedicated computing resources so that they don’t impact the day-to-day operations</span><a id="_idIndexMarker890"/><span class="koboSpan" id="kobo.263.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">the cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.265.1">
resource "azurerm_kubernetes_cluster_node_pool" "workload" {
  name                  = "workloadpool"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.main.id
  vm_size               = var.aks_workload_pool.vm_size
  enable_auto_scaling   = true
  min_count             = var.aks_workload_pool.min_node_count
  max_count             = var.aks_workload_pool.max_node_count
  vnet_subnet_id        = azurerm_subnet.kubernetes.id
  os_disk_type          = "Ephemeral"
  orchestrator_version  = var.aks_orchestration_version
  mode  = "User" # Define this node pool as a "user" aka workload node pool
  zones = [1, 2, 3]
  upgrade_settings {
    max_surge = "33%"
  }
  node_labels = {
    "role" = "workload"
  }
  node_taints = [
    "workload=true:NoSchedule"
  ]
}</span></pre> <p><span class="koboSpan" id="kobo.266.1">By setting a taint on the nodes within this node pool, we can ensure that only Kubernetes deployments that are explicitly targeted to this node pool will be scheduled here. </span><span class="koboSpan" id="kobo.266.2">By employing taints on your additional node pools, you can isolate Kubernetes system services from the default node pool and keep your workloads in their own space. </span><span class="koboSpan" id="kobo.266.3">This does have additional costs, but it will greatly improve the health and performance of the cluster. </span><span class="koboSpan" id="kobo.266.4">It is something you should do if you’re planning on deploying production workloads</span><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.267.1"> to your cluster – but if you’re just kic</span><a id="_idTextAnchor530"/><span class="koboSpan" id="kobo.268.1">king the tires, feel free to </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">skip it!</span></span></p>
<h3><span class="koboSpan" id="kobo.270.1">Identity and access management</span></h3>
<p><span class="koboSpan" id="kobo.271.1">Managed identities</span><a id="_idIndexMarker892"/><span class="koboSpan" id="kobo.272.1"> play an integral role in the configuration of AKS in several different ways. </span><span class="koboSpan" id="kobo.272.2">The first and most important is the managed identity that AKS will use to provision the </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">internal resources:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.274.1">
resource "azurerm_kubernetes_cluster" "main" {
  ...
</span><span class="koboSpan" id="kobo.274.2">  identity {
    type         = "UserAssigned"
    identity_ids = [azurerm_user_assigned_identity.cluster.id]
  }
  ...
</span><span class="koboSpan" id="kobo.274.3">}</span></pre> <p><span class="koboSpan" id="kobo.275.1">This identity needs to be assigned the </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">Managed Identity Operator</span></strong><span class="koboSpan" id="kobo.277.1"> role to perform </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">this function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.279.1">
resource "azurerm_role_assignment" "cluster_identity_operator" {
  scope                = azurerm_resource_group.main.id
  role_definition_name = "Managed Identity Operator"
  principal_id         = azurerm_user_assigned_identity.cluster.principal_id
}</span></pre> <p><span class="koboSpan" id="kobo.280.1">The preceding code</span><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.281.1"> creates this role assignment using a </span><strong class="bold"><span class="koboSpan" id="kobo.282.1">user-assigned managed identity</span></strong><span class="koboSpan" id="kobo.283.1">. </span><span class="koboSpan" id="kobo.283.2">We explored this topic in the previous chapter, so we know that this is a special type of managed identity that we explicitly provision and assign role assignments to. </span><span class="koboSpan" id="kobo.283.3">This is in contrast to the system-assigned identity, which is a managed identity that is automatically provisioned and managed by the </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">platform itself.</span></span></p>
<p><span class="koboSpan" id="kobo.285.1">There is another important</span><a id="_idIndexMarker894"/><span class="koboSpan" id="kobo.286.1"> identity that needs to be set on the AKS cluster: the managed identity used by the kubelet system service that’s deployed to each node within </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">the cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.288.1">
resource "azurerm_kubernetes_cluster" "main" {
  ...
</span><span class="koboSpan" id="kobo.288.2">  kubelet_identity {
    client_id                 = azurerm_user_assigned_identity.cluster_kubelet.client_id
    object_id                 = azurerm_user_assigned_identity.cluster_kubelet.principal_id
    user_assigned_identity_id = azurerm_user_assigned_identity.cluster_kubelet.id
  }
}</span></pre> <p><span class="koboSpan" id="kobo.289.1">The preceding code configures the cluster’s kubelet identity. </span><span class="koboSpan" id="kobo.289.2">This is a little inconsistent than how managed identities are typically attached within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">azurerm</span></strong><span class="koboSpan" id="kobo.291.1"> provider, so it’s important to get the correct outputs from the user-assigned identity to the right attributes of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">kubelet_identity</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.293.1"> block.</span></span></p>
<p><span class="koboSpan" id="kobo.294.1">As we learned in </span><a href="B21183_05.xhtml#_idTextAnchor278"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.295.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.296.1">, the kubelet system</span><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.297.1"> service processes orders from the scheduler. </span><span class="koboSpan" id="kobo.297.2">To do this, kubelet will need access to pull container images from our ACR. </span><span class="koboSpan" id="kobo.297.3">This will require the </span><strong class="source-inline"><span class="koboSpan" id="kobo.298.1">AcrPull</span></strong><span class="koboSpan" id="kobo.299.1"> role assignment to be added to the preceding </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">managed identity:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.301.1">
resource "azurerm_role_assignment" "cluster_kubelet_acr" {
  principal_id         = azurerm_user_assigned_identity.cluster_kubelet.principal_id
  role_definition_name = "AcrPull"
  scope                = azurerm_container_registry.main.id
}</span></pre> <h3><span class="koboSpan" id="kobo.302.1">Secrets management</span></h3>
<p><span class="koboSpan" id="kobo.303.1">To integrate with Azure’s secret</span><a id="_idIndexMarker896"/><span class="koboSpan" id="kobo.304.1"> management service, Key Vault, we need to take a couple of steps. </span><span class="koboSpan" id="kobo.304.2">The first is to simply enable the subsystem on the cluster itself. </span><span class="koboSpan" id="kobo.304.3">AKS has an extensible model for such features – including but not limited</span><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.305.1"> to enabling integrations</span><a id="_idIndexMarker898"/><span class="koboSpan" id="kobo.306.1"> with other Azure services</span><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.307.1"> and Kubernetes features such as </span><strong class="bold"><span class="koboSpan" id="kobo.308.1">Kubernetes Event Driven Architecture</span></strong><span class="koboSpan" id="kobo.309.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.310.1">KEDA</span></strong><span class="koboSpan" id="kobo.311.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.312.1">Azure Monitor</span></strong><span class="koboSpan" id="kobo.313.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.314.1">Open </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.315.1">Service Mesh</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.317.1">
resource "azurerm_kubernetes_cluster" "main" {
  ...
</span><span class="koboSpan" id="kobo.317.2">  key_vault_secrets_provider {
    secret_rotation_enabled  = true
    secret_rotation_interval = "5m"
  }
  ...
</span><span class="koboSpan" id="kobo.317.3">}</span></pre> <p><span class="koboSpan" id="kobo.318.1">The preceding code enables and configures secret rotation. </span><span class="koboSpan" id="kobo.318.2">This is just the first step in enabling AKS integration with Azure Key Vault; we also have to set up the CSI provider for pods to pull secrets from Key Vault. </span><span class="koboSpan" id="kobo.318.3">We’ll look at that in the next section when we start provisi</span><a id="_idTextAnchor531"/><span class="koboSpan" id="kobo.319.1">oning things</span><a id="_idIndexMarker900"/><span class="koboSpan" id="kobo.320.1"> to the Kubernetes </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">control plane.</span></span></p>
<h3><span class="koboSpan" id="kobo.322.1">Workload identity</span></h3>
<p><span class="koboSpan" id="kobo.323.1">To allow our pods</span><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.324.1"> to access other resources that are deployed to Azure, we need to allow them to impersonate a managed identity. </span><span class="koboSpan" id="kobo.324.2">Like the integration with Key Vault, we first need to enable this extension on the </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">AKS cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.326.1">
resource "azurerm_kubernetes_cluster" "main" {
  ...
</span><span class="koboSpan" id="kobo.326.2">  oidc_issuer_enabled       = true
  workload_identity_enabled = true
  ...
</span><span class="koboSpan" id="kobo.326.3">}</span></pre> <p><span class="koboSpan" id="kobo.327.1">The preceding</span><a id="_idIndexMarker902"/><span class="koboSpan" id="kobo.328.1"> code activates an internal </span><strong class="bold"><span class="koboSpan" id="kobo.329.1">OpenID Connect</span></strong><span class="koboSpan" id="kobo.330.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.331.1">OIDC</span></strong><span class="koboSpan" id="kobo.332.1">) endpoint that’s used to sign and issue </span><strong class="bold"><span class="koboSpan" id="kobo.333.1">JSON Web Tokens</span></strong><span class="koboSpan" id="kobo.334.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.335.1">JWTs</span></strong><span class="koboSpan" id="kobo.336.1">) for the service accounts</span><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.337.1"> within the cluster. </span><span class="koboSpan" id="kobo.337.2">After this is enabled, we’ll also need Azure federated identity credential, which, once linked to the AKS</span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.338.1"> cluster’s OIDC issuer endpoint and the managed identity to be used by the workloads, creates federation between the cluster and Microsoft Entra ID. </span><span class="koboSpan" id="kobo.338.2">This allows the pods using the corresponding Kubernetes service account to interact with Azure services using the privileges of the </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">managed identity:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.340.1">
resource "azurerm_federated_identity_credential" "main" {
  name                = azurerm_user_assigned_identity.workload.name
  resource_group_name = azurerm_resource_group.main.name
  audience            = ["api://AzureADTokenExchange"]
  issuer              = azurerm_kubernetes_cluster.main.oidc_issuer_url
  parent_id           = azurerm_user_assigned_identity.workload.id
  subject             = "system:serviceaccount:${var.k8s_namespace}:${var.k8s_service_account_name}"
}</span></pre> <p><span class="koboSpan" id="kobo.341.1">Just as we did in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.342.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.343.1"> when working with AWS, we’ll link this to a Kubernetes service account</span><a id="_idIndexMarker905"/><span class="koboSpan" id="kobo.344.1"> in the next</span><a id="_idTextAnchor532"/><span class="koboSpan" id="kobo.345.1"> section when we provision resources </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">to Kubernetes.</span></span></p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor533"/><span class="koboSpan" id="kobo.347.1">Kubernetes</span></h2>
<p><span class="koboSpan" id="kobo.348.1">As we saw in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.349.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.350.1">, we built</span><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.351.1"> out the Kubernetes deployments</span><a id="_idIndexMarker907"/><span class="koboSpan" id="kobo.352.1"> using the Terraform provider for Kubernetes. </span><span class="koboSpan" id="kobo.352.2">Like Packer and Docker, Kubernetes, in its own way, provides a control plane that operates consistently across cloud platforms. </span><span class="koboSpan" id="kobo.352.3">As a result, much of the Kubernetes deployment process is reusable, regardless of what cloud platform you choose. </span><span class="koboSpan" id="kobo.352.4">This is also one of the appeals of Kubernetes as a way to implement cloud agnostic or cloud portable workloads yet leverage the efficiency and elasticity that Kubernetes managed service </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">offerings provide.</span></span></p>
<p><span class="koboSpan" id="kobo.354.1">In this chapter, we won’t retread the same topics. </span><span class="koboSpan" id="kobo.354.2">If you happened to skip </span><em class="italic"><span class="koboSpan" id="kobo.355.1">Chapters 7</span></em><span class="koboSpan" id="kobo.356.1"> through </span><em class="italic"><span class="koboSpan" id="kobo.357.1">9</span></em><span class="koboSpan" id="kobo.358.1"> due to a lack of interest in AWS, I highly recommend going back and reviewing the corresponding section in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.359.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.360.1"> for more details ab</span><a id="_idTextAnchor534"/><span class="koboSpan" id="kobo.361.1">out the implementation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">Kubernetes deployments.</span></span></p>
<h3><span class="koboSpan" id="kobo.363.1">Provider setup</span></h3>
<p><span class="koboSpan" id="kobo.364.1">As we saw in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.365.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.366.1">, when executing</span><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.367.1"> Terraform using the Kubernetes provider to provision resources to the Kubernetes control plane, we don’t have to make many changes. </span><span class="koboSpan" id="kobo.367.2">We still authenticate against our target cloud platform, we still follow Terraform’s core workflow, and we still pass in additional input parameters for platform-specific resources that we need to reference. </span><span class="koboSpan" id="kobo.367.3">Most notably, information about the cluster, other Azure services, such as ACR, Key Vault, and managed identities, and other details might need to be put into Kubernetes ConfigMaps that can be used by the pods to point themselves at the endpoint of </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">their database:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.369.1">
data "azurerm_kubernetes_cluster" "main" {
    name                = var.kubernetes_cluster_name
    resource_group_name = var.resource_group_name
}</span></pre> <p><span class="koboSpan" id="kobo.370.1">Here, we’re using a layered approach to provision the infrastructure first and then provision to Kubernetes. </span><span class="koboSpan" id="kobo.370.2">As a result, we can reference the Kubernetes cluster using the data source for a resource that was provisioned by the Terraform workspace that’s responsible for the Azure infrastructure. </span><span class="koboSpan" id="kobo.370.3">This allows users to access important connectivity details without exporting them outside of Terraform and passing them around during the </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">deployment process.</span></span></p>
<p><span class="koboSpan" id="kobo.372.1">The preceding code is a reference</span><a id="_idIndexMarker909"/><span class="koboSpan" id="kobo.373.1"> to the AKS cluster that was provisioned in the previous deployment stage. </span><span class="koboSpan" id="kobo.373.2">Using this reference, we can initialize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">kubernetes</span></strong><span class="koboSpan" id="kobo.375.1"> provider by using several pieces of data to authenticate with </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">the cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.377.1">
provider "kubernetes" {
    host                   = data.azurerm_kubernetes_cluster.main.kube_admin_config[0].host
    client_key             = base64decode(data.azurerm_kubernetes_cluster.main.kube_admin_config[0].client_key)
    client_certificate     = base64decode(data.azurerm_kubernetes_cluster.main.kube_admin_config[0].client_certificate)
    cluster_ca_certificate = base64decode(data.azurerm_kubernetes_cluster.main.kube_admin_config[0].cluster_ca_certificate)
}</span></pre> <p><span class="koboSpan" id="kobo.378.1">The client key is the private key that’s used for authentication, the client certificate is the certificate that’s paired with the private key to perform authentication, and the cluster’s CA certificate is the certificate of the certificate authority that’s used to verify the Kubernetes </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">API server.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">In addition, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">helm</span></strong><span class="koboSpan" id="kobo.382.1"> provider</span><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.383.1"> can be configured using the same parameters. </span><span class="koboSpan" id="kobo.383.2">This can help provide pre-packaged templates of Kubernetes resources via </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">Hel</span><a id="_idTextAnchor535"/><span class="koboSpan" id="kobo.385.1">m charts:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.386.1">
provider "helm" {
  kubernetes {
    ...
</span><span class="koboSpan" id="kobo.386.2">  }
}</span></pre> <h3><span class="koboSpan" id="kobo.387.1">Secrets</span></h3>
<p><span class="koboSpan" id="kobo.388.1">In the previous section, we enabled</span><a id="_idIndexMarker911"/><span class="koboSpan" id="kobo.389.1"> the Key Vault extension on the cluster itself. </span><span class="koboSpan" id="kobo.389.2">Now, we need to provide a way for the pods to connect to Azure Key Vault. </span><span class="koboSpan" id="kobo.389.3">This requires</span><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.390.1"> us to use the Kubernetes secrets store </span><strong class="bold"><span class="koboSpan" id="kobo.391.1">Container Storage Interface</span></strong><span class="koboSpan" id="kobo.392.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.393.1">CSI</span></strong><span class="koboSpan" id="kobo.394.1">) driver. </span><span class="koboSpan" id="kobo.394.2">This configuration acts as a conduit, granting Workload Identity the necessary permissions to read specific secrets from the designated </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">Key Vault:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.396.1">
resource "kubernetes_manifest" "secret_provider_class" {
  manifest = {
    apiVersion = "secrets-store.csi.x-k8s.io/v1"
    kind       = "SecretProviderClass"
    metadata = {
      name      = "web-app-secrets"
      namespace = var.namespace
    }
    spec = {
      provider = "azure"
        secretObjects = [
        {
          data = [
            {
              key        = "db-admin-password"
              objectName = "db-admin-password"
            }
          ]
          secretName = "db-admin-password"
          type       = "Opaque"
        }
      ]
      parameters = {
        usePodIdentity = "false"
        clientID       = var.service_account_client_id
        keyvaultName   = var.keyvault_name
        cloudName      = ""
        objects = yamlencode([
          {
            objectName    = "db-admin-password"
            objectType    = "secret"
            objectVersion = ""
          }
        ])
        tenantId = var.tenant_id
      }
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.397.1">In the preceding code, we need to provision this Kubernetes resource into the namespace we plan on deploying our pods and specify the Key Vault, the managed identity that we configured with the Azure federated identity credential, and the Kubernetes</span><a id="_idIndexMarker913"/> <span class="No-Break"><span class="koboSpan" id="kobo.398.1">service account.</span></span></p>
<h3><span class="koboSpan" id="kobo.399.1">Workload identity</span></h3>
<p><span class="koboSpan" id="kobo.400.1">To ensure that our pods</span><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.401.1"> use the managed identity, we need to take a few actions that use both Azure-specific schema and standard Kubernetes schema by provisioning resources within Kubernetes and configurations within the deployment specifications of </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">our pods.</span></span></p>
<p><span class="koboSpan" id="kobo.403.1">The first thing we need to do is create a Kubernetes service account. </span><span class="koboSpan" id="kobo.403.2">This is a standard resource within Kubernetes but we use Azure-specific schema to associate it with the Azure federated </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">identity credential:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.405.1">
resource "kubernetes_service_account" "main" {
  metadata {
    namespace = var.namespace
    name      = var.service_account_name
    annotations = {
      "azure.workload.identity/client-id" = var.service_account_client_id
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.406.1">Using Terraform allows us to substitute dynamic values that are created during the earlier stage of the provisioning process. </span><span class="koboSpan" id="kobo.406.2">Kubernetes has its own way of doing things but it involves using Helm and has additional </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">implementation overhead.</span></span></p>
<p><span class="koboSpan" id="kobo.408.1">Now that the service account exists in Kubernetes and is linked to the appropriate Azure managed identity credential, the next step is to enable Azure Workload Identity within the deployment. </span><span class="koboSpan" id="kobo.408.2">To do this, we need to specify a special label, </span><strong class="source-inline"><span class="koboSpan" id="kobo.409.1">azure.workload.identity/use</span></strong><span class="koboSpan" id="kobo.410.1">, and set its value </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">true</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.414.1">
labels = {
    "azure.workload.identity/use" = "true"
}</span></pre> <p><span class="koboSpan" id="kobo.415.1">This will inform AKS to connect the pods within this deployment to the managed identity linked through the Azure federated </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">identity credential.</span></span></p>
<p><span class="koboSpan" id="kobo.417.1">The next step is to specify the corresponding Kubernetes service account that we already linked to the Azure federated identity credential in the previous section. </span><span class="koboSpan" id="kobo.417.2">This service account is set on the pod’s specification within </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">the deployment:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.419.1">
spec {
    ...
</span><span class="koboSpan" id="kobo.419.2">    service_account_name = "workload"
    ...
</span><span class="koboSpan" id="kobo.419.3">}</span></pre> <p><span class="koboSpan" id="kobo.420.1">Now that we have built</span><a id="_idIndexMarker915"/><span class="koboSpan" id="kobo.421.1"> out the three components of our architecture, in the next section, we’ll learn how to automate the deployment using Docker so that we can build and publish the container images. </span><span class="koboSpan" id="kobo.421.2">Then, we’ll use Terraform to provision our infrastructure and deploy our solution </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">to Kubernetes.</span></span></p>
<h1 id="_idParaDest-216"><a id="_idTextAnchor536"/><span class="koboSpan" id="kobo.423.1">Automating the deployment</span></h1>
<p><span class="koboSpan" id="kobo.424.1">In this section, we’ll learn how to automate</span><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.425.1"> the deployment process</span><a id="_idIndexMarker917"/><span class="koboSpan" id="kobo.426.1"> for container-based architectures. </span><span class="koboSpan" id="kobo.426.2">We’ll be employing similar techniques that we saw in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.427.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.428.1"> when we took this same journey down the Amazon. </span><span class="koboSpan" id="kobo.428.2">As a result, we’ll focus on what changes we need to make when we want to deploy to Microsoft Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">and AKS.</span></span></p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor537"/><span class="koboSpan" id="kobo.430.1">Docker</span></h2>
<p><span class="koboSpan" id="kobo.431.1">In </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.432.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.433.1">, we covered each step of the GitHub Actions</span><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.434.1"> workflow that executes Docker to build, tag, and push our Docker container images. </span><span class="koboSpan" id="kobo.434.2">Thanks to the nature of Docker’s cloud-agnostic architecture, this overwhelmingly stays the same. </span><span class="koboSpan" id="kobo.434.3">The only thing that changes is the way we must configure Docker so that it targets our Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">container registry.</span></span></p>
<p><span class="koboSpan" id="kobo.436.1">Like in </span><a href="B21183_08.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.437.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.438.1">, we need to connect to the container registry that we provisioned with Terraform. </span><span class="koboSpan" id="kobo.438.2">On Azure, that means we’ll need the Entra ID service principal’s client ID and </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">client secret:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.440.1">
- name: Login to Azure Container Registry
    uses: docker/login-action@v3
    with:
    registry: ${{ steps.terraform.outputs.registry_endpoint }}
    username: ${{ vars.DOCKER_ARM_CLIENT_ID }}
    password: ${{ secrets.DOCKER_ARM_CLIENT_SECRET }}</span></pre> <p><span class="koboSpan" id="kobo.441.1">This service principal is the same identity that we configured as inputs in Terraform that provision the infrastructure. </span><span class="koboSpan" id="kobo.441.2">As part of that process, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">AcrPush</span></strong><span class="koboSpan" id="kobo.443.1"> role assignment was associated with this identity. </span><span class="koboSpan" id="kobo.443.2">This grants it permission to publish images </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">to ACR:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.445.1">
- name: Build and push Docker image to ACR
    uses: docker/build-push-action@v5
    with:
    context: ${{ env.DOCKER_WORKING_DIRECTORY }}
    push: true
    tags: ${{ steps.terraform.outputs.registry_endpoint }}/${{ env.DOCKER_IMAGE_NAME }}:${{ steps.image-version.outputs.version }}</span></pre> <p><span class="koboSpan" id="kobo.446.1">The preceding code uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">docker\build-push-action</span></strong><span class="koboSpan" id="kobo.448.1"> to push the container image that we built in this GitHub Action to our Azure container registry. </span><span class="koboSpan" id="kobo.448.2">As we did in AWS, we reference the outputs</span><a id="_idIndexMarker919"/><span class="koboSpan" id="kobo.449.1"> fr</span><a id="_idTextAnchor538"/><span class="koboSpan" id="kobo.450.1">om the Terraform infrastructure stage to obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">ACR endpoint.</span></span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor539"/><span class="koboSpan" id="kobo.452.1">Terraform</span></h2>
<p><span class="koboSpan" id="kobo.453.1">In </span><a href="B21183_10.xhtml#_idTextAnchor474"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.454.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.455.1">, we comprehensively covered the process</span><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.456.1"> of creating a Terraform GitHub Action that authenticates with Azure using a Microsoft Entra ID service principal. </span><span class="koboSpan" id="kobo.456.2">Therefore, we won’t be delving into it any further. </span><span class="koboSpan" id="kobo.456.3">I encourage you to refer back to </span><a href="B21183_10.xhtml#_idTextAnchor474"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.457.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.458.1"> to review </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">the process.</span></span></p>
<h2 id="_idParaDest-219"><a id="_idTextAnchor540"/><span class="koboSpan" id="kobo.460.1">Kubernetes</span></h2>
<p><span class="koboSpan" id="kobo.461.1">When we automate Kubernetes</span><a id="_idIndexMarker921"/><span class="koboSpan" id="kobo.462.1"> with Terraform, we are just running </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">terraform apply</span></strong><span class="koboSpan" id="kobo.464.1"> again with a different root module. </span><span class="koboSpan" id="kobo.464.2">This time, the root module will configure the </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">kubernetes</span></strong><span class="koboSpan" id="kobo.466.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">helm</span></strong><span class="koboSpan" id="kobo.468.1"> providers in addition to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">azurerm</span></strong><span class="koboSpan" id="kobo.470.1"> provider. </span><span class="koboSpan" id="kobo.470.2">However, we won’t create new resources with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">azurerm</span></strong><span class="koboSpan" id="kobo.472.1"> provider; we will only obtain data sources to existing resources we provisioned in the previous </span><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">terraform apply</span></strong><span class="koboSpan" id="kobo.474.1"> command that provisioned the infrastructure </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">to Azure.</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">As a result, the GitHub Action that executes this process will look strikingly similar to how we executed Terraform with Azure. </span><span class="koboSpan" id="kobo.476.2">Some of the variables might change to include </span><a id="_idTextAnchor541"/><span class="koboSpan" id="kobo.477.1">things such as the container image details and </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">cluster information.</span></span></p>
<h1 id="_idParaDest-220"><a id="_idTextAnchor542"/><span class="koboSpan" id="kobo.479.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.480.1">In this chapter, we designed, built, and automated the deployment of a complete and end-to-end solution using container-based architecture. </span><span class="koboSpan" id="kobo.480.2">We built onto the foundations from </span><a href="B21183_10.xhtml#_idTextAnchor474"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.481.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.482.1">, where we worked with the foundational infrastructure of Azure virtual networks but layered on AKS to host our application in containers. </span><span class="koboSpan" id="kobo.482.2">In the next and final step in our Azure journey, we’ll be looking at serverless architecture, moving beyond the underlying infrastructure, and letting the platform itself take our solution to </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">new heights.</span></span></p>
</div>
</body></html>