- en: Real-Time Data Analysis - Azure Stream Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While some Azure components enable us to deliver data to the cloud, in most
    cases we also need something that is designed for analyzing and querying streamed
    data. One such service is Azure Stream Analytics, a real-time data analysis tool,
    which is able to read all messages sent through, for example, Event Hub, and transform,
    and save them using one of the predefined outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with Azure Stream Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available input and output types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying data using the query language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring the correct order of incoming data and performing checkpoints or replays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform the exercises in this chapter, you will need:'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Studio 2017 instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Azure subscription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Stream Analytics tools—[https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-install](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-install)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Stream Analytics introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed Azure Event Hub, which is a solution for
    receiving and processing thousands of messages per second, by introducing the
    implementation of event processor hosts. While it is great for workloads such
    as big data pipelines or IoT scenarios, it is not a solution to everything, especially
    if you want to avoid hosting VMs. Scaling such architectures can be cumbersome
    and nonintuitive; this is why there is Azure Stream Analytics, which is an event-processing
    engine designed for high volumes of data. It fills a gap where other services
    such as Event Hub or IoT Hub do not perform well (or where to do so they require
    much more skill and/or more sophisticated architecture), particularly for real-time
    analytics, anomaly detection, and geospatial analytics. It is an advanced tool
    for advanced tasks, which will greatly improve your cloud and message-processing
    skills.
  prefs: []
  type: TYPE_NORMAL
- en: Stream ingestions versus stream analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started, we will compare two topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stream ingestion**: This is a process where you introduce a service/API for
    receiving messages from your producers. Such a service is designed to ingest data
    only—it does nothing more (such as transforming or analyzing). To perform any
    kind of analysis of ingested data, you have to introduce your own processors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream analysis**: This is a process where you actually analyze the data.
    You search for anomalies, duplicates, or malformed data, process it, and push
    it further to other services for storing, presenting, and triggering other actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make things even clearer, we can take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b15af9bc-e1b6-4bf3-9539-95324c45e3d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It shows the four steps of data processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Produce**: Where data is actually produced by different services, devices,
    and clients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingest**: This is when the data is consumed from different sources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analyze**: During this step data is analyzed, transformed, and routed to
    appropriate services and components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use**: Storing, displaying, and processing data further in other services,
    such as PowerBI, Azure Functions, and many others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While Azure Event Hub or Azure IoT Hub is a part of the ingest step, Azure Stream
    Analytics is responsible for **analyzing**.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you are not limited to Azure services when it comes to ingesting data.
    In such a scenario, you can also use any kind of queue or API, as long as it is
    capable of processing thousands of events per second.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Stream Analytics concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Azure Stream Analytics, the most important concept is a **stream**. You
    can think about it as a flow of many events carrying data—they do not necessarily
    have to be the same or share schema. Analyzing such a stream is not a trivial
    task. If you have to decode hundreds of thousands of events, the process has to
    be quick, robust, and reliable. We will discuss the main concepts of this service
    to verify whether it is capable of acting as our analyzing solution and the main
    events processor:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully managed**: Azure Stream Analytics is a fully managed platform as a
    service(PaaS), so you do not have to worry about provisioning resources and scaling
    it—the runtime will take care of that, so you can focus on providing optimal queries
    for data analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An SQL-based query language**: To analyze data, Azure Stream Analytics uses
    an SQL-based query language, which enables developers to build advanced procedures
    quickly, which extract from a stream exactly what they want. Additionally, you
    can bring your own extensions such as ML solutions or user-defined aggregates
    to perform extra calculations, using tools unavailable to the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Azure Stream Analytics is focused on **streaming units **(**SUs**) instead
    of some hardcoded values of CPUs or memory. This is because it is designed to
    provide stable performance and recurrent execution time. What is more, thanks
    to this concept, you can easily scale your solution to meet your demands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low cost of ownership**: In Azure Stream Analytics you pay only for what
    you choose. As pricing depends on the number of SUs per hour, there is no additional
    cost to be incorporated in the overall payment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also some extra technical concepts (such as input/output types, checkpoints,
    or replays), which we will cover in the next parts of this chapter. To see the
    big picture of the whole pipeline using Azure Stream Analytics, please check the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e589130-614e-4d10-a8d3-2198eb66b992.png)'
  prefs: []
  type: TYPE_IMG
- en: Of course, there could be other references on this picture (additional services,
    user functions, and analyzers), but for the sake of simplicity, I did not include
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Stream Analytics offers a seamless integration with some native Azure
    services, such as Azure Event Hub, Azure IoT Hub, or Azure Blob Storage. Additionally,
    it can be easily configured to output data to an SQL database, Blob, or Event Azure
    Data Lake Store. To leverage those possibilities, you will have to define both
    input and output types, which you are interested in. This allows for data to be
    easily ingested (in the form of a stream), so a job, which you will write, can
    work on thousands of events, analyzing and processing them. In this section, you
    will learn how to get started with Azure Stream Analytics and to define both the
    inputsand outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Create Azure Stream Analytics in Azure portal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started, you will need to create an instance of Azure Stream Analytics.
    To do so, you have to click on + Create a resourceand search for `Stream Analytics
    job`. This will display a form, where you can enter all the necessary data to
    create a service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e62ebf9-4ae7-4a9e-9fd8-6cd399f8efa2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two fields, which at first you might overlook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hosting environment: Azure Stream Analytics can be hosted in two ways: as a
    native Azure service or deployed to an on-premise IoT Edge gateway device. IoT
    Edge is a topic beyond the scope of this book, so the natural choice will be Cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Streaming units (1 to 120): You have to select how many SUs you would like
    to provision for a job to process your events. The number of required SUs depends
    on the characteristics of your job, and additionally may vary depending on the
    input type of your choice. There is a link in the *Further reading*section, which
    describes in detail how many SUs you may need for your job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that you will pay €0.093/hour for each SU you choose, even when it
    is not working on a job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you click Create and open the Overview blade, you will see an empty dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/989b4ee6-dfd4-4c1a-883a-91dcb9a90482.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, both Inputs and Outputsare empty for now—we have to change
    this, so we can use them in our query. Both of the features are available on the
    left, in the JOB TOPOLOGYsection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb0061d0-855e-43dc-89ac-b6deb2a3dc9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding an input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To add an input, click on the Inputs blade. It will display an empty screen,
    where you have two possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: + Add stream input: Here you can add a link to services that enable you to ingest
    a stream. Currently available Azure components are Azure Event Hub, Azure IoT
    Hub, and Azure Blob Storage. The inputs can live (or not) in the same subscription,
    and such a connection supports compression (so you can pass a compressed stream using,
    for example, GZip or deflate).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: + Add reference input: Instead of ingesting data from a real-time stream, you
    can also use Azure Blob Storage and add a reference to it, so you can ingest so-called reference
    data. In that scenario, Azure Stream Analytics will load the whole data into memory,
    so it can perform lookups on it. It is an ideal solution for static or slowly
    changing data, and supports data up to the maximum size of 300 MB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here you can find an example of configuring Event Hub as an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79d74fab-61f3-495c-8db9-9616807fe09d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Depending on your choices (whether you have an Event Hub in your subscription
    or not, whether it exists or not), there will be different options available.
    In the previous example, I configured a new hub (which was nonexistent) to be
    the source of my data. There are some fields, however, which I would like to cover
    now:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event Hub consumer group: If you would like to make Azure Stream Analytics read
    data from the very beginning, enter a consumer group here. By default, it will
    use `$Default`, which is the default consumer group in Azure Event Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event serialization format: You can choose from JSON, Avro, and CSV. This allows
    you to deserialize events automatically, based on the used serialization format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event compression type: If you are using GZip or Deflate, here you can choose
    the right option, so the input will be automatically deserialized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that you need an actual Azure Event Hub namespace to be able to  create
    a hub from Azure Stream Analytics automatically.
  prefs: []
  type: TYPE_NORMAL
- en: After filling all the required fields, you will be able to click on the Create button
    to initialize the creation of a new input. Of course, you can add more than just
    one input as they will all be available in the input stream, so you will be able
    to work with the incoming events. Before you start your job, you will need at
    least one output, which we are about to add now.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To add an output, you have to click on the Outputs blade. It is similar to
    the Inputs one, but there are different kinds of output available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6df0fb16-6d94-4ad7-9180-e9f97ea21ce8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there are many different types of output available, which makes Azure
    Stream Analytics so flexible when it comes to pushing ingested data to different
    services. We can divide them into different categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Storage: SQL database, Blob storage, Table storage, Cosmos DB, and Data Lake
    Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reporting: Power BI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute: Azure Functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messaging: Event Hub, Service Bus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the category, you will have different options for what you can
    do with the processed events:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Storage: Storing data for further operations, archiving, and event log'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reporting: Near real-time reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute: An easy solution for achieving unlimited integration capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messaging: Pushing events further for different pipelines and systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here you can find a configuration for integrating Azure Table storage as an
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05d434e1-3b7e-4cdf-8b23-5bc4a81c4c3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Available fields depend heavily on the selected output type, so I will not focus
    on them in this chapter. You can find a reference to them in the *Further reading*section.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Stream Analytics query language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The strength of Azure Stream Analytics, besides the rich selection of Azure
    services that seamlessly integrate with it, lies in its query language, which
    allows you to analyze an input stream easily and output it to a required service.
    As it is an SQL-like language, it should be intuitive and easy to learn for most
    developers using this service. Even if you are not familiar with SQL, the many
    examples available and its simple syntax should make it easy for you.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a query
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the Azure portal, the query window for Azure Stream Analytics can be found
    either in the Overview or Query blade:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccdb9cfe-a9f2-4d77-9823-e423acf8c87c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding example, you can see a simple SQL-like query, which performs
    the following three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Selects data from the input using the given alias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chooses the particular columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushes them into a specific output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also click on the Edit querylink, so you will be routed to the Query screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7414db1b-be82-4085-9504-6cd0af031699.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, to be able to actually work with a query, you will need both
    an input and an output, as without them you will not be able to save it. In general,
    a query consists of three elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SELECT`: Where you are selecting columns from the input you are interested
    in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INTO`: Where you are telling the engine which output you are interested in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FROM`: Where you are selecting an input from which data should be fetched'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, the preceding statements are not the only ones, which are available—you
    can use plenty of different options, such as GROUP BY, LIKE, or HAVING. It all
    depends on the input stream and the schema of incoming data. For some jobs, you
    may only need to perform a quick transformation and extract the necessary columns;
    for others, you might require more sophisticated syntax for getting exactly what
    you want. You will find common query patterns in the link in the *Further reading*section.
    In the preceding example, in the `SELECT` part of the query, I have selected three
    columns, which are available when analyzing Azure Event Hub events. What is more,
    I used the `AS ` construct to tell the engine to actually rename fields to match
    those defined in the Outputs section. When I run my job, I can see that it actually
    passes events to my table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79802b23-11fa-4186-8a01-2d5182fb8ea7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, there are some problems with the current setup:'
  prefs: []
  type: TYPE_NORMAL
- en: We rely on the Event Hub fields, which might change in the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are missing the actual data of an event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are duplicated columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s assume each event has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, particular data changes over time. We can quickly change the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And adapt the configuration to change the output a little bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9668fd25-804b-4ef7-8cfa-e45792b367cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the basic constructs are only a few percent of the overall capability
    of the service. There are also inbuilt functions, which can be easily used in
    each query to enhance it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematical functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Aggregate functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Analytic functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Geospatial functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'String functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In addition to these, there are some more such as record functions, date/time
    functions, conversion, or array functions. The preceding examples are of course
    not all the available functions. You can find them all in the *Further reading*section.
    The important thing here is that some functions are deterministic (this means
    that they always return the same result if the same input values are used), and
    some are not—this is especially important when handling high loads and trying
    to avoid possible anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, you can merge different streams of data and push them to a single
    output (or vice versa—have a single input and distribute it to multiple outputs).
    This is a very powerful feature of this service, which makes ingesting and processing
    data much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Event ordering, checkpoints, and replays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we covered some basic topics of Azure Stream Analytics:
    how to configure inputs and outputs, querying data, and using the service. In
    the last part of this chapter, I will show you its more advanced features such
    as event ordering, checkpoints, and replays, which ensure that events are processed
    exactly in a way you would expect. These topics are in fact common subjects in
    many different messaging solutions, so you will be able to use knowledge from
    this chapter in your other projects.'
  prefs: []
  type: TYPE_NORMAL
- en: Event ordering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two concepts of events when it comes to their ordering:'
  prefs: []
  type: TYPE_NORMAL
- en: Application (or event) time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arrival time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is a clear distinction between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Application time**: This is a timestamp when an event was generated on the
    client (or application) side. It tells you exactly when it occurred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arrival time**: This is a system timestamp, which is not present in the original
    payload. It tells you when an event was received by a service and picked up for
    processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the input type, arrival time and application time will be different
    properties (`EventEnqueuedUtcTime `or `EnqueuedTime `for arrival time, whereas
    application time, in general, will be a generic property). What you have to remember
    is, depending on the selected scenario, you can process events as they come but
    out of order, or in order but delayed. This can be easily described using the
    following event sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Arrival**: `2018-09-02T12:17:49` **Application**: `2018-09-02T12:17:48`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Arrival**: `2018-09-02T12:17:50` **Application**:` 2018-09-02T12:17:44`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Arrival**: `2018-09-02T12:17:51` **Application**:`2018-09-02T12:17:46`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you process events as they come into the stream, they will be processed **out
    of order**—in fact, they occurred in a different order, so there is a possibility
    that some data will be outdated. The other option is to sort events by application
    time; in such a scenario, the process will be delayed, but the order will be preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you need to or not, processing events in order depends on the data schema
    and characteristics of the processed events. Processing them in order is more
    time-consuming, but sometimes you just cannot do it the other way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Stream Analytics has a feature named Event ordering, which allows you
    to make a decision about what to do with events, which are either out of order
    or outdated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/053f17d5-6cee-4585-a4ce-77f90fc5ddf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two options available:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Events that arrive late: This one allows you to process outdated events (for
    which the application time does not match the one processed as the last one) within
    a defined time window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Out of order events: It is possible that Azure Stream Analytics consider some
    of your events to be  out of order (this situation could happen, for instance,
    if your senders'' clocks are skewed). Here you can set a time window, during which
    this situation is acceptable).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, you can define an action, which will be performed if an event
    either arrived late or was out of order—for Drop, it will simply be removed, and
    if you select Adjust, processing will be suspended for some time when such situations
    occur.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints and replays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In fact, Azure Stream Analytics is a stateful service, which is able to track
    the event-processing progress. This makes it suitable for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Job recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful query logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different job start modes (now, custom, and when last stopped)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, there is a difference between what is possible after the checkpoint
    and when a replay is necessary. There are situations when the data stored within
    a checkpoint is not enough, and the whole replay is required; however, this may
    differ depending on your query. In fact, it depends on the query parallelization
    factor and can be described using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[The input event rate] x [The gap length] / [Number of processing partitions]*'
  prefs: []
  type: TYPE_NORMAL
- en: The more processors you have, the faster you can recover when something goes
    wrong. A good rule of thumb is to introduce more SUs in case your job fails and
    you have to close the gap quickly.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to consider when replaying data is the use of window functions in
    your queries (tumbling, hopping, sliding, or session)—they allow you to process
    data in different kinds of windows, but complicate the replay mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered Azure Stream Analytics, a service for processing
    streams of data in near real time. You have learned what the available inputs
    and outputs are and how to configure them. What is more, you were able to write
    your first query, and check how the query language works for analyzing and processing
    incoming events. If you need a PaaS  that can quickly read and transform events
    and push them to many different Azure services, Azure Stream Analytics is for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go through Azure Service Bus, an enterprise-class
    messaging solution that is in fact the foundation of Azure Event Hub, which we
    discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the payment model for Azure Stream Analytics?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a stream and the reference output?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between application and arrival time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which query construct do you need to select an ID from an input and push it
    to an output?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you process different inputs in the same query?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When is an event considered out of order?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it possible to get a substring from a property in a query? If so, which function
    can be used for that?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling and SUs: [https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different output types: [https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common query patterns: [https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Window functions: [https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
