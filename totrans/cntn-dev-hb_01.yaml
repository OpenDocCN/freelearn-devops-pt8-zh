- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Modern Infrastructure and Applications with Docker
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker的现代基础设施与应用程序
- en: Software engineering and development is always evolving and introducing new
    technologies in its architectures and workflows. Software containers appeared
    more than a decade ago, becoming particularly popular over the last five years
    thanks to Docker, which made the concept mainstream. Currently, every enterprise
    manages its container-based application infrastructure in production in both the
    cloud and on-premises distributed infrastructures. This book will teach you how
    to increase your development productivity using software containers so that you
    can create, test, share, and run your applications. You will use a container-based
    workflow and your final application artifact will be a Docker image-based deployment,
    ready to run in production environments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程与开发一直在不断发展，引入新技术到架构和工作流中。软件容器在十多年前就已出现，并在过去五年中特别流行，得益于Docker的普及，使得这一概念成为主流。目前，每个企业都在云端和本地分布式基础设施上管理其基于容器的应用程序基础设施。本书将教你如何通过使用软件容器来提高开发生产力，从而创建、测试、共享和运行你的应用程序。你将使用基于容器的工作流，最终的应用程序构件将是基于Docker镜像的部署，准备在生产环境中运行。
- en: This chapter will introduce software containers in the context of the current
    software development culture, which needs faster software supply chains made of
    moving, distributed pieces. We will review how containers work and how they fit
    into modern application architectures based on distributed components with very
    specific functionalities (microservices). This allows developers to choose the
    best language for each application component and distribute the total application
    load. We will learn about the kernel features that make software containers possible
    and learn how to create, share, and run application components as software containers.
    At the end of this chapter, we will learn about the different tools that can help
    us work with software containers and provide specific use cases for your laptop,
    desktop computer, and servers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍软件容器，尤其是在当前软件开发文化中的应用。现代开发文化需要更快速的软件供应链，由可移动、分布式的组件组成。我们将回顾容器的工作原理以及它们如何适应基于分布式组件（具有非常特定功能的微服务）的现代应用程序架构。这使得开发人员可以为每个应用组件选择最合适的语言，并分散整个应用程序的负载。我们将学习使软件容器成为可能的内核特性，并学习如何创建、共享和运行作为软件容器的应用程序组件。在本章的最后，我们将了解不同的工具，帮助我们使用软件容器，并为你的笔记本电脑、台式机和服务器提供具体的应用场景。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Evolution of application architecture, from monoliths to distributed microservice
    architectures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序架构的演变，从单体架构到分布式微服务架构
- en: Developing microservice-based applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发基于微服务的应用程序
- en: How containers fit in the microservices model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器如何适应微服务模型
- en: Understanding the main concepts, features, and components of software containers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解软件容器的主要概念、特性和组成部分
- en: Comparing virtualization and containers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较虚拟化和容器
- en: Building, sharing, and running containers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建、共享和运行容器
- en: Explaining Windows containers
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Windows容器
- en: Improving security using software containers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用软件容器提升安全性
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This book will teach you how to use software containers to improve your application
    development. We will use open source tools for building, sharing, and running
    containers, along with a few commercial ones that don’t require licensing for
    non-professional use. Also included in this book are some labs to help you practically
    understand the content that we’ll work through. These labs can be found at [https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter1](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter1).
    The *Code In Action* video for this chapter can be found at [https://packt.link/JdOIY](https://packt.link/JdOIY).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将教你如何使用软件容器来提高应用程序开发效率。我们将使用开源工具来构建、共享和运行容器，并结合一些不需要专业授权的商业工具。此外，本书还包含一些实验，帮助你实践理解我们所讨论的内容。这些实验可以在[https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter1](https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter1)找到。本章的*Code
    In Action*视频可以在[https://packt.link/JdOIY](https://packt.link/JdOIY)找到。
- en: From monoliths to distributed microservice architectures
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从单体架构到分布式微服务架构
- en: Application architectures are continuously evolving due to technological improvements.
    Throughout the history of computation, every time a technical gap is resolved
    in hardware and software engineering, software architects rethink how applications
    can be improved to take advantage of the new developments. For example, network
    speed increases made distributing application components across different servers
    possible, and nowadays, it’s not even a problem to distribute these components
    across data centers in multiple countries.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术的进步，应用架构不断演变。在计算历史上，每当硬件和软件工程中的技术差距得到解决时，软件架构师就会重新思考如何改进应用，以利用这些新的技术进展。例如，网络速度的提升使得将应用组件分布到不同服务器成为可能，如今，甚至将这些组件分布到多个国家的数据中心也不成问题。
- en: To take a quick look at how computers were adopted by enterprises, we must go
    back in time to the old mainframe days (before the 1990s). This can be considered
    the base for what we call **unitary architecture** – one big computer with all
    the processing functionality, accessed by users through terminals. Following this,
    the **client-server** model became very popular as technology also advanced on
    the user side. Server technologies improved while clients gained more and more
    functionality, freeing up the server load for publishing applications. We consider
    both models as **monoliths** as all application components run on one server;
    even if the databases are decoupled from the rest of the components, running all
    important components in a dedicated server is still considered monolithic. Both
    of these models were very difficult to upgrade when performance started to drop.
    In these cases, newer hardware with higher specifications was always required.
    These models also suffer from availability issues, meaning that any maintenance
    tasks required on either the server or application layer will probably lead to
    service outages, which affects the normal system uptime.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速了解计算机是如何被企业采纳的，我们必须回到早期的主机时代（1990年代之前）。这可以被视为我们今天所称的**单体架构**的基础——一台拥有所有处理功能的大型计算机，用户通过终端进行访问。在此之后，随着用户端技术的进步，**客户端-服务器**模型变得非常流行。服务器技术不断改进，而客户端则获得了越来越多的功能，减轻了服务器的负载，从而支持应用发布。我们认为这两种模型都是**单体**的，因为所有应用组件都运行在同一台服务器上；即使数据库与其他组件解耦，将所有重要组件运行在专用服务器上，仍然被视为单体架构。这两种模型在性能下降时都很难升级。在这种情况下，通常需要更高规格的硬件。这些模型也存在可用性问题，即任何对服务器或应用层的维护任务都可能导致服务中断，进而影响系统的正常运行时间。
- en: Exploring monolithic applications
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索单体应用
- en: '**Monolithic applications** are those in which all functionalities are provided
    by just one component, or a set of them so tightly integrated that they cannot
    be decoupled from one another. This makes them hard to maintain. They weren’t
    designed with reusability or modularity in mind, meaning that every time developers
    need to fix an issue, add some new functionality, or change an application’s behavior,
    the entire application is affected due to, for example, having to recompile the
    whole application’s code.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**单体应用**是指所有功能由一个组件或一组紧密集成的组件提供，这些组件彼此之间无法解耦，从而使得它们的维护变得困难。它们的设计并未考虑重用性或模块化，意味着每当开发人员需要修复一个问题、添加新功能或改变应用的行为时，整个应用都会受到影响，例如，可能需要重新编译整个应用程序的代码。'
- en: Providing high availability to monolithic applications required duplicated hardware,
    quorum resources, and continuous visibility between application nodes. This may
    not have changed too much today but we have many other resources for providing
    high availability. As applications grew in complexity and gained responsibility
    for many tasks and functionalities, we started to decouple them into a few smaller
    components (with specific functions such as the web server, database, and more),
    although core components were kept immutable. Running all application components
    together on the same server was better than distributing them into smaller pieces
    because network communication speeds weren’t high enough. Local filesystems were
    usually used for sharing information between application processes. These applications
    were difficult to scale (more hardware resources were required, usually leading
    to acquiring newer servers) and difficult to upgrade (testing, staging, and certification
    environments before production require the same hardware or at least compatible
    ones). In fact, some applications could run only on specific hardware and operating
    system versions, and developers needed workstations or servers with the same hardware
    or operating system to be able to develop fixes or new functionality for these
    applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how applications were designed in the early days, let’s introduce
    virtualization in data centers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of **virtualization** – providing a set of physical hardware resources
    for specific purposes – was already present in the mainframe days before the 1990s,
    but in those days, it was closer to the definition of **time-sharing** at the
    compute level. The concept we commonly associate with virtualization comes from
    the introduction of the **hypervisor** and the new technology introduced in the
    late 1990s that allowed for the creation of complete virtual servers running their
    own virtualized operating systems. This hypervisor software component was able
    to virtualize and share host resources in virtualized guest operating systems.
    In the 1990s, the adoption of Microsoft Windows and the emergence of Linux as
    a server operating system in the enterprise world established x86 servers as the
    industry standard, and virtualization helped the growth of both of these in our
    data centers, improving hardware usage and server upgrades. The virtualization
    layer simplified virtual hardware upgrades when applications required more memory
    or CPU and also improved the process of providing services with high availability.
    Data centers became smaller as newer servers could run dozens of virtual servers,
    and as physical servers’ hardware capabilities increased, the number of virtualized
    servers per node increased.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'In the late 1990s, the servers became services. This means that companies started
    to think about the services they provided instead of the way they did it. Cloud
    providers arrived to provide services to small businesses that didn’t want to
    acquire and maintain their own data centers. Thus, a new architecture model was
    created, which became pretty popular: the **cloud computing infrastructure** model.
    Amazon launched **Amazon Web Services** (**AWS**), providing storage, computation,
    databases, and other infrastructure resources. And pretty soon after that, Elastic
    Compute Cloud entered the arena of virtualization, allowing you to run your own
    servers with a few clicks. Cloud providers also allowed users to use their well-documented
    **application programming interfaces** (**APIs**) for automation, and the concept
    of **Infrastructure as Code** (**IaC**) was introduced. We were able to create
    our virtualization instances using programmatic and reusable code. This model
    also changed the service/hardware relationship and what started as a good idea
    at first – using cloud platforms for every enterprise service – became a problem
    for big enterprises, which saw increased costs pretty quickly based on network
    bandwidth usage and as a result of not sufficiently controlling their use of cloud
    resources. Controlling cloud service costs soon became a priority for many enterprises,
    and many open source projects started with the premise of providing cloud-like
    infrastructures. **Infrastructure elasticity** and **easy provisioning** are the
    keys to these projects. OpenStack was the first one, distributed in smaller projects,
    each one focused on different functionalities (storage, networking, compute, provisioning,
    and so on). The idea of having on-premises cloud infrastructure led software and
    infrastructure vendors into new alliances with each other, in the end providing
    new technologies for data centers with the required flexibility and resource distribution.
    They also provided APIs for quickly deploying and managing provisioned infrastructure,
    and nowadays, we can provision either cloud infrastructure resources or resources
    on our data centers using the same code with few changes.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年代末期，服务器变成了服务。这意味着公司开始考虑他们提供的服务，而不是如何提供服务。云服务提供商应运而生，向那些不想拥有和维护自己数据中心的小型企业提供服务。因此，创建了一种新的架构模型，并变得相当流行：**云计算基础设施**模型。亚马逊推出了**亚马逊网络服务**（**AWS**），提供存储、计算、数据库以及其他基础设施资源。很快，弹性计算云（Elastic
    Compute Cloud）进入了虚拟化领域，允许用户通过几次点击就能运行自己的服务器。云服务提供商还允许用户使用他们的文档完备的**应用程序编程接口**（**API**）进行自动化，并引入了**基础设施即代码**（**IaC**）的概念。我们可以通过编程和可重用的代码创建虚拟化实例。这个模型还改变了服务/硬件的关系，最初作为一个好主意——将云平台用于每个企业服务——最终变成了大企业的问题，这些企业很快就看到了基于网络带宽使用的成本增加，且由于没有充分控制其云资源的使用，成本进一步上升。控制云服务成本很快成为许多企业的优先事项，许多开源项目也基于提供云类基础设施的前提开始了。**基础设施弹性**和**简易配置**是这些项目的关键。OpenStack是第一个，它被分发成多个小项目，每个项目专注于不同的功能（存储、网络、计算、配置等）。拥有本地云基础设施的想法促使软件和基础设施供应商建立新的合作伙伴关系，最终为数据中心提供了具有所需灵活性和资源分配的新技术。他们还提供了用于快速部署和管理配置基础设施的API，现在，我们可以使用相同的代码，仅需少量更改，就能配置云基础设施资源或我们数据中心的资源。
- en: Now that we have a good idea of how server infrastructures work today, let’s
    go back to applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经对今天的服务器基础设施有了清晰的了解，接下来让我们回到应用程序。
- en: Three-tier architecture
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三层架构
- en: Even with these decoupled infrastructures, applications can still be monoliths
    if we don’t prepare them for separation into different components. Elastic infrastructures
    allow us to distribute resources and it would be nice to have distributed components.
    Network communications are essential and technological evolution has increased
    speeds, allowing us to consume network-provided services as if they were local
    and facilitating the use of distributed components.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有了这些解耦的基础设施，如果我们没有将应用程序准备好分离成不同的组件，它们仍然可能是单体应用。弹性基础设施允许我们分配资源，最好能够拥有分布式组件。网络通信至关重要，技术的进步提高了速度，使我们能够像使用本地服务一样使用网络提供的服务，并促进了分布式组件的使用。
- en: '**Three-tier architecture** is a software application architecture where the
    application is decoupled into three to five logical and physical computing layers.
    We have the **presentation tier**, or user interface; the **application tier**,
    or backend, where data is processed; and the **data tier**, where the data for
    use in the application is stored and managed, such as in a database. This model
    was used even before virtualization arrived on the scene, but you can imagine
    the improvement of being able to distribute application components across different
    virtual servers instead of increasing the number of servers in your data center.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**三层架构**是一种软件应用架构，其中应用被解耦成三到五个逻辑和物理计算层。我们有**表示层**，即用户界面；**应用层**，即后台，数据在此被处理；以及**数据层**，应用所需的数据在此存储和管理，比如在数据库中。即使在虚拟化技术出现之前，这种模型也已经被使用，但你可以想象，能够将应用组件分布到不同的虚拟服务器上，而不是增加数据中心中服务器的数量，这样的改进。'
- en: 'Just to recap before continuing our journey: the evolution of infrastructure
    and network communications has allowed us to run component-distributed applications,
    but we just have a few components per application in the three-tier model. Note
    that in this model, different roles are involved in application maintenance as
    different software technologies are usually employed. For example, we need database
    administrators, middleware administrators, and infrastructure administrators for
    systems and network communications. In this model, although we are still forced
    to use servers (virtual or physical), application component maintenance, scalability,
    and availability are significantly improved. We can manage each component in isolation,
    executing different maintenance tasks and fixes and adding new functionalities
    decoupled from the application core. In this model, developers can focus on either
    frontend or backend components. Some coding languages are specialized for each
    layer – for example, JavaScript was the language of choice for frontend developers
    (although it evolved for backend services too).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续我们的旅程之前，先回顾一下：基础设施和网络通信的发展使我们能够运行组件分布式的应用，但在三层模型中，每个应用只有少量组件。请注意，在这个模型中，由于通常采用不同的软件技术，不同的角色参与到应用的维护中。例如，我们需要数据库管理员、中间件管理员以及系统和网络通信的基础设施管理员。在这个模型中，尽管我们仍然不得不使用服务器（虚拟或物理），但应用组件的维护、可扩展性和可用性得到了显著提高。我们可以独立管理每个组件，执行不同的维护任务和修复，并在不依赖于应用核心的情况下增加新功能。在这个模型中，开发人员可以专注于前端或后端组件。一些编程语言专门为每一层设计——例如，JavaScript是前端开发人员的首选语言（尽管它也发展成为后端服务的语言）。
- en: As Linux systems grew in popularity in the late 1990s, applications were distributed
    into different components, and eventually different applications working together
    and running in different operating systems became a new requirement. Shared files,
    provided by network filesystems using either **network-attached storage** (**NAS**)
    or more complex **storage area network** (**SAN**) storage backends were used
    at first, but **Simple Object Access Protocol** (**SOAP**) and other queueing
    message technologies helped applications to distribute data between components
    and manage their information without filesystem interactions. This helped decouple
    applications into more and more distributed components running on top of different
    operating systems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Linux系统在1990年代末期的普及，应用被分布成不同的组件，最终，不同的应用在不同操作系统上协同工作，成为一种新的需求。最初通过网络文件系统提供的共享文件（使用**网络附加存储**（**NAS**）或更复杂的**存储区域网络**（**SAN**）存储后端）被使用，但**简单对象访问协议**（**SOAP**）和其他队列消息技术帮助应用在组件之间分发数据并管理其信息，而无需与文件系统交互。这有助于将应用解耦成更多分布式组件，运行在不同的操作系统之上。
- en: Microservices architecture
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微服务架构
- en: The **microservices architecture** model goes a step further, decoupling applications
    into smaller pieces with enough functionality to be considered components. This
    model allows us to manage a completely independent component life cycle, freeing
    us to choose whatever coding language fits best with the functionality in question.
    Application components are kept light in terms of functionality and content, which
    should lead to them using fewer host resources and responding faster to start
    and stop commands. Faster restarts are key to resilience and help us maintain
    our applications while up, with fewer outages. Application health should not depend
    on component-external infrastructure; we should improve components’ logic and
    resilience so that they can start and stop as fast as possible. This means that
    we can ensure that changes to an application are applied quickly, and in the case
    of failure, the required processes will be up and running in seconds. This also
    helps in managing the application components’ life cycle as we can upgrade components
    very fast and prepare circuit breakers to manage stopped dependencies.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**微服务架构**模型更进一步，将应用程序解耦为足够小的组件，每个组件都有足够的功能，可以视为独立的模块。该模型使我们能够管理完全独立的组件生命周期，允许我们选择最适合该功能的编程语言。应用程序组件在功能和内容方面保持轻量，这应该使它们占用更少的主机资源，并能够更快地响应启动和停止命令。更快的重启对于系统的弹性至关重要，并帮助我们在应用程序运行时减少故障停机。应用程序的健康状况不应依赖于组件外部的基础设施；我们应改进组件的逻辑和弹性，使其能够尽可能快速地启动和停止。这意味着我们可以确保应用程序的变更能够迅速应用，并且在发生故障时，所需的进程能在几秒钟内启动。这还帮助我们管理应用程序组件的生命周期，因为我们可以非常快速地升级组件，并准备断路器来管理停止的依赖。'
- en: Microservices use the **stateless** paradigm; therefore, application components
    should be stateless. This means that a microservice’s state must be abstracted
    from its logic or execution. This is key to being able to run multiple replicas
    of an application component, allowing us to run them distributed on different
    nodes from a pool.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务采用**无状态**范式；因此，应用组件应该是无状态的。这意味着微服务的状态必须与其逻辑或执行过程相分离。这对于能够运行多个副本的应用组件至关重要，使我们能够在不同节点上分布运行它们。
- en: This model also introduced the concept of *run everywhere*, where an application
    should be able to run its components on either cloud or on-premise infrastructures,
    or even a mix of both (for example, the presentation layer for components could
    run on cloud infrastructure while the data resides in our data center).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还引入了*随处运行*的概念，即应用程序应该能够在云端或本地基础设施上运行其组件，甚至是两者的混合（例如，组件的展示层可以运行在云基础设施上，而数据则存储在我们的数据中心）。
- en: 'Microservices architecture provides the following helpful features:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构提供以下有用特性：
- en: Applications are decoupled into different smaller pieces that provide different
    features or functionalities; thus, we can change any of them at any time without
    impacting the whole application.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序被解耦为多个较小的部分，提供不同的特性或功能；因此，我们可以随时更改其中任何一部分，而不会影响整个应用程序。
- en: Decoupling applications into smaller pieces lets developers focus on specific
    functionalities and allows them to use the most appropriate programming language
    for each component.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将应用程序解耦为更小的部分，使开发人员可以专注于特定的功能，并允许他们为每个组件使用最合适的编程语言。
- en: Interaction between application components is usually provided via **Representational
    State Transfer** (**REST**) API calls using HTTP. RESTful systems aim for fast
    performance and reliability and can scale without any problem.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用组件之间的交互通常通过**表现性状态转移**（**REST**）API调用使用HTTP来提供。RESTful系统旨在实现快速的性能和可靠性，并能够无障碍地扩展。
- en: Developers describe which methods, actions, and data they provide in their microservice,
    which are then consumed by other developers or users. Software architects must
    standardize how application components talk with each other and how microservices
    are consumed.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员描述他们的微服务提供哪些方法、操作和数据，这些方法、操作和数据会被其他开发人员或用户使用。软件架构师必须标准化应用组件之间的交互方式以及微服务的使用方式。
- en: Distributing application components across different nodes allows us to group
    microservices into nodes for the best performance, closer to data sources and
    with better security. We can create nodes with different features to provide the
    best fit for our application components.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将应用组件分布在不同的节点上，能够将微服务分组到节点中，以获得最佳的性能，更接近数据源并具备更好的安全性。我们可以创建具有不同特性的节点，以便为我们的应用组件提供最合适的环境。
- en: Now that we’ve learned what microservices architecture is, let’s take a look
    at its impact on the development process.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了什么是微服务架构，让我们来看看它对开发过程的影响。
- en: Developing distributed applications
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发分布式应用程序
- en: Monolith applications, as we saw in the previous section, are applications in
    which all functionalities run together. Most of these applications were created
    for specific hardware, operating systems, libraries, binary versions, and so on.
    To run these applications in production, you need a least one dedicated server
    with the right hardware, operating system, libraries, and so on, and developers
    require a similar node architecture and resources even just for fixing possible
    application issues. Adding to this, the pre-production environments for tasks
    such as certification and testing will multiply the number of servers significantly.
    Even if your enterprise had the budget for all these servers, any maintenance
    task as a result of any upgrade in any operating system-related component in production
    should always be replicated on all other environments. Automation helps in replicating
    changes between environments, but this is not easy. You have to replicate environments
    and maintain them. On the other hand, new node provisioning could have taken months
    in the old days (preparing the specifications for a new node, drawing up the budget,
    submitting it to your company’s approvals workflow, looking for a hardware provider,
    and so on). Virtualization helped system administrators provision new nodes for
    developers faster, and automation (provided by tools such as Chef, Puppet, and,
    my favorite, Ansible) allowed for the alignment of changes between all environments.
    Therefore, developers were able to obtain their development environments quickly
    and ensure they were using an aligned version of system resources, improving the
    process of application maintenance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 单体应用程序，正如我们在上一节中看到的，是所有功能共同运行的应用程序。这些应用程序大多数是为特定的硬件、操作系统、库、二进制版本等创建的。要在生产环境中运行这些应用程序，你至少需要一台配备正确硬件、操作系统、库等的专用服务器，而开发人员即便只是为了修复可能的应用问题，也需要类似的节点架构和资源。更不用说，像认证和测试等任务的预生产环境将显著增加服务器的数量。即使你的企业有足够的预算来购买这些服务器，任何由于操作系统相关组件升级导致的维护任务，都必须在所有其他环境中进行复制。这时，自动化有助于在环境之间复制更改，但这并不容易。你必须复制并维护这些环境。另一方面，过去（在虚拟化出现之前）新节点的配置可能需要数月时间（准备新节点的规格、制定预算、提交到公司审批流程、寻找硬件供应商等）。虚拟化帮助系统管理员为开发人员更快地配置新节点，自动化工具（如Chef、Puppet，以及我最喜欢的Ansible）帮助对所有环境之间的更改进行对齐。因此，开发人员能够迅速获取他们的开发环境，并确保他们使用的是对齐版本的系统资源，从而提高了应用程序维护的效率。
- en: Virtualization also worked very well with the three-tier application architecture.
    It was easy to run application components for developers in need of a database
    server to connect to while coding new changes. The problem with virtualization
    comes from the concept of replicating a complete operating system with server
    application components when we only need the software part. A lot of hardware
    resources are consumed for the operating system alone, and restarting these nodes
    takes some time as they are a complete operating system running on top of a hypervisor,
    itself running on a physical server with its own operating system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化与三层应用架构的兼容性也非常好。开发人员在需要连接数据库服务器来编写新更改时，能够轻松运行应用组件。虚拟化的问题出在复制一个完整操作系统及其服务器应用组件的概念上，而我们只需要软件部分。仅操作系统就消耗了大量硬件资源，且由于这些节点运行的是一个完整的操作系统，该系统又位于一个虚拟机监控器上，而虚拟机监控器又运行在一台具有自己操作系统的物理服务器上，因此重启这些节点需要一些时间。
- en: Anyhow, developers were hampered by outdated operating system releases and packages,
    making it difficult for them to enable the evolution of their applications. System
    administrators started to manage hundreds of virtual hosts and even with automation,
    they weren’t able to maintain operating systems and application life cycles in
    alignment. Provisioning virtual machines on cloud providers using their **Infrastructure-as-a-Service**
    (**IaaS**) platforms or using their **Platform-as-a-Service** (**PaaS**) environments
    and scripting the infrastructure using their APIs (IaC) helped but the problem
    wasn’t fully resolved due to the quickly growing number of applications and required
    changes. The application life cycle changed from one or two updates per year to
    dozens per day.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，开发者受限于过时的操作系统版本和软件包，这使得他们难以推动应用程序的发展。系统管理员开始管理数百个虚拟主机，即使使用自动化，也无法保持操作系统和应用生命周期的同步。通过使用云提供商的**基础设施即服务**（**IaaS**）平台，或者使用**平台即服务**（**PaaS**）环境，并通过其API（IaC）脚本化基础设施，虽然有所帮助，但由于应用程序数量的快速增长以及所需的更改，问题并未完全解决。应用生命周期从每年一两次更新变成了每天几十次更新。
- en: Developers started to use cloud-provided services and using scripts and applications
    quickly became more important than the infrastructure on which they were running,
    which today seems completely normal and logical. Faster network communications
    and distributed reliability made it easier to start deploying our applications
    anywhere, and data centers became smaller. We can say that developers started
    this movement and it became so popular that we finished decoupling application
    components from the underlying operating systems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者开始使用云提供的服务，并且使用脚本和应用程序变得比运行它们的基础设施更为重要，这在今天看起来是完全正常和合乎逻辑的。更快的网络通信和分布式可靠性使得我们能够更容易地在任何地方部署应用程序，数据中心也变得越来越小。我们可以说，正是开发者推动了这个运动，并且它变得如此流行，以至于我们最终将应用组件从底层操作系统中解耦。
- en: Software containers are the evolution of process isolation features that were
    learned throughout the development of computer history. Mainframe computers allowed
    us to share CPU time and memory resources many years ago. Chroot and jail environments
    were common ways of sharing operating system resources with users, who were able
    to use all the binaries and libraries prepared for them by system administrators
    in BSD operating systems. On Solaris systems, we had **zones** as resource containers,
    which acted as completely isolated virtual servers within a single operating system
    instance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 软件容器是计算机历史发展中学到的进程隔离特性的演进。多年前，大型机计算机使我们能够共享CPU时间和内存资源。Chroot和jail环境是共享操作系统资源的常见方式，用户可以使用系统管理员为他们在BSD操作系统中准备的所有二进制文件和库。在Solaris系统中，我们有**区域**作为资源容器，充当单一操作系统实例内完全隔离的虚拟服务器。
- en: So, why don’t we just isolate processes instead of full operating systems? This
    is the main idea behind containers. Containers use kernel features to provide
    process isolation at the operating system level, and all processes run on the
    same host but are isolated from each other. So, every process has its own set
    of resources sharing the same host kernel.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们不直接隔离进程，而不是完整的操作系统呢？这就是容器背后的主要思想。容器利用内核特性，在操作系统层面提供进程隔离，所有进程运行在同一主机上，但彼此隔离。因此，每个进程都有自己的一套资源，分享同一个主机内核。
- en: Linux kernels have featured this design of process grouping since the late 2000s
    in the form of **control groups** (**cgroups**). This feature allows the Linux
    kernel to manage, restrict, and audit groups of processes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自2000年代末以来，Linux内核就以**控制组**（**cgroups**）的形式具备了这一进程分组的设计。这个特性允许Linux内核管理、限制和审计进程组。
- en: Another very important Linux kernel feature that’s used with containers is **kernel
    namespaces**, which allow Linux to run processes wrapped with their process hierarchy,
    along with their own network interfaces, users, filesystem mounts, and inter-process
    communication. Using kernel namespaces and control groups, we can completely isolate
    a process within an operating system. It will run as if it were on its own, using
    its own operating system and limited CPU and memory (we can even limit its disk
    I/O).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与容器一起使用的非常重要的Linux内核特性是**内核命名空间**，它允许Linux运行与其进程层次结构绑定的进程，同时具备自己的网络接口、用户、文件系统挂载和进程间通信。通过使用内核命名空间和控制组，我们可以完全隔离操作系统中的进程。它将像在自己的操作系统中运行一样，使用自己有限的CPU和内存（我们甚至可以限制它的磁盘I/O）。
- en: The **Linux Containers** (**LXC**) project took this idea further and created
    the first working implementation of it. This project is still available, is still
    in progress, and was the key to what we now know as **Docker containers**. LXC
    introduced terms such as **templates** to describe the creation of encapsulated
    processes using kernel namespaces.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**Linux 容器**（**LXC**）项目进一步发展了这一理念，创造了第一个可行的实现。这个项目仍然存在，并且还在持续进展，它是我们现在所知的**Docker
    容器**的关键。LXC 引入了诸如**模板**等术语，用来描述使用内核命名空间创建封装进程。'
- en: Docker containers took all these concepts and created Docker Inc., an open source
    project that made it easy to run software containers on our systems. Containers
    ushered in a great revolution, just as virtualization did more than 20 years ago.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 容器将所有这些概念结合起来，创建了 Docker Inc.，一个开源项目，使得在我们的系统上运行软件容器变得简单。容器带来了一场伟大的革命，就像虚拟化在
    20 多年前所做的那样。
- en: Going back to microservices architecture, the ideal application decoupling would
    mean running defined and specific application functionalities as completely standalone
    and isolated processes. This led to the idea of running microservice applications’
    components within containers, with minimum operating system overhead.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回到微服务架构，理想的应用解耦意味着将定义和特定的应用功能作为完全独立且隔离的进程运行。这促生了将微服务应用组件运行在容器内的理念，且操作系统开销最小。
- en: What are containers?
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是容器？
- en: We can define a container as a process with all its requirements isolated using
    cgroups and namespace kernel features. A **process** is the way we execute a task
    within the operating system. If we define a **program** as the set of instructions
    developed using a programming language, included in an executable format on disk,
    we can say that a process is a program in action.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将容器定义为一个进程，所有的需求都通过 cgroups 和命名空间内核功能被隔离。**进程**是我们在操作系统内执行任务的方式。如果我们将**程序**定义为使用编程语言开发的指令集，并以可执行格式存储在磁盘上，那么我们可以说，进程就是程序在运行。
- en: The execution of a process involves the use of some system resources, such as
    CPU and memory, and although it runs on its own environment, it can use the same
    information as other processes sharing the same host system.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 进程的执行涉及使用一些系统资源，如 CPU 和内存，尽管它在自己的环境中运行，但它可以使用与其他共享同一主机系统的进程相同的信息。
- en: Operating systems provide tools for manipulating the behavior of processes during
    execution, allowing system administrators to prioritize the critical ones. Each
    process running on a system is uniquely identified by a **Process Identifier**
    (**PID**). A parent-child relationship between processes is developed when one
    process executes a new process (or creates a new thread) during its execution.
    The new process (or sub-process) that’s created will have as its parent the previous
    one, and so on. The operating system stores information about process relations
    using PIDs and parent PIDs. Processes may inherit a parent hierarchy from the
    user who runs them, so users own and manage their own processes. Only administrators
    and privileged users can interact with other users’ processes. This behavior also
    applies to child processes created by our executions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统提供了在执行过程中操控进程行为的工具，允许系统管理员优先处理关键进程。每个在系统上运行的进程都有一个唯一的**进程标识符**（**PID**）。当一个进程在执行过程中执行一个新进程（或创建一个新线程）时，会产生进程之间的父子关系。新创建的进程（或子进程）将以之前的进程为父进程，依此类推。操作系统使用
    PID 和父 PID 存储进程关系的信息。进程可能会继承从运行它们的用户那里来的父级层级，因此用户拥有并管理自己的进程。只有管理员和特权用户能够与其他用户的进程交互。这种行为同样适用于我们执行时创建的子进程。
- en: Each process runs on its own environment and we can manipulate its behavior
    using operating system features. Processes can access files as needed and use
    pointers to descriptors during execution to manage these filesystem resources.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 每个进程都在自己的环境中运行，我们可以使用操作系统的功能来操控它的行为。进程可以根据需要访问文件，并在执行过程中使用指针来描述符来管理这些文件系统资源。
- en: The operating system kernel manages all processes, scheduling them on its physical
    or virtualized CPUs, giving them appropriate CPU time, and providing them with
    memory or network resources (among others).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统内核管理所有进程，将它们调度到物理或虚拟化的 CPU 上，分配适当的 CPU 时间，并为它们提供内存或网络资源（等等）。
- en: These definitions are common to all modern operating systems and are key for
    understanding software containers, which we will discuss in detail in the next
    section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些定义适用于所有现代操作系统，并且是理解软件容器的关键，我们将在下一节详细讨论。
- en: Understanding the main concepts of containers
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解容器的主要概念
- en: We have learned that as opposed to virtualization, containers are processes
    running in isolation and sharing the host operating system kernel. In this section,
    we will review the components that make containers possible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，与虚拟化不同，容器是运行在隔离环境中的进程，并共享主机操作系统的内核。在本节中，我们将回顾使容器成为可能的各个组件。
- en: Kernel process isolation
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核进程隔离
- en: We already introduced kernel process namespace isolation as a key feature for
    running software containers. Operating system kernels provide namespace-based
    **isolation**. This feature has been present in Linux kernels since 2006 and provides
    different layers of isolation associated with the properties or attributes a process
    has when it runs on a host. When we apply these namespaces to processes, they
    will run their own set of properties and will not see the other processes running
    alongside them. Hence, kernel resources are partitioned such that each set of
    processes sees different sets of resources. Resources may exist in multiple spaces
    and processes may share them.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了内核进程命名空间隔离作为运行软件容器的关键特性。操作系统内核提供基于命名空间的**隔离**。自2006年以来，这一特性就存在于Linux内核中，并提供与进程在主机上运行时的属性或特征相关的不同隔离层级。当我们将这些命名空间应用于进程时，它们会运行自己的属性集，并且看不到与它们并行运行的其他进程。因此，内核资源被分割，使每组进程看到不同的资源集。资源可以存在于多个空间中，进程可能会共享这些资源。
- en: 'Containers, as they are host processes, run with their own associated set of
    kernel namespaces, such as the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 容器作为主机进程运行，具有自己的内核命名空间集，如下所示：
- en: '**Processes**: The container’s main process is the parent of others within
    the container. All these processes share the same process namespace.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进程**：容器的主进程是容器内其他进程的父进程。所有这些进程共享同一进程命名空间。'
- en: '**Network**: Each container receives a network stack with unique interfaces
    and IP addresses. Processes (or containers) sharing the same network namespace
    will get the same IP address. Communications between containers pass through host
    bridge interfaces.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：每个容器都分配一个独特的网络栈，包含唯一的接口和IP地址。共享同一网络命名空间的进程（或容器）将获得相同的IP地址。容器之间的通信通过主机桥接接口进行。'
- en: '**Users**: Users within containers are unique; therefore, each container gets
    its own set of users, but these users are mapped to real host user identifiers.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户**：容器内的用户是独立的；因此，每个容器都有自己的用户集，但这些用户会映射到主机的实际用户标识符。'
- en: '**Inter-process communication** (**IPC**): Each container receives its own
    set of shared memory, semaphores, and message queues so that it doesn’t conflict
    with other processes on the host.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进程间通信**（**IPC**）：每个容器都会获得一组独立的共享内存、信号量和消息队列，从而避免与主机上其他进程发生冲突。'
- en: '**Mounts**: Each container mounts a root filesystem; we can also attach remote
    and host local mounts.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挂载**：每个容器挂载一个根文件系统；我们还可以附加远程和主机本地挂载。'
- en: '**Unix time-sharing** (**UTS**): Each container is assigned a hostname and
    the time is synced with the underlying host.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unix时间共享**（**UTS**）：每个容器都会被分配一个主机名，并且时间会与底层主机同步。'
- en: Processes running inside a container sharing the same process kernel namespace
    will receive PIDs as if they were running alone inside their own kernel. The container’s
    main process is assigned PID 1 and other sub-processes or threads will get subsequent
    IDs, inheriting the main process hierarchy. The container will die if the main
    process dies (or is stopped).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器内运行的进程共享同一内核命名空间时，将获得类似于单独在自己内核内运行的PID。容器的主进程被分配PID 1，其他子进程或线程将获得后续ID，继承主进程的层次结构。如果主进程死亡（或被停止），容器也会死掉。
- en: 'The following diagram shows how our system manages container PIDs inside the
    container’s PID namespace (represented by the gray box) and outside, at the host
    level:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们的系统如何管理容器PID在容器的PID命名空间（由灰色框表示）内外的分配：
- en: '![Figure 1.1 – Schema showing a hierarchy of PIDs when you execute an NGINX
    web server with four worker processes](img/B19845_01_01.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 展示执行一个带有四个工作进程的NGINX web服务器时PID层次结构的示意图](img/B19845_01_01.jpg)'
- en: Figure 1.1 – Schema showing a hierarchy of PIDs when you execute an NGINX web
    server with four worker processes
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 展示执行一个带有四个工作进程的NGINX web服务器时PID层次结构的示意图
- en: In the preceding figure, the main process running inside a container is assigned
    PID 1, while the other processes are its children. The host runs its own PID 1
    process and all other processes run in association with this initial process.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，容器内运行的主进程被分配为 PID 1，而其他进程是其子进程。主机运行自己的 PID 1 进程，所有其他进程与这个初始进程关联运行。
- en: Control groups
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制组
- en: 'A **cgroup** is a feature provided by the Linux kernel that enables us to limit
    and isolate the host resources associated with processes (such as CPU, memory,
    and disk I/O). This provides the following features:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**cgroup** 是 Linux 内核提供的一项功能，允许我们限制和隔离与进程相关的主机资源（例如 CPU、内存和磁盘 I/O）。它提供了以下功能：'
- en: '**Resource limits**: Host resources are limited by using a cgroup and thus,
    the number of resources that a process can use, including CPU or memory'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源限制**：通过使用 cgroup 限制主机资源，因此进程可以使用的资源数量（包括 CPU 或内存）是有限的'
- en: '**Prioritization**: If resource contention is observed, the amount of host
    resources (CPU, disk, or network) that a process can use compared to processes
    in another cgroup can be controlled'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先级**：如果观察到资源争用，可以控制与另一个 cgroup 中的进程相比，进程可以使用的主机资源（CPU、磁盘或网络）数量'
- en: '**Accounting**: Cgroups monitor and report resource limits usage at the cgroup
    level'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计账**：Cgroups 在 cgroup 层级监控并报告资源限制的使用情况'
- en: '**Control**: We can manage the status of all processes in a cgroup'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制**：我们可以管理 cgroup 中所有进程的状态'
- en: The isolation of cgroups will not allow containers to bring down a host by exhausting
    its resources. An interesting fact is that you can use cgroups without software
    containers just by mounting a cgroup (cgroup type system), adjusting the CPU limits
    of this group, and finally adding a set of PIDs to this group. This procedure
    will apply to either cgroups-V1 or the newer cgroups-V2.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: cgroup 的隔离机制不会允许容器通过耗尽主机资源来使主机崩溃。有趣的是，你可以在没有软件容器的情况下使用 cgroup，只需挂载一个 cgroup（cgroup
    类型系统），调整该组的 CPU 限制，最后将一组 PID 添加到该组中。这个过程适用于 cgroups-V1 或更新的 cgroups-V2。
- en: Container runtime
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器运行时
- en: A **container runtime**, or **container engine**, is a piece of software that
    runs containers on a host. It is responsible for downloading container images
    from a registry to create containers, monitoring the resources available in the
    host to run the images, and managing the isolation layers provided by the operating
    system. The container runtime also reviews the current status of containers and
    manages their life cycle, starting again when their main process dies (if we declare
    them to be available whenever this happens).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器运行时**，或称为 **容器引擎**，是一种在主机上运行容器的软件。它负责从注册表下载容器镜像以创建容器，监控主机上可用的资源以运行这些镜像，并管理操作系统提供的隔离层。容器运行时还会检查容器的当前状态并管理其生命周期，在主进程死亡时重新启动（如果我们声明容器在这种情况下可以随时恢复）。'
- en: We generally group container runtimes into **low-level runtimes** and **high-level
    runtimes**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将容器运行时分为 **低级容器运行时** 和 **高级容器运行时**。
- en: Low-level runtimes are those simple runtimes focused only on software container
    execution. We can consider `ldd` command on our binaries and libraries and iterate
    this process with all its dependencies, and so on. We will get a complete list
    of all the files strictly required for the process and this would become the smallest
    image for the application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 低级容器运行时是那些仅专注于软件容器执行的简单运行时。我们可以考虑对二进制文件和库执行 `ldd` 命令，并迭代所有依赖项的过程。这样我们就能得到一个完整的文件列表，列出所有进程严格需要的文件，这将成为应用程序的最小镜像。
- en: High-level container runtimes usually implement the **Container Runtime Interface**
    (**CRI**) specification of the OCI. This was created to make container orchestration
    more runtime-agnostic. In this group, we have Docker, CRI-O, and Windows/Hyper-V
    containers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 高级容器运行时通常实现 OCI 的 **容器运行时接口** (**CRI**) 规范。这是为了使容器编排更具运行时无关性。在这一组中，我们有 Docker、CRI-O
    和 Windows/Hyper-V 容器。
- en: 'The CRI interface defines the rules so that we can integrate our container
    runtimes into container orchestrators, such as Kubernetes. Container runtimes
    should have the following characteristics:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: CRI 接口定义了规则，使我们能够将容器运行时集成到容器编排器中，例如 Kubernetes。容器运行时应具备以下特点：
- en: Be capable of starting/stopping pods
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够启动/停止 pod
- en: Deal with all containers (start, pause, stop, and delete them)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理所有容器（启动、暂停、停止和删除它们）
- en: Manage container images
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理容器镜像
- en: Provide metrics collection and access to container logs
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供度量收集和容器日志访问
- en: The Docker container runtime became mainstream in 2016, making the execution
    of containers very easy for users. CRI-O was created explicitly for the Kubernetes
    orchestrator by Red Hat to allow the execution of containers using any OCI-compliant
    low-level runtime. High-level runtimes provide tools for interacting with them,
    and that’s why most people choose them.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Docker容器运行时在2016年成为主流，使得用户可以轻松执行容器。CRI-O是由Red Hat专门为Kubernetes调度器创建的，旨在使用任何符合OCI标准的低级运行时执行容器。高级运行时提供与它们交互的工具，这也是大多数人选择它们的原因。
- en: A middle ground between low-level and high-level container runtimes is provided
    by Containerd, which is an industry-standard container runtime. It runs on Linux
    and Windows and can manage the complete container life cycle.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Containerd作为一种行业标准的容器运行时，提供了低级和高级容器运行时之间的中间地带。它可以在Linux和Windows上运行，并且可以管理整个容器生命周期。
- en: The technology behind runtimes is evolving very fast; we can even improve the
    interaction between containers and hosts using sandboxes (**gVisor** from Google)
    and virtualized runtimes (**Kata Containers**). The former increases containers’
    isolation by not sharing the host’s kernel with them. A specific kernel (the small
    **unikernel** with restricted capabilities) is provided to containers as a proxy
    to the real kernel. Virtualized runtimes, on the other hand, use virtualization
    technology to isolate a container within a very small virtual machine. Although
    both cases add some load to the underlying operating system, security is increased
    as containers don’t interact directly with the host’s kernel.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时背后的技术发展非常迅速；我们甚至可以通过沙箱技术（Google的**gVisor**）和虚拟化运行时（**Kata Containers**）来改善容器和宿主机之间的交互。前者通过不与宿主机共享内核来增加容器的隔离性。容器提供一个特定的内核（具有限制能力的小**unikernel**）作为代理，来替代真实的内核。而虚拟化运行时则使用虚拟化技术将容器隔离在一个非常小的虚拟机中。虽然这两种情况都会给底层操作系统增加一些负担，但通过容器不直接与宿主机内核交互，安全性得到了提升。
- en: Container runtimes only review the main process execution. If any other process
    running inside a container dies and the main process isn’t affected, the container
    will continue running.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时仅审查主进程的执行。如果容器内的其他进程死亡且不影响主进程，容器将继续运行。
- en: Kernel capabilities
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核能力
- en: Starting with Linux kernel release 2.2, the operating system divides process
    privileges into distinct units, known as **capabilities**. These capabilities
    can be enabled or disabled by operating system and system administrators.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从Linux内核2.2版本开始，操作系统将进程权限划分为不同的单元，称为**能力**。这些能力可以由操作系统和系统管理员启用或禁用。
- en: Previously, we learned that containers run processes in isolation using the
    host’s kernel. However, it is important to know that only a restricted set of
    these kernel capabilities are allowed inside containers unless they are explicitly
    declared. Therefore, containers improve their processes’ security at the host
    level because those processes can’t do anything they want. The capabilities that
    are currently available inside a container running on top of the Docker container
    runtime are `SETPCAP`, `MKNOD`, `AUDIT_WRITE`, `CHOWN`, `NET_RAW`, `DAC_OVERRIDE`,
    `FOWNER`, `FSETID`, `KILL`, `SETGID`, `SETUID`, `NET_BIND_SERVICE`, `SYS_CHROOT`,
    and `SETFCAP`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前了解到，容器通过使用宿主机的内核来运行进程。但重要的是要知道，除非显式声明，否则只有一小部分内核能力允许在容器内使用。因此，容器提高了进程在宿主机级别的安全性，因为这些进程不能做任何它们想做的事情。当前在基于Docker容器运行时运行的容器内可用的能力包括`SETPCAP`、`MKNOD`、`AUDIT_WRITE`、`CHOWN`、`NET_RAW`、`DAC_OVERRIDE`、`FOWNER`、`FSETID`、`KILL`、`SETGID`、`SETUID`、`NET_BIND_SERVICE`、`SYS_CHROOT`和`SETFCAP`。
- en: This set of capabilities allows, for example, processes inside a container to
    attach and listen on ports below `1024` (the `NET_BIND_SERVICE` capability) or
    use ICMP (the `NET_RAW` capability).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这一套功能允许，例如，容器内的进程附加并监听低于`1024`端口（`NET_BIND_SERVICE`能力）或使用ICMP（`NET_RAW`能力）。
- en: If our process inside a container requires us to, for example, create a new
    network interface (perhaps to run a containerized OpenVPN server), the `NET_ADMIN`
    capability should be included.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在容器内的进程需要，例如，创建一个新的网络接口（可能是为了运行一个容器化的OpenVPN服务器），应该包含`NET_ADMIN`能力。
- en: Important note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Container runtimes allow containers to run with full privileges using special
    parameters. The processes within these containers will run with all kernel capabilities
    and it could be very dangerous. You should avoid using privileged containers –
    it is best to take some time to verify which capabilities are needed by an application
    to work correctly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时允许容器以完全权限运行，使用特殊的参数。这些容器中的进程将使用所有内核功能，这可能非常危险。您应该避免使用特权容器——最好花些时间验证应用程序正确运行所需的功能。
- en: Container orchestrators
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器编排工具
- en: Now that we know that we need a runtime to execute containers, we must also
    understand that this will work in a standalone environment, without hardware high
    availability. This means that server maintenance, operating system upgrades, and
    any other problem at the software, operating system, or hardware levels may affect
    your application.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道我们需要一个运行时来执行容器，我们还必须理解，这将在一个独立的环境中工作，且没有硬件高可用性。这意味着服务器维护、操作系统升级以及在软件、操作系统或硬件层面上的任何问题都可能影响您的应用程序。
- en: High availability requires resource duplicity and thus more servers and/or hardware.
    These resources will allow containers to run on multiple hosts, each one with
    a container runtime. However, maintaining application availability in this situation
    isn’t easy. We need to ensure that containers will be able to run on any of these
    nodes; in the *Overlay filesystems* section, we’ll learn that synchronizing container-related
    resources within nodes involves more than just copying a few files. **Container
    orchestrators** manage node resources and provide them to containers. They schedule
    containers as needed, take care of their status, provide resources for persistence,
    and manage internal and external communications (in [*Chapter 6*](B19845_06.xhtml#_idTextAnchor134),
    *Fundamentals of Orchestration*, we will learn how some orchestrators delegate
    some of these features to different modules to optimize their work).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性需要资源的冗余，因此需要更多的服务器和/或硬件。这些资源将允许容器在多个主机上运行，每个主机都有一个容器运行时。然而，在这种情况下维持应用程序的可用性并不容易。我们需要确保容器能够在这些节点中的任何一个上运行；在*覆盖文件系统*一节中，我们将了解到，同步节点内与容器相关的资源不仅仅是复制几个文件。**容器编排工具**管理节点资源并将其提供给容器。它们根据需要调度容器，处理容器状态，为持久性提供资源，并管理内部和外部通信（在[*第6章*](B19845_06.xhtml#_idTextAnchor134)《编排基础》中，我们将学习一些编排工具如何将其中的一些功能委派给不同的模块，以优化它们的工作）。
- en: The most famous and widely used container orchestrator today is **Kubernetes**.
    It has a lot of great features to help manage clustered containers, although the
    learning curve can be tough. Also, **Docker Swarm** is quite simple and allows
    you to quickly execute your applications with high availability (or resilience).
    We will cover both in detail in [*Chapter 7*](B19845_07.xhtml#_idTextAnchor147),
    *Orchestrating with Swarm*, and [*Chapter 8*](B19845_08.xhtml#_idTextAnchor170),
    *Deploying Applications with the Kubernetes Orchestrator*. There were other opponents
    in this race but they stayed by the wayside while Kubernetes took the lead.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当今最著名且广泛使用的容器编排工具是**Kubernetes**。它有很多很棒的功能，帮助管理集群容器，尽管学习曲线可能较为陡峭。此外，**Docker
    Swarm**非常简单，并且允许您快速执行具有高可用性（或弹性）的应用程序。我们将在[*第7章*](B19845_07.xhtml#_idTextAnchor147)《使用Swarm进行编排》和[*第8章*](B19845_08.xhtml#_idTextAnchor170)《使用Kubernetes编排器部署应用程序》中详细讲解这两者。在这场竞争中还有其他对手，但它们都被抛在了后头，而Kubernetes则占据了主导地位。
- en: HashiCorp’s **Nomad** and Apache’s **Mesos** are still being used for very special
    projects but are out of scope for most enterprises and users. Kubernetes and Docker
    Swarm are community projects and some vendors even include them within their enterprise-ready
    solutions. Red Hat’s **OpenShift**, SUSE’s **Rancher**, Mirantis’ **Kubernetes
    Engine** (old Docker Enterprise platform), and VMware’s **Tanzu**, among others,
    all provide on-premises and some cloud-prepared custom Kubernetes platforms. But
    those who made Kubernetes the most-used platform were the well-known cloud providers
    – Google, Amazon, Azure, and Alibaba, among others, serve their own container
    orchestration tools, such as Amazon’s **Elastic Container Service** or **Fargate**,
    Google’s **Cloud Run**, and Microsoft’s **Azure Container Instances**, and they
    also package and manage their own Kubernetes infrastructures for us to use (Google’s
    GKE, Amazon’s EKS, Microsoft’s AKS, and so on). They provide **Kubernetes-as-a-Service**
    platforms where you only need an account to start deploying your applications.
    They also serve you storage, advanced networking tools, resources for publishing
    your applications, and even *follow-the-sun* or worldwide distributed architectures.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: HashiCorp 的 **Nomad** 和 Apache 的 **Mesos** 仍然用于一些非常特殊的项目，但对于大多数企业和用户来说超出了范围。Kubernetes
    和 Docker Swarm 是社区项目，一些厂商甚至将它们包含在企业级解决方案中。Red Hat 的 **OpenShift**、SUSE 的 **Rancher**、Mirantis
    的 **Kubernetes Engine**（旧版 Docker 企业平台）和 VMware 的 **Tanzu** 等，提供了本地部署的以及部分云准备的定制
    Kubernetes 平台。但使 Kubernetes 成为最常用平台的是那些著名的云提供商——Google、Amazon、Azure 和 Alibaba
    等，他们提供自己的容器编排工具，如 Amazon 的 **弹性容器服务** 或 **Fargate**，Google 的 **Cloud Run**，以及
    Microsoft 的 **Azure 容器实例**，他们还为我们打包并管理自己的 Kubernetes 基础设施（Google 的 GKE，Amazon
    的 EKS，Microsoft 的 AKS 等）。他们提供 **Kubernetes 即服务** 平台，你只需要一个账户就可以开始部署应用程序。他们还为你提供存储、先进的网络工具、发布应用程序的资源，甚至是
    *跟随太阳* 或全球分布式架构。
- en: There are many Kubernetes implementations. The most popular is probably OpenShift
    or its open source project, OKD. There are others based on a binary that launches
    and creates all of the Kubernetes components using automated procedures, such
    as Rancher RKE (or its government-prepared release, RKE2), and those featuring
    only the strictly necessary Kubernetes components, such as K3S or K0S, to provide
    the lightest platform for IoT and more modest hardware. And finally, we have some
    Kubernetes distributions for desktop computers, offering all the features of Kubernetes
    ready to develop and test applications with. In this group, we have Docker Desktop,
    Rancher Desktop, Minikube, and **Kubernetes in Docker** (**KinD**). We will learn
    how to use them in this book to develop, package, and prepare applications for
    production.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有很多实现。最流行的可能是 OpenShift 或其开源项目 OKD。还有一些基于二进制文件的项目，通过自动化程序启动并创建所有
    Kubernetes 组件，如 Rancher RKE（或其政府版本 RKE2），以及仅包含严格必要的 Kubernetes 组件的项目，如 K3S 或 K0S，以提供最轻量的平台，适用于物联网和更谦逊的硬件。最后，我们还有一些
    Kubernetes 发行版，专为桌面计算机提供，具备 Kubernetes 所有功能，准备好用于开发和测试应用程序。在这个组中，我们有 Docker Desktop、Rancher
    Desktop、Minikube 和 **Kubernetes in Docker**（**KinD**）。我们将在本书中学习如何使用它们来开发、打包和准备生产环境的应用程序。
- en: We shouldn’t forget solutions for running orchestrated applications based on
    multiple containers on standalone servers or desktop computers, such as **Docker
    Compose**. Docker has prepared a simple Python-based orchestrator for quick application
    development, managing the container dependencies for us. It is very convenient
    for testing all of our components together on a laptop with minimum overhead,
    instead of running a full Kubernetes or Swarm cluster. We will cover this tool,
    seeing as it has evolved a lot and is now part of the common Docker client command
    line, in [*Chapter 5*](B19845_05.xhtml#_idTextAnchor118), *Creating* *Multi-Container
    Applications*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应忽视基于多个容器在独立服务器或桌面计算机上运行编排应用程序的解决方案，例如**Docker Compose**。Docker 为我们准备了一个简单的基于
    Python 的编排工具，用于快速应用程序开发，管理容器依赖关系。这对于在笔记本电脑上以最小开销测试我们所有组件非常方便，而不是运行完整的 Kubernetes
    或 Swarm 集群。我们将在[*第 5 章*](B19845_05.xhtml#_idTextAnchor118)中介绍这个工具，*创建* *多容器应用程序*，因为它已经发展了很多，并且现在是常见的
    Docker 客户端命令行的一部分。
- en: Container images
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器镜像
- en: Earlier in this chapter, we mentioned that containers run thanks to **container
    images**, which are used as templates for executing processes in isolation and
    attached to a filesystem; therefore, a container image contains all the files
    (binaries, libraries, configurations, and so on) required by its processes. These
    files can be a subset of some operating system or just a few binaries with configurations
    built by yourself.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本章前面提到，容器能够运行是因为有了**容器镜像**，这些镜像作为模板用于在隔离环境中执行进程并附加到文件系统上；因此，容器镜像包含了其进程所需的所有文件（如二进制文件、库、配置文件等）。这些文件可以是某些操作系统的子集，或者只是由你自己构建的少量二进制文件和配置。
- en: Virtual machine templates are immutable, as are container templates. This immutability
    means that they don’t change between executions. This feature is key because it
    ensures that we get the same results every time we use an image for creating a
    container. Container behavior can be changed using configurations or command-line
    arguments through the container runtime. This ensures that images created by developers
    will work in production as expected, and moving applications to production (or
    even creating upgrades between different releases) will be smooth and fast, reducing
    the time to market.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机模板是不可变的，容器模板也是如此。这个不可变性意味着它们在执行之间不会发生变化。这个特性非常关键，因为它确保每次使用镜像创建容器时，我们都会得到相同的结果。容器的行为可以通过容器运行时的配置或命令行参数进行更改。这确保了开发人员创建的镜像在生产环境中能够按预期工作，并且将应用程序迁移到生产环境（甚至在不同版本之间创建升级）将变得平滑且快速，从而缩短上市时间。
- en: Container images are a collection of files distributed in layers. We shouldn’t
    add anything more than the files required by the application. As images are immutable,
    all these layers will be presented to containerized processes as read-only sets
    of files. But we don’t duplicate files between layers. Only files modified on
    one layer will be stored in the next layer above – this way, each layer retains
    the changes from the original base layer (referenced as the base image).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 容器镜像是分层分发的文件集合。我们不应添加除应用程序所需文件以外的任何内容。由于镜像是不可变的，这些层会作为只读文件集呈现给容器化进程。但我们不会在层之间重复文件。只有在某一层上修改的文件会存储在上面一层中——这样，每一层都会保留来自原始基础层（称为基础镜像）的更改。
- en: 'The following diagram shows how we create a container image using multiple
    layers:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了如何使用多个层创建容器镜像：
- en: '![Figure 1.2 – Schema of stacked layers representing a container image](img/B19845_01_02.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 表示容器镜像的堆叠层次结构示意图](img/B19845_01_02.jpg)'
- en: Figure 1.2 – Schema of stacked layers representing a container image
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 表示容器镜像的堆叠层次结构示意图
- en: A base layer is always included, although it could be empty. The layers above
    this base layer may include new binaries or just include new meta-information
    (which does not create a layer but a meta-information modification).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基础层始终会包含，即使它是空的。位于基础层之上的层可能包含新的二进制文件，或者仅包含新的元信息（这不会创建一个层，而只是修改元信息）。
- en: To easily share these templates between computers or even environments, these
    file layers are packaged into `.tar` files, which are finally what we call images.
    These packages contain all layered files, along with meta-information that describes
    the content, specifies the process to be executed, identifies the ports that will
    be exposed to communicate with other containerized processes, specifies the user
    who will own it, indicates the directories that will be kept out of container
    life cycle, and so on.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便在计算机之间或不同环境之间共享这些模板，这些文件层会被打包成 `.tar` 文件，这些文件最终就被称为镜像。这些包包含了所有的层文件，以及描述内容的元信息，指定要执行的进程，标识将暴露出来以与其他容器化进程进行通信的端口，指定将拥有该镜像的用户，指示将在容器生命周期中保持不变的目录等信息。
- en: We use different methods to create these images, but we aim to make the process
    reproducible, and thus we use Dockerfiles as recipes. In [*Chapter 2*](B19845_02.xhtml#_idTextAnchor036),
    *Building Container Images*, we will learn about the image creation workflow while
    utilizing best practices and diving deep into command-line options.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的方法来创建这些镜像，但我们的目标是使这个过程可重复，因此我们使用 Dockerfile 作为配方。在[*第 2 章*](B19845_02.xhtml#_idTextAnchor036)《构建容器镜像》中，我们将学习镜像创建的工作流程，同时使用最佳实践并深入探讨命令行选项。
- en: These container images are stored on registries. This application software is
    intended to store file layers and meta-information in a centralized location,
    making it easy to share common layers between different images. This means that
    two images using a common Debian base image (a subset of files from the complete
    operating system) will share these base files, thus optimizing disk space usage.
    This can also be employed on containers’ underlying host local filesystems, saving
    a lot of space.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Another result of the use of these layers is that containers using the same
    template image to execute their processes will use the same set of files, and
    only those files that get modified will be stored.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: All these behaviors related to the optimized use of files shared between different
    images and containers are provided by operating systems thanks to overlay filesystems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Overlay filesystems
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An **overlay filesystem** is a union mount filesystem (a way of combining multiple
    directories into one that appears to contain their whole combined content) that
    combines multiple underlying mount points. This results in a structure with a
    single directory that contains all underlying files and sub-directories from all
    sources.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Overlay filesystems merge content from directories, combining the file objects
    (if any) yielded by different processes, with the *upper* filesystem taking precedence.
    This is the magic behind container-image layers’ reusability and disk space saving.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how images are packaged and how they share content, let’s
    go back to learning a bit more about containers. As you may have learned in this
    section, containers are processes that run in isolation on top of a host operating
    system thanks to a container runtime. Although the kernel host is shared by multiple
    containers, features such as kernel namespaces and cgroups provide special containment
    layers that allow us to isolate them. Container processes need some files to work,
    which are included in the container space as immutable templates. As you may think,
    these processes will probably need to modify or create some new files found on
    container image layers, and a new read-write layer will be used to store these
    changes. The container runtime presents this new layer to the container to enable
    changes – we usually refer to this as the **container layer**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The following schema outlines the read-write layers coming from the container
    image template with the newly added container layer, where the container’s running
    processes store their file modifications:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Container image layers will always be read-only; the container
    adds a new layer with read-write capabilities](img/B19845_01_03.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Container image layers will always be read-only; the container
    adds a new layer with read-write capabilities
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The changes made by container processes are always *ephemeral* as the container
    layer will be lost whenever we remove the container, while image layers are immutable
    and will remain unchanged. With this behavior in mind, it is easy to understand
    that we can run multiple containers using the same container image.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure represents this situation where three different running
    containers were created from the same image:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Three different containers run using the same container image](img/B19845_01_04.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Three different containers run using the same container image
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed, this behavior leaves a very small footprint on our
    operating systems in terms of disk space. Container layers are very small (or
    at least they should be, and you as a developer will learn which files shouldn’t
    be left inside the container life cycle).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Container runtimes manage how these overlay folders will be included inside
    containers and the magic behind that. The mechanism for this is based on specific
    operating system drivers that implement **copy-on-write** filesystems. Layers
    are arranged one on top of the other and only files modified within them are merged
    on the upper layer. This process is managed at speed by operating system drivers,
    but some small overhead is always expected, so keep in mind that all files that
    are modified continuously by your application (logs, for example) should never
    be part of the container.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '*Copy-on-write* uses small layered filesystems or folders. Files from any layer
    are accessible to read access, but *write* requires searching for the file within
    the underlying layers and copying this file to the upper layer to store the changes.
    Therefore, the I/O overhead from reading files is very small and we can keep multiple
    layers for better file distribution between containers. In contrast, writing requires
    more resources and it would be better to leave big files and those subject to
    many or continuous modifications out of the container layer.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to notice that containers are not ephemeral at all. As
    mentioned previously, changes in the container layer are retained until the container
    is removed from the operating system; so, if you create a 10 GB file in the container
    layer, it will reside on your host’s disk. Container orchestrators manage this
    behavior, but be careful where you store your persistent files. Administrators
    should do container housekeeping and disk maintenance to avoid disk-pressure problems.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Developers should keep this in mind and prepare their applications using containers
    to be logically ephemeral and store persistent data outside the container’s layers.
    We will learn about options for persistence in [*Chapter 10*](B19845_10.xhtml#_idTextAnchor231),
    *Leveraging Application Data Management* *in Kubernetes*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: This thinking leads us to the next section, where we will discuss the intrinsic
    dynamism of container environments.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dynamism in container-based applications
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen how containers run using immutable storage (container images) and
    how the container runtime adds a new layer for managing changed files. Although
    we mentioned in the previous section that containers are not ephemeral in terms
    of disk usage, we have to include this feature in our application’s design. Containers
    will start and stop whenever you upgrade your application’s components. Whenever
    you change the base image, a completely new container will be created (remember
    the layers ecosystem described in the previous section). This will become even
    worse if you want to distribute these application components across a cluster
    – even using the same image will result in different containers being created
    on different hosts. Thus, this **dynamism** is inherited in these platforms.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: In the context of networking communications inside containers, we know that
    processes running inside a container share its network namespace, and thus they
    all get the same network stack and IP address. But every time a new container
    is created, the container runtime will provide a new IP address. Thanks to container
    orchestration and the **Domain Name System** (**DNS**) included, we can communicate
    with our containers. As IP addresses are dynamically managed by the container
    runtime’s internal **IP Address Management** (**IPAM**) using defined pools, every
    time a container dies (whether the main process is stopped, killed manually, or
    ended by an error), it will free its IP address and IPAM will assign it to a new
    container that might be part of a completely different application. Hence, we
    can trust the IP address assignment although we shouldn’t use container IP addresses
    in our application configurations (or even worse, write them in our code, which
    is a bad practice in every scenario). IP addresses will be dynamically managed
    by the IPAM container runtime component by default. We will learn about better
    mechanisms we can use to reference our application’s containers, such as service
    names, in [*Chapter 4*](B19845_04.xhtml#_idTextAnchor096), *Running* *Docker Containers*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Applications use fully qualified domain names (or short names if we are using
    internal domain communications, as we will learn when we use Docker Compose to
    run multi-container applications, and also when applications run in more complicated
    container orchestrations).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Because IP addresses are dynamic, special resources should be used to assign
    sets of IP addresses (or unique IP addresses, if we have just one process replica)
    to service names. In the same way, publishing application components requires
    some resource mappings, using **network address translation** (**NAT**) for communicating
    between users and external services and those running inside containers, distributed
    across a cluster in different servers or even different infrastructures (such
    as cloud-provided container orchestrators, for example).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re reviewing the main concepts related to containers in this chapter,
    we can’t miss out on the tools that are used for creating, executing, and sharing
    containers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Tools for managing containers
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we learned previously, the container runtime will manage most of the actions
    we can achieve with containers. Most of these runtimes run as **daemons** and
    provide an interface for interacting with them. Among these tools, Docker stands
    out as it provides *all the tools in a box*. Docker acts as a client-server application
    and in newer releases, both the client and server components are packaged separately,
    but in any case, both are needed by users. At first, when Docker Engine was the
    most popular and reliable container engine, Kubernetes adopted it as its runtime.
    But this marriage did not last long, and Docker Engine was deprecated in Kubernetes
    release 1.22\. This happened because Docker manages its own integration of Containerd,
    which is not standardized nor directly usable by the Kubernetes CRI. Despite this
    fact, Docker is still the most widely used option for developing container-based
    applications and the de facto standard for building images.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned Docker Desktop and Rancher Desktop earlier in this section. Both
    act as container runtime clients that use either the `docker` or `nerdctl` command
    lines. We can use such clients because in both cases, `dockerd` or `containerd`
    act as container runtimes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Developers and the wider community pushed Docker to provide a solution for users
    who prefer to run containers without having to run a privileged system daemon,
    which is dockerd’s default behavior. It took some time but finally, a few years
    ago, Docker published its rootless runtime with user privileges. During this development
    phase, another container executor arrived, called Podman, created by Red Hat to
    solve the same problem. This solution can run without root privileges and aims
    to avoid the use of a daemonized container runtime. The host user can run containers
    without any system privilege by default; only a few tweaks are required by administrators
    if the containers are to be run in a security-hardened environment. This made
    Podman a very secure option for running containers in production (without orchestration).
    Docker also included rootless containers by the end of 2019, making both options
    secure by default.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: As you learned at the beginning of this section, containers are processes that
    run on top of an operating system, isolated using its kernel features. It is quite
    evident why containers are so popular in microservice environments (one container
    runs a process, which is ultimately a microservice), although we can still build
    microservice-based applications without containers. It is also possible to use
    containers to run whole application components together, although this isn’t an
    ideal situation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll largely focus on software containers in the context of
    Linux operating systems. This is because they were only introduced in Windows
    systems much later. However, we will also briefly discuss them in the context
    of Windows.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: We shouldn’t compare containers with virtual nodes. As discussed earlier in
    this section, containers are mainly based on cgroups and kernel namespaces while
    virtual nodes are based on hypervisor software. This software provides sandboxing
    capabilities and specific virtualized hardware resources to guest hosts. We still
    need to prepare operating systems to run these virtual guest hosts. Each guest
    node will receive a piece of virtualized hardware and we must manage servers’
    interactions as if they were physical.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: We’ll compare these models side by side in the following section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Comparing virtualization and containers
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following schema represents a couple of virtual guest nodes running on
    top of a physical host:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Applications running on top of virtual guest nodes, running
    on top of a physical server](img/B19845_01_05.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Applications running on top of virtual guest nodes, running on
    top of a physical server
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: A physical server running its own operating system executes a hypervisor software
    layer to provide virtualization capabilities. A specific amount of hardware resources
    is virtualized and provisioned to these new virtual guest nodes. We should install
    new operating systems for these new hosts and after that, we will be able to run
    applications. Physical host resources are partitioned for guest hosts and both
    nodes are completely isolated from each other. Each virtual machine executes its
    own kernel and its operating system runs on top of the host. There is complete
    isolation between guests’ operating systems because the underlying host’s hypervisor
    software keeps them separated.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In this model, we require a lot of resources, even if we just need to run a
    couple of processes per virtual host. Starting and stopping virtual hosts will
    take time. Lots of non-required software and processes will probably run on our
    guest host and it will require some tuning to remove them.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: As we have learned, the microservices model is based on the idea of applications
    running decoupled in different processes with complete functionality. Thus, running
    a complete operating system within just a couple of processes doesn’t seem like
    a good idea.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Although automation will help us, we need to maintain and configure those guest
    operating systems in terms of running the required processes and managing users,
    access rights, and network communications, among other things. System administrators
    maintain these hosts as if they were physical. Developers require their own copies
    to develop, test, and certify application components. Scaling up these virtual
    servers can be a problem because in most cases, increasing resources require a
    complete reboot to apply the changes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Modern virtualization software provides API-based management, which enhances
    their usage and virtual node maintenance, but it is not enough for microservice
    environments. Elastic environments, where components should be able to scale up
    or down on demand, will not fit well in virtual machines.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s review the following schema, which represents a set of containers
    running on physical and virtual hosts:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – A set of containers running on top of physical and virtual hosts](img/B19845_01_06.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – A set of containers running on top of physical and virtual hosts
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: All containers in this schema share the same host kernel as they are just processes
    running on top of an operating system. In this case, we don’t care whether they
    run on a virtual or a physical host; we expect the same behavior. Instead of hypervisor
    software, we have a `/etc/hosts` and `/etc/nsswitch.conf` files would probably
    be required (along with some network libraries and their dependencies). The **attack
    surface** will be completely different than having a whole operating system full
    of binaries, libraries, and running services, regardless of whether the application
    uses them or not.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Containers are designed to run just one main process (and its threads or sub-processes)
    and this makes them lightweight. They can start and stop as fast as their main
    process does.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: All the resources consumed by a container are related to the given process,
    which is great in terms of the allocation of hardware resources. We can calculate
    our application’s resource consumption by observing the load of all its microservices.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: We define **images** as templates for running containers. These images contain
    all the files required by the container to work plus some meta-information providing
    its features, capabilities, and which commands or binaries will be used to start
    the process. Using images, we can ensure that all the containers created with
    one template will run the same. This eliminates infrastructure friction and helps
    developers prepare their applications to run in production. The configuration
    (and of course security information such as credentials) is the only thing that
    differs between the development, testing, certification, and production environments.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Software containers also improve application security because they run by default
    with limited privileges and allow only a set of system calls. They run anywhere;
    all we need is a container runtime to be able to create, share, and run containers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what containers are and the most important concepts involved,
    let’s try to understand how they fit into development processes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Building, sharing, and running containers
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Build, ship, and run*: you might have heard or read this quote years ago.
    Docker Inc. used it to promote the ease of using containers. When creating container-based
    applications, we can use Docker to build container images, share these images
    within environments, move the content from our development workstations to testing
    and staging environments, execute them as containers, and finally use these packages
    in production. Only a few changes are required throughout, mainly at the application’s
    configuration level. This workflow ensures application usage and immutability
    between the development, testing, and staging stages. Depending on the container
    runtime and container orchestrator chosen for each stage, Docker could be present
    throughout (Docker Engine and Docker Swarm). Either way, most people still use
    the Docker command line to create container images due to its great, always-evolving
    features that allow us, for example, to build images for different processor architectures
    using our desktop computers.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Adding **continuous integration** (**CI**) and **continuous deployment** (**CD**)
    (or **continuous delivery**, depending on the source) to the equation simplifies
    developers’ lives so they can focus on their application’s architecture and code.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: They can code on their workstations and push their code to a source code repository,
    and this event will trigger a CI/CD automation to build applications artifacts,
    compiling their code and providing the artifacts in the form of binaries or libraries.
    This automation can also include these artifacts inside container images. These
    become the new application artifacts and are stored in image registries (the backends
    that store container images). Different executions can be chained to test this
    newly compiled component together with other components in the integration phase,
    achieve verification via some tests in the testing phase, and so on, passing through
    different stages until it gets to production. All these chained workflows are
    based on containers, configuration, and the images used for execution. In this
    workflow, developers never explicitly create a release image; they only build
    and test development ones, but the same Dockerfile recipe is used on their workstations
    and in the CI/CD phases executed on servers. Reproducibility is key.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Developers can run multiple containers on their developer workstations as if
    they were using the real environment. They can test their code along with other
    components in their environment, allowing them to evaluate and discover problems
    faster and fix them even before moving their components to the CI/CD pipelines.
    When their code is ready, they can push it to their code repository and trigger
    the automation. Developers can build their development images, test them locally
    (be it a standalone component, multiple components, or even a full application),
    prepare their release code, then push it, and the CI/CD orchestrator will build
    the release image for them.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: In these contexts, images are shared between environments via the use of image
    registries. *Shipping* images from server to server is easy as the host’s container
    runtime will download the images from the given registries – but only those layers
    not already present on the servers will be downloaded, hence the layer distribution
    within container images is key.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The following schema outlines this simplified workflow:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Simplified schema representing a CI/CD workflow example using
    software containers to deliver applications to production](img/B19845_01_07.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Simplified schema representing a CI/CD workflow example using software
    containers to deliver applications to production
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Servers running these different stages can be either standalone servers, pools
    of nodes from orchestrated clusters, or even more complex dedicated infrastructures,
    including in some cases cloud-provided hosts or whole clusters. Using container
    images ensures the artifact’s content and infrastructure-specific configurations
    will run in the custom application environment in each case.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, we can imagine how we could build a full development chain
    using containers. We talked about Linux kernel namespaces already, so let’s continue
    by understanding how these isolation mechanisms work on Microsoft Windows.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Explaining Windows containers
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this chapter, we have focused on software containers within Linux operating
    systems. Software containers started on Linux systems, but due to their importance
    and advances in technology in terms of host resource usage, Microsoft introduced
    them in the Microsoft Windows Server 2016 operating system. Before this, Windows
    users and administrators were only capable of using software containers for Linux
    through virtualization. Thus, there was the Docker Toolbox solution, of which
    Docker Desktop formed a part, and installing this software on our Windows-based
    computer allowed us to have a terminal with the Docker command line, a fancy GUI,
    and a Hyper-V Linux virtual machine where containers would run. This made it easy
    for entry-level users to use software containers on their Windows desktops, but
    Microsoft eventually brought in a game-changer here, creating a new encapsulation
    model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Container runtimes are client-server applications, so we can serve the runtime
    to local (by default) and remote clients. When we use a remote runtime, we can
    use our clients to execute commands on this runtime using different clients, such
    as `docker` or `nerdctl`, depending on the server side. Earlier in this chapter,
    we mentioned that desktop solutions such as Docker Desktop or Rancher Desktop
    use this model, running a container runtime server where the common clients, executed
    from common Linux terminals or Microsoft PowerShell, can manage software containers
    running on the server side.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft provided two different software container models:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyper-V Linux Containers**: The old model, which uses a Linux virtual machine'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windows Server Containers**, also known as **Windows Process Containers**:
    This is the new model, allowing the execution of Windows operating-system-based
    applications'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the user’s perspective, the management and execution of containers running
    on Windows are the same, no matter which of the preceding models is in use, but
    only one model can be used per server, thus applying to all containers on that
    server. The differences here come from the isolation used in each model.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Process isolation** on Windows works in the same way it does on Linux. Multiple
    processes run on a host, accessing the host’s kernel, and the host provides isolation
    using namespaces and resources control (along with other specific methods, depending
    on the underlying operating system). As we already know, processes get their own
    filesystem, network, processes identifiers, and so on, but in this case, they
    also get their own Windows registry and object namespace.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Due to the very nature of the Microsoft Windows operating system, some system
    services and **dynamic linked libraries** (**DLLs**) are required within the containers
    and cannot be shared from the host. Thus, process containers need to contain a
    copy of these resources, which makes Windows images quite a lot bigger than Linux-based
    container images. You may also encounter some compatibility issues within image
    releases, depending on which base operating system (files tree) was used to generate
    it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The following schema represents both models side by side so that we can observe
    the main stack differences:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – A comparison of Microsoft Windows software container models](img/B19845_01_08.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – A comparison of Microsoft Windows software container models
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: We will use Windows Server containers when our application requires strong integration
    with the Microsoft operating system, for example, for integrating **Group Managed
    Service Accounts** (**gMSA**) or encapsulating applications that don’t run under
    Linux hosts.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: From my experience, Windows Server containers became very popular when they
    initially arrived, but as Microsoft improved the support of their applications
    for Linux operating systems, the fact that developers could create their applications
    in .NET Core for either Microsoft Windows or Linux, and the lack of many cloud
    providers offering this technology, made them almost disappear from the scene.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to mention that orchestration technology evolution helped
    developers move to Linux-only containers. Windows Server containers were supported
    only on top of Docker Swarm until 2019 when Kubernetes announced their support.
    Due to the large increase of Kubernetes’ adoption in the developer community and
    even in enterprise environments, Windows Server container usage reduced to very
    specific and niche use cases.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, Kubernetes supports Microsoft Windows Server hosts running as worker
    roles, allowing process container execution. We will learn about Kubernetes and
    host roles in [*Chapter 8*](B19845_08.xhtml#_idTextAnchor170), *Deploying Applications
    with the Kubernetes Orchestrator*. Despite this fact, you will probably not find
    many Kubernetes clusters running Windows Server container workloads.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned that containers improve application security. The next section
    will show you the improvements at the host and container levels that make containers
    *safer* *by default*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Improving security using software containers
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to introduce some of the features found on container
    platforms that help improve application security.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: If we keep in mind how containers run, we know that we first need a **host**
    with a container runtime. So, having a host with just the software required is
    the first security measure. We should use dedicated hosts in production for running
    container workloads. We do not need to concern ourselves with this while developing,
    but system administrators should prepare production nodes with minimal attack
    surfaces. We should never share these hosts for use in serving other technologies
    or services. This feature is so important that we can even find dedicated operating
    systems, such as Red Hat’s CoreOS, SuSE’s RancherOS, VMware’s PhotonOS, TalOS,
    or Flatcar Linux, just to mention the most popular ones. These are minimal operating
    systems that just include a container runtime. You can even create your own by
    using Moby’s LinuxKit project. Some vendors’ customized Kubernetes platforms,
    such as Red Hat’s OpenShift, create their clusters using CoreOS, improving the
    whole environment’s security.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: We will never connect to any cluster host to execute containers. Container runtimes
    work in client-server mode. Rather, we expose this engine service and simply using
    a client on our laptop or desktop computers will be more than enough to execute
    containers on the host.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Locally, clients connect to container runtimes using `/var/run/docker.sock`
    for `dockerd`, for example). Adding read-write access to this socket to specific
    users will allow them to use the daemon to build, pull, and push images or execute
    containers. Configuring the container runtime in this way may be worse if the
    host has a master role in an orchestrated environment. It is crucial to understand
    this feature and know which users will be able to run containers on each host.
    System administrators should keep their container runtimes’ sockets safe from
    untrusted users and only allow authorized access. These sockets are local and,
    depending on which runtime we are using, TCP or even SSH (in `dockerd`, for example)
    can be used to secure remote access. Always ensure **Transport Layer Security**
    (**TLS**) is used to secure socket access.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that container runtimes do not provide any **role-based
    access control** (**RBAC**). We will need to add this layer later with other tools.
    Docker Swarm does not provide RBAC, but Kubernetes does. RBAC is key for managing
    user privileges and multiple application isolation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: We should say here that, currently, desktop environments (Docker Desktop and
    Rancher Desktop) also work with this model, in which you don’t connect to the
    host running the container runtime. A virtualized environment is deployed on your
    system (using Qemu if on Linux, or Hyper-V or the newer Windows Subsystem for
    Linux on Windows hosts) and our client, using a terminal, will connect to this
    virtual container runtime (or the Kubernetes API when deploying workloads on Kubernetes,
    as we will learn in [*Chapter 8*](B19845_08.xhtml#_idTextAnchor170), *Deploying
    Applications with the* *Kubernetes Orchestrator*).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have to reiterate that container runtimes add only a subset of kernel
    capabilities by default to container processes. But this may not be enough in
    some cases. To improve containers’ security behavior, container runtimes also
    include a default **Secure Computing Mode** (**Seccomp**) profile. Seccomp is
    a Linux security facility that filters the system calls allowed inside containers.
    Specific profiles can be included and used by runtimes to add some required system
    calls. You, as the developer, need to notice when your application requires extra
    capabilities or uncommon system calls. The special features described in this
    section are used on host monitoring tools, for example, or if we need to add a
    new kernel module using system administration containers.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Container runtimes usually run as daemons; thus, they will quite probably run
    as root users. This means that any container can contain the host’s files inside
    (we will learn how we can mount volumes and host paths within containers in [*Chapter
    4*](B19845_04.xhtml#_idTextAnchor096), *Running Docker Containers*) or include
    the host’s namespaces (container processes may access host’s PIDs, networks, IPCs,
    and so on). To avoid the undesired effects of running container runtime privileges,
    system administrators should apply special security measures using **Linux Security
    Modules** (**LSM**), such as SELinux or AppArmor, among others.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: SELinux should be integrated into container runtimes and container orchestration.
    These integrations can be used to ensure, for example, that only certain paths
    are allowed inside containers. If your application requires access to the host’s
    files, non-default SELinux labels should be included to modify the default runtime
    behavior. Container runtimes’ software installation packages include these settings,
    among others, to ensure that common applications will run without problems. However,
    those with special requirements, such as those that are prepared to read hosts’
    logs, will require further security configurations.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: So far in this chapter, we have provided a quick overview of the key concepts
    related to containers. In the following section, we’ll put this into practice.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Labs
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first chapter, we covered a lot of content, learning what containers
    are and how they fit into the modern microservices architecture.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: In this lab, we will install a fully functional development environment for
    container-based applications. We will use Docker Desktop because it includes a
    container runtime, its client, and a minimal but fully functional Kubernetes orchestration
    solution.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: We could use Docker Engine in Linux directly (the container runtime only, following
    the instructions at [https://docs.docker.com/](https://docs.docker.com/)) for
    most labs but we will need to install a new tool for the Kubernetes labs, which
    requires a minimal Kubernetes cluster installation. Thus, even for just using
    the command line, we will use the Docker Desktop environment.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: We will use a Kubernetes desktop environment to minimize CPU and memory requirements.
    There are even lighter Kubernetes cluster alternatives such as KinD or K3S, but
    these may require some customization. Of course, you can also use any cloud provider’s
    Kubernetes environment if you feel more comfortable doing so.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker Desktop
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This lab will guide you through the installation of **Docker Desktop** on your
    laptop or workstation and how to execute a test to verify that it works correctly.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Docker Desktop can be installed on Microsoft Windows 10, most of the common
    Linux flavors, and macOS (the arm64 and amd64 architectures are both supported).
    This lab will show you how to install this software on Windows 10, but I will
    use Windows and Linux interchangeably in other labs as they mostly work the same
    – we will review any differences between the platforms when required.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We will follow the simple steps documented at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
    Docker Desktop can be deployed on Windows using **Hyper-V** or the newer **Windows
    Subsystem for Linux** 2 (**WSL 2**). This second option uses less compute and
    memory resources and is nicely integrated into Microsoft Windows, making it the
    preferred installation method, but note that WSL2 is required on your host before
    installing Docker Desktop. Please follow the instructions from Microsoft at [https://learn.microsoft.com/en-us/windows/wsl/install](https://learn.microsoft.com/en-us/windows/wsl/install)
    before installing Docker Desktop. You can install any Linux distribution because
    the integration will be automatically included.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the **Ubuntu** WSL distribution. It is available from the **Microsoft
    Store** and is simple to install:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Ubuntu in the Microsoft Store](img/B19845_01_09.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Ubuntu in the Microsoft Store
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'During the installation, you will be prompted for **username** and **password**
    details for this Windows subsystem installation:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – After installing Ubuntu, you will have a fully functional Linux
    Terminal](img/B19845_01_10.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – After installing Ubuntu, you will have a fully functional Linux
    Terminal
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: You can close this Ubuntu Terminal as the Docker Desktop integration will require
    you to open a new one once it has been configured.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: You may need to execute some additional steps at [https://docs.microsoft.com/windows/wsl/wsl2-kernel](https://docs.microsoft.com/windows/wsl/wsl2-kernel)
    to update WSL2 if your operating system hasn’t been updated.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s continue with the Docker Desktop installation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the installer from [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/):'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Docker Desktop download section](img/B19845_01_11.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Docker Desktop download section
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Once downloaded, execute the `Docker Desktop Installer.exe` binary. You will
    be asked to choose between Hyper-V or WSL2 backend virtualization; we will choose
    WSL2:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.12 – Choosing the WSL2 integration for better performance](img/B19845_01_12.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – Choosing the WSL2 integration for better performance
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking **Ok**, the installation process will begin decompressing the
    required files (libraries, binaries, default configurations, and so on). This
    could take some time (1 to 3 minutes), depending on your host’s disk speed and
    compute resources:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.13 – The installation process will take a while as the application
    files are decompressed and installed on your system](img/B19845_01_13.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – The installation process will take a while as the application
    files are decompressed and installed on your system
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'To finish the installation, we will be asked to log out and log in again because
    our user was added to new system groups (Docker) to enable access to the remote
    Docker daemon via operating system pipes (similar to Unix sockets):'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Docker Desktop has been successfully installed and we must
    log out](img/B19845_01_14.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Docker Desktop has been successfully installed and we must log
    out
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Once we log in, we can execute Docker Desktop using the newly added application
    icon. We can enable Docker Desktop execution on start, which could be very useful,
    but it may slow down your computer if you are short on resources. I recommend
    starting Docker Desktop only when you are going to use it.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we’ve accepted the Docker Subscription license terms, Docker Desktop will
    start. This may take a minute:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Docker Desktop is starting](img/B19845_01_15.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Docker Desktop is starting
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: You can skip the quick guide that will appear when Docker Desktop is running
    because we will learn more about this in the following chapters as we deep dive
    into building container images and container execution.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'We will get the following screen, showing us that Docker Desktop is ready:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.16 – Docker Desktop main screen](img/B19845_01_16.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – Docker Desktop main screen
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to enable WSL2 integration with our favorite Linux distribution:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.17 – Enabling our previously installed Ubuntu using WSL2](img/B19845_01_17.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – Enabling our previously installed Ubuntu using WSL2
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'After this step, we are finally ready to work with Docker Desktop. Let’s open
    a terminal using our Ubuntu distribution, execute `docker`, and, after that, `docker
    info`:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.18 – Executing some Docker commands just to verify container runtime
    integration](img/B19845_01_18.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – Executing some Docker commands just to verify container runtime
    integration
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have a fully functional Docker client command line associated
    with the Docker Desktop WSL2 server.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'We will end this lab by executing an `docker run-ti alpine` to download the
    Alpine image and execute a container using it:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Creating a container and executing some commands inside before
    exiting](img/B19845_01_19.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – Creating a container and executing some commands inside before
    exiting
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'This container execution left changes in Docker Desktop; we can review the
    current images present in our container runtime:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Docker Desktop – the Images view](img/B19845_01_20.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Figure 1.20 – Docker Desktop – the Images view
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also review the container, which is already dead because we exited by
    simply executing `exit` inside its shell:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.21 – Docker Desktop – the Containers view](img/B19845_01_21.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: Figure 1.21 – Docker Desktop – the Containers view
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Now, Docker Desktop works and we are ready to work through the following labs
    using our WSL2 Ubuntu Linux distribution.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the basics around containers and how they fit into
    modern microservices applications. The content presented in this chapter has helped
    you understand how to implement containers in distributed architectures, using
    already-present host operating system isolation features and container runtimes,
    which are the pieces of software required for building, sharing, and executing
    containers.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Software containers assist application development by providing resilience,
    high availability, scalability, and portability thanks to their very nature, and
    will help you create and manage the application life cycle.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will deep dive into the process of creating container
    images.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
