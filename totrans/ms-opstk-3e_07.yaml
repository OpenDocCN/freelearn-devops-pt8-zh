- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Running a Highly Available Cloud – Meeting the SLA
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行高可用云 – 满足 SLA
- en: “The past resembles the future more than one drop of water resembles another.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “过去比未来更像，就像一滴水不像另一滴水一样。”
- en: – Ibn Khaldun
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 伊本·赫尔敦
- en: One major aspect of successful cloud operating experiences is to prevent downtime
    and the failures of cloud resources and workloads. In [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Consideration* , we drafted an initial preparatory
    design to enable OpenStack services for redundancy. [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane – Shared Services* , and [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity and Flavors* , looked at some of the logical
    design patterns for OpenStack control plane deployments and various ways to segregate
    compute, such as cells and availability zones. OpenStack is designed to scale
    massively and providing hardware for dedicated OpenStack services can help isolate
    failures, but this requires mechanisms to keep services running during incidents.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的云操作经验的一个主要方面是防止停机以及云资源和工作负载的故障。在[*第 1 章*](B21716_01.xhtml#_idTextAnchor014)，*重新审视
    OpenStack – 设计考虑*中，我们草拟了一个初步的准备设计，以支持 OpenStack 服务的冗余。[*第 3 章*](B21716_03.xhtml#_idTextAnchor108)，*OpenStack
    控制平面 – 共享服务*，和[*第 4 章*](B21716_04.xhtml#_idTextAnchor125)，*OpenStack 计算 – 计算能力与规格*，探讨了
    OpenStack 控制平面部署的逻辑设计模式以及分离计算的一些方式，例如单元和可用性区域。OpenStack 被设计为大规模扩展，并且为专用的 OpenStack
    服务提供硬件可以帮助隔离故障，但这需要机制来确保在事故发生时服务能够继续运行。
- en: Ensuring **high availability** ( **HA** ) in the OpenStack world does not differ
    too much from any other complex IT system. One obligatory practice is to find
    and eliminate, through the logical OpenStack setup, any possible **single points
    of failure** ( **SPOFs** ), which differ from one design to another. Our goal
    in this chapter is to achieve HA in each layer of the private cloud infrastructure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 世界中，确保**高可用性**（**HA**）与任何其他复杂的 IT 系统并没有太大区别。一项必不可少的做法是通过逻辑的 OpenStack
    配置，找出并消除任何可能的**单点故障**（**SPOFs**），这些故障点在不同设计中有所不同。本章的目标是在私有云基础设施的每一层实现 HA。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Reviewing HA and failover strategies to ensure business continuity
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾 HA 和故障切换策略，以确保业务连续性
- en: Iterating through OpenStack control plane HA design patterns
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反复探讨 OpenStack 控制平面 HA 设计模式
- en: Deploying an OpenStack environment with additional cloud controllers for fault
    tolerance and redundancy using **kolla-ansible**
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **kolla-ansible** 部署具有额外云控制器的 OpenStack 环境，以实现容错和冗余
- en: Exploring different ways to achieve networking HA in Neutron with the latest
    OpenStack updates, including routing and distributed virtual router mechanisms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索使用最新 OpenStack 更新实现 Neutron 网络 HA 的不同方式，包括路由和分布式虚拟路由器机制
- en: Uncovering native OpenStack solutions to ensure instances failover using the
    Masakari OpenStack project
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭示本地 OpenStack 解决方案，通过 Masakari OpenStack 项目确保实例故障切换
- en: Exploring HA strategies
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 HA 策略
- en: A robust OpenStack cloud platform includes fault tolerance at every level of
    its architecture. This can be very successful if it is planned in advance. Starting
    with a small cluster is easy and achievable, but growing it is a challenge. The
    hallmark of the basic OpenStack component is that it can run on commodity hardware.
    OpenStack is designed to scale massively and provide HA by leveraging more advanced
    HA techniques at each level of the infrastructure. This can include **automatic
    failover** and **geo-redundancy** . [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity and Flavors* , introduced the concepts
    of cells, regions, and availability zones, which provide more robust and advanced
    fault tolerance and availability capabilities for massive OpenStack deployment
    at scale.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个稳健的 OpenStack 云平台包括每个层级的容错能力。如果提前规划得当，这种设计可以非常成功。从小规模集群开始很容易并且可以实现，但扩展它是一个挑战。基本的
    OpenStack 组件特点是能够在普通硬件上运行。OpenStack 设计之初就是为了大规模扩展，并通过在基础设施的每个层次上运用更先进的 HA 技术来提供
    HA。这可能包括**自动故障切换**和**地理冗余**。[*第 4 章*](B21716_04.xhtml#_idTextAnchor125)，*OpenStack
    计算 – 计算能力与规格*，介绍了单元、区域和可用性区域的概念，这些概念为大规模 OpenStack 部署提供了更强大和先进的容错和可用性能力。
- en: Measuring HA
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量 HA
- en: 'Service availability should be measured and defined by standard metrics. This
    can be summarized using the following formula:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 服务可用性应该通过标准的度量来衡量和定义。这可以用以下公式来总结：
- en: '*Availability = MTTF/ (MTTF +* *MTTR)*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*可用性 = MTTF / (MTTF +* *MTTR)*'
- en: 'In the preceding equation, we can see the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的方程中，我们可以看到以下内容：
- en: '**Mean time to failure (MTTF)** : An estimate of the average time that a system
    is functional before its failure'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均故障时间（MTTF）**：估计系统在故障前的平均运行时间'
- en: '**Mean time to repair (MTTR)** : An estimate of the average time to repair
    a part or component of a system'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均修复时间（MTTR）**：估计修复系统部件或组件的平均时间'
- en: 'Measuring HA in complex environments such as OpenStack would require a good
    understanding of the deployed cloud environment capabilities that can be tracked
    through performance metrics and KPIs, such as response time, system uptime, and
    downtime, for the **Repair Time Objective** ( **RTO** ) and **Repair Point Objective**
    ( **RPO** ). What’s more critical to end users is to expose a **Service-Level
    Agreement** ( **SLA** ) from the gathered metrics and KPIs and improve against
    them. A SLA identifies areas of improvement based on routinely gathered metrics
    and will boost your business continuity strategy. Availability management is an
    integral pillar of IT best practices that cannot be skipped, especially when operating
    a cloud environment running dozens of services, as with OpenStack. Creating those
    SLAs for each service in more depth would fill an entire book. For the sake of
    simplicity, make sure you engage in availability management practices and update
    the SLA per service by assigning an availability level and availability and downtime
    percentages, as shown in the following table:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 OpenStack 这样的复杂环境中衡量高可用性，需要对部署的云环境能力有良好的理解，这些能力可以通过性能指标和关键绩效指标（KPI）进行跟踪，例如响应时间、系统正常运行时间和停机时间，用于**修复时间目标**（**RTO**）和**修复点目标**（**RPO**）。对于最终用户而言，更为关键的是从收集到的指标和
    KPI 中揭示**服务级别协议**（**SLA**），并基于此进行改进。SLA 根据常规收集的指标确定改进的领域，并将促进您的业务连续性策略。可用性管理是
    IT 最佳实践的核心支柱，特别是在运行包括 OpenStack 在内的多个服务的云环境中，不能被忽视。为每项服务创建这些 SLA 的详细内容将占据整本书。为了简化，确保您参与可用性管理实践，并通过为每项服务分配可用性级别以及可用性和停机百分比来更新
    SLA，如下表所示：
- en: '| **Service** | **Availability Level** | **Availability** | **Downtime/Day**
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **服务** | **可用性级别** | **可用性** | **停机时间/天** |'
- en: '| Compute | One 9 | 90 | ~ 2.4 hours |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 计算 | 一个 9 | 90 | ~ 2.4 小时 |'
- en: '| Network | Two 9s | 99 | ~ 14 minutes |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | 两个 9 | 99 | ~ 14 分钟 |'
- en: '| Compute | Three 9s | 99.9 | ~ 86 seconds |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 计算 | 三个 9 | 99.9 | ~ 86 秒 |'
- en: '| Block storage | Four 9s | 99.99 | ~ 8.6 seconds |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 块存储 | 四个 9 | 99.99 | ~ 8.6 秒 |'
- en: '| Object storage | Five 9s | 99.999 | ~ 0.86 seconds |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 对象存储 | 五个 9 | 99.999 | ~ 0.86 秒 |'
- en: '| Image | Six 9s | 99.9999 | ~ 0.0086 seconds |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 镜像 | 六个 9 | 99.9999 | ~ 0.0086 秒 |'
- en: Table 7.1 – Example SLA with x-9s for OpenStack environment services
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1 – OpenStack 环境服务的 x-9s 示例 SLA
- en: 'When designing the architecture for an HA OpenStack setup, failures should
    be planned for at every single layer of the cloud architecture. This can be achieved
    thanks to advanced HA models and techniques. The latest OpenStack releases are
    even richer, with built-in features that embrace availability not only for core
    components but also for user workloads. For example, if a host fails, the application
    running on it will not be accessible anymore. Nova supports the feature to recover
    a guest instance by relocating it to a new healthy host. For an extended OpenStack
    setup, the cloud environment could scale even more domain failures by recalling
    availability zones under the Nova service. The essence of the HA design patterns
    can be summarized in the following points:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计 HA OpenStack 设置的架构时，应为云架构的每一层都规划好故障应对。这可以通过先进的 HA 模型和技术来实现。最新的 OpenStack
    版本甚至更加丰富，内置了不仅为核心组件，还为用户工作负载提供高可用性的功能。例如，如果一台主机故障，运行在其上的应用将无法再访问。Nova 支持通过将来宾实例迁移到新的健康主机来恢复故障实例。对于扩展的
    OpenStack 设置，云环境可以通过重新调用 Nova 服务下的可用区来扩展更多的域故障。HA 设计模式的核心可以总结为以下几点：
- en: Eliminate any SPOF for the control and data planes
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除控制平面和数据平面中的任何单点故障（SPOF）
- en: Adopt a geo-replicated design whenever possible
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下采用地理复制设计
- en: Automate monitoring and anomaly detection
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化监控和异常检测
- en: Plan and automate fast disaster recovery
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划并自动化快速灾难恢复
- en: Decouple and isolate OpenStack components as much as possible
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能地解耦并隔离 OpenStack 组件
- en: 'In the world of OpenStack, different levels of HA can be identified:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 的世界中，可以识别不同级别的高可用性（HA）：
- en: '**L1** : This includes physical hosts, network and storage devices, and hypervisors.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1**：这包括物理主机、网络和存储设备以及虚拟机管理程序（Hypervisors）。'
- en: '**L2** : This includes OpenStack services, including compute, network, and
    storage controllers, as well as databases and message queuing systems.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2**：这包括 OpenStack 服务，包括计算、网络和存储控制器，以及数据库和消息队列系统。'
- en: '**L3** : This includes the virtual machines running on hosts that are managed
    by OpenStack services.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L3**：这包括在主机上运行并由 OpenStack 服务管理的虚拟机。'
- en: '**L4** : This includes applications running in the virtual machines themselves.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L4**：这包括在虚拟机中运行的应用程序。'
- en: The main focus of supporting HA in OpenStack is on L1, L2, and L3.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 支持 OpenStack 高可用性（HA）的主要关注点是 L1、L2 和 L3。
- en: Designing for HA
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计高可用性（HA）
- en: 'One key aspect when designing systems is to take into account every possibility
    where an element of the system could fail. Each element has limits of some kind
    and would not be able to recover within at least a short period, which would affect
    other parts of the system and lead to the whole system becoming unresponsive.
    When looking at common design patterns aimed at maximizing scalability and availability,
    it is important to identify two main classes of services:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 设计系统时的一个关键方面是考虑系统中每个元素可能发生故障的所有情况。每个元素都有一定的限制，在至少短时间内无法恢复，这会影响系统的其他部分，导致整个系统变得无响应。在考察旨在最大化可扩展性和可用性的常见设计模式时，重要的是要识别出两类主要服务：
- en: '**Stateful service** : A service that depends on data from a previous request
    and interacts synchronously to preserve consistency. As such services rely on
    the state, a service failure can affect the whole system and require more backup
    and recovery mechanisms to maintain state information.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态服务**：一种依赖于先前请求数据并进行同步交互以保持一致性的服务。由于这类服务依赖于状态，因此服务故障可能会影响整个系统，并且需要更多的备份和恢复机制来维持状态信息。'
- en: '**Stateless service** : A service that does not require data or needs to save
    information (state) from a previous request or event. As such services do not
    store states across requests, a sudden failure won’t affect the rest of the system,
    which can operate in a different handler instance.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无状态服务**：一种不需要从先前请求或事件中保存数据或信息（状态）的服务。由于这类服务不在请求之间存储状态，因此突发故障不会影响系统的其余部分，系统可以在不同的处理器实例中继续运行。'
- en: 'As discussed in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack
    Control Plane – Shared Services* , the OpenStack control plane is mainly constructed
    of stateless services, including an API, scheduler, agents, and conductor components
    for all compute, network, image, monitoring, and storage services. The database
    and queuing message count as stateful services. This way, introducing HA into
    our initial setup would require verifying the right pattern for each service and
    component of the OpenStack deployment to ensure the continuity of the service
    during unexpected failures. There are a multitude of ways to achieve OpenStack
    control plane HA. This can be summarized in terms of the following design patterns,
    both of which were introduced in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane –* *Shared Services* :'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [*第3章*](B21716_03.xhtml#_idTextAnchor108) 中讨论的，*OpenStack 控制平面 – 共享服务*，OpenStack
    控制平面主要由无状态服务构成，包括 API、调度器、代理和处理器组件，涵盖所有计算、网络、镜像、监控和存储服务。数据库和消息队列系统属于有状态服务。这样，将
    HA 引入我们初步的设置中，就需要验证每个服务和组件的正确模式，以确保在突发故障期间服务的连续性。有多种方式可以实现 OpenStack 控制平面的 HA，这些方式可以通过以下设计模式来总结，这些模式都在
    [*第3章*](B21716_03.xhtml#_idTextAnchor108) 中介绍过，*OpenStack 控制平面 –* *共享服务*：
- en: '**Active/passive** : In the OpenStack control plane, failed services will be
    restarted on a second cloud controller node. With stateful services such as the
    database, the master node handles all read and write operations and a second node
    acts as a listener until a failover occurs, upon which the data entry point will
    be switched to the second node. This pattern is suitable for some stateful services
    but not for other stateless OpenStack services. Additionally, it does not fully
    comply with horizontal scaling in OpenStack.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动/被动**：在 OpenStack 控制平面中，故障服务会在第二个云控制器节点上重新启动。对于像数据库这样的有状态服务，主节点处理所有的读写操作，第二个节点充当监听者，直到发生故障转移，此时数据入口点会切换到第二个节点。此模式适用于一些有状态服务，但不适用于其他无状态的
    OpenStack 服务。此外，它不完全符合 OpenStack 中的横向扩展要求。'
- en: '**Active/active** : Here, the request load hitting the OpenStack control plane
    is distributed among active nodes that process in parallel. This mode in an OpenStack
    deployment brings the highest level of fault tolerance for the control and data
    plane services. In the event of one cloud controller failure, the rest of the
    load will be distributed to the second operational node, maintaining uninterrupted
    services. It is also suited to horizontal scaling as it adds a new node to accommodate
    any increased load without compromising the function of the cluster. As OpenStack
    core components are based on API calls and RPC to handle messages via the queue
    message system, performance is paramount. In active/active mode, service recovery
    has a shorter MTTR than active/passive mode, where a potentially long delay to
    failover can occur, causing some OpenStack services to time out.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动/主动**：在这种模式下，进入 OpenStack 控制平面的请求负载被分配到多个活动节点，这些节点并行处理。此模式在 OpenStack 部署中为控制平面和数据平面服务提供了最高级别的容错性。如果某个云控制器发生故障，剩余的负载将分配给第二个运行中的节点，从而保持服务不中断。该模式也适用于横向扩展，因为它可以添加新节点以适应增加的负载，而不影响集群的功能。由于
    OpenStack 核心组件基于 API 调用和 RPC 通过队列消息系统处理消息，因此性能至关重要。在主动/主动模式下，服务恢复的平均修复时间（MTTR）比主动/被动模式短，后者可能出现较长的故障转移延迟，导致某些
    OpenStack 服务超时。'
- en: Most of the OpenStack reference architectures adopt an active/active ground
    setup for fault tolerance and failover, mainly due to the nature of the OpenStack
    services that can scale horizontally easily and do not require additional mechanisms.
    On the other hand, some services can be configured in active/passive mode, depending
    on the nature of the service. In the next section, we will look at the different
    services that can be used to enable HA.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 OpenStack 参考架构采用主动/主动的基础架构设置，以实现容错和故障转移，主要是由于 OpenStack 服务的特性，这些服务可以轻松地横向扩展，并且不需要额外的机制。另一方面，某些服务可以根据服务的性质配置为主动/被动模式。在下一节中，我们将探讨可用于启用高可用性的不同服务。
- en: Preparing for HA
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为高可用性做好准备
- en: Each OpenStack control plane layer requires separate analysis to identify the
    best approaches to ensure its availability. Additional technical considerations
    will be taken into account when designing for fault tolerance in a production
    setup. It is important to note that a major factor in the different choices comes
    from the given cloud service provider’s experience with tooling or hardware solutions.
    In this section, we will go through some of the major adopted tools and design
    patterns to achieve a highly available and scalable OpenStack control plane.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 OpenStack 控制平面层都需要单独分析，以确定确保其可用性的最佳方法。在为生产环境设计容错时，还需要考虑其他技术因素。需要注意的是，不同选择的一个主要因素来自于给定云服务提供商在工具或硬件解决方案方面的经验。在本节中，我们将介绍一些主要采用的工具和设计模式，以实现高度可用且可扩展的
    OpenStack 控制平面。
- en: Designing with load balancing
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡设计
- en: Load balancing solution services can be found everywhere and can be used to
    efficiently distribute and serve incoming requests across a server pool. HAProxy
    has been widely adopted on dozens of large OpenStack production deployments.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡解决方案服务无处不在，可以高效地分配并服务来自服务器池的传入请求。HAProxy 已被广泛采用，在数十个大型 OpenStack 生产部署中得到了应用。
- en: 'The HAProxy setup involves the following two types of servers:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 设置涉及以下两种类型的服务器：
- en: '**Frontend server** : This server listens for requests coming from a specific
    IP and port, and determines where the connection or request should be forwarded'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前端服务器**：此服务器监听来自特定 IP 和端口的请求，并决定将连接或请求转发到何处。'
- en: '**Backend server** : A pool of servers in the cluster receiving the forwarded
    requests'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端服务器**：集群中接收转发请求的服务器池'
- en: 'It is also important to note the function layers that are involved in HAProxy
    load balancing:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同时也需要注意在 HAProxy 负载均衡中涉及的功能层：
- en: '**Layer 4** : Load balancing is performed in the transport layer in the OSI
    model. All the user traffic will be forwarded based on a specific IP address and
    port to the backend servers. For example, a load balancer might forward the internal
    OpenStack system’s request to the Horizon web backend group of backend servers.
    To do this, whichever backend Horizon is selected should respond to the request
    under scope. This is true in the case of all the servers in the web backend serving
    identical content.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第4层**：负载均衡在OSI模型的传输层执行。所有用户流量将根据特定的IP地址和端口转发到后台服务器。例如，负载均衡器可能会将内部OpenStack系统的请求转发到Horizon
    Web后台服务器组。为此，无论选择哪个后台Horizon服务器，都应该在其范围内响应请求。在所有Web后台服务器提供相同内容的情况下，这一规则适用。'
- en: '**Layer 7** : The application layer will be used for load balancing. This is
    a good way to load balance network traffic. Simply put, this mode allows you to
    forward requests to different backend servers based on the content of the request
    itself.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第7层**：将使用应用层进行负载均衡。这是一种非常有效的网络流量负载均衡方法。简而言之，这种模式允许你根据请求内容将请求转发到不同的后台服务器。'
- en: 'HAProxy supports several load balancing algorithms to dispatch a request to
    a server in the backend pool:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy支持多种负载均衡算法，以将请求调度到后台池中的服务器：
- en: '**Round robin** : Each server is exploited in turn. As a simple HAProxy setup,
    round robin is a dynamic algorithm that defines the server’s weight and adjusts
    it on the fly when the called instance hangs or starts slowly.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轮询**：每个服务器依次使用。作为一种简单的HAProxy配置，轮询是一种动态算法，它定义了服务器的权重，并在被调用的实例挂起或启动较慢时动态调整该权重。'
- en: '**Leastconn** : The selection of the server is based on the node that has the
    lowest number of connections.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最少连接数**：选择服务器基于拥有最低连接数的节点。'
- en: '**Source** : This algorithm ensures that the request will be forwarded to the
    same server based on a hash of the source IP, so long as the server is still up.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源**：该算法确保基于源IP的哈希值将请求转发到相同的服务器，只要服务器仍然在线。'
- en: '**Uniform resource identifier** ( **URI** ): This ensures that the request
    will be forwarded to the same server based on its URI. It is ideal to increase
    the cache hit rate in the case of proxy cache implementations.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一资源标识符**（**URI**）：这确保请求会基于URI转发到相同的服务器。它在代理缓存实现的情况下，理想用于提高缓存命中率。'
- en: HAProxy keeps an eye on the nodes’ backend availability by running health checks
    on particular IP addresses and ports. It disables any backend node that fails
    the health checks and discards it from the backend pool until it is healthy enough
    to start serving requests again.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy通过对特定IP地址和端口执行健康检查，监控节点的后台可用性。它会禁用任何未通过健康检查的后台节点，并将其从后台池中移除，直到它恢复健康，能够重新处理请求。
- en: Armed with a load balancer, an OpenStack service that is deployed in two or
    more nodes will be exposed by a **Virtual IP** ( **VIP** ). In active/active mode,
    the VIP is managed by the load balancer, which makes sure that a node has sufficient
    availability before it forwards the request.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 配备负载均衡器后，部署在两个或更多节点上的OpenStack服务将通过**虚拟IP**（**VIP**）暴露。在活动/活动模式下，VIP由负载均衡器管理，负载均衡器会确保在转发请求之前，节点的可用性足够。
- en: Important note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: If you consider separating the HAProxy deployment into its own physical setup,
    make sure that the VIPs can be reached over the public network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果考虑将HAProxy部署分离到独立的物理环境中，请确保VIP可以通过公共网络访问。
- en: 'As the use of VIPs combined with HAProxy adds an extra layer to protect OpenStack
    services from failures, the load balancing layer should not present a single point
    of failure. Depending on which software or hardware-based load balancing solution
    you adopt, make sure you increase its redundancy level. That should be reviewed
    as a critical networking setup as it defines the first interface of the OpenStack
    environment. This can be achieved by using a VIP software management tool such
    as **Keepalived** or **Pacemaker** to ensure a highly available load balancing
    layer. Keepalived is free software and employs the **Virtual Router Redundancy
    Protocol** ( **VRRP** ) to eliminate SPOFs by making IPs highly available. As
    shown in the following diagram, VRRP implements virtual routing to perform failover
    tasks between two or more servers in a static, default-routed environment:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将 VIP 与 HAProxy 结合使用增加了保护 OpenStack 服务免受故障的额外层，因此负载均衡层不应出现单点故障。根据您采用的软件或硬件负载均衡解决方案，确保增加其冗余级别。这应作为一个关键的网络设置进行审查，因为它定义了
    OpenStack 环境的第一个接口。可以通过使用诸如 **Keepalived** 或 **Pacemaker** 之类的 VIP 软件管理工具来实现这一目标，从而确保负载均衡层的高可用性。Keepalived
    是一款免费软件，使用 **虚拟路由冗余协议**（**VRRP**）通过使 IP 高可用来消除单点故障（SPOF）。如下图所示，VRRP 实现了虚拟路由，在静态的默认路由环境中执行两个或多个服务器之间的故障转移任务：
- en: '![Figure 7.1 – Load balancing and failover using HAProxy and Keepalived](img/B21716_07_01.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 使用 HAProxy 和 Keepalived 进行负载均衡和故障转移](img/B21716_07_01.jpg)'
- en: Figure 7.1 – Load balancing and failover using HAProxy and Keepalived
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 使用 HAProxy 和 Keepalived 进行负载均衡和故障转移
- en: Important note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: To ensure an easily determined quorum by Keepalived, we will empower our control
    plane with HA via three cloud controller nodes managed by Keepalived.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保 Keepalived 能轻松判断仲裁，我们将通过由 Keepalived 管理的三个云控制节点为我们的控制平面提供高可用性（HA）。
- en: HA for the database
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库的高可用性
- en: Databases have always been a critical subject when it comes to dealing with
    the growth of data being stored, along with performance and resiliency. This is
    because a database is not a simple service and does not scale as fast as a simple
    API. Any request that reaches an OpenStack API service results in the database’s
    size growing incrementally. In our deployment process, using a CI/CD pipeline
    will generate a few additional entries across several tables in each run. If not
    designed to scale and monitored closely, the database could be subject to failure
    and quickly become a bottleneck. Several open source and database vendors provide
    possible topologies to scale horizontally or vertically or both. The other factor
    is the type of database engine supported by OpenStack and the required experience
    of the cloud operation team to build and architect a highly available and scalable
    database solution in OpenStack.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库一直是处理数据存储增长、性能和弹性时的关键课题。这是因为数据库并不是一个简单的服务，且其扩展速度远不如简单的 API。任何到达 OpenStack
    API 服务的请求都会导致数据库的大小逐渐增加。在我们的部署过程中，使用 CI/CD 流水线的每次运行都会在多个表中生成一些额外的条目。如果没有设计成可扩展且进行紧密监控，数据库可能会面临故障，迅速成为瓶颈。多个开源和数据库供应商提供了可能的拓扑结构，以实现水平或垂直扩展，或两者兼具。另一个因素是
    OpenStack 支持的数据库引擎类型，以及云运维团队在 OpenStack 中构建和架构高可用、可扩展的数据库解决方案所需的经验。
- en: 'Since we’ve started our initial production draft with a single database based
    on MySQL, we can highlight the most commonly clustering topologies:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经开始使用基于 MySQL 的单一数据库的初始生产草案，我们可以突出最常见的集群拓扑：
- en: '**Master/slave replication** : A VIP will be switched to a slave node when
    the master node fails. A delay in the health check on the master node at failover
    time and thus a delay in assigning the VIP to the slave node could potentially
    result in data inconsistencies, as shown here:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主从复制**：当主节点失败时，VIP 将切换到从节点。在故障切换时，主节点健康检查的延迟以及将 VIP 分配给从节点的延迟，可能导致数据不一致，如下所示：'
- en: '![Figure 7.2 – Database master-slave replication](img/B21716_07_02.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 数据库主从复制](img/B21716_07_02.jpg)'
- en: Figure 7.2 – Database master-slave replication
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 数据库主从复制
- en: '**Multi-master replication manager** ( **MMM** ): By setting up two servers,
    both of them become masters by keeping only one acceptable write query at a given
    time. This is still not a very reliable solution for OpenStack database HA because
    in the event of failure of the master, it might lose a certain number of transactions,
    as illustrated here:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多主复制管理器** (**MMM**) : 通过设置两个服务器，它们都会成为主服务器，只在给定时间保持一个可接受的写入查询。这仍然不是一个非常可靠的
    OpenStack 数据库高可用性（HA）解决方案，因为在主服务器发生故障时，可能会丢失一定数量的事务，如下所示：'
- en: '![Figure 7.3 – Database MMM replication](img/B21716_07_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 数据库 MMM 复制](img/B21716_07_03.jpg)'
- en: Figure 7.3 – Database MMM replication
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 数据库 MMM 复制
- en: '**MySQL shared storage** : In this topology, both servers depend on redundant
    shared storage. As shown in the following figure, a separation is required between
    the servers processing the data and the storage devices. Note that an active node
    may exist at any point in time. If it fails, the other node will take over the
    VIP after checking the inactivity of the failed node, and turn it off. Such a
    solution is excellent in terms of uptime but may require a powerful storage/hardware
    system, which can be extremely expensive. The service will be resumed on a different
    node by mounting the shared storage within the taken VIP:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MySQL 共享存储** : 在此拓扑结构中，两个服务器依赖于冗余的共享存储。如下面的图所示，数据处理的服务器和存储设备之间需要分离。请注意，任何时候都可能存在一个活动节点。如果它失败，另一节点会在检查到故障节点的不活动后接管
    VIP，并将其关闭。此解决方案在正常运行时间方面非常优秀，但可能需要强大的存储/硬件系统，这可能非常昂贵。服务将在另一个节点上恢复，通过挂载共享存储到已接管的
    VIP：'
- en: '![Figure 7.4 – Database MySQL with shared storage](img/B21716_07_04.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 使用共享存储的数据库 MySQL](img/B21716_07_04.jpg)'
- en: Figure 7.4 – Database MySQL with shared storage
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 使用共享存储的数据库 MySQL
- en: '**Block-level replication** : One of the most adopted HA implementations is
    the **Distributed Replicated Block Device** ( **DRBD** ) replication. Simply put,
    it replicates data in the block device, which is the physical hard drive that’s
    shared between OpenStack MySQL nodes. DRBD can be a costless solution, but performance-wise,
    it is not sufficient when you’re relying on hundreds of nodes. It can also affect
    the scalability of the replicated cluster:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块级复制** : 最常采用的高可用性实现之一是 **分布式复制块设备** (**DRBD**) 复制。简而言之，它在块设备中复制数据，块设备即在
    OpenStack MySQL 节点之间共享的物理硬盘。DRBD 可以是一个无成本的解决方案，但从性能上讲，当依赖于数百个节点时，它并不足够。它也可能影响复制集群的可扩展性：'
- en: '![Figure 7.5 – Database block-level replication](img/B21716_07_05.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 数据库块级复制](img/B21716_07_05.jpg)'
- en: Figure 7.5 – Database block-level replication
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 数据库块级复制
- en: '**MySQL multi-master replication with Galera** : Based on **multi-master replication**
    , the **Galera** solution has a few performance challenges within an MMM architecture
    for MySQL/InnoDB database clusters. A requirement for the Galera setup to run
    properly is the presence of at least three nodes. As shown in the following diagram,
    synchronous replication is managed by Galera, where data is replicated across
    the whole cluster:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MySQL 多主复制与 Galera** : 基于 **多主复制**，**Galera** 方案在 MySQL/InnoDB 数据库集群的 MMM
    架构中存在一些性能挑战。Galera 配置正常运行的要求是至少有三个节点。如下面的图所示，同步复制由 Galera 管理，数据在整个集群中进行复制：'
- en: '![Figure 7.6 – Database multi-master with Galera replication](img/B21716_07_06.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 使用 Galera 复制的数据库多主](img/B21716_07_06.jpg)'
- en: Figure 7.6 – Database multi-master with Galera replication
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 使用 Galera 复制的数据库多主
- en: Regarding these topologies, any MySQL replication setup can be simple to set
    up and make HA-capable, but data can be lost during the failover process. MySQL
    multi-master replication with Galera is tightly designed to resolve such a conflict
    in the multi-master database environment. An issue you may face in a typical multi-master
    setup is that all the nodes try to update the same database with different data,
    especially when a synchronization problem occurs during the master failure. This
    is why Galera uses **certification-based replication** ( **CBR** ). The main mechanism
    of CBR is to assume that the database can roll back uncommitted changes and is
    transactional, in addition to applying replicated events in the same order across
    all the instances. Replication is truly parallel; each one has an ID check. The
    added value that Galera can bring to our MySQL (MariaDB in OpenStack) HA is the
    ease of scalability, such as joining a node to Galera in an automated fashion
    in a production environment. The end design brings an active/active multi-master
    topology with minimum latency and transaction losses. A best practice to ensure
    data consistency when implementing Galera for MariaDB in OpenStack is to keep
    writes committed to only one node of the three.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些拓扑，任何 MySQL 复制设置都可以简单地设置并使其具有高可用性，但在故障切换过程中可能会丢失数据。Galera 提供的 MySQL 多主复制系统被精心设计来解决这种多主数据库环境中的冲突。你在典型的多主设置中可能会遇到的问题是，所有节点都试图用不同的数据更新相同的数据库，尤其是在主节点故障期间发生同步问题时。这就是为什么
    Galera 使用**基于认证的复制**（**CBR**）。CBR 的主要机制是假设数据库可以回滚未提交的更改，并且是事务性的，此外，所有实例上的复制事件将以相同的顺序应用。复制是真正的并行化；每个都需要进行
    ID 检查。Galera 给我们的 MySQL（OpenStack 中的 MariaDB）高可用性带来的附加值是可扩展性，诸如在生产环境中自动将节点加入 Galera。最终的设计带来了最小延迟和事务丢失的主动/主动多主拓扑。在
    OpenStack 中为 MariaDB 实现 Galera 时，确保数据一致性的最佳实践是将写操作仅提交到三台节点中的一个。
- en: HA for the message bus
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消息总线的高可用性
- en: 'RabbitMQ is mainly responsible for communication between different OpenStack
    services. The issue is fairly simple: no queue, no OpenStack service intercommunication.
    RabbitMQ should be considered another critical service that needs to be available
    and survive failures.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ 主要负责不同 OpenStack 服务之间的通信。问题很简单：没有队列，就没有 OpenStack 服务的相互通信。RabbitMQ
    应被视为另一个关键服务，需要保持可用并能够承受故障。
- en: Important note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: A variety of queuing messages systems such as Qpid or ZeroMQ are mature enough
    to support their own cluster setup without the need for you to run other resource
    managers or clustering software solutions alongside them.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 各种队列消息系统，如 Qpid 或 ZeroMQ，已经足够成熟，可以支持自身的集群设置，无需与其他资源管理器或集群软件一起运行。
- en: 'RabbitMQ is a robust messaging system that can achieve scalability in an active/active
    way through one of the following patterns:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ 是一个强大的消息系统，可以通过以下模式之一以主动/主动方式实现可扩展性：
- en: '**Clustering** : Any data or state needed for the RabbitMQ broker to be operational
    is replicated across all nodes.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群**：RabbitMQ 经纪人所需的任何数据或状态都会在所有节点之间复制。'
- en: '**Mirrored queues** : As the message queue cannot survive in the nodes in which
    it resides, RabbitMQ can act in active/active HA message queues. Simply put, queues
    will be mirrored on other nodes within the same RabbitMQ cluster. Thus, any node
    failure will lead to automatically switching to one of the queue mirrors.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**镜像队列**：由于消息队列无法在其所在的节点中生存，因此 RabbitMQ 可以作为主动/主动高可用消息队列。简而言之，队列会在同一 RabbitMQ
    集群中的其他节点上进行镜像。因此，任何节点故障都会自动切换到其中一个队列镜像。'
- en: '**Quorum queues** : This is a modern version of queues using a variant of the
    **Raft** protocol (enabling members of a distributed system to agree on a set
    of values and share data in the event of failure) with a distributed consensus
    algorithm and implementing replicated FIFO. Each quorum queue has a leader and
    multiple followers with replicated queues hosted in different hosts.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法定队列**：这是一种现代版的队列，使用**Raft**协议的变体（使分布式系统的成员能够在发生故障时就一组值达成一致并共享数据），通过分布式共识算法并实现复制的
    FIFO。每个法定队列都有一个领导者和多个追随者，复制队列托管在不同的主机上。'
- en: 'RabbitMQ has deprecated the implementation of mirrored queues in favor of quorum
    queues due to the problems of its predecessor that it solves. This includes synchronization
    failing and performance issues. With the huge message bus traffic in OpenStack,
    quorum queues can boost not just availability but also the consistency of the
    messages, as shown in the following diagram:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于镜像队列存在的同步失败和性能问题，RabbitMQ 已弃用镜像队列的实现，转而采用 quorum 队列。这解决了前者的一些问题。在 OpenStack
    中，随着大量消息总线流量的传输，quorum 队列不仅能够提高可用性，还能增强消息的一致性，如下图所示：
- en: '![Figure 7.7 – RabbitMQ broker quorum pattern](img/B21716_07_07.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – RabbitMQ 经纪人 quorum 模式](img/B21716_07_07.jpg)'
- en: Figure 7.7 – RabbitMQ broker quorum pattern
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – RabbitMQ 经纪人 quorum 模式
- en: In the next section, we will extend our deployment by introducing the aforementioned
    elements to enable HA and redundancy in the OpenStack environment.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过引入上述元素，扩展我们的部署，启用 OpenStack 环境中的高可用性（HA）和冗余。
- en: Deploying for HA
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署高可用性（HA）
- en: 'In this section, we will extend our initial production deployment by extending
    our OpenStack control plane with a highly available configuration composed of
    the following set of nodes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将通过扩展我们的 OpenStack 控制平面，采用由以下节点组成的高可用配置，来扩展最初的生产部署：
- en: '**Virtual** **IP** : **10.0.0.47**'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟** **IP**：**10.0.0.47**'
- en: '**HAProxy 01** ( **hap1.os.packtpub** ): **10.0.0.20**'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HAProxy 01**（**hap1.os.packtpub**）：**10.0.0.20**'
- en: '**HAProxy 02** ( **hap2.os.packtpub** ): **10.0.0.21**'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HAProxy 02**（**hap2.os.packtpub**）：**10.0.0.21**'
- en: '**Cloud Controller 01** ( **cc01.os.packtpub** ): **10.0.0.100**'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloud Controller 01**（**cc01.os.packtpub**）：**10.0.0.100**'
- en: '**Cloud Controller 02** ( **cc02.os.packtpub** ): **10.0.0.101**'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloud Controller 02**（**cc02.os.packtpub**）：**10.0.0.101**'
- en: '**Cloud Controller 03** ( **cc03.os.packtpub** ): **10.0.0.102**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloud Controller 03**（**cc03.os.packtpub**）：**10.0.0.102**'
- en: 'The HA version of our OpenStack environment will require that we apply the
    following configurations in the **globals.yml** file:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 OpenStack 环境的 HA 版本要求我们在 **globals.yml** 文件中应用以下配置：
- en: 'Enable HAProxy, which will use Keepalived by default for the HA settings:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用 HAProxy，默认情况下将使用 Keepalived 来设置 HA：
- en: '[PRE0]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Assign a VIP that is not used for the management network where the HAProxy
    hosts are connected and Keepalived is running. Optionally, the external and internal
    VIPs can be separated. The following setting will use the same internal VIP address:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 VIP 分配一个不用于管理网络的地址，HAProxy 主机连接并运行 Keepalived 的网络可以使用此地址。可选地，可以将外部和内部 VIP
    分开。以下设置将使用相同的内部 VIP 地址：
- en: '[PRE1]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The RabbitMQ quorum is the default implementation for message-queue HA in OpenStack.
    Some older versions in **kolla-ansible** use mirrored queues referenced with the
    **om_enable_rabbitmq_high_availability** setting. Make sure this is disabled and
    uses quorum queues instead by checking the **ansible/group_vars/all.yml** file
    or adding the following variable set if it does not exist in the **globals.yml**
    file:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RabbitMQ quorum 是 OpenStack 中消息队列高可用性的默认实现。在某些旧版本的 **kolla-ansible** 中，使用镜像队列，并通过
    **om_enable_rabbitmq_high_availability** 设置引用它们。请确保禁用此设置，改用 quorum 队列，可以通过检查 **ansible/group_vars/all.yml**
    文件来确认，或者如果在 **globals.yml** 文件中不存在该变量集，则添加以下变量集：
- en: '[PRE2]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Important note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Populating the quorum queues in a running environment requires manually restarting
    all OpenStack services using the **kolla-ansible stop --tags <service-tags>**
    and **kolla-ansible deploy --tags <service-tags>** command lines, where **<service-tags>**
    is the name of a given OpenStack service. It is recommended to automate the service
    restart using the pipeline for consistent configuration and durable queues.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行环境中填充 quorum 队列需要使用 **kolla-ansible stop --tags <service-tags>** 和 **kolla-ansible
    deploy --tags <service-tags>** 命令手动重启所有 OpenStack 服务，其中 **<service-tags>** 是指定
    OpenStack 服务的名称。建议通过管道自动化服务重启，以保证配置的一致性和队列的持久性。
- en: 'The next configuration update is to adjust the **multi_packtpub_prod** file.
    The following layout suggests the deployment of three cloud controller nodes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个配置更新是调整 **multi_packtpub_prod** 文件。以下布局建议部署三个云控制器节点：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A new host group of two load balancers will be added, running HAProxy and Keepalived:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将添加一个新的主机组，包含两台负载均衡器，分别运行 HAProxy 和 Keepalived：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As stated in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control
    Plane – Shared Services* , one of the best practices regarding production environments
    is to start hosting workloads in the cloud environment only when it is configured
    with HA (at a minimum) within its core services. As no production workload has
    been run yet, it is recommended to clean up the running environment by firing
    off the following command line:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第 3 章*](B21716_03.xhtml#_idTextAnchor108)所述，*OpenStack 控制平面 – 共享服务*，关于生产环境的最佳实践之一是，只有在核心服务至少配置了
    HA（高可用性）后，才开始在云环境中托管工作负载。由于尚未运行生产工作负载，因此建议通过执行以下命令行来清理运行环境：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will clean up the OpenStack service containers and associated volumes.
    The new pipeline run will deploy all containers from the same image.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将清理 OpenStack 服务容器及相关卷。新的流水线运行将从相同的镜像部署所有容器。
- en: The database deployment for Galera InnoDB will run the **wsrep** service across
    the three controller nodes. One common issue when deploying additional nodes in
    a running environment is the failure of one or both nodes to read binary logs
    and update the replication status. RabbitMQ quorum also requires additional manual
    tweaks to clean the existing exchanges and move to durable queues while using
    quorum across different cloud controller nodes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Galera InnoDB 的数据库部署将在三个控制节点上运行**wsrep**服务。在运行环境中部署额外节点时，常见问题之一是一个或两个节点无法读取二进制日志并更新复制状态。RabbitMQ
    仲裁也需要额外的手动调整，以清理现有的交换并在不同的云控制器节点之间使用仲裁时迁移到持久队列。
- en: Commit the changes before running the pipeline. Rolling out the new multi-node
    infrastructure will take longer than the very first run as the new cloud controller
    nodes and load balancers will be deployed in addition to multi-master database
    quorum queues deployment across different nodes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行流水线之前，先提交更改。由于将同时部署新的云控制器节点和负载均衡器，并在不同节点上部署多主数据库仲裁队列，滚动更新新的多节点基础设施将比第一次运行花费更多时间。
- en: 'Once the pipeline finishes the multi-node deployment, observe the Docker images
    that have been loaded for HAProxy and Keepalived by running the following command
    line in any of the controller nodes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦流水线完成多节点部署，在任何控制节点上运行以下命令行，观察为 HAProxy 和 Keepalived 加载的 Docker 镜像：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will get this output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 7.8 – Listing the HAProxy and Keepalived Kolla images](img/B21716_07_08.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 列出 HAProxy 和 Keepalived Kolla 镜像](img/B21716_07_08.jpg)'
- en: Figure 7.8 – Listing the HAProxy and Keepalived Kolla images
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 列出 HAProxy 和 Keepalived Kolla 镜像
- en: 'In addition to the different containers for OpenStack services, observe the
    container running HAProxy and Keepalived:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 OpenStack 服务的不同容器外，还应注意运行 HAProxy 和 Keepalived 的容器：
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出内容：
- en: '![Figure 7.9 – Listing the HAProxy and Keepalived Kolla containers](img/B21716_07_09.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 列出 HAProxy 和 Keepalived Kolla 容器](img/B21716_07_09.jpg)'
- en: Figure 7.9 – Listing the HAProxy and Keepalived Kolla containers
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 列出 HAProxy 和 Keepalived Kolla 容器
- en: 'Optionally, verify that all compute nodes can be listed as part of the OpenStack
    environment and check the service status, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，验证所有计算节点是否可以作为 OpenStack 环境的一部分列出，并检查服务状态，方法如下：
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.10 – Listing the enabled Nova services in all OpenStack environments](img/B21716_07_10.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 列出所有 OpenStack 环境中启用的 Nova 服务](img/B21716_07_10.jpg)'
- en: Figure 7.10 – Listing the enabled Nova services in all OpenStack environments
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 列出所有 OpenStack 环境中启用的 Nova 服务
- en: 'Each HAProxy instance that’s deployed in each cloud controller node is assigned
    a priority ID that’s used by Keepalived to refer to the elected master node. The
    generated file in each controller node can be found in the **/etc/kolla/keepalived/keepalived.conf**
    file. The following is a snippet of the Keepalived configuration that was generated
    on one of the cloud controller nodes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 每个在云控制器节点上部署的 HAProxy 实例都分配了一个优先级 ID，Keepalived 使用这个 ID 来引用选举出的主节点。每个控制节点中生成的文件可以在**/etc/kolla/keepalived/keepalived.conf**文件中找到。以下是其中一个云控制器节点上生成的
    Keepalived 配置片段：
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The **kolla_internal_vip_51** configuration block defines a random unique ID
    for the VIP that can be set in the **globals.yml** file by changing the **keepalived_virtual_router_id**
    variable. The default value that’s shown is **51** , which refers to the **virtual_router_id**
    value in the Keepalived configuration. Once a cluster managed by Keepalived is
    launched, a priority number will be assigned for each node, where the higher priority
    is the most preferred node to hold the VIP and hence is elected as the master.
    In this example, a priority of **40** has been assigned to the current cloud controller
    node. A quick check on the Kolla Keepalived container log is used to validate
    each cloud controller state. In the following example, the current cloud controller
    is assigned a master state:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla_internal_vip_51** 配置块定义了一个随机唯一的 VIP ID，可以通过更改 **globals.yml** 文件中的
    **keepalived_virtual_router_id** 变量来设置。显示的默认值是 **51**，它指的是 Keepalived 配置中的 **virtual_router_id**
    值。一旦由 Keepalived 管理的集群启动，每个节点将分配一个优先级，优先级越高，越优先获得 VIP，从而被选为主节点。在这个例子中，当前云控制器节点的优先级为
    **40**。可以通过快速检查 Kolla Keepalived 容器日志来验证每个云控制器的状态。在以下示例中，当前云控制器被分配了主节点状态：'
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And we get the following output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 7.11 – Validating the Keepalived master assignment](img/B21716_07_11.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 验证 Keepalived 主节点分配](img/B21716_07_11.jpg)'
- en: Figure 7.11 – Validating the Keepalived master assignment
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 验证 Keepalived 主节点分配
- en: 'Upon being elected as a master, Keepalived assigns a VIP of **10.0.0.47** to
    the cloud controller node. In this example, cloud controller 02 ( **10.0.0.101**
    ) has been assigned the VIP. This can be checked by firing off the following command
    line in the host:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在被选为主节点后，Keepalived 将 **10.0.0.47** 的 VIP 分配给云控制器节点。在这个例子中，云控制器 02（**10.0.0.101**）被分配了该
    VIP。可以通过在主机中执行以下命令来检查：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It gives the following output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 它给出了以下输出：
- en: '![Figure 7.12 – Checking the Keepalived VIP](img/B21716_07_12.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 检查 Keepalived VIP](img/B21716_07_12.jpg)'
- en: Figure 7.12 – Checking the Keepalived VIP
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 检查 Keepalived VIP
- en: Enabling HAProxy and Keepalived in our OpenStack deployment ensures HA for most
    OpenStack services. On the other hand, OpenStack networking may require additional
    hardening to enable the fault tolerance capability, as will be depicted in the
    following subsection.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 OpenStack 部署中启用 HAProxy 和 Keepalived 可确保大多数 OpenStack 服务的高可用性。另一方面，OpenStack
    网络可能需要额外的加固才能启用故障容错能力，具体内容将在下一个子部分中描述。
- en: HA for networking
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络的高可用性（HA）
- en: The OpenStack network service involves different composites, including Neutron
    server L2, L3, metadata, and DHCP agents. L2 agents are installed on every compute
    node and there is no need to maintain their HA setup. The DHCP and metadata agents
    run across multiple nodes and support a highly available setup by default.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络服务涉及不同的组件，包括 Neutron 服务器 L2、L3、元数据和 DHCP 代理。L2 代理安装在每个计算节点上，无需维护其
    HA 配置。DHCP 和元数据代理在多个节点上运行，默认支持高可用性配置。
- en: 'On the other hand, L3 agents require more tweaking to attain HA as they are
    the ones responsible for the following aspects:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，L3 代理需要更多的调整才能实现高可用性（HA），因为它们负责以下几个方面：
- en: Managing virtual routers per tenant
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个租户管理虚拟路由器
- en: Providing external connectivity to instances
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实例提供外部连接
- en: Managing floating IPs to instances for external network access
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实例管理浮动 IP 以访问外部网络
- en: 'Before the **Icehouse** release, there was no built-in solution to resolve
    the L3 agent HA issue. Some workarounds involve utilizing an external cluster
    solution using Pacemaker and **Corosync** . New HA modes adopted for Neutron in
    OpenStack have been introduced since the **Juno** release, including the following
    options:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **Icehouse** 版本之前，没有内置的解决方案来解决 L3 代理的 HA 问题。一些解决方法是利用外部集群方案，例如使用 Pacemaker
    和 **Corosync**。自 **Juno** 版本以来，OpenStack 为 Neutron 引入了新的 HA 模式，包括以下选项：
- en: '**Virtual Router Redundancy** **Protocol** ( **VRRP** )'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟路由冗余** **协议**（**VRRP**）'
- en: '**Distributed Virtual** **Routing** ( **DVR** )'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式虚拟** **路由**（**DVR**）'
- en: The next few sections will look at how a redundant Neutron router setup can
    be achieved using VRRP and Keepalived.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几个部分将讨论如何使用 VRRP 和 Keepalived 实现冗余的 Neutron 路由器配置。
- en: Routing redundancy with VRRP
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 VRRP 实现路由冗余
- en: 'We briefly introduced the concept of VRRP earlier in this chapter. In the networking
    context, VRRP and Keepalived are configured in Neutron to fail over within a brief
    period between router namespaces. As shown in the following diagram, routers can
    be seen in the form of groups, where each group presents an active router that
    is currently forwarding traffic to instances:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们简要介绍了VRRP的概念。在网络上下文中，VRRP和Keepalived在Neutron中配置，以在路由器命名空间之间实现短时间的故障转移。如下面的图示所示，路由器可以按组的形式呈现，每个组中的活动路由器正在将流量转发到实例：
- en: '![Figure 7.13 – Routing redundancy with VRRP](img/B21716_07_13.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 使用VRRP的路由冗余](img/B21716_07_13.jpg)'
- en: Figure 7.13 – Routing redundancy with VRRP
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 使用VRRP的路由冗余
- en: Additionally, the instance traffic is spread among all network nodes based on
    the scheduling for the master and the rest of the backup routers. Based on the
    same concept of the Keepalived mechanism, the master router configures its VIP
    internally and keeps informing the router group about its state and priority.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，实例流量会根据主路由器及其余备份路由器的调度分布在所有网络节点之间。基于Keepalived机制的相同概念，主路由器会在内部配置其VIP，并持续向路由器组通告其状态和优先级。
- en: Each VRRP group elects a master router based on the assigned priorities, where
    the router with the highest ID will be selected as the master and the rest remain
    as backups. A new election poll will only take place when the master router stops
    sending its VRRP advertisements to the assigned group and will be marked as a
    failed master.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 每个VRRP组根据分配的优先级选举一个主路由器，ID值最高的路由器将被选为主路由器，其余的作为备份路由器。只有当主路由器停止向指定组发送VRRP广告时，才会进行新的选举投票，此时主路由器将被标记为失败。
- en: Every newly created HA router will add a new router namespace where its L3 agent
    starts Keepalived. Under the hood, routers configured in HA mode will be able
    to communicate via a specific HA network that is not visible to users. The HA
    network interface is denoted by **ha** .
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 每个新创建的HA路由器将添加一个新的路由器命名空间，在该命名空间中其L3代理启动Keepalived。在背后，配置为HA模式的路由器将能够通过特定的HA网络进行通信，该网络对用户不可见。HA网络接口表示为**ha**。
- en: Important note
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The active router needs to periodically inform the standby ones about its state,
    determined by the **advertisement interval timer** . If a backup router does not
    receive such information, it will start a new master router election process based
    on the last advertised VRRP. This election is based on priority, where the router
    with the highest value will be elected as the master. Priorities range from **0**
    to **255** , with **255** being the highest priority.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃路由器需要定期向备份路由器通告其状态，这一状态由**广告间隔计时器**决定。如果备份路由器未收到此类信息，它将基于最后一次广告的VRRP开始新的主路由器选举过程。该选举基于优先级，优先级值最高的路由器将被选为主路由器。优先级范围从**0**到**255**，其中**255**为最高优先级。
- en: By default, Neutron automatically creates an HA pool range network of **169.254.192.0/18**
    , which is used by tenant routers with HA mode enabled. The next configuration
    demonstrates a router resiliency setup in Neutron using VRRP and Keepalived. Based
    on our initial design, a Neutron node will be added that runs an L3 agent and
    is configured with the Open vSwitch mechanism driver, which supports HA routers.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Neutron会自动创建一个HA池范围网络**169.254.192.0/18**，该网络由启用HA模式的租户路由器使用。以下配置展示了在Neutron中使用VRRP和Keepalived进行路由器容错的设置。根据我们的初始设计，将会添加一个运行L3代理并配置了Open
    vSwitch机制驱动的Neutron节点，该驱动支持HA路由器。
- en: 'Add a second network node to the inventory file that runs an L3 agent and,
    optionally, the DHCP and metadata agents, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在清单文件中添加第二个网络节点，该节点运行L3代理，并可选地运行DHCP和元数据代理，如下所示：
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the pipeline and make sure the L3 agent is up and running by running the
    following command line on the controller node:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 运行流水线，并通过在控制节点上运行以下命令，确保L3代理正在正常运行：
- en: '[PRE13]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will be as shown:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.14 – Listing the installed Neutron L3 agents](img/B21716_07_14.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 列出已安装的Neutron L3代理](img/B21716_07_14.jpg)'
- en: Figure 7.14 – Listing the installed Neutron L3 agents
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 列出已安装的Neutron L3代理
- en: Important note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Make sure the new network node is connected to the same network segment as the
    first network node, as illustrated in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Considerations* . The second node will use the
    same Neutron mechanism driver for Open vSwitch, as configured in the **globals.yml**
    file.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 确保新网络节点连接到与第一个网络节点相同的网络段，如 [*第 1 章*](B21716_01.xhtml#_idTextAnchor014) *重温 OpenStack
    – 设计考虑* 中所示。第二个节点将使用与 **globals.yml** 文件中配置的 Open vSwitch 相同的 Neutron 机制驱动程序。
- en: 'Set the following configuration to **yes** in the **globals.yml** file to enable
    the HA state for the Neutron L3 agent:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **globals.yml** 文件中将以下配置设置为 **yes** 以启用 Neutron L3 代理的 HA 状态：
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Run the pipeline to apply the new changes in the Neutron configuration file.
    The Neutron service should be restarted, with the **l3_ha** setting being set
    to **True** in the **/etc/neutron/neutron.conf** file. The default number of maximum
    L3 agents that will be scheduled on a virtual router is set to **3** to construct
    the VRRP virtual router. This can be modified in the **roles/neutron/defaults/main.yml**
    file by setting the value of **max_l3_agents_per_router** .
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 运行管道以应用 Neutron 配置文件中的新更改。应该重启 Neutron 服务，并在 **/etc/neutron/neutron.conf** 文件中将
    **l3_ha** 设置为 **True**。虚拟路由器上最多可调度的 L3 代理数量默认为 **3**，用于构建 VRRP 虚拟路由器。可以通过设置 **roles/neutron/defaults/main.yml**
    文件中的 **max_l3_agents_per_router** 的值来修改此设置。
- en: 'With these new settings, any newly created router is considered an HA router
    and not a legacy one. Create a new router by setting the **–** **ha** flag:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些新设置，任何新创建的路由器都被视为 HA 路由器，而不是传统路由器。通过设置 **–** **ha** 标志来创建新路由器：
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Walking through the different network nodes running an L3 agent, the newly
    created router can be observed via its created namespace in the first network
    node:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 L3 代理的不同网络节点中，可以通过其在第一个网络节点中创建的命名空间观察到新创建的路由器：
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once an HA router is created, the master router will be assigned a virtual
    IP of **169.254.0.1** at any given time:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了 HA 路由器，主路由器将在任何给定时间分配一个虚拟 IP 地址 **169.254.0.1**：
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.15 – Listing the router namespaces and HA scope for the master router](img/B21716_07_15.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 列出主路由器的路由器命名空间和 HA 范围](img/B21716_07_15.jpg)'
- en: Figure 7.15 – Listing the router namespaces and HA scope for the master router
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 列出主路由器的路由器命名空间和 HA 范围
- en: 'Let’s check the output in the second node running the backup router:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查在第二个节点上运行备份路由器的输出：
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We get the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![Figure 7.16 – Listing the router namespaces and HA scope for the backup router](img/B21716_07_16.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – 列出备份路由器的路由器命名空间和 HA 范围](img/B21716_07_16.jpg)'
- en: Figure 7.16 – Listing the router namespaces and HA scope for the backup router
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 列出备份路由器的路由器命名空间和 HA 范围
- en: 'Neutron automatically reserves a new dedicated HA network that is only visible
    to administrators and does not belong to any OpenStack project upon the creation
    of HA routers:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron 在创建 HA 路由器时，会自动保留一个新的专用 HA 网络，该网络仅对管理员可见，并且不属于任何 OpenStack 项目：
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is shown as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.17 – Listing the reserved HA network](img/B21716_07_17.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – 列出保留的 HA 网络](img/B21716_07_17.jpg)'
- en: Figure 7.17 – Listing the reserved HA network
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 列出保留的 HA 网络
- en: 'Keepalived is configured to run in each namespace by using a persistent configuration
    file located at **/var/lib/neutron/ha_confs/ROUTER_NETNS/keepalived.conf** , where
    **ROUTER_NETNS** is the router namespace of the HA router. Failover events are
    also logged in **neutron-keepalived-state-change.log** under the same directory.
    The following extract from a log shows a switch router during a failover event
    on the first network node:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Keepalived 被配置为在每个命名空间中运行，通过使用位于 **/var/lib/neutron/ha_confs/ROUTER_NETNS/keepalived.conf**
    的持久配置文件，其中 **ROUTER_NETNS** 是 HA 路由器的路由器命名空间。故障切换事件也会记录在同一目录下的 **neutron-keepalived-state-change.log**
    中。以下是日志中的一个提取，显示了在第一个网络节点发生故障切换事件时的路由器切换：
- en: '![Figure 7.18 – Verifying router failover in log entries](img/B21716_07_18.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.18 – 验证路由器故障切换日志条目](img/B21716_07_18.jpg)'
- en: Figure 7.18 – Verifying router failover in log entries
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – 验证路由器故障切换日志条目
- en: Neutron routing fault tolerance is a critical requirement to handle OpenStack
    networking availability. The OpenStack Neutron design was not limited only to
    router redundancy using VRRP. Later, Neutron introduced the DVR implementation.
    We’ll explore this in the next subsection.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 中子路由容错是处理 OpenStack 网络可用性的关键要求。OpenStack Neutron 设计不仅仅局限于使用 VRRP 进行路由器冗余。后来，Neutron
    引入了 DVR 实现。我们将在下一小节中探讨这个问题。
- en: Routing redundancy with DVR
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 DVR 进行路由冗余
- en: Like HA routers, DVR operates across multiple compute nodes. Using DVR, network
    load is distributed across operating routers, reducing the traffic load on the
    network node. L3 agents run in compute nodes and traffic flows for *east-west*
    (instance-to-instance) and *north-south* (from an instance to external networks
    with floating IPs or vice versa) are routed across them instead of a single centralized
    network node.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 像 HA 路由器一样，DVR 在多个计算节点上运行。使用 DVR，网络负载分配到各个操作中的路由器上，从而减少了网络节点的流量负担。L3 代理在计算节点中运行，东西向（实例到实例）和南北向（从实例到外部网络，使用浮动
    IP 或反向流动）的流量都通过这些节点路由，而不是单一的集中式网络节点。
- en: 'Configuring DVR in Neutron requires the usage of the Open vSwitch mechanism
    driver and the L3 agent to be installed on the compute nodes. Update the inventory
    file by adding the L3 agent to the desired compute nodes, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Neutron 中配置 DVR 需要使用 Open vSwitch 机制驱动程序，并且 L3 代理必须安装在计算节点上。通过以下方式更新库存文件，将
    L3 代理添加到所需的计算节点：
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the pipeline and verify that the L3 agent is up and running in the compute
    nodes by running the following command line on the controller node:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 运行管道并通过在控制节点上运行以下命令，验证 L3 代理是否在计算节点中正常运行：
- en: '[PRE21]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We get the following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 7.19 – Listing the installed Neutron L3 agents](img/B21716_07_19.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.19 – 列出已安装的 Neutron L3 代理](img/B21716_07_19.jpg)'
- en: Figure 7.19 – Listing the installed Neutron L3 agents
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 列出已安装的 Neutron L3 代理
- en: 'Enable DVR routing in the **globals.yml** file by setting the following configuration
    line:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置以下配置行，在 **globals.yml** 文件中启用 DVR 路由：
- en: '[PRE22]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Run the pipeline and observe the configuration updates in the **/etc/neutron/neutron.conf**
    Neutron configuration file, as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 运行管道并观察 **/etc/neutron/neutron.conf** Neutron 配置文件中的配置更新，如下所示：
- en: '[PRE23]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Each compute node with an installed L3 agent should host the **/etc/neutron/plugins/ml2/openvswitch_agent.ini**
    Open vSwitch agent configuration file with the following settings:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 每个安装了 L3 代理的计算节点应该包含 **/etc/neutron/plugins/ml2/openvswitch_agent.ini** Open
    vSwitch 代理配置文件，配置文件应包含以下设置：
- en: '[PRE24]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Run the pipeline to populate the DVR configuration in both the agent and Neutron
    server configuration settings.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 运行管道，在代理和 Neutron 服务器配置设置中填充 DVR 配置。
- en: 'As an admin, create a new router by specifying the **--distributed** argument,
    as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 作为管理员，通过指定 **--distributed** 参数创建新路由器，如下所示：
- en: '[PRE25]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In each compute node, validate the existence of the same **qrouter** namespace
    ID:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个计算节点中，验证相同 **qrouter** 命名空间 ID 的存在：
- en: '[PRE26]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The router connecting both networks to different compute nodes is the same
    router instance with the same namespace created in each compute node. Typically,
    the **qr** interfaces corresponding to the same namespace will have the same interface
    names and IP addresses in the compute nodes. For **east-west** connectivity, the
    traffic flow between **instance01** hosted in **cn01.os** reaching **instance02**
    spawned in **cn02.os** can be visualized as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 连接两个网络到不同计算节点的路由器是相同的路由器实例，在每个计算节点中都创建了相同的命名空间。通常，属于相同命名空间的 **qr** 接口会在计算节点中具有相同的接口名称和
    IP 地址。对于 **东西向** 连接，**instance01** 所在的 **cn01.os** 与在 **cn02.os** 中启动的 **instance02**
    之间的流量流动可以如下图所示：
- en: '![Figure 7.20 – East-west traffic in DVR mode](img/B21716_07_20.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.20 – DVR 模式下的东西向流量](img/B21716_07_20.jpg)'
- en: Figure 7.20 – East-west traffic in DVR mode
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – DVR 模式下的东西向流量
- en: 'As shown in the preceding diagram, traffic flowing from **instance01** and
    **instance02** (east-west) takes the following path:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，从 **instance01** 和 **instance02**（东西向）流动的流量将经过以下路径：
- en: Traffic from **instance01** is forwarded from its local gateway through the
    router namespace.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自 **instance01** 的流量从其本地网关通过路由器命名空间进行转发。
- en: The router in **cn01.os** replaces the source MAC address with its interface
    MAC address.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**cn01.os** 中的路由器将源 MAC 地址替换为其接口 MAC 地址。'
- en: The router forwards the traffic to the integration bridge in **cn01.os** .
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 路由器将流量转发到 **cn01.os** 中的集成桥接。
- en: Packets are then forwarded to the provider bridge in **cn01.os** .
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包随后被转发到 **cn01.os** 中的提供者桥接。
- en: The source MAC address of the router interface in the packet is replaced by
    the MAC address of the compute node, **cn01.os** .
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包中路由器接口的源MAC地址被计算节点**cn01.os**的MAC地址替换。
- en: Traffic is forwarded to the destination compute node, **cn02.os** , through
    the physical network.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流量通过物理网络转发到目标计算节点**cn02.os**。
- en: Traffic reaches the **cn02.os** host via the provider bridge.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流量通过提供者桥接器到达**cn02.os**主机。
- en: Traffic is forwarded to the integration bridge in the **cn02.os** host.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流量被转发到**cn02.os**主机中的集成桥接器。
- en: At the integration bridge level, the source MAC address is replaced by the router
    MAC interface in **cn02.os** , as dictated in the router namespace.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集成桥接器层面，源MAC地址被替换为**cn02.os**中的路由器MAC接口地址，按照路由器命名空间的规定。
- en: Traffic is forwarded to the destination, **instance02** .
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流量被转发到目标**instance02**。
- en: Return traffic from **instance02** follows the routing path from **cn02.os**
    through its respective bridges and routers.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**instance02**返回的流量遵循从**cn02.os**到其各自的桥接器和路由器的路由路径。
- en: The DVR implementation, much like using VRRP, is simple as it comes with Neutron’s
    built-in features.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: DVR实现类似于使用VRRP，其简便性在于它包含了Neutron的内建功能。
- en: This section outlined the building blocks of enabling HA in the underlying OpenStack
    infrastructure and control plane services. In the following section, we will deep
    dive into the different options to implement fault tolerance for workloads running
    in OpenStack.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了在OpenStack基础架构和控制平面服务中启用HA的构建模块。在接下来的章节中，我们将深入探讨为在OpenStack中运行的工作负载实现容错的不同选项。
- en: Managing instance failover
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理实例故障转移
- en: Providing HA to cloud resources is a critical topic that OpenStackers have had
    to handle since the early releases of OpenStack. Workload users seek different
    ways to increase the availability of their virtual machines through manual tooling
    and scripting, which can be overlooked if not efficiently managed. With the introduction
    of the **Masakari** project in OpenStack, cloud operators can offer workload users
    an automated service that ensures HA for KVM-based instances, reducing the need
    for manual scripting and seamlessly integrating within the OpenStack ecosystem.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为云资源提供HA是OpenStack开发者自OpenStack早期版本以来一直面临的一个关键话题。工作负载用户寻求通过手动工具和脚本增加虚拟机的可用性，如果没有有效管理，这些方法可能会被忽视。随着**Masakari**项目在OpenStack中的引入，云运营商可以为工作负载用户提供一个自动化服务，确保KVM基础的实例具备HA，从而减少了手动脚本编写的需求，并能够无缝集成到OpenStack生态系统中。
- en: 'Masakari also uses Corosync and Pacemaker. As a native HA load balancing stack
    solution for the Linux platform, Pacemaker is a cluster resource manager that
    depends on Corosync to control and organize HA across the hosts in OpenStack.
    Corosync ensures that cluster communication is based on the messaging layer and
    manages the VIP assignment to one of the nodes. Once a cluster of workload instances
    is created, Masakari provides failure detection of the hosts running the instances,
    which is where Corosync comes into play by making sure a virtual IP is assigned
    to a functional host. Masakari is composed of an API that deals with REST requests
    and an engine component that executes recovery requests to the Nova service. As
    shown in the following figure, Masakari primarily provides instance HA via three
    forms of monitors:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Masakari还使用Corosync和Pacemaker。作为Linux平台的原生HA负载均衡堆栈解决方案，Pacemaker是一个集群资源管理器，它依赖Corosync来控制并组织OpenStack中主机间的HA。Corosync确保集群通信基于消息传递层，并将VIP分配给某个节点。一旦创建了工作负载实例的集群，Masakari就可以提供对运行实例的主机的故障检测，此时Corosync发挥作用，确保将虚拟IP分配给一个正常运行的主机。Masakari由一个处理REST请求的API和一个执行恢复请求到Nova服务的引擎组件组成。如下图所示，Masakari主要通过三种监控形式提供实例HA：
- en: '![Figure 7.21 – Instance HA monitors using Masakari](img/B21716_07_21.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.21 – 使用Masakari的实例HA监控](img/B21716_07_21.jpg)'
- en: Figure 7.21 – Instance HA monitors using Masakari
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21 – 使用Masakari的实例HA监控
- en: 'The main Masakari components can be summarized as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Masakari的主要组件可以总结如下：
- en: '**Instance restart** : An instance failure detected by an agent running in
    the compute node will be restarted. Instance restart is managed by the **masakari-instancemonitor**
    Masakari monitor process.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例重启**：由计算节点中运行的代理检测到的实例故障将会重启。实例重启由**masakari-instancemonitor** Masakari监控进程进行管理。'
- en: '**Instance evacuation** : Instances will be evacuated to another healthy compute
    node upon detection of hypervisor failure. The Masakari glossary defines a **failover
    segment** as a group of compute nodes that hosts evacuated instances if one of
    the compute nodes in the same segment goes down. Instance evacuation is managed
    by the **masakari-hostmonitor** Masakari process monitor.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例疏散**：当检测到虚拟化服务器故障时，实例将被疏散到另一个健康的计算节点。Masakari 术语中定义的 **故障切换段** 是指一组计算节点，在同一段中的计算节点出现故障时，负责承载疏散的实例。实例疏散由
    **masakari-hostmonitor** Masakari 进程监控管理。'
- en: '**Process monitor** : This is managed by the **masakari-processmonitor** service,
    which runs on the compute node to collect the status of the different processes
    running on the hypervisor machine, including libvirtd and the Nova compute service.
    The monitor makes sure that, during the failure of one process, no more instances
    are scheduled to run on the affected compute node and will instead be handled
    by other nodes.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进程监控**：由 **masakari-processmonitor** 服务管理，该服务运行在计算节点上，收集虚拟化服务器机器上不同进程的状态，包括
    libvirtd 和 Nova 计算服务。该监控确保在一个进程发生故障时，受影响的计算节点不会再调度更多实例，而是由其他节点处理。'
- en: The next section describes how to deploy the Masakari service in OpenStack using
    **kolla-ansible** .
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分描述了如何使用 **kolla-ansible** 在 OpenStack 中部署 Masakari 服务。
- en: Deploying Masakari
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 Masakari
- en: 'Masakari is composed of its API, monitors, and engine, all of which are installed
    across the cloud controller and compute nodes. Starting with the **globals.yml**
    file, enable the Masakari service by editing the following variable:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Masakari 由其 API、监控器和引擎组成，这些组件被安装在云控制器和计算节点上。从 **globals.yml** 文件开始，通过编辑以下变量启用
    Masakari 服务：
- en: '[PRE27]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To control the instances’ HA using the instance evacuation option, enable the
    following setting in the **globals.yml** file:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过实例疏散选项控制实例的 HA，请在 **globals.yml** 文件中启用以下设置：
- en: '[PRE28]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the **multi_packtpub_prod** inventory file, assign the Masakari services
    to run the API and engine services in the cloud controller nodes:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **multi_packtpub_prod** 清单文件中，将 Masakari 服务分配到云控制器节点上，运行 API 和引擎服务：
- en: '[PRE29]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'At the cloud controller level, the instance evacuation monitor requires that
    the compute nodes that will deploy Peacemaker and Corosync under the hood to be
    monitored:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在云控制器级别，实例疏散监控要求对将要部署 Peacemaker 和 Corosync 的计算节点进行监控：
- en: '[PRE30]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, **hacluster** is an extra Ansible role that provides support to Masakari
    for compute node failure monitoring and recovering instances on a healthy hypervisor
    node, as required for Pacemaker and Corosync:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**hacluster** 是一个额外的 Ansible 角色，提供对 Masakari 的支持，用于计算节点故障监控和在健康的虚拟化服务器节点上恢复实例，符合
    Pacemaker 和 Corosync 的要求：
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The instance restart monitor requires the instance status to be monitored by
    the compute nodes themselves:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 实例重启监控要求计算节点本身对实例状态进行监控：
- en: '[PRE32]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Run the pipeline and make sure the Masakari containers are up and running by
    checking the new Masakari containers. To do so, run the following command line
    in a cloud controller node:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 运行流水线，确保 Masakari 容器正在正常运行，可以通过检查新的 Masakari 容器来确认。为此，请在云控制器节点上运行以下命令行：
- en: '[PRE33]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will get the following output:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下输出：
- en: '![Figure 7.22 – Listing Masakari Kolla containers](img/B21716_07_22.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.22 – 列出 Masakari Kolla 容器](img/B21716_07_22.jpg)'
- en: Figure 7.22 – Listing Masakari Kolla containers
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.22 – 列出 Masakari Kolla 容器
- en: 'To simulate a VM HA scenario, we will spawn an instance in one compute node
    and use the Masakari instance restart monitor mode:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟虚拟机 HA 场景，我们将在一个计算节点中启动实例，并使用 Masakari 实例重启监控模式：
- en: '[PRE34]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Set the **HA_Enabled** flag property of the created instance to **True** :'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 将创建实例的 **HA_Enabled** 标志属性设置为 **True**：
- en: '[PRE35]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Check which compute node the instance is running and kill the instance process
    ID to simulate an instance failure:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 检查实例运行的计算节点，并杀死实例的进程 ID 来模拟实例故障：
- en: '[PRE36]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is the output:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 7.23 – Listing the virtual machines for the HA test](img/B21716_07_23.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.23 – 列出 HA 测试的虚拟机](img/B21716_07_23.jpg)'
- en: Figure 7.23 – Listing the virtual machines for the HA test
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.23 – 列出 HA 测试的虚拟机
- en: 'Log in to the compute node and kill the instance PID:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到计算节点并杀死实例的 PID：
- en: '[PRE37]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By watching the Masakari log file outputs in **/var/log/kolla/masakari** ,
    observe the status of the instance reboot in the same hypervisor host:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看 **/var/log/kolla/masakari** 中的 Masakari 日志文件输出，观察同一虚拟化服务器主机上实例重启的状态：
- en: '![Figure 7.24 – Validating the Masakari instance respawning process in the
    log entries](img/B21716_07_24.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.24 – 验证日志条目中 Masakari 实例重生过程](img/B21716_07_24.jpg)'
- en: Figure 7.24 – Validating the Masakari instance respawning process in the log
    entries
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24 – 验证日志条目中 Masakari 实例重生过程
- en: 'Within a few seconds, the killed instance will be spawned in the same compute
    node. This can be checked via the newly created instance PID:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟内，被终止的实例将会在同一计算节点上重新创建。可以通过新创建的实例 PID 来检查这一点：
- en: '[PRE38]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As we can see, Masakari can be a great addition to ensure workload fault tolerance
    without the need to operate the HA capability separately for each instance, thus
    avoiding heavy manual effort. Masakari is getting more popular, and several production
    deployments have proven its advantages in helping workload administrators and
    cloud developers tackle failover challenges at the instance level.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Masakari 可以很好地帮助确保工作负载的容错性，而无需为每个实例单独操作 HA 功能，从而避免了繁重的手动工作。Masakari 正在变得越来越流行，多个生产环境的部署已经证明了它在帮助工作负载管理员和云开发者解决实例级故障转移挑战方面的优势。
- en: Summary
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: 'In this chapter, we enabled a critical architectural safeguard design in our
    OpenStack environment by covering the HA pillar across the control and data planes.
    We now have numerous ways to construct a highly available OpenStack environment,
    depending on which HA strategy is preferred. In this chapter, we empowered the
    control plane services using HAProxy and Keepalived. Other deployments could employ
    Corosync, Pacemaker, and a selection of vendor solutions for load balancing. As
    demonstrated in this chapter, more than a single design pattern can be applied
    for common infrastructure services, such as databases and message queues, to achieve
    failover at best. Networking HA in OpenStack has achieved another level of maturity
    that enables cloud operators to choose between two options to minimize any potential
    risks of connectivity loss for the tenant networks: routing with VRRP and DVR.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过覆盖控制平面和数据平面的 HA 支柱，启用了 OpenStack 环境中的一个关键架构安全设计。现在，我们有多种方法可以构建一个高度可用的
    OpenStack 环境，具体取决于首选的 HA 策略。在本章中，我们使用 HAProxy 和 Keepalived 强化了控制平面服务。其他部署可以采用
    Corosync、Pacemaker 和一些厂商的负载均衡解决方案。正如本章所示，多个设计模式可以应用于常见的基础设施服务，如数据库和消息队列，以实现最佳的故障转移。OpenStack
    中的网络 HA 已经达到了一个新的成熟水平，云运营商可以选择两种方式，以最小化租户网络连接丢失的潜在风险：通过 VRRP 和 DVR 路由。
- en: This chapter also explored an attractive capability for cloud users to achieve
    failover with native OpenStack services for their workloads running in instances
    using Masakari. We also showed some additional snippets for managing HA in OpenStack
    layers using **kolla-ansible** . There is still some ongoing amelioration on the
    infrastructure code using Kolla to automate cell and region deployments for compute
    services in OpenStack that was not covered in this chapter. HA and redundancy
    are critical elements that are required for you to start welcoming production
    workloads in your OpenStack environment and with this chapter, you should be ready
    to get started. Alongside ensuring the HA of services, we need to keep an eye
    on them and act proactively when things start to boil.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还探讨了一个对云用户来说非常有吸引力的能力，通过使用 Masakari 和原生 OpenStack 服务实现工作负载的故障转移，帮助他们管理运行在实例中的工作负载。我们还展示了一些额外的代码片段，用于通过**kolla-ansible**
    管理 OpenStack 层的 HA。目前，利用 Kolla 自动化 OpenStack 中计算服务的单元和区域部署的基础设施代码仍在不断改进，这在本章中没有涉及。HA
    和冗余是你开始在 OpenStack 环境中迎接生产工作负载所需的关键要素，通过本章的学习，你应该已经准备好开始了。除了确保服务的 HA 外，我们还需要时刻关注它们，并在问题发生时主动采取措施。
- en: The next chapter will discuss the next operational excellence pillar of the
    OpenStack journey. It will explore ways you can monitor your cloud environment
    for the early detection of anomalies and deep dive into more fine-grained options
    for introspection and the analysis of collected OpenStack services logs.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论 OpenStack 之旅中的下一个运营卓越支柱。它将探讨如何监控你的云环境，以便早期检测异常，并深入研究更细粒度的自省选项以及收集的 OpenStack
    服务日志分析。
