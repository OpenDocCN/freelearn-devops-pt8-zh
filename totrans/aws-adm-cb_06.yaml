- en: Database Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a database with automatic failover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a NAT gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a database read-replica
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promoting a read-replica to master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a one-time database backup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restoring a database from a snapshot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Migrating a database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating DynamoDB performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a persistent storage service is a key component of effectively using
    the AWS cloud for your systems. By ensuring that you have a highly available,
    fault-tolerant location to store your application state in, you can stop depending
    on individual servers for your data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a database with automatic failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we're going to create a MySQL RDS database instance configured
    in multi-AZ mode to facilitate automatic failover.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Database with automatic failover
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The default VPC will work fine for this example. Once you are comfortable with
    creating databases, you may want to consider a VPC containing private subnets
    that you can use to segment your database away from the Internet and other resources
    (in the style of a three tier application). Either way, you''ll need to note down
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The ID of the VPC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CIDR range of the VPC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IDs of at least two subnets in your VPC. These subnets need to be in different
    Availability Zones, for example, `us-east-1a` and `us-east-1b`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new CloudFormation template. We''re going to add a total of 12 parameters
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first three parameters will contain the values we mentioned in the *Getting
    ready* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re also going to add the database credentials as parameters. This is good
    practice as it means we''re not storing any credentials in our infrastructure
    source code. Note that the password contains the `NoEcho` parameter set to `true`.
    This stops CloudFormation from outputting the password wherever the CloudFormation
    stack details are displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The next block of parameters pertains to cost and performance. They should
    be mostly self-explanatory. Refer to the AWS documentation on database instance
    types should you wish to change the instance class for this example. We''re supplying
    a default value of 10 GB for the storage size and choosing a magnetic (`standard`)
    volume for the storage type. `gp2` offers better performance, but it costs a little
    more:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to set some additional parameters for our database. These are the MySQL
    engine version and port. Refer to the AWS documentation for a list of all the
    available versions. We are setting a default value for this parameter as the latest
    version of MySQL at the time of writing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we are going to define some parameters relating to backup and availability.
    We want our database to run in *multi-AZ* mode, we set this to `true` by default.
    We also set a backup retention period of `1` day by default; you might want to
    choose a period larger than this. If you set this value to `0`, backups will be
    disabled (not recommended!):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re done with the parameters for this template; we can now go ahead and
    start defining our `Resources`. First of all, we want a security group for our
    DB to reside in. This security group allows inbound access to the database port
    from the CIDR range we''ve defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define a `DBSubnetGroup` resource. This resource is used to
    declare which subnet(s) our DB will reside in. We define two subnets for this
    resource so that the primary and standby servers will reside in separate Availability
    Zones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define our RDS instance resource. We specify it as being a MySQL
    database and the rest of the properties are made up of the parameters and resources
    that we''ve defined previously. Lots of `!Ref` is required here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For good measure, we can add an output to this template that will return the
    hostname for this RDS database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can provision the database via the CloudFormation web console or use a
    CLI command like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a multi-AZ configuration, AWS will provision a standby MySQL instance in
    a separate Availability Zone. Changes to your database will be replicated to the
    standby DB instance in a synchronous fashion. If there is a problem with your
    primary DB instance AWS will automatically failover to the standby, promote it
    to be the primary DB, and provision a new standby.
  prefs: []
  type: TYPE_NORMAL
- en: You don't have access to query standby databases directly. So you can't use
    it to handle all of your read queries, for example. If you wish to use additional
    database instances to increase read capacity, you'll need to provision a *read-replica*.
    We'll cover those in a separate recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Backups will always be taken from the standby instance, which means there is
    no interruption to your DB availability. This is not the case if you opted against
    deploying your DB in multi-AZ mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you deploy this example it will take roughly 20 minutes or more for the
    stack to report completion. This is because the RDS service needs to go through
    the following process in order to provision a fully working multi-AZ database:'
  prefs: []
  type: TYPE_NORMAL
- en: Provision the primary database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back up the primary database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provision the standby database using the backup from the primary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure both databases for synchronous replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WARNING
  prefs: []
  type: TYPE_NORMAL
- en: Be careful about making changes to your RDS configuration after you've started
    writing data to it, especially when using CloudFormation updates. Some RDS configuration
    changes require the database to be re-provisioned, which can result in data loss.
    We'd recommend using CloudFormation change sets, which will give you an opportunity
    to see which changes are about to cause destructive behavior. The CloudFormation
    RDS docs also provide some information on this.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can define a maintenance window for your RDS instance. This is the time
    period when AWS will perform maintenance tasks such as security patches or minor
    version upgrades. If you don't specify a maintenance window (which we don't in
    this example), one is chosen for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a NAT gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unless required, your instances should not be publicly exposed to the Internet.
    When your instances are on the Internet, you have to assume that they will be
    attacked at some stage.
  prefs: []
  type: TYPE_NORMAL
- en: This means most of your workloads should run on instances in private subnets.
    Private subnets are those that are not connected directly to the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: In order to give your private instances access to the Internet you use **network
    address translation** (**NAT**). A NAT gateway allows your instances to initiate
    a connection to the Internet, without allowing connections from the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe, you must have the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: A VPC with an **Internet gateway** (**IGW**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A public subnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A private subnet route table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need the IDs for the public subnet and private subnet route table.
    Both of these resources should be in the same AZ.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start with the usual CloudFormation template version and description:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The template must take the following required parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `Resources` section, define an Elastic IP that will be assigned to the
    NAT gateway:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the NAT gateway resource, assigning it the EIP you just defined in the
    public subnet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, define the route to the NAT gateway and associate it with the private
    subnet''s route table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The parameters required for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A public subnet ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A private subnet route table ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The public subnet ID is needed to host the NAT gateway, as it must have Internet
    access. The private subnet route table will be updated with a route to the NAT
    gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Using the AWS NAT gateway service means that AWS takes care of hosting and securing
    the service for you. The service will be hosted redundantly in a single AZ.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the recipe multiple times to deploy NAT gateways in each of your
    private subnets. Just make sure the public subnet and the private subnet are in
    the same AZ.
  prefs: []
  type: TYPE_NORMAL
- en: To cater for the unlikely event of an AZ outage (unlikely, but possible) you
    should deploy a NAT gateway per subnet. This means if one NAT gateway goes offline,
    instances in the other AZ can continue to access the Internet as normal. You *are*
    deploying your application in multiple AZs, aren't you?
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will only work if you have created your own private subnets, as
    the default subnets in a new AWS account are all *public*. Instances in a public
    subnet have direct access to the Internet (via an IGW), so they do not need a
    NAT gateway.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Building a secure network* recipe in [Chapter 7](de50c1bf-fc87-4674-9719-c55280a6b60d.xhtml),
    *Networking*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a database read-replica
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will show you how to create an RDS read-replica. You can use read-replicas
    in order to increase the performance of your application by off-loading database
    reads to a separate database instance. You can provision up to five read-replicas
    per source DB.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Read-only database slaves
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an RDS DB deployed with backup retention enabled. We are going
    to build upon the DB deployed in the previous *Creating a database with automatic
    failover* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''re going to need the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: The identifier for your source RDS instance, for example, `eexocwv5k5kv5z`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A unique identifier for the read-replicate we're going to create, for example, `read-replica-1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the AWS CLI, type this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDS will now go ahead and create a new read-replica for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some parameters are inherited from the source instance and can''t be defined
    at the time of creation:'
  prefs: []
  type: TYPE_NORMAL
- en: Storage engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CLI command accepts some parameters that we could have defined, but didn''t
    to keep things simple. They will instead be inherited from the source database.
    The main two are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--db-instance-class`: The same class as the source instance is used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--db-subnet-group-name`: The source instance''s subnet group will be used
    and a subnet is chosen at random (hence, an Availability Zone is chosen at random)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read-replicas are deployed in a single Availability Zone; there is no standby
    read-replica.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's not possible to enable backups on read-replicas during time of creation.
    This must be configured afterwards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default storage type is `standard` (magnetic). You can increase performance
    by choosing `gp2` or using provisioned IOPS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's possible to add MySQL indexes directly to a read-replica to further increase
    read performance. These indexes are not required to be present on the primary
    DB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using read-replicas for availability purposes is more of a complimentary DR
    strategy and shouldn't be used in place of multi-AZ RDS. A multi-AZ configuration
    gives you the benefit of failure detection and automatic failover.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to deploy a read-replica in an entirely different region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the replication between a primary and standby DB (which is synchronous),
    replication to a read-replica is asynchronous. This means that it's possible for
    a read-replica to fall behind the primary. Keep this in mind when sending time
    sensitive read queries to your read-replicas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promoting a read-replica to master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re going to show you how to promote an RDS read-replica to be a primary
    instance. There are a few reasons you might like to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: To handle a table migration that would typically cause a large amount of downtime,
    especially when messing with columns or indexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because you need to implement sharding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery from failure, should you choose not to deploy your existing primary
    in multi-AZ mode (not recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You're going to need the unique ID, which has been assigned to an RDS read-replica.
    If you followed the previous *Creating a database with automatic failover*, and
    *Creating a database read-replica* recipes, then you'll be all set.
  prefs: []
  type: TYPE_NORMAL
- en: It's also a good idea to have backups enabled on this read-replica prior to
    promoting it. This shortens the promotion process because you won't need to wait
    for a backup to be taken. You'll want to set the backup retention period to a
    value between `1` and `8`.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling backups on your read-replica will cause it to reboot!
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to enable backups, you can use the following CLI command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can drop the `--apply-immediately` parameter if you prefer to wait for the
    reboot to happen during the configured maintenance window. But you'll still want
    to wait until after the reboot happens before you continue with the promotion
    process.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that you have the most up-to-date data before promotion you'll want
    to stop all write traffic to the current source primary DB before going ahead.
    It's also a good idea to make sure that the replication lag on your read-replica
    is `0` (you can check this in CloudWatch).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the following command to promote your read-replica to a primary DB instance.
    This command will cause your read-replica to reboot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you wish to then go ahead and configure your new primary RDS instance to
    run in a multi-AZ configuration then you''ll need to run this additional command.
    Expect to wait a while for this operation to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Creating a one-time database backup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're now going to show you how to make a one-off snapshot of your database.
    You might opt to do this if you have a specific requirement around keeping a point
    in time backup of your DB. You might also want to take a snapshot for the purpose
    of creating a new working copy of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to proceed you''re going to need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The identifier for the RDS instance you wish to back up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A unique identifier that you'd like to assign to this snapshot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The snapshot identifier has some constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: It needs to start with a letter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must not be longer than 255 characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your primary database isn't running in a multi-AZ configuration then be aware
    that creating a snapshot will cause an outage. In a multi-AZ configuration the
    snapshot is taken on the standby instance so no outage occurs.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Type the following AWS CLI command to initiate the creation of a snapshot.
    You''ll need to wait for a few minutes for the snapshot to complete before you
    can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Restoring a database from a snapshot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll now talk through how to restore a database from a snapshot. This process
    creates a new database that will retain a majority of the configuration of the
    database that the snapshot was taken from.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ll need the following pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: The ID of the snapshot you wish to restore from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A name or identifier that you wish to give to the database we're about to create
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS does not allow RDS services in your account to share the same identifier.
    If the source database is still online you'll need to make sure to choose a different
    identifier (or rename the source database).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have noticed that this command creates a new database in the default
    security group. This happens because the `restore-db-instance-from-db-snapshot`
    doesn''t accept a security group ID as a parameter. You''ll have to run a second
    command to assign a nondefault security group to the new database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `modify-db-instance` command will return an error unless the state of the
    target database is `available`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, security group names aren't valid with this command; you'll need to use
    a security group ID instead, for example, `sg-7603d50a`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous command includes the parameter for enabling multi-AZ on the new
    DB. If you'd like the new DB to be running in single-AZ mode only then can you
    simply remove this flag.
  prefs: []
  type: TYPE_NORMAL
- en: Migrating a database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use **Database Migration Service** (**DMS**) to move
    an external database into **Relational Database Service** (**RDS**).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike many of the other recipes, this will be performed manually through the
    web console.
  prefs: []
  type: TYPE_NORMAL
- en: Most database migrations are one-off, and there are many steps involved. We
    suggest that you first perform the process manually via the console before automating
    it, if required (which you can do with the AWS CLI tool or SDKs).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An external database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RDS database instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source database in this example is called **employees**, so substitute your
    own database name as required.
  prefs: []
  type: TYPE_NORMAL
- en: Both databases must be accessible from the replication instance that will be
    created as part of the recipe. The simplest way to do this is to allow access
    to the databases from the Internet, but obviously this has security implications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Navigate to the DMS console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Create Migration to start the migration wizard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Specify the details for your replication instance. Unless you have a specific
    VPC configuration, the defaults will be fine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While waiting for the replication instance to be ready, fill out the source
    and target endpoint information, including server hostname and port, and the username
    and password to use when connecting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the instance is ready, the interface will update and you can proceed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to confirm and create the source and target endpoints, click on the
    Run test button for each of your databases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the endpoints have been successfully tested and created, define your
    task. In this recipe, we will simply migrate the data (without ongoing replication):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity, drop the tables in the target database (which should be empty)
    to ensure parity between the databases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, define the mappings between the two databases. In this case, we will
    migrate all the tables (by using the wildcard `%`) in the employees database on
    the source:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click Add selection rule you will see your rule in the selection rules
    list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the task is defined you have finished the wizard. You will then see the
    task being created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the status of the task is Ready you can select it and click on the Start/Resume
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When complete, you will see the task''s details updated in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/image_06_015.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At a high level, this is what the DMS architecture looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06236_06_16.png)'
  prefs: []
  type: TYPE_IMG
- en: Both the **Source** and **Target** databases are external to **DMS**. They are
    represented internally by endpoint resources that are references to the databases.
    Endpoints can be reused between different tasks if needed.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe starts by defining the replication instance details. Keep in mind
    that the DMS migration process works best when the migration/transform between
    the two databases is kept *in memory*. This means that for larger jobs you should
    allocate a more powerful instance. If the process needs to temporarily write data
    to disk (such as swap) then the performance and throughput will be much lower.
    This can have flow-on effects, particularly for tasks that include ongoing replication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the two endpoints are defined. It is very important to verify your endpoint
    configuration by using the built-in testing feature so that your tasks do not
    fail later in the process. Generally, if the connectivity test fails, it is one
    of two main issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Network connectivity issues between the replication instance and the database.
    This is particularly an issue for on-premise databases, which are usually specifically
    restricted from being accessed externally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User permissions issues: For example, in the case of MySQL, the root user cannot
    be used to connect to the database externally, so this default user cannot be
    used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the task involves defining your migration type. The recipe uses the
    simplest type; migrate tables. This means that the data will be copied between
    the two databases, and will be complete when the data is propagated. We also get
    to define the behavior on the target database. For simplicity, we have configured
    the task to drop the tables in the target database ensuring that the two databases
    look as similar as possible, even if the tables are renamed, or the table mappings
    change. For the task table mappings we use the wildcard symbol `%` to match all
    tables in the source database. Obviously, you could be more selective if you only
    wanted to match a subset of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Once the replication instance, endpoints, and task are defined the wizard ends
    and you are returned to the DMS console. After the task is finished creating it
    can be started.
  prefs: []
  type: TYPE_NORMAL
- en: As it is a *migrate existing data-type* task, it will complete once all the
    data has been propagated to the target database.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is obviously a simple example of what DMS can do. There are other features
    and performance aspects that you should consider in more advanced scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Database engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this example uses two MySQL databases, it is possible to migrate from
    one database engine to a complete database engine, for example, Oracle to MySQL.
    Unfortunately, this can be a complex process, and while this functionality is
    very useful it is beyond the scope of this recipe. Due to the differences in the
    various engines, there are some limitations on what you can migrate and transform.
  prefs: []
  type: TYPE_NORMAL
- en: See the *AWS Schema Conversion Tool* documentation for more details on what
    can be migrated between different database engines.
  prefs: []
  type: TYPE_NORMAL
- en: Ongoing replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are also some limits around the ongoing propagation of data—only table
    data can be migrated. Things such as indexes, users, and permissions cannot be
    replicated continually.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-AZ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For ongoing replication tasks, you may want to create a multi-AZ replication
    instance so that the impact of any interruptions of services are minimized. Obviously
    you will need to have a similarly configured (such as multi-AZ) RDS instance as
    your target to get the full benefit!
  prefs: []
  type: TYPE_NORMAL
- en: For best performance, when setting up your replication instance you should make
    sure it is in the *same* AZ as your target RDS instance.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating DyanmoDB performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DynamoDB** (**DDB**) is the managed NoSQL database service from AWS.'
  prefs: []
  type: TYPE_NORMAL
- en: As DDB pricing is based on the amount of read and write capacity units provisioned,
    it is important to be able to calculate the requirements for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe uses a written formula to estimate the required **read capacity
    units** (**RCU**) and **write capacity units** (**WCU**) that should be allocated
    to you DDB table.
  prefs: []
  type: TYPE_NORMAL
- en: It is also crucial to remember that while new partitions will be automatically
    added to a DDB table, they cannot be automatically taken away. This means that
    excessive partitioning can cause long-term impacts to your performance, so you
    should be aware of them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All of these calculations assume that you have chosen a good partition key
    for your data. A good partition key ensures the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is evenly spread across all the available partitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read and write activity is spread evenly in time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, choosing a good partition key is very data-specific, and beyond
    the scope of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: All reads are assumed to be strongly consistent.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start with the size of the items, in **kilobytes** (**KB**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*ItemSize = Size of the items (rows) in KB*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Work out the required number of RCUs required by dividing the number by *4*,
    and rounding up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*RCU Per Item = ItemSize / 4 (rounded up)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the expected number of read operations per second. This is one of the
    numbers you will use to provision your table with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Required RCU = Expected Number of Reads * RCU Per Item*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the number by *3,000* to calculate the number of DDB partitions required
    to reach the capacity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Read Partitions = Required RCU / 3,000*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, work out the write capacity required by dividing the item size by *1*,
    and rounding up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*WCU Per Item = ItemSize / 1 (rounded up)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the expected number of write operations per second. This is one of the
    numbers you will use to provision your table with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Required WCU = Expected Number of Writes * WCU Per Item*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the number by *1,000* to calculate the number of DDB partitions required
    to reach the capacity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Write Partitions = Required WCU / 1,000*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add these two values to get the capacity partitions required (rounding up to
    a whole number):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Capacity Partitions = Read Partitions + Write Partitions (rounded up)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Work out the minimum number of partitions required by the amount of data you
    plan to store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Size Partitions = Total Size in GB / 10 (rounded up)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the partition requirements for your use case, take the maximum
    of your previous calculations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Required Partitions = Maximum value between Capacity Partitions and Size Partitions*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since your allocated capacity is spread evenly across partitions, divide the
    RCU and WCU values to get the per-partition performance of your table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Partition Read Throughput = Required RCU / Required Partitions*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Partition Write Throughput = Required WCU / Required Partitions*'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Behind the scenes, DDB throughput is controlled by the number of partitions
    that are allocated to your table. It is important to consider how your data will
    be spread across these partitions to ensure you get the performance you expect
    *and have paid for*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start this recipe by calculating the size of the items in your database,
    for throughput purposes. DDB has a minimum size it will consider, and even if
    an operation uses less than this size, it is rounded up in terms of allocated
    throughput used. The minimum size depends on the type of operation:'
  prefs: []
  type: TYPE_NORMAL
- en: Read operations are calculated in 4-K blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write operations are calculated in 1-K blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then work out what the required RCU and WCU is, based on the expected number
    of operations. These values are what can then be used to provision the DDB table,
    as they represent the minimum required throughput (in optimal conditions).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have these values, you can use them to provision your table.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we calculate the throughput per partition key. These calculations rely
    on knowing what the performance of each partition is expected to be. The numbers
    3,000 (for RCUs) and 1,000 (for WCUs) represent the capacity of a single DDB partition.
    By expressing the capacity in terms of partition performance (reads and writes)
    and adding them together we get the minimum number of partitions required from
    a capacity point of view.
  prefs: []
  type: TYPE_NORMAL
- en: We then do the same calculation for total data size. Each DDB partition can
    handle up to 10 GB of data. Any more than that will need to be split between multiple
    partitions.
  prefs: []
  type: TYPE_NORMAL
- en: The specific values for partition capacity (for reads, writes, and size) have
    been stable for a while, but may change in the future. Double-check that the current
    values are the same as used here for complete accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the minimum partitions for both capacity and size, we take the
    highest value and work with that. This ensures we meet both the capacity and size
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we take the provisioned capacity and divide it by the number of partitions.
    This gives us the throughput performance for each partition key, which we can
    then use to confirm against our use case.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many nuances to using DDB efficiently and effectively. Here are some
    of the more important/impactful things to note.
  prefs: []
  type: TYPE_NORMAL
- en: Burst capacity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a burst capacity available to tables that go over their allocated capacity.
    Unused read and write capacity can be retained for up to five minutes (such as
    300 seconds, for calculation purposes). Relying on this capacity is not good practice,
    and it will undoubtedly cause issues at some stage in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DDB tables automatically send data to CloudWatch metrics. This is the quickest
    and easiest way to confirm that your calculations and provision capacity are meeting
    your needs. It also helps you keep an eye on your usage to track your throughput
    needs over time. All metrics appear in the *AWS/DynamoDB* namespace. Some of the
    most interesting metrics for throughput calculations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ConsumedReadCapacityUnits`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConsumedWriteCapacityUnits`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReadThrottleEvents`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WriteThrottleEvents`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other metrics available; see the *Amazon DynamoDB Metrics and Dimensions*
    documentation for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually consistent reads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using eventually consistent reads (as opposed to strongly consistent reads)
    *halves* the RCU requirements for calculation purposes. In this recipe, we have
    used strongly consistent reads because it works with all workloads, but you should
    confirm that your use case actually requires it. Use eventually consistent reads
    if it does not.
  prefs: []
  type: TYPE_NORMAL
- en: By reducing the required provisioned capacity for reads, you effectively reduce
    your *cost* for using DDB.
  prefs: []
  type: TYPE_NORMAL
