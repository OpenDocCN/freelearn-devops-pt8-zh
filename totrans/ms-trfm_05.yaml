- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Beyond VMs – Core Concepts of Containers and Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越虚拟机——容器和 Kubernetes 的核心概念
- en: 'In the previous chapter, we familiarized ourselves with **virtual machine**
    (**VM**) architecture and the core concepts and mechanics needed to automate VM-based
    solutions. In this book, we will build end-to-end solutions covering the three
    significant hyperscalars—**Amazon Web Services** (**AWS**), Azure, and **Google
    Cloud Platform** (**GCP**)—and covering three cloud computing paradigms: VMs,
    containers, and serverless. In this chapter, we will look at the core concepts
    needed to tackle container-based architecture solutions using the managed Kubernetes
    offerings from each cloud platform.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们熟悉了**虚拟机（VM）**架构和自动化基于虚拟机的解决方案所需的核心概念和机制。在本书中，我们将构建端到端的解决方案，涵盖三大超大规模云服务提供商——**亚马逊
    Web 服务（AWS）**、Azure 和 **谷歌云平台（GCP）**——并涵盖三种云计算范式：虚拟机、容器和无服务器。在本章中，我们将探讨使用各云平台提供的托管
    Kubernetes 服务来解决基于容器的架构问题所需的核心概念。
- en: To accomplish this, we must understand the basics of containers, Kubernetes,
    and how they fit within the Terraform ecosystem. As with VMs and the surrounding
    toolchains used for configuration management and the **build-versus-bake** dilemma,
    with container-based architecture, we need to make some decisions about where
    the boundary between Terraform and other tools will exist and how best to integrate
    configuration management of our containers and container orchestrators with the
    cloud infrastructure that we provision to host them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们必须了解容器、Kubernetes 的基础知识，以及它们如何融入 Terraform 生态系统。就像虚拟机和用于配置管理的工具链，以及**构建与烘焙**的困境一样，在基于容器的架构中，我们需要做出一些决策，关于
    Terraform 与其他工具之间的边界在哪里，以及如何最好地将容器和容器编排工具的配置管理与我们提供的云基础设施进行集成，以托管这些容器。
- en: 'The chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Understanding key concepts of container architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解容器架构的关键概念
- en: Leveraging Docker to build container images
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Docker 构建容器镜像
- en: Working with container registries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用容器注册表
- en: Understanding key concepts of container orchestration and Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解容器编排和 Kubernetes 的关键概念
- en: Understanding Kubernetes manifests
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 清单
- en: Leveraging the Kubernetes provider to provision Kubernetes resources
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Kubernetes 提供者来配置 Kubernetes 资源
- en: Leveraging the Helm provider to provision Kubernetes resources
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Helm 提供者来配置 Kubernetes 资源
- en: Understanding key concepts of container architecture
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解容器架构的关键概念
- en: '**VMs** are great when you want minimal changes to operate your applications
    and software in the cloud, but they also have drawbacks. With the maximum control
    you get from having a full VM—of whatever size you happened to provision—you are
    free to use as many (or as few) of the VM’s resources as you can. However, many
    organizations have found that their fleet of VMs is plagued by low utilization
    even when best practices in workload isolation or the **single responsibility
    principle** (**SRP**) are followed.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟机（VM）** 在你希望最小化对应用程序和软件操作变更时，在云中运行是非常有效的，但它们也有缺点。通过拥有完整的虚拟机（无论你配置的是何种大小），你可以自由使用虚拟机的任何资源（多或少）。然而，许多组织发现，即使在遵循工作负载隔离的最佳实践或**单一职责原则（SRP）**的情况下，它们的虚拟机阵列仍然面临低利用率的问题。'
- en: Inversely, when maximum utilization is the objective, organizations load up
    a single VM with so many disparate services and components that each VM—while
    highly utilized—becomes a bit of a quagmire to manage and maintain. The VM will
    have a myriad of dependency conflicts, with resource contention cropping up between
    the horde of independent but cohabitating processes within the same VM.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，当最大化利用率成为目标时，组织会将许多不同的服务和组件加载到一个单一的虚拟机中，以至于每个虚拟机——虽然被高度利用——却变成了一种难以管理和维护的困境。虚拟机会有无数的依赖冲突，并且在同一个虚拟机内，独立但共存的进程之间会出现资源争用。
- en: This dilemma between workload isolation and resource utilization is the problem
    that container technology aims to solve and where container orchestrators, such
    as Kubernetes, help by bringing resiliency and scalability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作负载隔离与资源利用之间的矛盾是容器技术旨在解决的问题，也是容器编排工具（如 Kubernetes）通过提供弹性和可扩展性来帮助解决的地方。
- en: In this book, we will build an end-to-end solution using Kubernetes-based container
    technology on AWS, Azure, and GCP. To do so, you must understand some critical
    concepts that transcend cloud platforms to help you navigate the architecture
    and relevant Terraform resources within the respective cloud platform’s Terraform
    provider.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用基于 Kubernetes 的容器技术，在 AWS、Azure 和 GCP 上构建端到端的解决方案。为此，你需要理解一些关键概念，这些概念超越了云平台，帮助你在各个云平台的
    Terraform 提供者中导航架构和相关的 Terraform 资源。
- en: Containers
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器
- en: '**Containers** allow you to package your applications into an isolated environment
    logically separated from other applications without the overhead incurred by virtualizing
    the underlying physical hardware and the resource consumption of a full-on operating
    system. Whether it is Windows or Linux, the operating system consumes resources
    that take away from your capacity.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器** 允许你将应用程序打包到一个逻辑上与其他应用程序隔离的环境中，而无需虚拟化底层物理硬件和完整操作系统所带来的开销。无论是 Windows
    还是 Linux，操作系统都会消耗资源，影响你的计算能力。'
- en: 'Containers use two Linux kernel primitives: *namespaces* and *control groups*.
    These constructs allow the container runtime to set up an isolated environment
    within the Linux operating system. Namespaces are all about isolation, which allows
    us to split the operating system into multiple virtual operating systems with
    their own process tree, root filesystem, user, and so on. Each container might
    feel like a regular operating system, but it’s not. Control groups police the
    allocation of the host system’s resources—including CPU, memory, and disk I/O—to
    ensure that the actual physical server is not overwhelmed by resources consumed
    by the containers.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 容器使用两个 Linux 内核原语：*命名空间* 和 *控制组*。这些构造使容器运行时能够在 Linux 操作系统中设置一个隔离的环境。命名空间的核心是隔离，它允许我们将操作系统分割成多个虚拟操作系统，每个虚拟系统都有自己的进程树、根文件系统、用户等。每个容器可能感觉像一个常规操作系统，但实际上并不是。控制组负责监管主机系统资源的分配——包括
    CPU、内存和磁盘 I/O——以确保实际的物理服务器不会因容器消耗的资源而被压垮。
- en: The last component that enables containers is a layered filesystem. This is
    similar to how we used to build VM images—only with better isolation between layers.
    When we build a VM layer, when we apply changes to and create a new VM image,
    we can no longer sort out the base layer from the top layer. Containers can apply
    filesystem layers that contain only the differences between the lower layers.
    This approach creates an extremely compact and highly efficient way of layering
    changes onto each container image to compose the final filesystem that the container
    operates on—with the topmost layer being writable by the container itself.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 启用容器的最后一个组件是分层文件系统。这类似于我们以前构建虚拟机镜像的方式——只是层之间有更好的隔离。当我们构建虚拟机层时，当我们对虚拟机镜像进行更改并创建新的镜像时，我们无法再将基础层从顶层分离出来。容器可以应用只包含下层差异的文件系统层。这种方法创造了一种极为紧凑且高效的方式，将更改分层到每个容器镜像上，从而组成容器操作的最终文件系统——最顶层是可由容器本身写入的。
- en: One of the key benefits of containers is their efficiency. Unlike VMs, which
    require separate operating systems and resource allocations for each instance,
    containers directly leverage the host system’s kernel. This approach means they
    consume fewer resources and start up much faster than their VM counterparts. Multiple
    containers can run simultaneously on a single host, thus using system resources
    more efficiently. This allows us to create higher-density workloads—thus reducing
    the waste of valuable system resources such as CPU and memory to idleness, and
    when working in the cloud, this waste is like pouring money down the drain!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 容器的一个关键优势是它们的高效性。与虚拟机不同，虚拟机需要为每个实例分配独立的操作系统和资源，而容器直接利用主机系统的内核。这意味着它们消耗的资源更少，并且启动速度比虚拟机更快。多个容器可以在单个主机上同时运行，从而更有效地利用系统资源。这使得我们能够创建更高密度的工作负载——从而减少宝贵的系统资源（如
    CPU 和内存）空闲时的浪费，而在云端工作时，这种浪费就像把钱倒进水沟里！
- en: 'Now that we have a solid understanding of what a container is and how it differs
    from a VM, let’s look at the de facto tool for managing the configuration of an
    individual container: **Docker**. While this book isn’t about Docker per se, if
    you are going to be a master of Terraform and work with container-based architectures,
    you will inevitably come into contact with this tool either directly or need to
    integrate it into the **continuous integration/continuous deployment** (**CI/CD**)
    process.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对容器是什么以及它与虚拟机的区别有了清晰的理解，让我们来看看管理单个容器配置的事实标准工具：**Docker**。虽然本书的主题并非专门关于
    Docker，但如果你打算精通 Terraform 并与基于容器的架构一起工作，你不可避免地会直接接触到这个工具，或者需要将其集成到 **持续集成/持续部署**（**CI/CD**）过程中。
- en: Leveraging Docker to build container images
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 Docker 构建容器镜像
- en: The Docker engine makes the process of setting up containers much simpler. It
    provides a consistent meta-language for describing containers and command-line
    tools for building, interrogating, and running container images.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 引擎使得容器设置过程更加简单。它提供了一种一致的元语言来描述容器，并提供命令行工具来构建、查询和运行容器镜像。
- en: Writing a Dockerfile
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写 Dockerfile
- en: Docker uses a simple syntax that you can use to define basic information about
    your container. This basic structure includes what base image to build onto (`FROM`),
    who the author is (`MAINTAINER`), files to copy and commands to execute (`COPY`
    and `RUN`), and what the entry point process should be (`CMD`).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 使用一种简单的语法，你可以用它来定义容器的基本信息。这个基本结构包括构建基础镜像的指令（`FROM`）、作者（`MAINTAINER`）、要复制的文件和执行的命令（`COPY`
    和 `RUN`），以及入口点进程（`CMD`）。
- en: Much of this is similar to the structure of a Packer template, except for the
    entry point process. With Packer, it’s just a VM; whatever processes are running,
    based on how you configure, it will be running. With Docker, you need to explicitly
    state what process to start because containers run a single process in isolation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 Packer 模板的结构相似，除了入口点的过程。使用 Packer 时，它只是一个虚拟机；无论运行哪些进程，都会根据你的配置启动。而在 Docker
    中，你需要明确指出要启动哪个进程，因为容器在隔离中只运行一个进程。
- en: You can also configure the runtime further by setting the working directory,
    adding environment variables, and exposing network ports.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过设置工作目录、添加环境变量以及暴露网络端口来进一步配置运行时。
- en: 'A simple Dockerfile looks like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的 Dockerfile 看起来像这样：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that we are building from a base image called `python:3-7slim` and copying
    the current folder’s contents to the container’s `/app` directory. This step will
    copy the `app.py` script into the container so that it is available when we set
    it as the execution point at the bottom of the file. This Python script sets up
    a web server and exposes it to port `80`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在从一个名为 `python:3-7slim` 的基础镜像构建，并将当前文件夹的内容复制到容器的 `/app` 目录中。这一步将 `app.py`
    脚本复制到容器中，以便我们在文件底部设置它作为执行点时可以使用。这个 Python 脚本设置了一个 Web 服务器，并将其暴露到端口 `80`。
- en: Building a Docker image
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 Docker 镜像
- en: 'Just as with Terraform, Docker uses the current working directory to derive
    its context. Therefore, when building a Docker image, you need to execute the
    `docker build` command from the same directory where your Dockerfile resides.
    However, you can override this by specifying a different path:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 Terraform 一样，Docker 使用当前工作目录来推导其上下文。因此，在构建 Docker 镜像时，你需要从 Dockerfile 所在的同一目录执行
    `docker build` 命令。然而，你可以通过指定不同的路径来覆盖这一点：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `-t` flag lets you tag your image with a memorable name. The `.` instance
    may seem out of place, but it tells Docker to look for the Dockerfile in the current
    directory.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`-t` 标志让你为镜像打上一个易于记住的标签。` . ` 实例可能看起来不太合适，但它告诉 Docker 在当前目录中查找 Dockerfile。'
- en: 'After the build completes, you can see your image listed by running the following
    command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完成后，你可以运行以下命令查看镜像：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Running Docker images
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 Docker 镜像
- en: 'Docker images are like the VM images we built using Packer, which represent
    a VM we have yet to start. They have potential energy but need to be launched
    as the operating system disk of a VM to achieve kinetic energy and become a running
    VM. Docker images are the same for containers. We need to start a container using
    the image and specify the runtime configuration:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 镜像就像我们使用 Packer 构建的虚拟机镜像，它们代表一个尚未启动的虚拟机。它们有潜在的能量，但需要作为虚拟机的操作系统磁盘启动，才能转化为动能，变成一个运行中的虚拟机。Docker
    镜像对于容器也是一样的。我们需要使用镜像启动一个容器，并指定运行时配置：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this case, because we exposed port `80` in the container, we need to map
    a port to the container’s port `80`. The `-p` flag maps a network port inside
    the container to a port on the host machine. This setting will route traffic from
    port `4000` on the host to port `80` on the container.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于我们在容器中暴露了端口`80`，我们需要将一个端口映射到容器的端口`80`。`-p`标志将容器内部的网络端口映射到主机机器的端口。这个设置将把主机的`4000`端口的流量路由到容器的`80`端口。
- en: You can run as many containers as your host machine can handle. You are constrained
    only by the technical resources of the host machine. Sometimes, the cloud platform
    imposes constraints depending on what SKU of VM your host machine is running.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行任意数量的容器，前提是主机机器能够承载。你唯一的限制是主机机器的技术资源。有时，云平台会根据你主机所运行的虚拟机 SKU 强加一些限制。
- en: 'To see which containers are running, you can execute the following Docker command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看正在运行的容器，你可以执行以下 Docker 命令：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This section should help you understand the basic principles of working with
    Docker images. While there are many more commands and flags you can use with Docker
    to manage your images and containers, this is out of the scope of this book. I’m
    providing you with enough theory and practice to be productive in building container-based
    architectures using Terraform.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容应帮助你理解与 Docker 镜像合作的基本原理。虽然 Docker 提供了许多其他命令和标志来管理你的镜像和容器，但这些超出了本书的范围。我将为你提供足够的理论和实践，使你能够高效地构建基于容器的架构，并使用
    Terraform 进行管理。
- en: 'In this section, we familiarized ourselves with the command-line tool used
    to create container images: Docker.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们熟悉了用于创建容器镜像的命令行工具：Docker。
- en: In the next section, we’ll look at how to publish these container images that
    we create with Docker to container registries so that we can deploy containers
    with them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨如何将我们使用 Docker 创建的容器镜像发布到容器注册表，以便我们可以使用它们部署容器。
- en: Working with container registries
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与容器注册表合作
- en: A **container registry** is just a server-side application that acts as central
    storage and allows you to distribute container images to the host machines that
    need to run them. This approach is advantageous when leveraging a CI/CD pipeline
    where you need a central location to pull down your container images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器注册表**只是一个服务器端应用，作为中央存储并允许你将容器镜像分发到需要运行这些镜像的主机机器。采用这种方法在利用 CI/CD 流水线时非常有优势，因为你需要一个中央位置来拉取容器镜像。'
- en: They often provide versioning, labeling, and sharing mechanisms that let you
    keep track of different versions of your container images, maintain stable releases,
    and share images with others—either within your organization or publicly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通常提供版本控制、标签和共享机制，允许你跟踪不同版本的容器镜像、保持稳定的发布版本，并与他人共享镜像——无论是在你的组织内部，还是公开共享。
- en: 'Just as with `git`, anybody can set up a container registry on their own, but
    several managed services provide best-in-class service offerings on each of the
    respective clouds. There is also a cloud-agnostic and community-oriented solution:
    Docker Hub. Docker Hub is the default registry where Docker looks for images,
    and you can use it for both images you want to share publicly or keep private
    for internal purposes. It offers a free tier and paid plans with more storage
    and features.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`git`一样，任何人都可以自行设置容器注册表，但多个托管服务在各自的云平台上提供最佳服务。此外，还有一个云无关且面向社区的解决方案：Docker
    Hub。Docker Hub 是 Docker 查找镜像的默认注册表，你可以用它来共享你希望公开的镜像，或将其私密用于内部目的。它提供免费套餐以及更多存储和功能的付费套餐。
- en: Docker Hub
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker Hub
- en: The mechanics of interacting with a container registry are broadly similar depending
    on the service—with only slight variations. As an example, because it is the default
    container registry that Docker uses, I’ll show you how to use **Docker Hub** to
    authenticate, tag, push, and pull your images.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与容器注册表交互的机制大体相似，具体取决于所使用的服务——只有少许差异。举个例子，因为它是 Docker 默认使用的容器注册表，我将展示如何使用**Docker
    Hub**进行身份验证、标记、推送和拉取镜像。
- en: 'First, you need to authenticate. Depending on your registry service, this step
    might require additional tools. However, you won’t need to install any other tools
    for Docker Hub but, naturally, you will need to register an account on Docker
    Hub:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要进行身份验证。根据你的注册表服务，这一步可能需要额外的工具。然而，使用 Docker Hub 时，你不需要安装其他工具，但你需要在 Docker
    Hub 上注册一个帐户：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding command will initiate an interactive login process where you must
    supply your Docker Hub username and password.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将启动一个交互式登录过程，你需要提供你的 Docker Hub 用户名和密码。
- en: 'Before you can push your image to a registry, you must tag it with the registry’s
    address:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在将镜像推送到注册表之前，你必须用注册表的地址对其进行标记：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding command first specifies the `my-image` source image of a specific
    version, `1.0`. Then, it specifies a target image under my `markti` Docker Hub
    account for the same image and version. It’s crucial to synchronize the image
    name and version between your local and remote environments to maintain consistency
    between the environments. After your image is tagged, you can push it to the registry:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令首先指定了特定版本`1.0`的`my-image`源镜像。然后，它指定了同一镜像和版本在我的`markti` Docker Hub账户下的目标镜像。保持本地和远程环境之间的镜像名称和版本同步对于确保环境的一致性至关重要。在镜像标记完成后，你可以将其推送到注册表：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding command pushes the image to the remote container registry. Now,
    you can pull the image with the appropriate permissions using your Docker Hub
    username as the registry name, the container image name, and the tag:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将镜像推送到远程容器注册表。现在，你可以使用合适的权限，通过使用 Docker Hub 用户名作为注册表名称、容器镜像名称和标签来拉取镜像：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Remember that container registries might have slightly different naming conventions
    and authentication processes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，容器注册表可能会有稍微不同的命名规范和认证流程。
- en: In this section, we looked at how to work with container registries, which serve
    as a critical infrastructure for our container-based architecture. In the next
    section, we’re ready to look at Kubernetes—both from an architectural standpoint
    and at its practical usage as a developer, operator, and within CI/CD pipelines.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用容器注册表，它们是容器架构中至关重要的基础设施。在下一节中，我们将准备好从架构角度和开发者、运维人员以及 CI/CD 流水线中的实际使用来学习
    Kubernetes。
- en: Understanding key concepts of container orchestration and Kubernetes
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解容器编排和 Kubernetes 的关键概念
- en: '**Kubernetes** is a platform that expands on the responsibilities of the container
    runtime, which operates at an individual host level. Kubernetes’ job is to perform
    this across multiple nodes. As we learned in the first section of this chapter,
    the container runtime uses a Linux operating system construct—control groups—to
    protect the health of the operating system by ensuring that the physical (or virtual)
    host that the containers are running on remains healthy. Kubernetes essentially
    does the same thing but across many, many servers.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes** 是一个扩展容器运行时职责的平台，容器运行时在单个主机级别操作。Kubernetes的工作是跨多个节点执行这一任务。正如我们在本章第一节中学到的，容器运行时使用
    Linux 操作系统构造——控制组——通过确保容器运行的物理（或虚拟）主机保持健康，从而保护操作系统的健康。Kubernetes 基本上做的就是同样的事情，只不过是跨多个服务器进行。'
- en: Most applications or systems will naturally be organized into different components,
    layers, or microservices—each with its own responsibilities and corresponding
    application code and technology stack that implements its functionality. Each
    component within such a system will have its own container that has this software
    installed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数应用程序或系统通常会被组织成不同的组件、层或微服务——每个组件都有自己的责任、相应的应用程序代码和技术栈来实现其功能。系统中的每个组件都会有自己的容器，里面安装了相应的软件。
- en: When we deploy systems using VMs, we do so in such a way that the same component
    is deployed to two more VMs, and we ensure that these VMs do not share the same
    underlying physical equipment. This separation could be as simple as a different
    physical host in the same rack, all the way up to a different physical host in
    an entirely different data center—sometimes separated by many tens, if not hundreds,
    of miles. This allows us to achieve **high availability** (**HA**) and resiliency
    during an outage or an issue affecting some underlying component of the physical
    hardware.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用虚拟机（VM）部署系统时，我们会以这样的方式部署：同一个组件会部署到两个或更多的虚拟机上，并确保这些虚拟机不共享相同的底层物理设备。这种隔离可以是简单的不同物理主机在同一机架上，甚至可以是完全不同数据中心中的不同物理主机——有时这些主机之间可能相距数十，甚至数百英里。这使得我们能够在发生故障或影响底层硬件组件的问题时，实现**高可用性**（**HA**）和恢复力。
- en: Unlike when using VMs, our application components don’t sit on isolated VMs;
    they sit on the cluster nodes, oftentimes with pods from other applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用虚拟机时不同，我们的应用程序组件并不是在独立的虚拟机上运行，而是部署在集群节点上，通常与来自其他应用程序的 pod 一起运行。
- en: 'Kubernetes tries to make sure that our application containers don’t sit on
    the same node. That way, if one of the cluster’s nodes fails, our application
    will not go down. Kubernetes also takes it a step further by intelligently reorganizing
    the containers on other health nodes. In order to do this, Kubernetes maintains
    a divide between its own internal **logical layer** and the underlying **physical
    layer** and maps the device by assigning logical deployments, or pods, to physical
    deployments and nodes. This separation between the logical and the physical layers
    is one of Kubernetes’ huge advantages and what makes it so effective at managing
    applications and services on top of a potentially unlimited underlying physical
    infrastructure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 尝试确保我们的应用程序容器不会部署在同一节点上。这样，如果集群中的某个节点发生故障，我们的应用程序就不会宕机。Kubernetes
    还进一步智能地将容器重新安排到其他健康节点上。为了实现这一点，Kubernetes 在其内部**逻辑层**和底层**物理层**之间保持分隔，并通过分配逻辑部署或
    pod 到物理部署和节点来映射设备。这种逻辑层与物理层的分离是 Kubernetes 的巨大优势之一，也是它在管理应用程序和服务时能在潜在的无限物理基础设施之上发挥如此高效的原因。
- en: '![Figure 5.1 – Logical-physical divide](img/B21183_05_1.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 逻辑-物理分离](img/B21183_05_1.jpg)'
- en: Figure 5.1 – Logical-physical divide
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 逻辑-物理分离
- en: That’s pretty much it, but there are a lot of ways we can customize how our
    application’s components are deployed to Kubernetes to meet the specific needs
    of our application.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容，但我们可以通过多种方式自定义应用程序组件在 Kubernetes 上的部署，以满足我们应用程序的特定需求。
- en: Kubernetes is flexible enough to run on a fleet of VMs on a cloud provider or
    physical bare-metal servers down to running on a single computer—such as your
    laptop. This flexibility makes it an ideal choice for hybrid cloud scenarios.
    It streamlines the problematic task of integration testing by allowing developers
    to run a copy of the entire solution on their laptop that closely mimics a production
    environment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 足够灵活，能够在云提供商的虚拟机群或物理裸金属服务器上运行，甚至可以在单台计算机上运行——例如你的笔记本电脑。这种灵活性使它成为混合云场景的理想选择。它通过允许开发人员在本地运行整个解决方案的副本，轻松模拟生产环境，从而简化了集成测试的繁琐工作。
- en: Kubernetes offers a rich set of features that fulfill most of the needs for
    running workloads at scale, such as service discovery, secrets management, horizontal
    scaling, automated rollouts and rollbacks, and self-healing capabilities—making
    it an ideal candidate to run both stateless and stateful applications at scale
    while avoiding vendor lock-in.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了丰富的功能集，满足大部分大规模运行工作负载的需求，例如服务发现、密钥管理、水平扩展、自动化发布和回滚，以及自愈能力——使其成为运行无状态和有状态应用程序的大规模解决方案，同时避免供应商锁定的理想候选。
- en: Kubernetes architecture is a set of loosely coupled and extensible components.
    This modularity allows adaptations for different cloud providers to integrate
    with their specific solutions for networking, storage, service mesh, and so on.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 架构由一组松散耦合且可扩展的组件构成。这种模块化设计使得可以根据不同的云提供商进行适配，整合它们特定的网络、存储、服务网格等解决方案。
- en: As with Terraform, Google designed Kubernetes to encourage the adoption of **infrastructure
    as code** (**IaC**) by leveraging a declarative approach for defining your application’s
    runtime environment. Due to the extensibility of both Terraform and Kubernetes,
    several integration options exist. In this chapter, we’ll discuss a few of those
    approaches and trade-offs that come along with each—but before we do that, we
    need to introduce some critical concepts of Kubernetes’ internal architecture
    and operating model. Only with this foundation can we maximize the potential of
    leveraging Terraform and Kubernetes together.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Terraform 一样，谷歌设计 Kubernetes 的目的是通过采用声明式方法来鼓励**基础设施即代码**（**IaC**）的使用，以定义应用程序的运行时环境。由于
    Terraform 和 Kubernetes 都具有扩展性，因此存在多种集成选项。在本章中，我们将讨论其中的一些方法及其伴随的权衡——但在此之前，我们需要介绍
    Kubernetes 内部架构和操作模型的一些关键概念。只有在掌握这些基础知识之后，我们才能最大化地发挥 Terraform 和 Kubernetes 联合使用的潜力。
- en: Kubernetes architecture
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 架构
- en: Kubernetes is a distributed software system, and its design is relatively similar
    to that of other such systems. Because its responsibility spans a cluster of interconnected
    computer systems that can scale from just a few to literally thousands, it is
    organized like an army. There are officers, soldiers, and a central command. The
    soldiers are organized into smaller sub-groups, and each needs to maintain continuous
    contact with the central command in order to operate effectively by receiving
    new orders and providing the status of the current situation. The central commands
    receive status reports from the various officers that oversee their soldiers and
    operating orders and determine whether different areas of the battlefield need
    more troops or fewer troops, issuing orders to reallocate different sub-groups
    of the soldiers to different locations across the battlefield. Let’s dive into
    each component and their roles.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个分布式软件系统，它的设计与其他类似系统相似。因为它的责任范围涵盖一个由互联计算机系统组成的集群，这个集群的规模可以从几个节点扩展到几千个节点，所以它的组织方式就像一支军队。里面有军官、士兵和中央指挥部。士兵们被分成较小的子群体，每个子群体都需要与中央指挥部保持持续的联系，以便通过接收新的命令和提供当前情况的状态来有效地运作。中央指挥部接收来自各个军官的状态报告，这些军官负责监管他们的士兵以及执行命令，判断是否需要在不同战区增兵或减兵，并下达命令将不同的士兵小组重新部署到战场的不同位置。让我们深入了解每个组件及其角色。
- en: Master node
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主节点
- en: 'The **master node** is the central command of the Kubernetes cluster—it’s essentially
    where the generals of the army operate. For smaller skirmishes, there is typically
    only one central command, but for truly epic entanglement, you might need more
    than one for each theatre of war. It oversees the entire system and makes high-level
    decisions. As with any good central command, it must perform several functions:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**主节点**是Kubernetes集群的中央指挥——它本质上就是军队的将军们所在的地方。对于较小的冲突，通常只有一个中央指挥，但对于真正史诗般的交战，你可能需要为每个战区配备多个指挥部。它负责监管整个系统并做出高层决策。像任何好的中央指挥部一样，它必须执行几个重要职能：'
- en: '**API server**: Any army must take input from its civilian government, which
    provides objectives to complete and defines what success looks like. In many ways,
    this is very much like the role of the API server. Instead of taking input from
    politicians via that red telephone, it takes input from the end user (usually
    a system administrator or software developer) over a REST-based interface. The
    definition of success looks a bit different as well, which is the definition of
    how the end user’s applications and services should be deployed and how to tell
    if they are healthy.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API服务器**：任何军队都必须接收来自其民间政府的指令，政府为其提供任务目标并定义什么才算成功。从某种程度上讲，这与API服务器的角色非常相似。它不像通过红色电话从政治家那里获取指令，而是通过基于REST的接口从最终用户（通常是系统管理员或软件开发者）处获取输入。成功的定义也有所不同，这个定义就是最终用户的应用程序和服务应该如何部署，以及如何判断它们是否健康。'
- en: '**Controller manager**: Napoleon Bonaparte famously said, “*An army marches
    on its stomach*,” which highlights the importance of sound logistics when waging
    war. An army is more than just boots and guns. You need food and water, uniforms
    and tents, and fuel for your trucks and trains. The controller manager performs
    a similar function as it is responsible for monitoring inventories and distributing
    resources so that the desired state of the army is maintained and they are empowered
    to accomplish their mission.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器管理器**：拿破仑·波拿巴曾经名言：“*一支军队依赖粮草*”，这突显了在战争中良好后勤的重要性。一支军队不仅仅是靴子和枪支，还需要食物、水、制服和帐篷，甚至卡车和火车的燃料。控制器管理器执行的功能类似，它负责监控资源库存并分配资源，以确保军队的理想状态得以维持，并且能够执行任务。'
- en: '**Scheduler**: Our very own George Washington famously said “*Discipline is
    the soul of an army*”—and to enforce that discipline, an army must have an officer
    corps that efficiently executes orders across the field of battle, assigning soldiers
    across the battlefield to where they are most needed. In this sense, it assigns
    pods to appropriate nodes based on resource availability and the objective to
    be accomplished.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**：我们的乔治·华盛顿曾名言：“*纪律是军队的灵魂*”——为了加强纪律，一支军队必须有一个有效的指挥系统，能够在战场上执行命令，合理分配士兵到最需要的地方。从这个意义上讲，调度器根据资源的可用性和任务目标的不同，将容器组（pods）分配到合适的节点上。'
- en: '`etcd` plays this role in Kubernetes by maintaining configuration data, the
    state of the cluster, and creating a **single source of** **truth** (**SSOT**).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd` 在 Kubernetes 中扮演这一角色，通过维护配置数据、集群状态，并创建一个**单一真实数据源**（**SSOT**）。'
- en: Worker nodes
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作节点
- en: '**Worker nodes** are the battlefields where the soldiers of this army do what
    must be done to achieve the objective. They are the physical (or virtual) machines
    where your containers run. On any battlefield, there must be a sergeant who commands
    a squad of soldiers. The sergeant of Kubernetes is called the **Kubelet**. As
    with a sergeant, the Kubelet is autonomous within its area of the battlefield,
    executing orders received from central command and commanding the troops within
    their squad—the pods—and it maintains the chain of command with its superiors
    at central command—or the master node—that might delegate new orders.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作节点**是这个军队的战场，士兵们在这里完成必须执行的任务，以实现目标。它们是物理（或虚拟）机器，容器就在这些机器上运行。在任何战场上，都必须有一名指挥士兵的小队长。在
    Kubernetes 中，小队长被称为 **Kubelet**。像小队长一样，Kubelet 在其战场区域内具有自主性，执行从指挥部接收到的命令，并指挥其小队——Pods——同时它保持与上级指挥部（或主节点）的指挥链，接收并执行新的指令。'
- en: The containers running within the node, being monitored by their attentive sergeant,
    the Kubelet, need a container runtime in order to operate. There are several different
    container runtimes, such as `containerd`, **CRI-O**, or Docker, which we learned
    about in the first section of this chapter. Although there are many container
    runtimes, we still use the same tooling—Docker—to build images. The runtime is
    really only responsible for running the containers. There are some other details
    to it, and it’s definitely a rabbit hole, but this is what we need to know within
    the context of this book.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 节点中运行的容器在 Kubelet 的精心监控下需要一个容器运行时来运行。有多种容器运行时，例如 `containerd`、**CRI-O** 或 Docker，我们在本章的第一部分已经了解过它们。虽然有许多容器运行时，我们仍然使用相同的工具——Docker——来构建镜像。运行时实际上只负责运行容器，虽然其中有一些其他细节，绝对是一个深入的领域，但在本书的背景下，这就是我们需要了解的内容。
- en: With soldiers distributed across an expansive battlefield, there needs to be
    a way for messages to be sent back and forth between the soldiers, their officers,
    and the central command. On the battlefield, this has changed throughout history
    from flags, banners, smoke signals, drums, horns, and bugles to modern times with
    telegraph, radio, and satellite communication. For the pods, this is the network
    traffic that is being routed to the node. The **kube-proxy**, as with the Kubelet,
    runs on every node and is responsible for routing network traffic to the correct
    destination.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在战场上，士兵们分布广泛，因此需要有一种方式将信息在士兵、指挥官和指挥部之间传递。在战场上，这种方式随着历史的变化从旗帜、横幅、烟雾信号、鼓声、号角到现代的电报、电台和卫星通信不断演变。对于
    Pods 来说，这就是被路由到节点的网络流量。**kube-proxy** 就像 Kubelet 一样，运行在每个节点上，负责将网络流量路由到正确的目的地。
- en: Pods
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pods
- en: That’s enough about the big hats. It’s time to talk soldiers. A soldier is the
    smallest participant on the battlefield, and soldiers, collectively, are the primary
    force in military operations. The same is true for pods within the context of
    Kubernetes. **Pods** are where all the work actually happens. Everything else
    going on inside a Kubernetes cluster is to facilitate the effectiveness of pods
    in achieving their individual objectives, much like the myriad of characters that
    support our frontline troopers on the battlefield by making sound strategic decisions,
    allocating resources, organizing soldiers into units, and assigning orders.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多关于“大人物”的事情，接下来该谈谈士兵了。士兵是战场上最小的参与者，而士兵们共同构成了军事行动中的主要力量。在 Kubernetes 中，Pods
    就是这一角色。**Pods** 是所有工作实际发生的地方。集群中的其他一切运作，都是为了支持 Pods 达成其各自的目标，就像战场上许多角色通过制定明智的战略决策、分配资源、组织士兵成单位并下达命令，支持我们前线的战士一样。
- en: A pod is not a container but a Kubernetes-specific construct and, as with the
    soldier, the smallest unit of deployment within a cluster. A pod can have one
    or more containers inside of it that share resources and configurations to perform
    a common objective.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 不是容器，而是 Kubernetes 特有的构造，就像士兵一样，它是集群中最小的部署单元。一个 Pod 可以包含一个或多个容器，这些容器共享资源和配置，执行共同的目标。
- en: Instead of directly deploying individual containers, you create a pod and place
    containers within it. When you declare more than one container within the same
    pod, you are tightly coupling them together—in that they share the same network
    namespace, **inter-process communication** (**IPC**), namespace, and filesystem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与其直接部署单个容器，不如创建一个 Pod 并将容器放入其中。当你在同一个 Pod 内声明多个容器时，你实际上是在将它们紧密地绑定在一起——因为它们共享相同的网络命名空间、**进程间通信**（**IPC**）、命名空间和文件系统。
- en: 'The following diagram illustrates these core components of Kubernetes architecture:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Kubernetes 架构的核心组件：
- en: '![Figure 5.2 – Key Kubernetes architectural components](img/B21183_05_2.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 关键的 Kubernetes 架构组件](img/B21183_05_2.jpg)'
- en: Figure 5.2 – Key Kubernetes architectural components
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 关键的 Kubernetes 架构组件
- en: Now that we understand the core components of the architecture, we’ll delve
    into a couple of other important topics. I do want to call out that this book
    is about mastering Terraform, and while part of that journey is understanding
    the architectures that you will be designing and provisioning with Terraform,
    this book does not intend to be an in-depth guide to Kubernetes. Hence, I am focusing
    on just the key concepts that you need to be aware of when building solutions
    with these technologies using Terraform.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了架构的核心组件，接下来将深入探讨其他几个重要主题。我想特别指出，本书的核心是掌握 Terraform，尽管这一过程的一部分是了解你将要设计和配置的架构，但本书并不打算成为
    Kubernetes 的深入指南。因此，我将重点关注在使用 Terraform 构建解决方案时需要了解的关键概念。
- en: Services
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务
- en: For more complex military operations, we may need to allocate a larger military
    unit to complete the mission successfully. This is where we have a lieutenant
    that would command multiple squads. The lieutenant delegates orders to the appropriate
    squads, with each deployed to a different area on the battlefield. This is similar
    to the role of a **service** in Kubernetes, which allows us to group pods together
    with a common purpose and distribute them across multiple nodes. The service is
    responsible for load balancing across pods, and any incoming requests intended
    for those pods would be addressed to the service to route accordingly—much like
    how orders from a captain or higher in the chain of command would be delegated
    down to a lieutenant, and they would take the necessary steps to dole them out
    to the squads under their command.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的军事行动，我们可能需要分配更大的军事单位来完成任务。这就像是有一个中尉指挥多个小队。中尉将命令下达给适当的小队，每个小队被部署到战场的不同区域。这与
    Kubernetes 中的 **服务** 角色类似，它允许我们将多个 Pod 组在一起，赋予它们共同的目标，并分布到多个节点上。服务负责在 Pod 之间进行负载均衡，任何针对这些
    Pod 的传入请求都会首先被路由到服务，就像上级指挥官下达的命令通过中尉传达给小队，后者再将命令分配给自己指挥的小队一样。
- en: In this way, the service plays a crucial role in workloads that require a stable
    endpoint for communicating with pods, such as a web application or a REST API.
    This is because Kubernetes assigns a stable IP address and DNS name to the service,
    which remains unchanged even if the underlying pods change, enabling other applications
    or services within or outside the cluster to establish a reliable connection with
    the service.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，服务在需要与 Pod 进行稳定通信的工作负载中起着至关重要的作用，例如 Web 应用或 REST API。因为 Kubernetes 会为服务分配一个稳定的
    IP 地址和 DNS 名称，即使底层的 Pod 发生变化，这些信息也保持不变，从而使得集群内外的其他应用或服务能够与该服务建立可靠的连接。
- en: Namespaces
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名空间
- en: 'Lastly, we need to cover an important concept of Kubernetes’ logical model:
    the **namespace**. The namespace provides complete separation from all services
    and pods deployed within the cluster at the logical level. Namespaces do not apply
    to the physical resources of clusters, such as nodes or persistent volumes. They
    only apply within the logical realm of Kubernetes as it relates to pods and other
    related resources. You can think of it as branches within the military. Resources
    in different namespaces, as with soldiers in different branches of the army, share
    a central command, and they can communicate and coordinate with each other, but
    they are isolated in terms of chain of command and resource allocation. Therefore,
    pods in different namespaces can operate on the same nodes but can’t coexist in
    the same service since that, too, has a namespace.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered the key components of Kubernetes architecture. There is definitely
    a lot more that is out of the scope of this book, but this should give you enough
    of the conceptual overhead to understand Kubernetes architecture at a high level.
    Next, we’ll delve a bit deeper into some of the resources that are used to configure
    pods and services.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Configuration and secrets
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key areas where Terraform and Kubernetes will likely interact is
    the area of configuration and secrets. This is because, quite often, Terraform
    is provisioning other resources that will supply endpoint URLs, authentication
    credentials, logging, or identity configuration. Therefore, it’s important to
    understand which Kubernetes resources should be used to connect these configuration
    settings to the appropriate place in your Kubernetes deployments.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **ConfigMap** is a special kind of Kubernetes resource that can be used to
    provide non-sensitive configuration to a pod. The configuration is stored as a
    set of key-value pairs, which can be used to configure either environment variables
    for containers or command-line arguments for an application that you want to run
    inside the container.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: A pod can reference one or more ConfigMap objects, and then the application
    can reference the keys in the key-value pairs to obtain their values. This creates
    a separation of the application, which is running in the pod, from the configuration,
    which is stored in a ConfigMap. This means that the same ConfigMap can be used
    by more than one pod specification.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: By default, only other pods within the same namespace can access ConfigMaps.
    If you want more granular security, you can apply for **role-based access** **control**
    (**RBAC**).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While Kubernetes does have an internal method for storing secrets and making
    them available to your pods, when you are deploying to the cloud, you will often
    use a cloud-specific secret provider instead. There are a number of advantages
    to leveraging an external secret store. First, with an external secret store,
    you would have more centralized management, which would make it easier for operators
    to manage the environment. Second, most external secret providers offer features
    and capabilities that the built-in secret storage in Kubernetes doesn’t have,
    such as the ability to version and rotate secrets. Lastly, offloading secret storage
    reduces the burden on the `etcd` database on the cluster, thus freeing up more
    resources for workloads running in your pods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: When you leverage an external secret store, Terraform will likely be provisioning
    it along with the secrets that your pods will need. In order to take advantage
    of an external secret store, you will need to provision a `SecretProviderClass`
    resource that is specific to the external secret store you plan on using. It will
    provide a bridge between your pods and the secrets you store there. There are
    often platform-native configurations depending on the cloud platform you are using
    to configure this provider. Most managed Kubernetes service offerings provide
    built-in support for the corresponding secret storage service and streamline the
    authentication and authorization required for your pods to access secrets.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we will be working with the Managed Kubernetes offerings of three
    cloud platforms: Amazon **Elastic Kubernetes Service** (**EKS**), **Azure Kubernetes
    Service** (**AKS**), and **Google Kubernetes** **Engine** (**GKE**).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Continuous deployment (CD)
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes has a multitude of ways to provision resources. It has both imperative
    and declarative covered with the `kubectl` command-line tool and Kubernetes YAML
    manifests (which also use the `kubectl` command-line tool) respectively. Because
    this is a book on Terraform, I think it’s clear the approach we would prefer!
    Yes—declarative! And because Kubernetes also has its own REST API, it’s possible
    to build a Terraform provider that communicates with it as well. All of these
    approaches, using `kubectl` either with imperative commands or YAML manifests
    or using `terraform` and the `kubernetes` Terraform provider, are examples of
    the traditional push model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Push model
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `kubectl` commands, either just plain old `bash` or YAML manifest files,
    using `kubectl apply -``f foo.yaml`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – CI/CD pipeline with Terraform and Kubernetes command-line interface](img/B21183_05_3.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – CI/CD pipeline with Terraform and Kubernetes command-line interface
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the cloud environment is defined in `kubectl` is executed to create
    deployments on the newly created or existing Kubernetes cluster. The Kubernetes
    cluster’s existence will depend on whether it was the first time `terraform apply`
    was executed or not.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The next method is to use Terraform for both of these stages, replacing the
    `kubectl` stage with a second Terraform stage, this time using a second Terraform
    root module that only uses the Kubernetes provider for Terraform. The Terraform
    root module that provisioned the cloud environment stays in its own folder and
    is completely isolated from this second Terraform code base:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – CI/CD pipeline with Terraform using the Kubernetes provider
    for Terraform](img/B21183_05_4.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – CI/CD pipeline with Terraform using the Kubernetes provider for
    Terraform
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The first Terraform stage still uses our target cloud platform’s Terraform provider
    to provision the Kubernetes cluster and other required resources within our cloud
    environment. Likewise, the CI/CD pipeline still passes the Kubernetes cluster
    configuration that is output from this first Terraform stage to the second Terraform
    stage where we provision Kubernetes resources to our Kubernetes cluster using
    the Kubernetes provider for Terraform.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Pull model
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An alternative to the push model is the **pull model**, which flips things
    upside down. Instead of the Kubernetes resources being provisioned by some actor
    outside of the Kubernetes cluster itself, the CI/CD pipeline installs a CD service
    on the cluster, and this service connects to a specified source code repository
    containing Kubernetes YAML manifests and provisions the resources on the Kubernetes
    cluster:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – CI/CD pipeline with Terraform and ArgoCD](img/B21183_05_5.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – CI/CD pipeline with Terraform and ArgoCD
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: This approach takes advantage of the immutable and declarative aspects of YAML-based
    Kubernetes deployments and creates an SSOT for a Kubernetes deployment within
    a Git source code repository. As a result, this approach has become more and more
    identified as a best practice when it comes to fully embracing GitOps, which we’ll
    delve into more detail in the next chapter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we took a high-level look at Kubernetes—what purpose it serves,
    how it works, and how it can interconnect our containers with the underlying infrastructure
    that we provision. These are all critical things to understand as we use Terraform
    to provision and manage the Kubernetes infrastructure that we’ll use to run our
    containers. Next, let’s look at how Kubernetes natively handles deployments before
    we contrast that with what we can do with Terraform’s Kubernetes providers.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Kubernetes manifests
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in the previous section, `kubectl` is a command-line application
    that can be used to either imperatively or declaratively execute commands on a
    Kubernetes cluster. You can use `kubectl` to deploy resources and inspect and
    manage cluster resources, among other common operational activities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes manifests
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deploying resources to a Kubernetes cluster, you can either use `kubectl`
    commands directly to perform operations to provision resources or use YAML manifests
    to define the desired state of resources and use `kubectl` to execute against
    these manifests. These two different ways of using `kubectl` parallel the way
    there are imperative ways to provision resources to cloud platforms such as AWS
    and Azure through their respective command-line applications and the way Terraform
    provisions the desired state of resources during `terraform apply`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’re using `kubectl` commands directly, you’re giving instructions right
    away in the command line. For example, if you want to create a deployment, you
    might issue a command such as this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, `kubectl` will create a deployment for `nginx` with mostly default
    settings, and it will do so immediately.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: This method can be useful for quick, one-off creations or when you need to make
    an immediate change.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'When using YAML manifests, you’re writing the desired state of your resources
    in a declarative manner. For example, a deployment might be written like this
    in a YAML file:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You would then use `kubectl` to apply this file, like so:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This tells Kubernetes to make the cluster’s actual state match the desired state
    described in the file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this approach is that the file serves as a **source of truth**
    (**SOT**) for the resource configuration. The files can be version-controlled,
    making it easy to track changes, roll back if needed, and reuse configurations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Generally, it’s considered a best practice to manage your Kubernetes resources
    using configuration files, especially in production environments. That being said,
    direct `kubectl` commands are useful for debugging and quick prototyping tasks,
    but you should consider using a declarative approach to manage resources in the
    long term.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Deployment manifest
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When creating an application in Kubernetes, you use a deployment to specify
    how you want it to be configured. Kubernetes will then automatically adjust the
    current state of the application to match your desired configuration:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This deployment manifest describes a desired state that includes running three
    instances (or replicas) of the `my-app` application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Service manifest
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **service** is a method of grouping a collection of pods that form an application,
    allowing them to be presented as a network service:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This service manifest will create a network service that will route traffic
    to the `my-app` pods on port `8080`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Configuration and secrets
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because Kubernetes is where we will host our applications and services, we need
    to have a way to provide runtime configuration settings, both non-sensitive and
    secret.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in the previous section, a ConfigMap is how we pass non-sensitive
    data into our pods. The ConfigMap is a key area where Terraform and Kubernetes
    integration takes place because many of the configuration settings are likely
    generated by Terraform. This is an important consideration when designing how
    you provision to Kubernetes, as you want to minimize the manual steps required
    to provision to Kubernetes. We’ll look at strategies on how to avoid this in future
    sections covering Kubernetes and Helm providers:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This ConfigMap is named `my-config`, and it holds a key-value pair of `my-value:`
    `Hello, Kubernetes!`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, when we want to reference this ConfigMap from one of our deployments,
    we simply use the `configMapRef` block to pull in the correct value from the ConfigMap
    and set an environment variable inside our container:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this deployment, the `my-app` application has a `MY_VALUE` environment variable
    whose value is pulled from the `my-config` ConfigMap, and when the pod is running,
    it can get a `Hello, Kubernetes!` value from that environment variable.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as with the non-sensitive configuration settings, many of our secrets will
    be provisioned by Terraform using the target cloud platform’s secret management
    service. As a result, we won’t be using the Kubernetes `Secret` resource but will
    be defining a `SecretProviderClass` resource that will enable integration with
    the cloud platform’s secret management service and pull in the desired secrets.
    Because this is cloud platform-specific, we’ll cover this in more detail in each
    of the solutions we build on AWS, Azure, and GCP, using their respective managed
    Kubernetes offerings.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at how Kubernetes handles deployments natively—both
    using its own `kubectl` command-line utility and its own YAML-based deployment
    manifests, which allow us to describe Kubernetes resources we want to provision
    in a declarative way—similar to what Terraform allows us to do with the underlying
    cloud infrastructure. In the next section, we’ll look at the Kubernetes provider,
    which gives us a way of managing Kubernetes natively using Terraform.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Using the Kubernetes provider to provision Kubernetes resources
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes provider for Terraform is a plugin that allows Terraform to manage
    resources on a Kubernetes cluster. This includes creating, updating, and deleting
    resources such as deployments, services, and pods.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: When using the Kubernetes Terraform provider, your infrastructure description
    is written in HCL instead of YAML. This is the language used by Terraform to describe
    infrastructure and service configurations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes Terraform provider
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in the previous section, because Kubernetes has a REST API that
    acts as a uniform control plane for all management operations, it’s possible to
    create a Terraform provider that we can use to automate it in the same fashion
    that we do with the AWS, Azure, and GCP cloud platforms.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Just as with other cloud platforms, we need to authenticate against the control
    plane. One big difference with Kubernetes is that the management control plane
    is hosted on the Kubernetes cluster itself—more specifically, as we discussed
    in the *Understanding key concepts of container orchestration and Kubernetes*
    section of this chapter, on the master node. This means we need to specify the
    endpoint address of the Kubernetes cluster. This is usually provided by the Terraform
    resource that provisions the Kubernetes cluster on the target cloud platform.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: In order to authenticate with the Kubernetes cluster, we need to typically use
    a cluster certificate, but some cloud platforms support more sophisticated authentication
    methods that tie into your organization’s directory systems such as Microsoft
    Entra ID.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of what the provider configuration would typically look
    like when using certificate-based authentication:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here’s what each field is for:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '`host`: The hostname (in the form of URI) of the Kubernetes master. It can
    be sourced from the `KUBE_HOST` environment variable.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`client_certificate`: This is used for client authentication against the Kubernetes
    REST API.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`client_key`: This is paired with `client_certificate` and is used as part
    of the **Transport Layer Security** (**TLS**) handshake that happens between the
    Terraform provider and the Kubernetes REST API.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster_ca_certificate`: This is the **certificate authority** (**CA**) for
    the Kubernetes cluster and is used to verify the authenticity of the Kubernetes
    cluster’s REST API.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another common method for configuring the Kubernetes provider for Terraform
    is to use a `kube_config` file:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this situation, all of the details needed to connect and authenticate with
    the cluster are stored within the file. We just need to point the provider at
    the location where the file exists. By default, this location is `~/.kube/config`.
    Of course, this file can contain multiple cluster connections, each referred to
    as a *context*. Therefore, we may need to specify the context. However, if you
    are running in a CI/CD pipeline, this is very unlikely because you will likely
    use a custom path.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes resources
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you use the Kubernetes provider for Terraform, we get the same declarative
    model that we get with Kubernetes’ native YAML manifests, but we get all the features
    and capabilities of HCL. This allows us to pass input variables, generate dynamic
    local values, and use string interpolation—the works!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the downside of all this is that we have to use HCL to define Kubernetes
    resources. This goes against the grain of the Kubernetes ecosystem as most Kubernetes
    documentation and practitioners asking and answering questions online will be
    using YAML. If we can tolerate the translation from YAML into HCL, then it might
    be worth considering using the Kubernetes provider for Terraform:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding example is of an HCL equivalent of the Kubernetes YAML that provisions
    a Kubernetes deployment resource. Notice the prolific use of curly braces, which
    can be rather jarring for somebody who is used to looking at YAML.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the trade-offs
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With this approach, your Kubernetes resources are defined in HCL, and you then
    use the `terraform apply` command to create or update those resources as opposed
    to using `kubectl` either imperatively or declaratively.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: As with the native YAML approach for Kubernetes, this process is also declarative,
    meaning you describe what you want but leverage Terraform to figure out how to
    do it. This is similar to how Kubernetes itself works, but you’re using the Terraform
    provider to generate the plan and do the work.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: While it may seem like a great thing to use one language—HCL—to manage other
    parts of your infrastructure (such as cloud resources on AWS or GCP) and use it
    to manage your Kubernetes resources, however, because most Kubernetes documentation
    and samples are in YAML, you will be spending a significant amount of time mapping
    from YAML into HCL. This can make it difficult to learn and effectively manage
    Kubernetes at scale.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it is usually better to let Terraform manage the underlying infrastructure
    that Kubernetes sits on while managing Kubernetes using its own declarative approach
    using YAML and `kubectl`. However, if you can overcome the translation from YAML
    into HCL—or an even better option that we’ll address later: encapsulate your Kubernetes
    deployments into Helm charts—then it might be easier to use Terraform’s Kubernetes
    provider to eliminate the additional integration with `kubectl` commands embedded
    in `bash` scripts that you’ll have to do at the end of your `terraform` `apply`
    operation.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: There might also be certain Kubernetes resources that are tightly coupled with
    your cloud platform and the configuration that Terraform manages for you. These
    might be individual or standalone resources that connect a Kubernetes Service
    account to a cloud platform identity or a ConfigMap that sources the bulk of its
    values from Terraform outputs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at how we can use Terraform to provision resources
    to Kubernetes and compared and contrasted this approach to the native Kubernetes
    options using `kubectl`—both imperatively and declaratively using YAML-based manifests.
    In the next section, we’ll look at the Helm provider to see if it provides a better
    alternative to the options we’ve evaluated thus far.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the Helm provider to provision Kubernetes resources
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed previously, Kubernetes has a built-in declarative model based
    on YAML that allows you to provide resources to your cluster. However, as we saw,
    one of the challenges of using this model is that there is no way to use dynamic
    values inside your YAML-based specifications. That’s where Helm comes in. In this
    section, we’ll look at what Helm is exactly, its basic structure, how to use it,
    and how we can integrate it with our Terraform pipelines or use it directly with
    the Helm provider for Terraform.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: What is Helm?
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helm is widely referred to as a package manager for Kubernetes, but I find this
    definition a bit perplexing as a software developer who is used to working with
    package managers for software libraries such as Maven, NuGet, or `npm` or operating
    system package managers such as `apt` or Chocolatey. I suppose at some levels,
    they share a similarity in aggregating multiple components into a single, versioned
    package and providing a convenient way to pull these packages into other projects
    for reuse.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: However, I think a big difference and a unique part of Helm’s architecture is
    the nature of the templating engine. At its core, Helm allows you to create templates
    containing one or more Kubernetes YAML manifests and allows you to infuse more
    dynamic customization within your Kubernetes resources, thus making your Kubernetes
    deployments much more reusable and easier to manage and maintain. These templates
    are referred to as **charts** or **Helm charts**.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'In many ways, a Helm chart reminds me more of what a Terraform module is rather
    than a traditional package management software—whether it’s `apt` or NuGet. The
    similarities abound when comparing a Terraform module with a Helm chart. They
    both operate within a folder and define a method for taking input variables and
    producing outputs:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Terraform module inputs, outputs, and resources](img/B21183_05_6.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Terraform module inputs, outputs, and resources
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'A Terraform module encapsulates an aggregation of several Terraform resources
    (or other modules) defined within `.tf` files, and HCL allows you to implement
    any number of dynamic configurations using built-in capabilities of the language:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Helm chart inputs, outputs, and resources](img/B21183_05_7.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Helm chart inputs, outputs, and resources
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, a Helm chart performs a similar aggregation but with Kubernetes
    resources that are defined within `.yaml` files and use Kubernetes YAML-based
    markup. Helm defines its own templating engine based on Go templates that offers
    a wide range of features that allow you to implement a similar level of dynamic
    configuration that you can achieve with HCL.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the basic structure of a Helm chart is quite simple. It is
    not as simple as a Terraform module because we have nested folders that preclude
    users from being able to cleanly nest Helm charts within each other. Sub-charts
    need to be created in a special `charts` directory and can be completely encapsulated
    within this folder or simply reference an existing chart hosted elsewhere. This
    is similar to how Terraform modules work in that you can reference a local module
    or one hosted at any number of remote locations. A subtle difference is how Terraform
    modules can be declared in any `.tf` file, and their definition simply needs to
    be stored in another local folder or remote location:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Helm chart anatomy](img/B21183_05_8.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Helm chart anatomy
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Chart.yaml` file is a special file inside the Helm chart that acts as
    the main entry point file that contains key identification metadata and other
    dependencies such as other Helm charts defined either locally or in a remote location:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `values.yaml` file is a file that defines the input variables for a Helm
    chart. This is an example where in HCL we have no restriction on where we put
    input variables, by convention—and for our own sanity, we put input variables
    into a `variables.tf` file. In Helm, this convention of isolating input variable
    declarations is canonized into a well-known file that is recognized beyond a simple
    convention:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `templates` folder is where all our YAML-based manifests will go. However,
    the YAML is a bit different because it will most likely have many dynamic values
    injected into it using a Go templating convention (`{{` and `}}`) to denote symbolic
    references that Helm will resolve using the Go templating engine:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Helm charts can then be installed onto a Kubernetes cluster using a different
    command-line tool called `helm`. This tool performs a number of different functions,
    including autogenerating a basic chart structure, packaging charts for distribution,
    managing chart repositories, and installing charts onto the cluster.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Both `kubectl` and `helm` use the same method to authenticate with a Kubernetes
    cluster, but they are used for different purposes when managing the cluster, just
    as with `kubectl`, which can apply declarative Kubernetes configuration using
    the following command:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `helm` command can be used to provision a Helm chart to a Kubernetes cluster
    using the following command:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In this regard, Helm could similarly be integrated into a Terraform CI/CD pipeline
    that first provisions the cloud environment using Terraform and the relevant cloud
    platform provider (for example, `aws`, `azurerm`, or `googlecloud`) and then uses
    the `helm` command-line tool to install Helm charts onto the Kubernetes cluster
    using connection and authentication information provided by the output of the
    Terraform stage of the pipeline:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Helm chart anatomy: Terraform and Helm integration in a CI/CD
    pipeline](img/B21183_05_9.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9 – Helm chart anatomy: Terraform and Helm integration in a CI/CD
    pipeline'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we’ll look at how the same process could be streamlined
    using the Helm provider for Terraform, thus replacing the `bash` scripts executing
    `helm` commands imperatively and managing it with Terraform.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The Helm Terraform provider
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we looked at how Helm works, the structure of a Helm
    chart, and how its structure and functionality compare and contrast to Terraform
    modules. Now, we’ll look at how we can use Terraform to manage our Kubernetes
    environment using the Helm provider for Terraform. This provider is a close brother
    to the Kubernetes provider for Terraform because they both interact with the Kubernetes
    REST API as the control plan for managing Terraform resources.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using Terraform with Helm is that it enables you to manage
    your Kubernetes applications alongside your other infrastructure, using the same
    configuration language and tooling. As we know, Helm allows us to create parameterized
    templates using Kubernetes’ declarative YAML manifests and a templating language,
    but we still need to use `bash` scripts to execute `helm` commands and pass in
    parameters to the Helm chart. Some Helm charts can have very complicated configurations
    with dozens of parameters. So, using Terraform eliminates the additional integration
    with external `bash` scripts that execute `helm` commands.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, it also allows Kubernetes practitioners to develop Kubernetes
    templates in their native toolset. So, if you have Kubernetes specialists in your
    organization who want to build their own custom Helm charts, this allows them
    to keep doing their thing while plugging into a declarative deployment approach
    using Terraform. This also allows you to leverage the massive ecosystem that already
    exists for Helm and Kubernetes without any additional translation into HCL.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the Kubernetes provider, you need to initialize the provider first
    by declaring it as a required provider:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, in your root module, you need to create an instance of the provider.
    The provider configuration for the Helm provider closely resembles that of the
    Kubernetes provider:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In fact, both the Helm and Kubernetes providers can be used side by side in
    the same Terraform module in case some additional Kubernetes resources need to
    be provisioned to augment what’s in the Helm chart itself.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The Helm provider can be used to create a two-stage Terraform CI/CD pipeline
    where the first stage provisions the cloud environment using Terraform and the
    corresponding cloud platform’s provider. The second stage uses the cluster connection
    and authentication settings output by the first stage to configure the Helm provider
    and runs `terraform apply` again using a different Terraform code base containing
    the Helm configuration:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Helm chart anatomy: Terraform and Helm integration in a CI/CD
    pipeline](img/B21183_05_10.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10 – Helm chart anatomy: Terraform and Helm integration in a CI/CD
    pipeline'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The Terraform code base for the second stage is often quite small, only using
    a single resource. The `helm_release` resource is the only resource in the provider—which
    is quite different if you have ever used one of the cloud platform providers such
    as AWS, Azure, or GCP!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'The `helm_release` resource simply takes the inputs that we would expect to
    pass to the `helm install` command by specifying the chart name and version and
    an external repository (if necessary):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This concludes the section on the Helm provider.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the basic concepts needed to understand containers,
    container orchestrators, and the ways you can provision and manage container-based
    infrastructure using both Kubernetes native tooling via `kubectl` and Helm and
    the corresponding Terraform providers for both Kubernetes and Helm.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: This is the end of the cross-platform, cloud-agnostic knowledge that we need
    to build both VM- and container-based architectures across all three hyperscalars.
    Since serverless is inherently platform-specific and offers significant abstraction
    from the underlying infrastructure, I will cover each hyperscalar’s offering in
    its respective chapter.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move beyond cloud architecture paradigms and spend
    some time understanding how teams deliver IAC solutions using CI/CD pipelines
    that fuse the infrastructure provisioning, configuration management, and application
    deployment processes into a cohesive, end-to-end workflow.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
