<html><head></head><body>
<div id="_idContainer088">
<h1 class="chapter-number" id="_idParaDest-151"><a id="_idTextAnchor365"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-152"><a id="_idTextAnchor366"/><span class="koboSpan" id="kobo.2.1">Getting Started on AWS – Building Solutions with AWS EC2</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Now that we have a good foundation of the concepts needed to build real-world cloud solutions and automate them</span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.4.1"> using </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">Infrastructure as Code</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">IaC</span></strong><span class="koboSpan" id="kobo.8.1">) with Terraform, we will start our journey with arguably the most popular cloud</span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.9.1"> platform: </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">Amazon Web </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.11.1">Services</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.12.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.13.1">AWS</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">In this chapter, we’ll take a step-by-step approach to designing, building, and automating a solution using AWS’s </span><strong class="bold"><span class="koboSpan" id="kobo.16.1">virtual machine</span></strong><span class="koboSpan" id="kobo.17.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.18.1">VM</span></strong><span class="koboSpan" id="kobo.19.1">) service – </span><strong class="bold"><span class="koboSpan" id="kobo.20.1">Elastic Cloud Compute</span></strong><span class="koboSpan" id="kobo.21.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">EC2</span></strong><span class="koboSpan" id="kobo.23.1"> for short. </span><span class="koboSpan" id="kobo.23.2">We’ll also explore several other AWS services that are crucial for ensuring </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.24.1">our solution’s robustness and production readiness, such as secrets management, logging, and </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">network security.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">We have much to accomplish, but this is where the rubber hits the road. </span><span class="koboSpan" id="kobo.26.2">We’ll begin to really apply the concepts we’ve been discussing and put them into practice </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">on AWS.</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.30.1">Laying </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">the foundation</span></span></li>
<li><span class="koboSpan" id="kobo.32.1">Designing </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.34.1">Building </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">the solution</span></span></li>
<li><span class="koboSpan" id="kobo.36.1">Automating </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">the deployme</span><a id="_idTextAnchor367"/><span class="koboSpan" id="kobo.38.1">nt</span></span></li>
</ul>
<h1 id="_idParaDest-153"><a id="_idTextAnchor368"/><span class="koboSpan" id="kobo.39.1">Laying the foundation</span></h1>
<p><span class="koboSpan" id="kobo.40.1">Cloud infrastructure is only as</span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.41.1"> good as the applications and services deployed to it, so for this book, we will be building our sample architectures around a function use case for a fictional company called Söze Enterprises. </span><span class="koboSpan" id="kobo.41.2">Söze Enterprises was founded by a mysterious Turkish billionaire, Keyser Söze, who wants to take autonomous vehicles to the next level by building a platform that will allow both land and air vehicles – from any manufacturer – to coordinate their actions to improve safety and efficiency. </span><span class="koboSpan" id="kobo.41.3">Somehow, Keyser has already got Elon onboard, so it’s only a matter of time before the other EV vendors </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">follow suit.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">We have</span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.44.1"> inherited a team from one of Söze Enterprises’ other divisions that has a strong core team of C# .NET developers, so we’ll be building version 1.0 of the platform using .NET technologies. </span><span class="koboSpan" id="kobo.44.2">The elusive CEO, Keyser, was seen hobnobbing with Jeff Bezos in Monaco over the weekend, and word has come down from corporate that we will be using AWS to host the platform. </span><span class="koboSpan" id="kobo.44.3">Since the team doesn’t have a ton of experience with containers and timelines are tight, we’ve decided to build a simple three-tier architecture and host on VMs using AWS’s EC2 service. </span><span class="koboSpan" id="kobo.44.4">We’ve decided to use a Linux operating system to make it easier to convert containers in </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">the future:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<span class="koboSpan" id="kobo.46.1"><img alt="Figure 7.1 – Logical architecture for the autonomous vehicle platform" src="image/B21183_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.47.1">Figure 7.1 – Logical architecture for the autonomous vehicle platform</span></p>
<p><span class="koboSpan" id="kobo.48.1">The platform will need a frontend, which will be a web UI built using ASP.NET Core Blazor. </span><span class="koboSpan" id="kobo.48.2">The frontend will be powered by a REST API backend, which will be built using ASP.NET Core Web API. </span><span class="koboSpan" id="kobo.48.3">Having our core functionality encapsulated into a REST API will allow autonomous vehicles to communicate directly with the platform and allow us to expand by adding client interfaces with additional frontend technologies such as native mobile apps and virtual or mixed reality in the future. </span><span class="koboSpan" id="kobo.48.4">The backend will use a PostgreSQL database for persistent storage since it’s lightweight, industry-standard, and </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">relatively inexpensi</span><a id="_idTextAnchor369"/><span class="koboSpan" id="kobo.50.1">ve.</span></span></p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor370"/><span class="koboSpan" id="kobo.51.1">Designing the solution</span></h1>
<p><span class="koboSpan" id="kobo.52.1">Due to </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.53.1">the tight timelines the team is facing, we want to keep the cloud architecture simple. </span><span class="koboSpan" id="kobo.53.2">Therefore, we’ll be keeping it simple and using tried and tested services from AWS to implement the platform as opposed to trying to learn something new. </span><span class="koboSpan" id="kobo.53.3">The first decision we have to make is what AWS service each component of our logical architecture will be </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">hosted on.</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">Our application architecture consists of three components: a frontend, a backend, and a database. </span><span class="koboSpan" id="kobo.55.2">The frontend and backend are application components and need to be hosted on a cloud service that provides general computing, while the database needs to be hosted on a cloud database service. </span><span class="koboSpan" id="kobo.55.3">There are many options for both types </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">of services:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.57.1"><img alt="Figure 7.2 – Logical architecture for the autonomous vehicle platform" src="image/B21183_07_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.58.1">Figure 7.2 – Logical architecture for the autonomous vehicle platform</span></p>
<p><span class="koboSpan" id="kobo.59.1">Since we’ve</span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.60.1"> decided that we’re going to use VMs to host our application, we have narrowed down the different services that we can use to host our application, and we have decided that AWS EC2 is the ideal choice for our current situation. </span><span class="koboSpan" id="kobo.60.2">There are other options, such as </span><strong class="bold"><span class="koboSpan" id="kobo.61.1">Elastic Beanstalk</span></strong><span class="koboSpan" id="kobo.62.1">, that also use VMs, but we want to have total control over the solution and maintain as many cross-platform capabilities as possible in case we ever have to migrate to a different </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">cloud platform:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.64.1"><img alt="Figure 7.3 – Source control structure of our repository" src="image/B21183_07_3..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.65.1">Figure 7.3 – Source control structure of our repository</span></p>
<p><span class="koboSpan" id="kobo.66.1">This solution will</span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.67.1"> consist of six parts. </span><span class="koboSpan" id="kobo.67.2">We still have the application code and Packer templates for both the frontend and backend. </span><span class="koboSpan" id="kobo.67.3">Then, we have GitHub Actions to implement our CI/CD process and Terraform to provision our AWS infrastructure and reference the Packer-built VM images for our </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">EC2 instan</span><a id="_idTextAnchor371"/><span class="koboSpan" id="kobo.69.1">ces.</span></span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor372"/><span class="koboSpan" id="kobo.70.1">Cloud architecture</span></h2>
<p><span class="koboSpan" id="kobo.71.1">The </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.72.1">first part of our design is adapting our solution’s architecture to the target cloud platform: AWS. </span><span class="koboSpan" id="kobo.72.2">This involves mapping</span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.73.1"> application architecture components to AWS services and thinking through the configuration of those services so that they meet the requirements of </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">our sol</span><a id="_idTextAnchor373"/><span class="koboSpan" id="kobo.75.1">ution.</span></span></p>
<h3><span class="koboSpan" id="kobo.76.1">Virtual network</span></h3>
<p><span class="koboSpan" id="kobo.77.1">VMs must </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.78.1">be deployed within a virtual network. </span><span class="koboSpan" id="kobo.78.2">On AWS, we use the AWS EC2 service to provide our VMs, and we use </span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.79.1">AWS </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">Virtual Private Cloud</span></strong><span class="koboSpan" id="kobo.81.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.82.1">VPC</span></strong><span class="koboSpan" id="kobo.83.1">) to provide our virtual network. </span><span class="koboSpan" id="kobo.83.2">When working on AWS, the term </span><em class="italic"><span class="koboSpan" id="kobo.84.1">EC2 instance</span></em><span class="koboSpan" id="kobo.85.1"> is used interchangeably with the term </span><em class="italic"><span class="koboSpan" id="kobo.86.1">virtual machine</span></em><span class="koboSpan" id="kobo.87.1">. </span><span class="koboSpan" id="kobo.87.2">Likewise, the term </span><em class="italic"><span class="koboSpan" id="kobo.88.1">VPC</span></em><span class="koboSpan" id="kobo.89.1"> is used interchangeably with the term </span><em class="italic"><span class="koboSpan" id="kobo.90.1">virtual network</span></em><span class="koboSpan" id="kobo.91.1">. </span><span class="koboSpan" id="kobo.91.2">In this book, I will try to use industry-standard terminology wherever possible. </span><span class="koboSpan" id="kobo.91.3">You should get in the habit of thinking this way as this will allow your knowledge and skills to better transition between the different </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">cloud platforms:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.93.1"><img alt="Figure 7.4 – AWS virtual network architecture" src="image/B21183_07_4..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.94.1">Figure 7.4 – AWS virtual network architecture</span></p>
<p><span class="koboSpan" id="kobo.95.1">As we’ve discussed</span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.96.1"> previously, a virtual network is divided into a set of subnets. </span><span class="koboSpan" id="kobo.96.2">On AWS, a virtual network is scoped to a specific region, and a subnet is scoped to an Availability Zone within that region. </span><span class="koboSpan" id="kobo.96.3">Therefore, to build highly available systems on AWS, we must distribute our workloads across multiple Availability Zones. </span><span class="koboSpan" id="kobo.96.4">Therefore, if one Availability Zone experiences an outage, our workload, when deployed into the other Availability Zone, will prevent disruption to the </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">end users.</span></span></p>
<p><span class="koboSpan" id="kobo.98.1">Our application’s VMs need to be provisioned into subnets within a virtual network. </span><span class="koboSpan" id="kobo.98.2">The frontend of our application needs to be accessible over the internet, while the backend only needs to be accessible to the frontend. </span><span class="koboSpan" id="kobo.98.3">Therefore, we should provision separate subnets for the internet-accessible frontend and our private backend. </span><span class="koboSpan" id="kobo.98.4">This is a common pattern when it comes to creating </span><em class="italic"><span class="koboSpan" id="kobo.99.1">public</span></em><span class="koboSpan" id="kobo.100.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.101.1">private</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.102.1"> subnets:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.103.1"><img alt="Figure 7.5 – Public and private subnets for the frontend and backend application components" src="image/B21183_07_5..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.104.1">Figure 7.5 – Public and private subnets for the frontend and backend application components</span></p>
<p><span class="koboSpan" id="kobo.105.1">In this pattern, two </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.106.1">pairs of public and private subnets are created. </span><span class="koboSpan" id="kobo.106.2">Each pair is provisioned in the same Availability Zone. </span><span class="koboSpan" id="kobo.106.3">The reason why each pair shares the same Availability Zone is due to the dependency between the frontend and the backend. </span><span class="koboSpan" id="kobo.106.4">For example, if there is an outage affecting the Availability Zone of the backend, the frontend won’t be able to operate. </span><span class="koboSpan" id="kobo.106.5">Likewise, if there is an outage affecting the Availability Zone of the frontend, no traffic will be routed to the backend. </span><span class="koboSpan" id="kobo.106.6">We can create as many pairs of these public/private subnets as there are Availability Zones within a region. </span><span class="koboSpan" id="kobo.106.7">Most regions have four to five Availability Zones, but usually, two to three Availability Zones are sufficient for most workloads. </span><span class="koboSpan" id="kobo.106.8">After that, you are more likely to benefit from setting up a </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">multi-region dep</span><a id="_idTextAnchor374"/><span class="koboSpan" id="kobo.108.1">loyment.</span></span></p>
<h3><span class="koboSpan" id="kobo.109.1">Network routing</span></h3>
<p><span class="koboSpan" id="kobo.110.1">There are a </span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.111.1">few other components that we need to set up within this virtual network to enable our VMs to function properly. </span><span class="koboSpan" id="kobo.111.2">In AWS, when you provision a VM in a virtual network, you won’t have internet access! </span><span class="koboSpan" id="kobo.111.3">For most connected applications, internet access is required to allow connectivity to third-party services. </span><span class="koboSpan" id="kobo.111.4">Without this, operators would be inconvenienced as they would be unable to perform operating system upgrades and patches using internet-hosted </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">package repositories:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.113.1"><img alt="Figure 7.6 – Internet and NAT gateways enable internet access for VMs within the subnets" src="image/B21183_07_6..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.114.1">Figure 7.6 – Internet and NAT gateways enable internet access for VMs within the subnets</span></p>
<p><span class="koboSpan" id="kobo.115.1">The internet gateway is attached to the virtual network at the region level, providing internet access to the entire VPC, while the NAT gateways are deployed into each public subnet at</span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.116.1"> the Availability Zone level to allow EC2 instances in private subnets to access the internet without being directly accessible from the internet. </span><span class="koboSpan" id="kobo.116.2">Each NAT gateway also needs its own static public IP address to grant access. </span><span class="koboSpan" id="kobo.116.3">This can be achieved by using the Elastic IP service </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">on AWS:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.118.1"><img alt="Figure 7.7 – Route tables associated with the subnets that direct traffic to the correct gateway" src="image/B21183_07_7..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.119.1">Figure 7.7 – Route tables associated with the subnets that direct traffic to the correct gateway</span></p>
<p><span class="koboSpan" id="kobo.120.1">The final step in establishing internet access to our VMs in private subnets is routing internet-bound traffic to the correct NAT gateway for each subnet; VMs in public subnets can directly access the internet. </span><span class="koboSpan" id="kobo.120.2">This can be done using route tables. </span><span class="koboSpan" id="kobo.120.3">In the public subnet, we route internet traffic to the internet gateway. </span><span class="koboSpan" id="kobo.120.4">In the private subnet, we route internet traffic to the </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">NAT </span><a id="_idTextAnchor375"/><span class="koboSpan" id="kobo.122.1">gateway.</span></span></p>
<h3><span class="koboSpan" id="kobo.123.1">Load balancing</span></h3>
<p><span class="koboSpan" id="kobo.124.1">Now that</span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.125.1"> our subnets have been set up and connected using proper routing tables, we can provision our VMs. </span><span class="koboSpan" id="kobo.125.2">To achieve high availability, we need at least one VM to be provisioned for each subnet for both the frontend and the backend of our solution. </span><span class="koboSpan" id="kobo.125.3">We can increase the number of VMs in each subnet to achieve even more reliability </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">or scale:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.127.1"><img alt="Figure 7.8 – VMs provisioned for our virtual network" src="image/B21183_07_8..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.128.1">Figure 7.8 – VMs provisioned for our virtual network</span></p>
<p><span class="koboSpan" id="kobo.129.1">The </span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.130.1">problem with the current design is that we need a way for our system to respond correctly to an outage affecting one of our Availability Zones. </span><span class="koboSpan" id="kobo.130.2">This is where a load balancer comes in. </span><span class="koboSpan" id="kobo.130.3">It allows us to get the double benefit of routing traffic to healthy endpoints and distributing the load evenly across </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">our resources.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">In AWS, the </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">Application Load Balancer</span></strong><span class="koboSpan" id="kobo.134.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.135.1">ALB</span></strong><span class="koboSpan" id="kobo.136.1">) service performs this function. </span><span class="koboSpan" id="kobo.136.2">The load balancer’s job </span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.137.1">is to be the single point of contact for clients to send requests to. </span><span class="koboSpan" id="kobo.137.2">The load balancer then forwards that traffic to VMs and routes the corresponding responses </span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.138.1">back to the client from where the </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">request originated:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.140.1"><img alt="Figure 7.9 – Load balancer forwarding traffic to VMs across Availability Zones" src="image/B21183_07_9..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.141.1">Figure 7.9 – Load balancer forwarding traffic to VMs across Availability Zones</span></p>
<p><span class="koboSpan" id="kobo.142.1">In the AWS ALB, the first thing we need to set up is the listener. </span><span class="koboSpan" id="kobo.142.2">On this listener, you specify a port, a protocol, and one or more actions you want to perform when a request is received. </span><span class="koboSpan" id="kobo.142.3">The most basic type of action is to forward the request to a </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">target group.</span></span></p>
<p><span class="koboSpan" id="kobo.144.1">In our solution, the </span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.145.1">target group will consist of a set of VMs. </span><span class="koboSpan" id="kobo.145.2">The target group specifies what port and protocol the request should be sent to, as well as a health probe with a specific application path. </span><span class="koboSpan" id="kobo.145.3">The health probe can optionally be set up on a different port and protocol, where it provides several different settings to control how frequently it should be probed and how to evaluate whether the endpoint is healthy or unhealthy. </span><span class="koboSpan" id="kobo.145.4">Healthy is usually indicated by an HTTP status code of </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">200</span></strong><span class="koboSpan" id="kobo.147.1">. </span><span class="koboSpan" id="kobo.147.2">Anything else is </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">considered unhealthy.</span></span></p>
<p><span class="koboSpan" id="kobo.149.1">For both our frontend and backend, we have a simple set of VMs for the target group with an endpoint configured for the HTTP protocol on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">5000</span></strong><span class="koboSpan" id="kobo.151.1"> (the default port for </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">ASP.NET Core).</span></span></p>
<p><span class="koboSpan" id="kobo.153.1">The frontend </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.154.1">is an </span><strong class="bold"><span class="koboSpan" id="kobo.155.1">ASP.NET Core Blazor</span></strong><span class="koboSpan" id="kobo.156.1"> application. </span><span class="koboSpan" id="kobo.156.2">As a</span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.157.1"> result, it uses </span><strong class="bold"><span class="koboSpan" id="kobo.158.1">SignalR</span></strong><span class="koboSpan" id="kobo.159.1"> (which abstracts WebSocket communication) to provide real-time connectivity between the web browser and the server. </span><span class="koboSpan" id="kobo.159.2">As a result, we need to enable sticky sessions so that this can function properly. </span><span class="koboSpan" id="kobo.159.3">Sticky sessions will allow the client to continue to use the same VM, thus allowing the WebSocket to stay alive and not be disrupted by changing which web server it </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">communicates with.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">For the health probe, the frontend will use the root path of the web application, while the backend will use a special path that routes to a controller that’s been configured to respond to the </span><a id="_idTextAnchor376"/><span class="No-Break"><span class="koboSpan" id="kobo.162.1">health probe.</span></span></p>
<h3><span class="koboSpan" id="kobo.163.1">Network security</span></h3>
<p><span class="koboSpan" id="kobo.164.1">Now</span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.165.1"> that our virtual network has been fully configured and our VMs have been set up behind load balancers, we need to think about what network traffic we want to allow through the system. </span><span class="koboSpan" id="kobo.165.2">In AWS, this can be controlled by creating security groups, which allow traffic to be sent between components of your architecture on specific ports using </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">specific protocols.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">The first step in this process is to think through the logical stops for our network traffic as it makes its way through </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">our solution:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<span class="koboSpan" id="kobo.169.1"><img alt="Figure 7.10 – Logical components of our architecture" src="image/B21183_07_10..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.170.1">Figure 7.10 – Logical components of our architecture</span></p>
<p><span class="koboSpan" id="kobo.171.1">The </span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.172.1">application components, including the frontend and the backend, are on this list, followed by the database. </span><span class="koboSpan" id="kobo.172.2">However, these aren’t the only places where our network traffic flows. </span><span class="koboSpan" id="kobo.172.3">Since we introduced load balancers in front of both the frontend and the backend, we have two additional stops for </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">network traffic.</span></span></p>
<p><span class="koboSpan" id="kobo.174.1">The next step is to think about how each component communicates with others. </span><span class="koboSpan" id="kobo.174.2">This includes both the port and protocol but also the direction of the traffic. </span><span class="koboSpan" id="kobo.174.3">To do this, we need to think about the network traffic from the perspective of </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">each component:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.176.1"><img alt="Figure 7.11 – Frontend load balancer network traffic flow" src="image/B21183_07_11..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.177.1">Figure 7.11 – Frontend load balancer network traffic flow</span></p>
<p><span class="koboSpan" id="kobo.178.1">From the perspective of the frontend load balancer, we’ll </span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.179.1">be receiving traffic from the internet on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">80</span></strong><span class="koboSpan" id="kobo.181.1"> using the HTTP protocol. </span><span class="koboSpan" id="kobo.181.2">This inbound traffic is called </span><strong class="bold"><span class="koboSpan" id="kobo.182.1">ingress</span></strong><span class="koboSpan" id="kobo.183.1">. </span><span class="koboSpan" id="kobo.183.2">Due to the target group configuration, we’ll be forwarding those requests to the frontend on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">5000</span></strong><span class="koboSpan" id="kobo.185.1"> using the HTTP protocol. </span><span class="koboSpan" id="kobo.185.2">This outbound traffic is</span><a id="_idIndexMarker535"/> <span class="No-Break"><span class="koboSpan" id="kobo.186.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.187.1">egress</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.189.1"><img alt="Figure 7.12 – Frontend network traffic flow" src="image/B21183_07_12..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.190.1">Figure 7.12 – Frontend network traffic flow</span></p>
<p><span class="koboSpan" id="kobo.191.1">From the perspective of the frontend, we’ll be receiving traffic from the frontend load balancer on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">5000</span></strong><span class="koboSpan" id="kobo.193.1"> using the HTTP protocol. </span><span class="koboSpan" id="kobo.193.2">The C# application code will make requests to the REST web API hosted in the backend, but we’ll be routing all our requests to the backend through the backend load balancer on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">80</span></strong><span class="koboSpan" id="kobo.195.1"> using the </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">HTTP protocol:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.197.1"><img alt="Figure 7.13 – Backend load balancer network traffic flow" src="image/B21183_07_13..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.198.1">Figure 7.13 – Backend load balancer network traffic flow</span></p>
<p><span class="koboSpan" id="kobo.199.1">From the</span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.200.1"> perspective of the backend load balancer, we’ll be receiving traffic from the frontend on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">80</span></strong><span class="koboSpan" id="kobo.202.1"> using the HTTP protocol. </span><span class="koboSpan" id="kobo.202.2">Due to the target group configuration, we’ll be forwarding those requests to the backend on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">5000</span></strong><span class="koboSpan" id="kobo.204.1"> using the </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">HTTP protocol:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<span class="koboSpan" id="kobo.206.1"><img alt="Figure 7.14 – Backend network traffic flow" src="image/B21183_07_14..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.207.1">Figure 7.14 – Backend network traffic flow</span></p>
<p><span class="koboSpan" id="kobo.208.1">From the perspective of the backend, we’ll be receiving traffic from the backend load balancer on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.209.1">5000</span></strong><span class="koboSpan" id="kobo.210.1"> using the HTTP protocol. </span><span class="koboSpan" id="kobo.210.2">The C# application code will be making requests to the PostgreSQL database on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.211.1">5432</span></strong><span class="koboSpan" id="kobo.212.1"> using the </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">HT</span><a id="_idTextAnchor377"/><span class="koboSpan" id="kobo.214.1">TPS protocol.</span></span></p>
<h3><span class="koboSpan" id="kobo.215.1">Secrets management</span></h3>
<p><span class="koboSpan" id="kobo.216.1">Secrets </span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.217.1">such as database credentials or service access keys need to be stored securely. </span><span class="koboSpan" id="kobo.217.2">Each cloud platform has a service that provides </span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.218.1">this functionality. </span><span class="koboSpan" id="kobo.218.2">On AWS, this service is called </span><strong class="bold"><span class="koboSpan" id="kobo.219.1">AWS </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.220.1">Secrets Manager</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<span class="koboSpan" id="kobo.222.1"><img alt="Figure 7.15 – Secrets stored in AWS Secrets Manager can be accessed by VMs once they have the necessary IAM privileges" src="image/B21183_07_15..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.223.1">Figure 7.15 – Secrets stored in AWS Secrets Manager can be accessed by VMs once they have the necessary IAM privileges</span></p>
<p><span class="koboSpan" id="kobo.224.1">You </span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.225.1">simply create secrets on this service using a consistent naming convention, then construct an IAM role that has permission to access these secrets. </span><span class="koboSpan" id="kobo.225.2">The following IAM policy will grant permission to just secrets that start </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">fleetportal/</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.229.1">
{
    “Version”: “2012-10-17”,
    “Statement”: [
        {
            “Effect”: “Allow”,
            “Action”: “secretsmanager:GetSecretValue”,
            “Resource”:
“arn:aws:secretsmanager:region:account
id:secret:fleetportal/*”
        }
    ]
}</span></pre> <p><span class="koboSpan" id="kobo.230.1">The values for </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">region</span></strong><span class="koboSpan" id="kobo.232.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">account-id</span></strong><span class="koboSpan" id="kobo.234.1"> will need to be altered to reflect where the secrets were created. </span><span class="koboSpan" id="kobo.234.2">It’s important to note that an AWS account is typically used as a security boundary for an application and an environment. </span><span class="koboSpan" id="kobo.234.3">So, we would likely have separate AWS accounts for our solution’s development and production environments, as well as any other environments we may need. </span><span class="koboSpan" id="kobo.234.4">This will isolate our secrets manager secrets within the context of the AWS account and </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">the region.</span></span></p>
<p><span class="koboSpan" id="kobo.236.1">The two </span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.237.1">main attributes we use to grant permissions are </span><strong class="source-inline"><span class="koboSpan" id="kobo.238.1">action</span></strong><span class="koboSpan" id="kobo.239.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">resource</span></strong><span class="koboSpan" id="kobo.241.1">. </span><span class="koboSpan" id="kobo.241.2">When implementing the principle of least privilege, it’s important to be as specific as possible about the actions that are required for a particular identity. </span><span class="koboSpan" id="kobo.241.3">If access is not required, don’t grant it. </span><span class="koboSpan" id="kobo.241.4">Likewise, we should ensure the resources we grant these permissions to are as narrow as possible. </span><span class="koboSpan" id="kobo.241.5">It’s easy to be lazy and leave </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">*</span></strong><span class="koboSpan" id="kobo.243.1"> in the resources or the actions. </span><span class="koboSpan" id="kobo.243.2">Still, we need to be aware that a malicious attacker could use overly generous permissions to move laterally within </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">o</span><a id="_idTextAnchor378"/><span class="koboSpan" id="kobo.245.1">ur environments.</span></span></p>
<h3><span class="koboSpan" id="kobo.246.1">VMs</span></h3>
<p><span class="koboSpan" id="kobo.247.1">Now</span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.248.1"> that we have everything we need for our solution, we can finish by talking about where our application components will run: in VMs that have been provisioned using </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">AWS EC2.</span></span></p>
<p><span class="koboSpan" id="kobo.250.1">When provisioning VMs on AWS, you have two options. </span><span class="koboSpan" id="kobo.250.2">First, you can provide static VMs. </span><span class="koboSpan" id="kobo.250.3">In this approach, you need to specify key characteristics for every VM. </span><span class="koboSpan" id="kobo.250.4">Alternatively, you can</span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.251.1"> use an </span><strong class="bold"><span class="koboSpan" id="kobo.252.1">AWS Auto Scaling group</span></strong><span class="koboSpan" id="kobo.253.1"> to dynamically provision and manage the VMs. </span><span class="koboSpan" id="kobo.253.2">In this approach, you provide the Auto Scaling group with some configuration and parameters on when to scale up and when to scale down, at which point the Auto Scaling group will take care of </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">everything else.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">When provisioning a static VM on AWS, you need to associate it with an </span><strong class="bold"><span class="koboSpan" id="kobo.256.1">AWS key pair</span></strong><span class="koboSpan" id="kobo.257.1"> to ensure </span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.258.1">that you can connect to its operating system. </span><span class="koboSpan" id="kobo.258.2">This will allow your operators to perform diagnostics and update or patch the software and </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">operating system.</span></span></p>
<p><span class="koboSpan" id="kobo.260.1">All VMs need to be connected to a virtual network, so when you set up a static VM, you need to specify the network configuration. </span><span class="koboSpan" id="kobo.260.2">This can be accomplished by creating a network interface and associating it with the VM. </span><span class="koboSpan" id="kobo.260.3">The network interface connects the VM to the appropriate subnet, which is the place where you attach one or more </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">security groups.</span></span></p>
<p><span class="koboSpan" id="kobo.262.1">The</span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.263.1"> internal configuration of your VM is controlled by two critical attributes: the VM image and the user data. </span><span class="koboSpan" id="kobo.263.2">As we discussed in </span><a href="B21183_04.xhtml#_idTextAnchor239"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.264.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.265.1">, the VM image can either be a vanilla installation of an operating system or it can be a fully configured version of your application. </span><span class="koboSpan" id="kobo.265.2">The decision of </span><strong class="bold"><span class="koboSpan" id="kobo.266.1">build versus bake</span></strong><span class="koboSpan" id="kobo.267.1"> is up </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">to you.</span></span></p>
<p><span class="koboSpan" id="kobo.269.1">User data allows you to run the </span><em class="italic"><span class="koboSpan" id="kobo.270.1">last mile</span></em><span class="koboSpan" id="kobo.271.1"> configuration when the VM starts up. </span><span class="koboSpan" id="kobo.271.2">This can be done using industry-standard </span><strong class="source-inline"><span class="koboSpan" id="kobo.272.1">cloud-init</span></strong><span class="koboSpan" id="kobo.273.1"> configuration to perform a wide variety of tasks such as setting up users/groups, setting up environment variables, or </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">mounting disks:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<span class="koboSpan" id="kobo.275.1"><img alt="Figure 7.16 – Resource VMs created statically" src="image/B21183_07_16..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.276.1">Figure 7.16 – Resource VMs created statically</span></p>
<p><span class="koboSpan" id="kobo.277.1">AWS can dynamically manage your VMs based on the load that they incur. </span><span class="koboSpan" id="kobo.277.2">This is done using an Auto Scaling group. </span><span class="koboSpan" id="kobo.277.3">This Auto Scaling group is responsible for provisioning the VMs. </span><span class="koboSpan" id="kobo.277.4">Consequently, this means that the Auto Scaling group needs to have the key characteristics that define your VM set on its launch template. </span><span class="koboSpan" id="kobo.277.5">The Auto Scaling group uses this launch template to specify the configuration of each VM that </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">it provisions:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.279.1"><img alt="Figure 7.17 – VMs created and managed dynamically using an Auto-Scaling Group" src="image/B21183_07_17..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.280.1">Figure 7.17 – VMs created and managed dynamically using an Auto-Scaling Group</span></p>
<p><span class="koboSpan" id="kobo.281.1">Besides </span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.282.1">this launch template, the Auto Scaling group simply needs to be told what subnets the VMs should be provisioned into and under what circumstances it should provision or de-provision VMs from the set that i</span><a id="_idTextAnchor379"/><span class="koboSpan" id="kobo.283.1">t </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">actively manages.</span></span></p>
<h3><span class="koboSpan" id="kobo.285.1">Monitoring</span></h3>
<p><span class="koboSpan" id="kobo.286.1">AWS </span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.287.1">has a cross-cutting service </span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.288.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.289.1">CloudWatch</span></strong><span class="koboSpan" id="kobo.290.1"> that can capture logs and telemetry from various AWS services you consume within your solutions. </span><span class="koboSpan" id="kobo.290.2">We’ll be using this as the primary logging mechanism within this book. </span><span class="koboSpan" id="kobo.290.3">Many services support CloudWatch out of the box with minimal to no configuration to get it working. </span><span class="koboSpan" id="kobo.290.4">At the same time, other services and scenarios require permissions to be granted to allow that service to </span><a id="_idTextAnchor380"/><span class="koboSpan" id="kobo.291.1">log in </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">to CloudWatch.</span></span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor381"/><span class="koboSpan" id="kobo.293.1">Deployment architecture</span></h2>
<p><span class="koboSpan" id="kobo.294.1">Now that </span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.295.1">we have a good idea of what our cloud architecture is going to look like for our solution on AWS, we need to come up with a plan for how to provision our environment</span><a id="_idTextAnchor382"/><span class="koboSpan" id="kobo.296.1">s and deploy </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">our code.</span></span></p>
<h3><span class="koboSpan" id="kobo.298.1">VM configuration</span></h3>
<p><span class="koboSpan" id="kobo.299.1">In our solution, we </span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.300.1">have two VM roles: the frontend role, which is responsible for handling web page requests from the end user’s web browser, and the backend role, which is responsible for handling REST API requests from the web application. </span><span class="koboSpan" id="kobo.300.2">Each of these roles has different code and a different configuration that needs to be set. </span><span class="koboSpan" id="kobo.300.3">Each will require its own Packer template to build a VM image that we can use to launch a VM </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">on AWS:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.302.1"><img alt="Figure 7.18 – Packer pipeline to build a VM image for the frontend" src="image/B21183_07_18..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.303.1">Figure 7.18 – Packer pipeline to build a VM image for the frontend</span></p>
<p><span class="koboSpan" id="kobo.304.1">A GitHub Actions workflow </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.305.1">that triggers off changes to the frontend application code and the frontend Packer template will execute </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">packer build</span></strong><span class="koboSpan" id="kobo.307.1"> and create a new VM image for the </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">solution’s frontend.</span></span></p>
<p><span class="koboSpan" id="kobo.309.1">Both the frontend and the backend will have an identical GitHub workflow that executes </span><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">packer build</span></strong><span class="koboSpan" id="kobo.311.1">. </span><span class="koboSpan" id="kobo.311.2">The key difference between the workflows is the code bases that they execute against. </span><span class="koboSpan" id="kobo.311.3">Both the frontend and the backend might have slightly different operating system configurations, and both will require different deployment packages for their respective </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">application components:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.313.1"><img alt="Figure 7.19 – Packer pipeline to build a VM image for the backend" src="image/B21183_07_19..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.314.1">Figure 7.19 – Packer pipeline to build a VM image for the backend</span></p>
<p><span class="koboSpan" id="kobo.315.1">It’s important </span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.316.1">to note that the application code will be baked into the VM image rather than copied to an already running VM. </span><span class="koboSpan" id="kobo.316.2">This means that to update the software running on the VMs, each VM will need to be restarted so that it has a new VM image containing the latest copy of </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">the code.</span></span></p>
<p><span class="koboSpan" id="kobo.318.1">This approach makes the VM image an immutable deployment artifact that is versioned and updated each time there is a release of the application code tha</span><a id="_idTextAnchor383"/><span class="koboSpan" id="kobo.319.1">t needs to </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">be deployed.</span></span></p>
<h3><span class="koboSpan" id="kobo.321.1">Cloud environment configuration</span></h3>
<p><span class="koboSpan" id="kobo.322.1">Once</span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.323.1"> the VM images have been built for both the frontend and the backend, we can execute the final workflow, which will both provision and deploy our solution </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">to AWS:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<span class="koboSpan" id="kobo.325.1"><img alt="Figure 7.20 – VM images are used as inputs to the Terraform code, which provisions the environment on AWS" src="image/B21183_07_20..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.326.1">Figure 7.20 – VM images are used as inputs to the Terraform code, which provisions the environment on AWS</span></p>
<p><span class="koboSpan" id="kobo.327.1">The Terraform code base will have two input variables for the version of the VM image for both the frontend and the backend. </span><span class="koboSpan" id="kobo.327.2">When new versions of the application software need to be deployed, the input parameters for these versions will be incremented to reflect the target version for deployment. </span><span class="koboSpan" id="kobo.327.3">When the workflow is executed, </span><strong class="source-inline"><span class="koboSpan" id="kobo.328.1">terraform apply</span></strong><span class="koboSpan" id="kobo.329.1"> will simply replace the existing VMs with VMs </span><a id="_idTextAnchor384"/><span class="koboSpan" id="kobo.330.1">using the new </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">VM image.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">Now that</span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.333.1"> we have a solid plan for how we will implement both the cloud architecture using AWS and the deployment architecture using GitHub Actions, let’s start building! </span><span class="koboSpan" id="kobo.333.2">In the next section, we’ll break down the HashiCorp configuration language code that we used to implement the Terraform and </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">Packer solutions.</span></span></p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor385"/><span class="koboSpan" id="kobo.335.1">Building the solution</span></h1>
<p><span class="koboSpan" id="kobo.336.1">With our design in place, all we need to do is write the code tha</span><a id="_idTextAnchor386"/><span class="koboSpan" id="kobo.337.1">t implements </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">the design.</span></span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor387"/><span class="koboSpan" id="kobo.339.1">Packer</span></h2>
<p><span class="koboSpan" id="kobo.340.1">Our solution</span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.341.1"> has a frontend and a backend application component. </span><span class="koboSpan" id="kobo.341.2">Although the application code is radically different, the way we</span><a id="_idTextAnchor388"/><span class="koboSpan" id="kobo.342.1"> build a VM image </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">is not.</span></span></p>
<h3><span class="koboSpan" id="kobo.344.1">AWS plugin</span></h3>
<p><span class="koboSpan" id="kobo.345.1">As we </span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.346.1">discussed in </span><a href="B21183_04.xhtml#_idTextAnchor239"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.347.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.348.1">, Packer – like Terraform – is an extensible command-line executable. </span><span class="koboSpan" id="kobo.348.2">Each cloud platform provides a plugin for Packer that encapsulates the integration with </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">its services:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.350.1">
packer {
  required_plugins {
    amazon = {
      source  = “github.com/hashicorp/amazon”
      version = “~&gt; 1.2.6”
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.351.1">Plugins need to be declared within a Packer solution. </span><span class="koboSpan" id="kobo.351.2">At the time of writing, the latest version of the AWS Packer plugin </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">is </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">1.2.6</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.355.1">The</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.356.1"> AWS plugin for Packer provides an </span><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">amazon-ebs</span></strong><span class="koboSpan" id="kobo.358.1"> builder that will generate an AMI by creating a new VM from a base image, executing the provisioners, taking</span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.359.1"> an </span><strong class="bold"><span class="koboSpan" id="kobo.360.1">Elastic Block Store</span></strong><span class="koboSpan" id="kobo.361.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.362.1">EBS</span></strong><span class="koboSpan" id="kobo.363.1">) disk image snapshot, and </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.364.1">creating an </span><strong class="bold"><span class="koboSpan" id="kobo.365.1">Amazon Machine Image</span></strong><span class="koboSpan" id="kobo.366.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.367.1">AMI</span></strong><span class="koboSpan" id="kobo.368.1">) from it. </span><span class="koboSpan" id="kobo.368.2">This behavior is controlled by the </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">Amazon builder:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.370.1">
data “amazon-ami” “ubuntu2204” {
  filters = {
    architecture        = “x86_64”
    virtualization-type = “hvm”
    root-device-type    = “ebs”
    name                = “ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*”
  }
  owners      = [“099720109477”]
  most_recent = true
  region      = var.aws_primary_region
}</span></pre> <p><span class="koboSpan" id="kobo.371.1">The first input to the Amazon </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">amazon-ebs</span></strong><span class="koboSpan" id="kobo.373.1"> builder is the base image to use when creating the initial VM against which the Packer template’s provisioners will be executed. </span><span class="koboSpan" id="kobo.373.2">The preceding code references the latest version of the Ubuntu </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">22.04</span></strong><span class="koboSpan" id="kobo.375.1"> VM image within the target </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">AWS region:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.377.1">
source “amazon-ebs” “vm” {
  region        = var.aws_primary_region
  ami_name      = “${var.image_name}-${var.image_version}”
  instance_type = var.aws_instance_type
  ssh_username  = “ubuntu”
  ssh_interface = “public_ip”
  communicator  = “ssh”
  source_ami    = data.amazon-ami.ubuntu2204.id
}</span></pre> <p><span class="koboSpan" id="kobo.378.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">amazon-ebs</span></strong><span class="koboSpan" id="kobo.380.1"> builder references the </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">amazon-ami</span></strong><span class="koboSpan" id="kobo.382.1"> data source to ensure that the correct </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.383.1">base image is used before the provisioners are executed. </span><span class="koboSpan" id="kobo.383.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.384.1">ami_name</span></strong><span class="koboSpan" id="kobo.385.1"> is probably the most important attribute on this block as it dictates the version name that the VM image will be referenced by </span><a id="_idTextAnchor389"/><span class="koboSpan" id="kobo.386.1">in </span><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">terraform </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">apply</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.389.1"> operations.</span></span></p>
<h3><span class="koboSpan" id="kobo.390.1">Operating system configuration</span></h3>
<p><span class="koboSpan" id="kobo.391.1">To </span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.392.1">avoid access control issues, it’s a good idea to establish the context for the provisioners to be </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">executed within:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.394.1">
locals {
  execute_command = “chmod +x {{ .Path }}; {{ .Vars }} sudo -E sh ‘{{ .Path }}’”
}</span></pre> <p><span class="koboSpan" id="kobo.395.1">This is a standard </span><strong class="source-inline"><span class="koboSpan" id="kobo.396.1">execute_command</span></strong><span class="koboSpan" id="kobo.397.1"> parameter that can be used to set the context for all provisioners. </span><span class="koboSpan" id="kobo.397.2">It allows you to eliminate any unnecessary </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">sudo</span></strong><span class="koboSpan" id="kobo.399.1"> commands within your installation scripts. </span><span class="koboSpan" id="kobo.399.2">The preceding </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">execution_command</span></strong><span class="koboSpan" id="kobo.401.1"> parameter will allow your Packer template scripts to execute as a </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">privileged user.</span></span></p>
<p><span class="koboSpan" id="kobo.403.1">Our solution is built using ASP.NET Core. </span><span class="koboSpan" id="kobo.403.2">Therefore, we need to install .NET 6.0 SDK for our solution to work properly on the VMs. </span><span class="koboSpan" id="kobo.403.3">Ubuntu, like other Debian-based distributions of Linux, uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.404.1">apt</span></strong><span class="koboSpan" id="kobo.405.1"> command-line application to perform package management. </span><span class="koboSpan" id="kobo.405.2">By default, Ubuntu includes several public repositories that include most of the common software packages. </span><span class="koboSpan" id="kobo.405.3">However, sometimes, you need to set up additional package repositories when the default repositories don’t work. </span><span class="koboSpan" id="kobo.405.4">Microsoft hosts a package repository for </span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">apt</span></strong><span class="koboSpan" id="kobo.407.1">, that houses the correct software package we need to install .NET 6.0 on Ubuntu. </span><span class="koboSpan" id="kobo.407.2">Therefore, we need to add that repository before we can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">apt</span></strong><span class="koboSpan" id="kobo.409.1"> to install .</span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">NET 6.0.</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">Our Packer template includes a file called </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">dotnet.pref</span></strong><span class="koboSpan" id="kobo.413.1"> that has the </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">following contents:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.415.1">
Package: *
Pin: origin “packages.microsoft.com”
Pin-Priority: 1001</span></pre> <p><span class="koboSpan" id="kobo.416.1">We </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.417.1">use the Packer </span><strong class="source-inline"><span class="koboSpan" id="kobo.418.1">file</span></strong><span class="koboSpan" id="kobo.419.1"> provisioner to copy this file to the correct location on </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">the VM:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.421.1">
provisioner “shell” {
  execute_command = local.execute_command
  inline = [
    “cp /tmp/dotnet.pref /etc/apt/preferences.d/dotnet.pref”
  ]
}</span></pre> <p><span class="koboSpan" id="kobo.422.1">Then, we execute the </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">install-dotnet6-prereq.sh</span></strong><span class="koboSpan" id="kobo.424.1"> bash script, which downloads a </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">.deb</span></strong><span class="koboSpan" id="kobo.426.1"> file and installs it using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.427.1">dpkg</span></strong><span class="koboSpan" id="kobo.428.1"> tool. </span><span class="koboSpan" id="kobo.428.2">This registers the third-party repository hosted by Microsoft with the Debian package </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">management tool.</span></span></p>
<p><span class="koboSpan" id="kobo.430.1">Now, we can simply run </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">apt-get update -y</span></strong><span class="koboSpan" id="kobo.432.1"> to get the latest version of the packages from all repositories, and we are ready to install .</span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">NET 6.0:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.434.1">
provisioner “shell” {
  execute_command = local.execute_command
  inline = [
    “apt-get install dotnet-sdk-6.0 -y”
  ]
}</span></pre> <p><span class="koboSpan" id="kobo.435.1">If we don’t include the </span><strong class="source-inline"><span class="koboSpan" id="kobo.436.1">packages.microsoft.com</span></strong><span class="koboSpan" id="kobo.437.1"> repository, then this </span><strong class="source-inline"><span class="koboSpan" id="kobo.438.1">apt-get install</span></strong><span class="koboSpan" id="kobo.439.1"> command will fail with an error message saying that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.440.1">dotnet-sdk</span><a id="_idTextAnchor390"/><span class="koboSpan" id="kobo.441.1">-6.0</span></strong><span class="koboSpan" id="kobo.442.1"> package could not </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">be found.</span></span></p>
<h3><span class="koboSpan" id="kobo.444.1">Setting up a service in Linux</span></h3>
<p><span class="koboSpan" id="kobo.445.1">Most </span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.446.1">applications run as processes within Linux that run perpetually. </span><span class="koboSpan" id="kobo.446.2">This is often the case when the application needs to listen for network traffic – such as a web server. </span><span class="koboSpan" id="kobo.446.3">Another great benefit of setting up a service in Linux is that the operating system can auto-start the service every time the VM reboots. </span><span class="koboSpan" id="kobo.446.4">To do that, you need to set up a service </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">definition file:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.448.1">
[Unit]
Description=Fleet Portal
[Service]
WorkingDirectory=/var/www/fleet-portal
ExecStart=/usr/bin/dotnet /var/www/fleet-portal/FleetPortal.dll
Restart=always
RestartSec=10  # Restart service after 10 seconds if the dotnet service crashes
SyslogIdentifier=fleet-portal
User=fleet-portal-svc
Environment=ASPNETCORE_ENVIRONMENT=Production
Environment=DOTNET_PRINT_TELEMETRY_MESSAGE=false
[Install]
WantedBy=multi-user.target</span></pre> <p><span class="koboSpan" id="kobo.449.1">This service file needs to be copied to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">/etc/systemd/system</span></strong><span class="koboSpan" id="kobo.451.1"> folder. </span><span class="koboSpan" id="kobo.451.2">By running the </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">systemctl</span></strong><span class="koboSpan" id="kobo.453.1"> command, it will be enabled so that the operating system will automatically start the service when the machine reboots. </span><span class="koboSpan" id="kobo.453.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.454.1">systemctl</span></strong><span class="koboSpan" id="kobo.455.1"> command is also </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.456.1">useful to </span><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">start</span></strong><span class="koboSpan" id="kobo.458.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">stop</span></strong><span class="koboSpan" id="kobo.460.1">, and check the </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">status</span></strong><span class="koboSpan" id="kobo.462.1"> value of </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">your service.</span></span></p>
<p><span class="koboSpan" id="kobo.464.1">It’s best practice to run services using their own identity. </span><span class="koboSpan" id="kobo.464.2">This allows you to grant the service access to only the resources on the VM that </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">it needs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.466.1">
  provisioner “shell” {
    execute_command = local.execute_command
    inline = [
      “groupadd fleet-portal-svc”,
      “useradd -g fleet-portal-svc fleet-portal-svc”,
      “mkdir -p /var/www/fleet-portal”,
      “chown -R fleet-portal-svc:fleet-portal-svc /var/www/fleet-portal”
    ]
  }</span></pre> <p><span class="koboSpan" id="kobo.467.1">The preceding code sets up a local user and group for the service to run under and changes the ownership of the application’s folder at </span><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">/var/www/fleet-portal</span></strong><span class="koboSpan" id="kobo.469.1"> so that the service’s user account has sufficient access to the application’s executable and supporting files. </span><span class="koboSpan" id="kobo.469.2">Both the user and the application’s working directory are specified in the service </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">definition file.</span></span></p>
<p><span class="koboSpan" id="kobo.471.1">Once the user is ready, we can install the service definition file and enable </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">the service:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.473.1">
provisioner “shell” {
  execute_command = local.execute_command
  inline = [
    “cp /tmp/fleet-portal.service /etc/systemd/system/fleet-portal.service”,
    “systemctl enable fleet-portal.service”
  ]
}</span></pre> <p><span class="koboSpan" id="kobo.474.1">This</span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.475.1"> concludes the operating system configuration, which can be baked into the VM image. </span><span class="koboSpan" id="kobo.475.2">Any additional configuration steps require more information from the cloud env</span><a id="_idTextAnchor391"/><span class="koboSpan" id="kobo.476.1">ironment that </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">Terraform provisions.</span></span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor392"/><span class="koboSpan" id="kobo.478.1">Terraform</span></h2>
<p><span class="koboSpan" id="kobo.479.1">As we</span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.480.1"> discussed in our design, our solution </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.481.1">is made up of two application components: the frontend and the backend. </span><span class="koboSpan" id="kobo.481.2">Each has an application code base that needs to be deployed. </span><span class="koboSpan" id="kobo.481.3">Since this is the first time we will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">aws</span></strong><span class="koboSpan" id="kobo.483.1"> provider, we’ll look at the basic provider setup and how to configure the backend before we look at the nuts and bolts o</span><a id="_idTextAnchor393"/><span class="koboSpan" id="kobo.484.1">f each component of </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">our architecture.</span></span></p>
<h3><span class="koboSpan" id="kobo.486.1">Provider setup</span></h3>
<p><span class="koboSpan" id="kobo.487.1">We need</span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.488.1"> to specify all the providers that we intend to use in this solution within the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">required_providers</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.490.1"> block:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.491.1">
terraform {
  required_providers {
    aws = {
      source  = “hashicorp/aws”
      version = “~&gt; 5.17”
    }
    cloudinit = {
      source  = “hashicorp/cloudinit”
      version = “~&gt; 2.3.2”
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.492.1">We also need to configure the AWS provider to ensure that it uses the desired target region using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">primary_region</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.494.1">input variable:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.495.1">
provider “aws” {
  region = var.primary_region
}</span></pre> <p><span class="koboSpan" id="kobo.496.1">Sometimes, you</span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.497.1"> may want to add a secondary region in the future, so it’s a good idea to establish the primary region when you start the project. </span><span class="koboSpan" id="kobo.497.2">Even if you only deploy to one region, you still have a </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.498.1">primary region</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.500.1">The AWS provider does require some additional parameters to specify the credentials to use to connect to AWS, but because these are sensitive values, we don’t want to embed them into the code. </span><span class="koboSpan" id="kobo.500.2">We’ll pass those values in later when we automate the deployment using the standard AWS </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">AWS_ACCESS_KEY_ID</span></strong><span class="koboSpan" id="kobo.502.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">AWS_SECRET_ACCESS_KEY</span></strong><span class="koboSpan" id="kobo.504.1"> environment variables. </span><span class="koboSpan" id="kobo.504.2">It’s important to note that there are many different ways to configure the AWS provider to authenticate with AWS. </span><span class="koboSpan" id="kobo.504.3">I recommend using environment variables as it is a consistent approach across cloud platforms and other Terraform providers, and it integrates easily with different pipeline tools, such as GitHub Actions, which we’ll be using i</span><a id="_idTextAnchor394"/><span class="koboSpan" id="kobo.505.1">n the next section and </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">future chapters.</span></span></p>
<h3><span class="koboSpan" id="kobo.507.1">Backend</span></h3>
<p><span class="koboSpan" id="kobo.508.1">Because</span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.509.1"> we will be using a CI/CD pipeline to provision and maintain our environment in the long term, we need to set up a remote backend for our Terraform state. </span><span class="koboSpan" id="kobo.509.2">Because our solution will be hosted on AWS, we’ll use the AWS </span><strong class="bold"><span class="koboSpan" id="kobo.510.1">Simple Storage Service</span></strong><span class="koboSpan" id="kobo.511.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.512.1">S3</span></strong><span class="koboSpan" id="kobo.513.1">) backend to store our </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">Terraform state.</span></span></p>
<p><span class="koboSpan" id="kobo.515.1">Just like the AWS provider, we don’t want to hard code the backend configuration in our code, so we’ll simply set up a placeholder for </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">the backend:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.517.1">
terraform {
  ...
  </span><span class="koboSpan" id="kobo.517.2">backend “s3” {
  }
}</span></pre> <p><span class="koboSpan" id="kobo.518.1">We’ll </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.519.1">configure the backend’s parameters using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">-backend-config</span></strong><span class="koboSpan" id="kobo.521.1"> parameters when we run </span><strong class="source-inline"><span class="koboSpan" id="kobo.522.1">terraform init</span></strong><span class="koboSpan" id="kobo.523.1"> in our </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">CI/CD pipeline.</span></span></p>
<p><span class="koboSpan" id="kobo.525.1">It’s important to ensure that the AWS IAM identity you use to authenticate with AWS has access to this S3 bucket. </span><span class="koboSpan" id="kobo.525.2">Other</span><a id="_idTextAnchor395"/><span class="koboSpan" id="kobo.526.1">wise, you will get </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">authentication errors.</span></span></p>
<h3><span class="koboSpan" id="kobo.528.1">Input variables</span></h3>
<p><span class="koboSpan" id="kobo.529.1">It’s good</span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.530.1"> practice to pass in short names that identify the application’s name and the application’s environment. </span><span class="koboSpan" id="kobo.530.2">This allows you to embed consistent naming conventions across the resources that make up your solution, which makes it easier to identify and track resources from the </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">AWS Console.</span></span></p>
<p><span class="koboSpan" id="kobo.532.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">primary_region</span></strong><span class="koboSpan" id="kobo.534.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.535.1">vpc_cidr_block</span></strong><span class="koboSpan" id="kobo.536.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">az_count</span></strong><span class="koboSpan" id="kobo.538.1"> input variables drive key architectural characteristics of the deployment. </span><span class="koboSpan" id="kobo.538.2">They can’t be hard-coded as it would limit the reusability of the Terraform </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">code base.</span></span></p>
<p><span class="koboSpan" id="kobo.540.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.541.1">vpc_cidr_block</span></strong><span class="koboSpan" id="kobo.542.1"> input variable establishes the virtual network address space, which is often tightly regulated by an enterprise governance body. </span><span class="koboSpan" id="kobo.542.2">There is usually a process to ensure that teams across an organization do not use IP address ranges that conflict, thus making it impossible to allow those two applications to integrate or integrate with shared network resources within the enterprise in </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">the future.</span></span></p>
<p><span class="koboSpan" id="kobo.544.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.545.1">az_count</span></strong><span class="koboSpan" id="kobo.546.1"> input variable allows us to configure how much redundancy we want within our solution. </span><span class="koboSpan" id="kobo.546.2">This will affect the high availability of the solution but also the cost of the deployment. </span><span class="koboSpan" id="kobo.546.3">As you can imagine, cost is also a tightly regulated character</span><a id="_idTextAnchor396"/><span class="koboSpan" id="kobo.547.1">istic of cloud </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">infrastructure deployments.</span></span></p>
<h3><span class="koboSpan" id="kobo.549.1">Consistent naming and tagging</span></h3>
<p><span class="koboSpan" id="kobo.550.1">The AWS console is designed in such a way that it’s rather difficult to get an application-centric view</span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.551.1"> of your deployment. </span><span class="koboSpan" id="kobo.551.2">This is why it’s extremely important to leave breadcrumbs within the resources that you deploy that indicate what application and environment they belong to. </span><span class="koboSpan" id="kobo.551.3">Almost all resources within the AWS provider have a </span><strong class="source-inline"><span class="koboSpan" id="kobo.552.1">map</span></strong><span class="koboSpan" id="kobo.553.1"> attribute </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">tags</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.557.1">
resource “aws_vpc” “main” {
  cidr_block = var.vpc_cidr_block
  tags = {
    Name        = “${var.application_name}-${var.environment_name}-network”
    application = var.application_name
    environment = var.environment_name
  }
}</span></pre> <p><span class="koboSpan" id="kobo.558.1">You should make a habit of setting both the AWS console-recognized </span><strong class="source-inline"><span class="koboSpan" id="kobo.559.1">Name</span></strong><span class="koboSpan" id="kobo.560.1"> tag and a tagging scheme for your devices that establishes application and environment ownership of that resource. </span><span class="koboSpan" id="kobo.560.2">For our solution, we use two top-level input variables, </span><strong class="source-inline"><span class="koboSpan" id="kobo.561.1">application_name</span></strong><span class="koboSpan" id="kobo.562.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">environment_name</span></strong><span class="koboSpan" id="kobo.564.1">, to set this context, and we’ll embed these values on all the resources that </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">we provision.</span></span></p>
<p><span class="koboSpan" id="kobo.566.1">AWS can create an application-centric view within the AWS console using something called a resource group. </span><span class="koboSpan" id="kobo.566.2">Unlike on other platforms, a resource group on AWS is not a strong boundary around a set of resources but a loosely coupled relationship between resources derived from a common </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">tagging scheme:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.568.1">
resource “aws_resourcegroups_group” “main” {
  name = “${var.application_name}-${var.environment_name}”
  resource_query {
    query = jsonencode(
      {
        ResourceTypeFilters = [
          “AWS::AllSupported”
        ]
        TagFilters = [
          {
            Key    = “application”
            Values = [var.application_name]
          },
          {
            Key    = “environment”
            Values = [var.environment_name]
          }
        ]
      }
    )
  }
}</span></pre> <p><span class="koboSpan" id="kobo.569.1">The</span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.570.1"> preceding code creates an AWS resource group that creates a central location where you can access all of your related resources from one place. </span><span class="koboSpan" id="kobo.570.2">Simply adding </span><strong class="source-inline"><span class="koboSpan" id="kobo.571.1">application</span></strong><span class="koboSpan" id="kobo.572.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.573.1">environment</span></strong><span class="koboSpan" id="kobo.574.1"> tags to all your resources will </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">include them.</span></span></p>
<h3><span class="koboSpan" id="kobo.576.1">Virtual network</span></h3>
<p><span class="koboSpan" id="kobo.577.1">Because our</span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.578.1"> solution is a standard three-tier architecture, we are configuring our virtual network into public and private subnets for the frontend and backend </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">application components.</span></span></p>
<p><span class="koboSpan" id="kobo.580.1">We want to distribute our VMs across the Availability Zones to ensure the high availability of our solution. </span><span class="koboSpan" id="kobo.580.2">Rather than hard-code the Availability Zones or just take the first two, we</span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.581.1"> can randomly select the number of Availability Zones we want from the list of available ones for the given region using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.582.1">aws_availability_zones</span></strong><span class="koboSpan" id="kobo.583.1"> data source and a </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">random_shuffle</span></strong><span class="koboSpan" id="kobo.585.1"> resource from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">random</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.587.1"> provider:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.588.1">
data “aws_availability_zones” “available” {
  state = “available”
}
resource “random_shuffle” “az” {
  input        = data.aws_availability_zones.available.names
  result_count = var.az_count
}</span></pre> <p><span class="koboSpan" id="kobo.589.1">If the </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">az_count</span></strong><span class="koboSpan" id="kobo.591.1"> input variable has a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.592.1">2</span></strong><span class="koboSpan" id="kobo.593.1">, then the preceding code will randomly select two Availability Zones from the region of the current AWS provider. </span><span class="koboSpan" id="kobo.593.2">Remember that the AWS provider is scoped to a particular region, and when we initialized the provider, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.594.1">primary_region</span></strong><span class="koboSpan" id="kobo.595.1"> input variable to set </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">that value.</span></span></p>
<p><span class="koboSpan" id="kobo.597.1">Rather than hard-code the address space for our subnets, it would be nice if we could calculate our subnets’ address space using HCL’s built-in functions. </span><span class="koboSpan" id="kobo.597.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.598.1">cidrsubnet</span></strong><span class="koboSpan" id="kobo.599.1"> function allows us to take an address space and split it into smaller </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">address spaces:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.601.1">
locals {
  azs_random = random_shuffle.az.result
  public_subnets = { for k, v in local.azs_random :
    k =&gt; {
      cidr_block        = cidrsubnet(var.vpc_cidr_block, var.cidr_split_bits, k)
      availability_zone = v
    }
  }
  private_subnets = { for k, v in local.azs_random :
    k =&gt; {
      cidr_block        = cidrsubnet(var.vpc_cidr_block, var.cidr_split_bits, k + var.az_count)
      availability_zone = v
    }
  }
}</span></pre> <p><span class="koboSpan" id="kobo.602.1">The</span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.603.1"> preceding code will generate two maps, one for the public subnets and another for the private subnets. </span><span class="koboSpan" id="kobo.603.2">It accomplishes this by taking the randomly selected Availability Zones and using </span><strong class="source-inline"><span class="koboSpan" id="kobo.604.1">cidrsubnet</span></strong><span class="koboSpan" id="kobo.605.1"> to grab the next available block of </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">/24</span></strong><span class="koboSpan" id="kobo.607.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.608.1">256</span></strong><span class="koboSpan" id="kobo.609.1"> IP addresses for each (this is more than enough for our application to scale to a huge number of VMs in each Availability Zone in both the frontend and </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">the backend):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.611.1">
public_subnets = {
  “0” = {
    “availability_zone” = “us-west-2c”
    “cidr_block” = “10.0.0.0/24”
  }
  “1” = {
    “availability_zone” = “us-west-2a”
    “cidr_block” = “10.0.1.0/24”
  }
}
private_subnets = {
  “0” = {
    “availability_zone” = “us-west-2c”
    “cidr_block” = “10.0.2.0/24”
  }
  “1” = {
    “availability_zone” = “us-west-2a”
    “cidr_block” = “10.0.3.0/24”
  }
}</span></pre> <p><span class="koboSpan" id="kobo.612.1">The </span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.613.1">preceding code is the value that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.614.1">public_subnets</span></strong><span class="koboSpan" id="kobo.615.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.616.1">private_subnets</span></strong><span class="koboSpan" id="kobo.617.1"> maps will have when evaluated with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">vpc_cidr_block</span></strong><span class="koboSpan" id="kobo.619.1"> value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.620.1">10.0.0.0/16</span></strong><span class="koboSpan" id="kobo.621.1">, a </span><strong class="source-inline"><span class="koboSpan" id="kobo.622.1">cidr_split_bits</span></strong><span class="koboSpan" id="kobo.623.1"> value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">8</span></strong><span class="koboSpan" id="kobo.625.1"> and an </span><strong class="source-inline"><span class="koboSpan" id="kobo.626.1">az_count</span></strong><span class="koboSpan" id="kobo.627.1"> value </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.629.1">2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.631.1">By manipulating these input variables, we can reasonably size the virtual network and its corresponding subnets so that we don’t monopolize available address spaces for other applications that we may want to provision within the broader organization. </span><span class="koboSpan" id="kobo.631.2">For example, setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.632.1">vpc_cidr_block</span></strong><span class="koboSpan" id="kobo.633.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">10.0.0.0/22</span></strong><span class="koboSpan" id="kobo.635.1"> allocates a total IP address count of </span><strong class="source-inline"><span class="koboSpan" id="kobo.636.1">1024</span></strong><span class="koboSpan" id="kobo.637.1"> to our application. </span><span class="koboSpan" id="kobo.637.2">With an </span><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">az_count</span></strong><span class="koboSpan" id="kobo.639.1"> value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.640.1">2</span></strong><span class="koboSpan" id="kobo.641.1"> and a </span><strong class="source-inline"><span class="koboSpan" id="kobo.642.1">cidr_split_bits</span></strong><span class="koboSpan" id="kobo.643.1"> value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">2</span></strong><span class="koboSpan" id="kobo.645.1">, we can allocate address space for our four subnets, each with </span><strong class="source-inline"><span class="koboSpan" id="kobo.646.1">/24</span></strong><span class="koboSpan" id="kobo.647.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.648.1">256</span></strong><span class="koboSpan" id="kobo.649.1"> IP addresses. </span><span class="koboSpan" id="kobo.649.2">This gives us sufficient room for our application to scale without over-allocating valuable IP </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">address space:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.651.1">
resource “aws_subnet” “frontend” {
  for_each = local.public_subnets
  vpc_id            = aws_vpc.main.id
  availability_zone = each.value.availability_zone
  cidr_block        = each.value.cidr_block
}</span></pre> <p><span class="koboSpan" id="kobo.652.1">We </span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.653.1">create each subnet by iterating over the corresponding map of subnet address spaces. </span><span class="koboSpan" id="kobo.653.2">The preceding code demonstrates how we can use this map to set the correct Availability Zone and address space for </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">each subnet.</span></span></p>
<h3><span class="koboSpan" id="kobo.655.1">Network routing</span></h3>
<p><span class="koboSpan" id="kobo.656.1">As per </span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.657.1">our design, the public subnets route internet traffic to the </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">internet gateway:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.659.1">
resource “aws_route_table” “frontend” {
  vpc_id = aws_vpc.main.id
  route {
    cidr_block = “0.0.0.0/0”
    gateway_id = aws_internet_gateway.main.id
  }
}
resource “aws_route_table_association” “frontend” {
  for_each = aws_subnet.frontend
  subnet_id      = each.value.id
  route_table_id = aws_route_table.frontend.id
}</span></pre> <p><span class="koboSpan" id="kobo.660.1">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.661.1">aws_route_table</span></strong><span class="koboSpan" id="kobo.662.1"> resource to define the route and then </span><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">aws_route_table_association</span></strong><span class="koboSpan" id="kobo.664.1"> to link the route table to the </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">corresponding subnet.</span></span></p>
<p><span class="koboSpan" id="kobo.666.1">The </span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.667.1">private subnets route their internet traffic to a NAT gateway, which is provisioned in each </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">private subnet:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.669.1">
resource “aws_eip” “nat” {
  for_each = local.private_subnets
}
resource “aws_nat_gateway” “nat” {
  for_each = local.private_subnets
  allocation_id = aws_eip.nat[each.key].id
  subnet_id     = aws_subnet.backend[each.key].id
  depends_on = [aws_internet_gateway.main]
}</span></pre> <p><span class="koboSpan" id="kobo.670.1">Because each private subnet has its own NAT gateway, we need a route table for each subnet to route the traffic to the correct </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">NAT gateway:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.672.1">
resource “aws_route_table” “backend” {
  for_each = local.private_subnets
  vpc_id = aws_vpc.main.id
  route {
    cidr_block     = “0.0.0.0/0”
    nat_gateway_id = aws_nat_gateway.nat[each.key].id
  }
}
resource “aws_route_table_association” “backend” {
  for_each = local.private_subnets
  subnet_id      = aws_subnet.backend[each.key].id
  route_table_id = aws_route_table.backend[each.key].id
}</span></pre> <p><span class="koboSpan" id="kobo.673.1">Notice</span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.674.1"> that, unlike the public subnets, which share the same route table, we need to iterate on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.675.1">private_subnets</span></strong><span class="koboSpan" id="kobo.676.1"> map to create a different route table for each private subnet and associate it with the corresponding private subnet using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">each</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.678.1"> symbol.</span></span></p>
<h3><span class="koboSpan" id="kobo.679.1">Load balancing</span></h3>
<p><span class="koboSpan" id="kobo.680.1">As per </span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.681.1">our design, we need two AWS ALB instances – one for the frontend and another for the backend. </span><span class="koboSpan" id="kobo.681.2">We’ll use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.682.1">aws_lb</span></strong><span class="koboSpan" id="kobo.683.1"> resource and related resources with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.684.1">aws_lb</span></strong><span class="koboSpan" id="kobo.685.1"> prefix to provision the </span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.686.1">target group and </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">listener configuration:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.688.1">
resource “aws_lb_target_group” “frontend_http” {
  name                          = “${var.application_name}-${var.environment_name}-frontend-http”
  port                          = 5000
  protocol                      = “HTTP”
  vpc_id                        = aws_vpc.main.id
  slow_start                    = 0
  load_balancing_algorithm_type = “round_robin”
  stickiness {
    enabled = true
    type    = “lb_cookie”
  }
  health_check {
    enabled             = true
    port                = 5000
    interval            = 30
    protocol            = “HTTP”
    path                = “/”
    matcher             = 200
    healthy_threshold   = 3
    unhealthy_threshold = 3
  }
}</span></pre> <p><span class="koboSpan" id="kobo.689.1">Notice that the sticky session configuration needed for the ASP.NET Core Blazor Web application’s WebSocket configuration is implemented by a nested </span><strong class="source-inline"><span class="koboSpan" id="kobo.690.1">stickiness</span></strong><span class="koboSpan" id="kobo.691.1"> block. </span><span class="koboSpan" id="kobo.691.2">Likewise, the</span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.692.1"> health probe is implemented by a nested </span><strong class="source-inline"><span class="koboSpan" id="kobo.693.1">health_check</span></strong><span class="koboSpan" id="kobo.694.1"> block. </span><span class="koboSpan" id="kobo.694.2">This structure will be identical for both the frontend and the backend, but the configuration will differ slightly, with the backend not requiring sticky sessions and having a different path for the </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">health probe.</span></span></p>
<p><span class="koboSpan" id="kobo.696.1">The VMs are explicitly included in the target group using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.697.1">aws_lb_target_group_attachment</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.698.1"> resource:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.699.1">
resource “aws_lb_target_group_attachment” “frontend_http” {
  for_each = aws_instance.frontend
  target_group_arn = aws_lb_target_group.frontend_http.arn
  target_id        = each.value.id
  port             = 5000
}</span></pre> <p><span class="koboSpan" id="kobo.700.1">Notice that we are iterating over the corresponding </span><strong class="source-inline"><span class="koboSpan" id="kobo.701.1">aws_instance</span></strong><span class="koboSpan" id="kobo.702.1"> resource map and referencing the AWS EC2 instance ID </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.704.1">each.value.id</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.706.1">Finally, we must provision the AWS </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">ALB itself:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.708.1">
resource “aws_lb” “frontend” {
  name               = “${var.application_name}
${var.environment_name}-frontend”
  internal           = false
  load_balancer_type = “application”
  subnets            = [for subnet in values(aws_subnet.frontend) : subnet.id]
  security_groups    = [aws_security_group.frontend_lb.id]
  tags = {
    Name        = “${var.application_name}-${var.environment_name}-frontend-lb”
    application = var.application_name
    environment = var.environment_name
  }
}</span></pre> <p><span class="koboSpan" id="kobo.709.1">Notice that we are dynamically constructing a list of subnets using the corresponding </span><strong class="source-inline"><span class="koboSpan" id="kobo.710.1">aws_subnet</span></strong><span class="koboSpan" id="kobo.711.1"> resource map. </span><span class="koboSpan" id="kobo.711.2">When a resource block is provisioned with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.712.1">count</span></strong><span class="koboSpan" id="kobo.713.1"> value, that resource </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.714.1">block becomes a list, while when it is provisioned with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.715.1">for_each</span></strong><span class="koboSpan" id="kobo.716.1"> iterator, it becomes a map. </span><span class="koboSpan" id="kobo.716.2">This is an important detail to pay attention to when you want to reference it from </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">other resources.</span></span></p>
<p><span class="koboSpan" id="kobo.718.1">Lastly, we must connect our AWS ALB to the target group using </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">the listener:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.720.1">
resource “aws_lb_listener” “frontend_http” {
  load_balancer_arn = aws_lb.frontend.arn
  port              = “80”
  protocol          = “HTTP”
  default_action {
    type             = “forward”
    target_group_arn =
aws_lb_target_group.frontend_http.arn
  }
}</span></pre> <h3><span class="koboSpan" id="kobo.721.1">Network security</span></h3>
<p><span class="koboSpan" id="kobo.722.1">As per </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.723.1">our design, we have three logical components of our solution architecture through which network traffic will pass. </span><span class="koboSpan" id="kobo.723.2">Each needs its own security group and set of rules to allow ingress and </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">egress traffic:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.725.1">
resource “aws_security_group” “frontend_lb” {
  name        = “${var.application_name}-${var.environment_name}-frontend-lb-sg”
  description = “Security group for the load balancer”
  vpc_id      = aws_vpc.main.id
}</span></pre> <p><span class="koboSpan" id="kobo.726.1">A security group is created using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.727.1">aws_security_group</span></strong><span class="koboSpan" id="kobo.728.1"> resource and attached to a </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">virtual network.</span></span></p>
<p><span class="koboSpan" id="kobo.730.1">Not all components within the architecture will need both ingress and egress rules, but it’s important to think about all the ways network traffic should be allowed to flow through </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">the system:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.732.1">
resource “aws_security_group_rule” “frontend_lb_ingress_http” {
  type              = “ingress”
  from_port         = 80
  to_port           = 80
  protocol          = “tcp”
  security_group_id = aws_security_group.frontend_lb.id
  cidr_blocks       = [“0.0.0.0/0”]
}
resource “aws_security_group_rule” “frontend_lb_egress_http” {
  type                     = “egress”
  from_port                = 5000
  to_port                  = 5000
  protocol                 = “tcp”
  security_group_id        = aws_security_group.frontend_lb.id
  source_security_group_id = aws_security_group.frontend.id
}</span></pre> <p><span class="koboSpan" id="kobo.733.1">The</span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.734.1"> preceding code establishes the rules we designed for the frontend load balancer, which allows traffic in from the internet (for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.735.1">0.0.0.0/0</span></strong><span class="koboSpan" id="kobo.736.1">) and allows traffic out to the frontend VMs (for </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">example, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.738.1">aws_security_group.frontend.id</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">).</span></span></p>
<h3><span class="koboSpan" id="kobo.740.1">Secrets management</span></h3>
<p><span class="koboSpan" id="kobo.741.1">To allow our</span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.742.1"> VMs to access our AWS Secrets Manager resources, we need to define an IAM role and associate it with our VMs. </span><span class="koboSpan" id="kobo.742.2">This will allow our VMs to operate under the security context defined by the IAM policies attached to this </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">IAM role:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.744.1">
resource “aws_iam_role” “backend” {
  name = “${var.application_name}-${var.environment_name}-backend”
  assume_role_policy = jsonencode({
    Version = “2012-10-17”
    Statement = [
      {
        Action = “sts:AssumeRole”
        Effect = “Allow”
        Sid    = “”
        Principal = {
          Service = “ec2.amazonaws.com”
        }
      },
    ]
  })
}</span></pre> <p><span class="koboSpan" id="kobo.745.1">The </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.746.1">preceding code creates the IAM role for the backend VMs, which need access to the PostgreSQL database’s connection string that we will store in AWS Secrets Manager. </span><span class="koboSpan" id="kobo.746.2">The IAM role itself doesn’t do anything unless there is a policy defined. </span><span class="koboSpan" id="kobo.746.3">We need to attach a policy definition to the role to grant specific privileges to </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">the VMs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.748.1">
resource “aws_iam_role_policy” “backend” {
  name = “${var.application_name}-${var.environment_name}-backend”
  role = aws_iam_role.backend.id
  policy = jsonencode({
    Version = “2012-10-17”
    Statement = [
      {
        Action = [
          “secretsmanager:GetSecretValue”,
        ]
        Effect   = “Allow”
        Resource = “arn:aws:secretsmanager:secret:${var.application_name}/${var.environment_name}/*”
      },
    ]
  })
}</span></pre> <p><span class="koboSpan" id="kobo.749.1">The </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.750.1">preceding code grants access to all VMs operating with this IAM role associated with accessing AWS Secrets Manager secrets that begin with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.751.1">fleet-ops/dev</span></strong><span class="koboSpan" id="kobo.752.1"> prefix. </span><span class="koboSpan" id="kobo.752.2">We must build this prefix using our standard naming convention input variables, </span><strong class="source-inline"><span class="koboSpan" id="kobo.753.1">application_name</span></strong><span class="koboSpan" id="kobo.754.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.755.1">environment_name</span></strong><span class="koboSpan" id="kobo.756.1">, which have </span><strong class="source-inline"><span class="koboSpan" id="kobo.757.1">fleet-ops</span></strong><span class="koboSpan" id="kobo.758.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.759.1">dev</span></strong><span class="koboSpan" id="kobo.760.1"> as values, respectively. </span><span class="koboSpan" id="kobo.760.2">When we provision the production version of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.761.1">fleet-ops</span></strong><span class="koboSpan" id="kobo.762.1"> platform, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.763.1">environment_name</span></strong><span class="koboSpan" id="kobo.764.1"> input variable will be set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.765.1">prod</span></strong><span class="koboSpan" id="kobo.766.1">, ensuring that the VMs in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.767.1">dev</span></strong><span class="koboSpan" id="kobo.768.1"> environment don’t have access to the secrets in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.769.1">prod</span></strong><span class="koboSpan" id="kobo.770.1"> environment. </span><span class="koboSpan" id="kobo.770.2">Deploying the different environments of our application into isolated AWS accounts would also create a more secure </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">security boundary.</span></span></p>
<h3><span class="koboSpan" id="kobo.772.1">VMs</span></h3>
<p><span class="koboSpan" id="kobo.773.1">When </span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.774.1">provisioning static VMs, we have much more control over the configuration of each machine. </span><span class="koboSpan" id="kobo.774.2">Some VMs have specific network and storage configurations to meet </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">workload demands:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.776.1">
resource “aws_network_interface” “frontend” {
  for_each = aws_subnet.frontend
  subnet_id = each.value.id
}
resource “aws_network_interface_sg_attachment” “frontend” {
  for_each = aws_instance.frontend
  security_group_id    = aws_security_group.frontend.id
  network_interface_id = each.value.primary_network_interface_id
}</span></pre> <p><span class="koboSpan" id="kobo.777.1">The </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.778.1">preceding code creates a network interface that we can then attach to a VM. </span><span class="koboSpan" id="kobo.778.2">Notice that we are iterating over the frontend subnets. </span><span class="koboSpan" id="kobo.778.3">This will ensure we have exactly one VM in each subnet (and consequently each Availability Zone). </span><span class="koboSpan" id="kobo.778.4">This network interface is where we attach the security group for VMs in </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">the frontend.</span></span></p>
<p><span class="koboSpan" id="kobo.780.1">Finally, we provision the VM using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">aws_instance</span></strong><span class="koboSpan" id="kobo.782.1"> resource, taking care to use the correct instance type, network interface, and </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">AWS AMI:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.784.1">
resource “aws_instance” “frontend” {
  for_each = aws_subnet.frontend
  ami           = data.aws_ami.frontend.id
  instance_type = var.frontend_instance_type
  key_name      = data.aws_key_pair.main.key_name
  user_data     = data.cloudinit_config.frontend.rendered
  monitoring    = true
  network_interface {
    network_interface_id = aws_network_interface.frontend[each.key].id
    device_index         = 0
  }
}</span></pre> <p><span class="koboSpan" id="kobo.785.1">AWS has a </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.786.1">cross-cutting service called CloudWatch that collects logs and telemetry across the various AWS services. </span><span class="koboSpan" id="kobo.786.2">To enable CloudWatch on your EC2 instances, you simply need to add the </span><strong class="source-inline"><span class="koboSpan" id="kobo.787.1">monitoring</span></strong><span class="koboSpan" id="kobo.788.1"> attribute and set it </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.790.1">true</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.792.1">Monitoring</span></h3>
<p><span class="koboSpan" id="kobo.793.1">Depending </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.794.1">on the service and its available configuration options within the Terraform resources used to provision it, to activate CloudWatch, you might need to go through the process of provisioning additional resources and setting up additional IAM permissions to grant the respective resource to write </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">to CloudWatch.</span></span></p>
<p><span class="koboSpan" id="kobo.796.1">The first thing we need to set up is an IAM policy that will allow the specific service access to assume an IAM role. </span><span class="koboSpan" id="kobo.796.2">In this case, we are granting VPC Flow Logs access to assume an </span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">IAM role:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.798.1">
data “aws_iam_policy_document” “vpc_assume_role” {
  statement {
    effect = “Allow”
    principals {
      type        = “Service”
      identifiers = [“vpc-flow-logs.amazonaws.com”]
    }
    actions = [“sts:AssumeRole”]
  }
}</span></pre> <p><span class="koboSpan" id="kobo.799.1">We’ll use this policy when we set up the IAM role to grant the VPC Flow Logs service access to this particular IAM role. </span><span class="koboSpan" id="kobo.799.2">This will be important later when we link </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">everything together:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.801.1">
resource “aws_iam_role” “vpc” {
  name               = “${var.application_name}-${var.environment_name}-network”
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}</span></pre> <p><span class="koboSpan" id="kobo.802.1">The </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.803.1">preceding code allows VPC Flow Logs to assume this role, eventually granting it access to writing logs </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">to CloudWatch.</span></span></p>
<p><span class="koboSpan" id="kobo.805.1">Next, we need to set up another IAM policy that will grant access to write to CloudWatch logs. </span><span class="koboSpan" id="kobo.805.2">You can further narrow the scope of an access policy by narrowing the allowed actions and the allowed resources the policy grants </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">access to:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.807.1">
data “aws_iam_policy_document” “cloudwatch” {
  statement {
    effect = “Allow”
    actions = [
      “logs:CreateLogGroup”,
      “logs:CreateLogStream”,
      “logs:PutLogEvents”,
      “logs:DescribeLogGroups”,
      “logs:DescribeLogStreams”,
    ]
    resources = [“*”]
  }
}</span></pre> <p><span class="koboSpan" id="kobo.808.1">In the</span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.809.1"> preceding code, we do a good job of being specific about the types of operations we want to grant access to by giving specific operations such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.810.1">logs:PutLogEvents</span></strong><span class="koboSpan" id="kobo.811.1">. </span><span class="koboSpan" id="kobo.811.2">However, the resources are set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">*</span></strong><span class="koboSpan" id="kobo.813.1">, a very wide access level. </span><span class="koboSpan" id="kobo.813.2">We should consider narrowing that down to just the resources that </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">we need.</span></span></p>
<p><span class="koboSpan" id="kobo.815.1">The next step is to attach the policy to the </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">IAM role:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.817.1">
resource “aws_iam_role_policy” “cloudwatch” {
  name   = “${var.application_name}-${var.environment_name}-network-cloudwatch”
  role   = aws_iam_role.vpc.id
  policy = data.aws_iam_policy_document.cloudwatch.json
}</span></pre> <p><span class="koboSpan" id="kobo.818.1">At this point, we have an IAM role that is allowed to write to CloudWatch and we have allowed VPC Flow Logs to assume </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">this role.</span></span></p>
<p><span class="koboSpan" id="kobo.820.1">Next, we need to create a CloudWatch log group that will store the logs </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">from VPC:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.822.1">
resource “aws_cloudwatch_log_group” “vpc” {
  name = “${var.application_name}-${var.environment_name}-network”
}</span></pre> <p><span class="koboSpan" id="kobo.823.1">Finally, we’ll connect VPC Flow Logs to the log group and assign the IAM role it should use to gain access to write </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">to CloudWatch:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.825.1">
resource “aws_flow_log” “main” {
  iam_role_arn    = aws_iam_role.vpc.arn
  log_destination = aws_cloudwatch_log_group.vpc.arn
  traffic_type    = “ALL”
  vpc_id          = aws_vpc.main.id
}</span></pre> <p><span class="koboSpan" id="kobo.826.1">The </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.827.1">preceding code also links our VPC to the VPC Flow Logs service, thus completing the flow and placing the networking logs in the corresponding CloudWatch </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">log group.</span></span></p>
<p><span class="koboSpan" id="kobo.829.1">With that, we have implemented the Packer and Terraform solutions and have a working code base that will build VM images for both our frontend and backend application components while also provisioning our cloud environment into AWS. </span><span class="koboSpan" id="kobo.829.2">In the next section, we’ll dive into YAML and Bash and implement GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">Actions workflows.</span></span></p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor397"/><span class="koboSpan" id="kobo.831.1">Automating the deployment</span></h1>
<p><span class="koboSpan" id="kobo.832.1">As we</span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.833.1"> discussed in our design, our solution is made up of two application components: the frontend and the backend. </span><span class="koboSpan" id="kobo.833.2">Each</span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.834.1"> has a code base consisting of application code and an operating system configuration encapsulated within a Packer template. </span><span class="koboSpan" id="kobo.834.2">These two application components are then deployed into a cloud environment on AWS, which is defined within our Terraform </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">code base.</span></span></p>
<p><span class="koboSpan" id="kobo.836.1">There is an additional code base that we have yet to discuss: our automation pipelines. </span><span class="koboSpan" id="kobo.836.2">We will be implementing our automation pipelines using </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">GitHub Actions:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<span class="koboSpan" id="kobo.838.1"><img alt="Figure 7.21 – Source code structure within our GitHub repository" src="image/B21183_07_21..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.839.1">Figure 7.21 – Source code structure within our GitHub repository</span></p>
<p><span class="koboSpan" id="kobo.840.1">In GitHub Actions, automation pipelines are called workflows and they are stored in a particular</span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.841.1"> folder</span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.842.1"> within the source code repository, namely </span><strong class="source-inline"><span class="koboSpan" id="kobo.843.1">/.github/workflows</span></strong><span class="koboSpan" id="kobo.844.1">. </span><span class="koboSpan" id="kobo.844.2">Each of our code bases is stored in a separate folder. </span><span class="koboSpan" id="kobo.844.3">Our solutions source code repository’s folder structure looks </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">like this:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.846.1">
- .github
    - workflows
- dotnet
    - backend
    - frontend
- packer
    - backend
    - frontend
- terraform</span></pre> <p><span class="koboSpan" id="kobo.847.1">As per our </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.848.1">design, we will have GitHub Actions workflows that will execute Packer and build VM images for both the frontend (for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.849.1">packer-frontend.yaml</span></strong><span class="koboSpan" id="kobo.850.1">) and the backend (for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.851.1">packer-backend.yaml</span></strong><span class="koboSpan" id="kobo.852.1">). </span><span class="koboSpan" id="kobo.852.2">We’ll also have workflows that will run </span><strong class="source-inline"><span class="koboSpan" id="kobo.853.1">terraform plan</span></strong><span class="koboSpan" id="kobo.854.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.855.1">terraform apply</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.857.1">
- .github
    - workflows
        - packer-backend.yaml
        - packer-frontend.yaml
        - terraform-apply.yaml
        - terraform-plan.yaml</span></pre> <p><span class="koboSpan" id="kobo.858.1">Each folder </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.859.1">path will allow us to control which GitHub Actions workflows should trigger so that we aren’t unnecessarily running workflows when no applicable changes have </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">been made.</span></span></p>
<p><span class="koboSpan" id="kobo.861.1">Because we are following GitFlow, we’ll have a main branch where the production version of all of our code will reside. </span><span class="koboSpan" id="kobo.861.2">Developers, whether they are working on updates to the application code (for example, C#), the operating system configuration (for example, the Packer template), or the cloud environment configuration (for example, the Terraform template), will create a branch off of </span><strong class="source-inline"><span class="koboSpan" id="kobo.862.1">main</span></strong><span class="koboSpan" id="kobo.863.1"> with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.864.1">feature/*</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.865.1">naming convention.</span></span></p>
<p><span class="koboSpan" id="kobo.866.1">Once they’ve done this, they can submit a pull request. </span><span class="koboSpan" id="kobo.866.2">This indicates that the developer believes their code changes are ready to be merged back into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.867.1">main</span></strong><span class="koboSpan" id="kobo.868.1"> branch – in other words, their code changes are ready </span><span class="No-Break"><span class="koboSpan" id="kobo.869.1">for production!</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<span class="koboSpan" id="kobo.870.1"><img alt="Figure 7.22 – GitFlow’s pull request process" src="image/B21183_07_22..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.871.1">Figure 7.22 – GitFlow’s pull request process</span></p>
<p><span class="koboSpan" id="kobo.872.1">The pull request </span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.873.1">is a great time to perform some checks on our solution’s code. </span><span class="koboSpan" id="kobo.873.2">For the application code, this could take the form of a build, static code analysis, and unit or integration tests. </span><span class="koboSpan" id="kobo.873.3">Each of these actions tests a </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.874.1">different aspect of the application code. </span><span class="koboSpan" id="kobo.874.2">The build (that is, compiling the C# code base) is one of the most basic tests that we can perform. </span><span class="koboSpan" id="kobo.874.3">It simply tests whether the application code is valid C# and is devoid of inherent language syntax errors. </span><span class="koboSpan" id="kobo.874.4">Static code analysis can cover a wide range of code quality checks, including readability and maintainability or security and vulnerability assessments. </span><span class="koboSpan" id="kobo.874.5">The unit and integration tests check the functionality of the software components working individually and together to accomplish the underlying business purpose of the software. </span><span class="koboSpan" id="kobo.874.6">Executing</span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.875.1"> these tests regularly is known as </span><strong class="bold"><span class="koboSpan" id="kobo.876.1">continuous integration</span></strong><span class="koboSpan" id="kobo.877.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.878.1">CI</span></strong><span class="koboSpan" id="kobo.879.1">) and</span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.880.1"> is half of the famous and often </span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.881.1">elusive </span><strong class="bold"><span class="koboSpan" id="kobo.882.1">CI/CD pipeline</span></strong><span class="koboSpan" id="kobo.883.1">, where </span><strong class="bold"><span class="koboSpan" id="kobo.884.1">CD</span></strong><span class="koboSpan" id="kobo.885.1"> stands for </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.886.1">continuous delivery</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.888.1">The CI pipeline cuts down on routine work surrounding the built-in quality of the application code. </span><span class="koboSpan" id="kobo.888.2">Without it, these checks would need to be performed by humans through exhaustive code reviews and manual testing. </span><span class="koboSpan" id="kobo.888.3">We still need to do code reviews and manual testing, but a good CI pipeline will reduce the effort that humans need </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">to perform.</span></span></p>
<p><span class="koboSpan" id="kobo.890.1">Now that we’ve covered what built-in quality controls we can put on application code, what can we do with our operating system and our cloud environment configuration? </span><span class="koboSpan" id="kobo.890.2">Is there a way to test IaC without provisioning the infrastructure? </span><span class="koboSpan" id="kobo.890.3">There is, but there </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">are limitations.</span></span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor398"/><span class="koboSpan" id="kobo.892.1">Packer</span></h2>
<p><span class="koboSpan" id="kobo.893.1">Because</span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.894.1"> the VM image acts as an immutable artifact that contains a versioned copy of the application code and operating system configuration, we need to update this artifact any time something changes in either the application code or the operating </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">system configuration:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.896.1">
on:
  push:
    branches: 
    - main
    paths:
    - ‘src/packer/frontend/**’
    - ‘src/dotnet/frontend/**’</span></pre> <p><span class="koboSpan" id="kobo.897.1">This means that we need a trigger on both code bases that affect the final artifact for Packer, which includes the application code and the operating system configuration within the Packer template itself. </span><span class="koboSpan" id="kobo.897.2">With GitHub Actions, we can add a list of </span><strong class="source-inline"><span class="koboSpan" id="kobo.898.1">paths</span></strong><span class="koboSpan" id="kobo.899.1"> that will trigger </span><span class="No-Break"><span class="koboSpan" id="kobo.900.1">our workflow.</span></span></p>
<p><span class="koboSpan" id="kobo.901.1">We should build a new VM image every time there is a pull request and every time there is a push onto </span><strong class="source-inline"><span class="koboSpan" id="kobo.902.1">main</span></strong><span class="koboSpan" id="kobo.903.1">. </span><span class="koboSpan" id="kobo.903.2">When Packer is executed, it is essentially doing a pretty rigorous integration test. </span><span class="koboSpan" id="kobo.903.3">Therefore, it’s useful to have it performed as part of our CI process. </span><span class="koboSpan" id="kobo.903.4">That means we need to have a VM image that is tested and verified to be production-ready before we push the code into the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.904.1">main</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.905.1"> branch:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<span class="koboSpan" id="kobo.906.1"><img alt="Figure 7.23 – VM image versioning" src="image/B21183_07_23..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.907.1">Figure 7.23 – VM image versioning</span></p>
<p><span class="koboSpan" id="kobo.908.1">Our Packer workflow will generate a unique name and version for each VM image it produces. </span><span class="koboSpan" id="kobo.908.2">We can build tests into our Packer template to verify that the web server is running and listening on port </span><strong class="source-inline"><span class="koboSpan" id="kobo.909.1">5000</span></strong><span class="koboSpan" id="kobo.910.1">. </span><span class="koboSpan" id="kobo.910.2">Using this version of the image, we can also launch a new VM and inspect the operating system’s configuration ourselves to make sure everything is </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">in order.</span></span></p>
<p><span class="koboSpan" id="kobo.912.1">When we are confident that the code changes to either the application code or the operating system configuration are fully functional, we can approve the pull request and merge it into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.913.1">main</span></strong><span class="koboSpan" id="kobo.914.1"> branch. </span><span class="koboSpan" id="kobo.914.2">This will trigger a new version of the VM image from the production-ready code in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.915.1">main</span></strong><span class="koboSpan" id="kobo.916.1"> branch. </span><span class="koboSpan" id="kobo.916.2">We can use the new version of this production-ready VM image to update our cloud environment configuration when we are ready to deploy these changes to </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">our environments.</span></span></p>
<p><span class="koboSpan" id="kobo.918.1">The </span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.919.1">GitHub Actions workflow needs some ground rules to be established that control the specific versions of software and key locations within the code base. </span><span class="koboSpan" id="kobo.919.2">It’s important to always be specific. </span><span class="koboSpan" id="kobo.919.3">This means using specific versions of software instead of relying on the internet Gods to decide which version you’ll use. </span><span class="koboSpan" id="kobo.919.4">This might work well when you are running things locally on your machine and are there to solve the inevitable problems and conflicts that arise, but for an automation pipeline, there is no human there to correct things as they are happening; there are only assumptions – assumptions about what version of the software </span><span class="No-Break"><span class="koboSpan" id="kobo.920.1">you’re using.</span></span></p>
<p><span class="koboSpan" id="kobo.921.1">We’ll use two pieces of software: the .NET SDK and Packer. </span><span class="koboSpan" id="kobo.921.2">Likewise, we have two code bases: the C# .NET code base for the application and the HCL code base for Packer. </span><span class="koboSpan" id="kobo.921.3">As such, we must establish where these code bases are very clearly and upfront. </span><span class="koboSpan" id="kobo.921.4">Setting pipeline variables for them is a very useful way of accomplishing this as it ensures they are featured prominently in the YAML file and are stored in a reusable variable in case they will be repeated </span><span class="No-Break"><span class="koboSpan" id="kobo.922.1">multiple times:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.923.1">
env:
  DOTNET_VERSION: ‘6.0.401’ # The .NET SDK version to use
  PACKER_VERSION: ‘1.9.4’ # The version of Packer to use
  WORKING_DIRECTORY: “./src/packer/frontend”
  DOTNET_WORKING_DIRECTORY: “./src/dotnet/frontend/FleetPortal”</span></pre> <p><span class="koboSpan" id="kobo.924.1">Now that we have the triggers and some variables set for our workflow, we need to structure the jobs. </span><span class="koboSpan" id="kobo.924.2">For each Packer template, we will have two jobs: one that builds the C# .NET application code and produces a deployment package and another that runs </span><strong class="source-inline"><span class="koboSpan" id="kobo.925.1">packer build</span></strong><span class="koboSpan" id="kobo.926.1"> to produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">VM image:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.928.1">
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      ...
  </span><span class="koboSpan" id="kobo.928.2">packer:
    runs-on: ubuntu-latest
    steps:
      ...</span></pre> <p><span class="koboSpan" id="kobo.929.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.930.1">build</span></strong><span class="koboSpan" id="kobo.931.1"> job </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.932.1">performs a pretty standard .NET build process, which includes restoring package dependencies from NuGet (the .NET package manager), building the code, running unit and integration tests, publishing a deployable artifact, and storing that artifact so that it can be used by future jobs within </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">the pipeline:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.934.1"><img alt="Figure 7.24 – Packer workflow" src="image/B21183_07_24..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.935.1">Figure 7.24 – Packer workflow</span></p>
<p><span class="koboSpan" id="kobo.936.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.937.1">packer</span></strong><span class="koboSpan" id="kobo.938.1"> job immediately downloads the </span><strong class="source-inline"><span class="koboSpan" id="kobo.939.1">.zip</span></strong><span class="koboSpan" id="kobo.940.1"> file containing the deployment artifact and </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.941.1">puts it into a location where the Packer template’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.942.1">file</span></strong><span class="koboSpan" id="kobo.943.1"> provisioner expects it. </span><span class="koboSpan" id="kobo.943.2">Then, it generates a unique version of the name for the VM image that will be produced </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">if successful:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.945.1">
- id: image-version
  name: Generate Version Number
  run: |
   echo “version=$(date +’%Y.%m’).${{ github.run_number }}” &gt;&gt; “$GITHUB_OUTPUT”</span></pre> <p><span class="koboSpan" id="kobo.946.1">It does this by using Bash to generate the current year and month and appends </span><strong class="source-inline"><span class="koboSpan" id="kobo.947.1">github.run_number</span></strong><span class="koboSpan" id="kobo.948.1"> to ensure uniqueness if we happen to be running this pipeline more than once </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">per day.</span></span></p>
<p><span class="koboSpan" id="kobo.950.1">Next, it obtains the public IP address for the VM on which the GitHub Actions workflow </span><span class="No-Break"><span class="koboSpan" id="kobo.951.1">is running:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.952.1">
- id: agent-ipaddress
  name: Check Path
  working-directory: ${{ env.WORKING_DIRECTORY }}
  run: |
    ipaddress=$(curl -s http://checkip.amazonaws.com)
    echo $ipaddress
    echo “ipaddress=$ipaddress” &gt;&gt; “$GITHUB_OUTPUT”</span></pre> <p><span class="koboSpan" id="kobo.953.1">It does this so </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.954.1">that when it runs </span><strong class="source-inline"><span class="koboSpan" id="kobo.955.1">packer build</span></strong><span class="koboSpan" id="kobo.956.1">, it can configure Packer’s plugin for AWS to poke a hole in the firewall to allow SSH traffic from the GitHub Actions machine to the temporary VM running on AWS where the Packer provisioners </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">are executed.</span></span></p>
<p><span class="koboSpan" id="kobo.958.1">Next, it installs a specific version </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">of Packer:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.960.1">
- id: setup
  name: Setup `packer`
  uses: hashicorp/setup-packer@main
  with:
    version: ${{ env.PACKER_VERSION }}</span></pre> <p><span class="koboSpan" id="kobo.961.1">Finally, it executes </span><strong class="source-inline"><span class="koboSpan" id="kobo.962.1">packer build</span></strong><span class="koboSpan" id="kobo.963.1">, making sure to specify the </span><strong class="source-inline"><span class="koboSpan" id="kobo.964.1">AWS_ACCESS_KEY_ID</span></strong><span class="koboSpan" id="kobo.965.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.966.1">AWS_SECRET_ACCESS_KEY</span></strong><span class="koboSpan" id="kobo.967.1"> environment variables that the AWS plugin relies upon to authenticate to AWS’s </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">REST APIs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.969.1">
- id: build
  name: Packer Build
  env:
    AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    PKR_VAR_image_version: ${{ steps.image-version.outputs.version }}
    PKR_VAR_agent_ipaddress: ${{ steps.agent-ipaddress.outputs.ipaddress }}
  working-directory: ${{ env.WORKING_DIRECTORY }}
  run: |
    packer init ./
    packer build -var-file=variables.pkrvars.hcl ./</span></pre> <p><span class="koboSpan" id="kobo.970.1">It also specifies two input variables to the Packer template using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.971.1">PKR_VAR_</span></strong><span class="koboSpan" id="kobo.972.1"> prefixed</span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.973.1"> environment variable technique so that it includes the image version and the build agent IP address, both of which were dynamically generated within the GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">Actions workflow.</span></span></p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor399"/><span class="koboSpan" id="kobo.975.1">Terraform</span></h2>
<p><span class="koboSpan" id="kobo.976.1">With both </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.977.1">of our VM images built and their versions input into our </span><strong class="source-inline"><span class="koboSpan" id="kobo.978.1">tfvars</span></strong><span class="koboSpan" id="kobo.979.1"> file, our Terraform automation pipeline is ready to take the reins and not only provision our environment but deploy our solution (although not technically). </span><span class="koboSpan" id="kobo.979.2">The deployment was technically done within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.980.1">packer build</span></strong><span class="koboSpan" id="kobo.981.1"> process, with the physical deployment packages being copied to the home directory and the Linux service setup primed and ready. </span><span class="koboSpan" id="kobo.981.2">Terraform finishes the job by launching VMs using </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">these images:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.983.1">
on:
  push:
    branches: 
    - main
    paths:
    - ‘src/terraform/**’</span></pre> <p><span class="koboSpan" id="kobo.984.1">This means that we only need to trigger the Terraform automation pipeline when the Terraform code base changes. </span><span class="koboSpan" id="kobo.984.2">This could include configuration changes to the resources simply be an updated VM image version within the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.985.1">tfvars</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.986.1"> file:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<span class="koboSpan" id="kobo.987.1"><img alt="Figure 7.25 – The terraform apply workflow" src="image/B21183_07_25..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.988.1">Figure 7.25 – The terraform apply workflow</span></p>
<p><span class="koboSpan" id="kobo.989.1">As a result, the Terraform pipeline is quite simple. </span><span class="koboSpan" id="kobo.989.2">We simply need to execute either </span><strong class="source-inline"><span class="koboSpan" id="kobo.990.1">terraform plan</span></strong><span class="koboSpan" id="kobo.991.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.992.1">terraform apply</span></strong><span class="koboSpan" id="kobo.993.1">, depending on whether we want to evaluate or execute the changes for our </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">cloud environment.</span></span></p>
<p><span class="koboSpan" id="kobo.995.1">In keeping with the </span><em class="italic"><span class="koboSpan" id="kobo.996.1">always be specific</span></em><span class="koboSpan" id="kobo.997.1"> mantra, we must dutifully designate the version of Terraform that we want to use and specify the location for the Terraform code base using </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">pipeline variables:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.999.1">
env:
  TERRAFORM_VERSION: ‘1.5.7’
  WORKING_DIRECTORY: “./src/terraform”</span></pre> <p><span class="koboSpan" id="kobo.1000.1">Next, we</span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.1001.1"> must install the particular version of Terraform using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1002.1">setup-terraform</span></strong><span class="koboSpan" id="kobo.1003.1"> GitHub Action published by HashiCorp, which will handle the details of its installation </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">for us:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1005.1">
    - id: setup
      name: Setup `terraform`
      uses: hashicorp/setup-terraform@main
      with:
        version: ${{ env.TERRAFORM_VERSION }}</span></pre> <p><span class="koboSpan" id="kobo.1006.1">Finally, it executes </span><strong class="source-inline"><span class="koboSpan" id="kobo.1007.1">terraform apply</span></strong><span class="koboSpan" id="kobo.1008.1"> again, making sure to include the AWS credentials and the target backend location for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">Terraform state:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1010.1">
- id: apply
  name: Terraform Apply
  env:
    AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    BACKEND_BUCKET_NAME: ${{ vars.BUCKET_NAME }}
    BACKEND_REGION: ${{ vars.BUCKET_REGION }}
  working-directory: ${{ env.WORKING_DIRECTORY }}
  run: |
    terraform init \
      -backend-config=’bucket=’$BACKEND_BUCKET_NAME \
      -backend-config=’region=’$BACKEND_REGION \
      -backend-config=”key=aws-vm-sample”
    terraform apply -target “random_shuffle.az” -auto-approve
    terraform apply -auto-approve</span></pre> <p><span class="koboSpan" id="kobo.1011.1">The backend</span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.1012.1"> configuration is set using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1013.1">-backend-config</span></strong><span class="koboSpan" id="kobo.1014.1"> command-line argument, which frees us from having to hardcode these settings in our </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">source code.</span></span></p>
<p><span class="koboSpan" id="kobo.1016.1">Notice that we execute </span><strong class="source-inline"><span class="koboSpan" id="kobo.1017.1">terraform apply</span></strong><span class="koboSpan" id="kobo.1018.1"> twice. </span><span class="koboSpan" id="kobo.1018.2">First, we perform a targeted apply on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1019.1">random_shuffle.az</span></strong><span class="koboSpan" id="kobo.1020.1"> resource, after which we perform a general apply. </span><span class="koboSpan" id="kobo.1020.2">The targeted apply ensures that the Availability Zones we are targeting have been selected before we calculate the IP address space for our networks. </span><span class="koboSpan" id="kobo.1020.3">The need for this is driven by the dynamic nature of calculating the address space using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1021.1">cidrsubnet</span></strong><span class="koboSpan" id="kobo.1022.1"> function. </span><span class="koboSpan" id="kobo.1022.2">If we wanted to avoid this targeted apply approach, we could opt for a more hard-coded approach of the Availability Zones and the corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">address spaces.</span></span></p>
<p><span class="koboSpan" id="kobo.1024.1">That’s it! </span><span class="koboSpan" id="kobo.1024.2">With the completion of our Terraform GitHub Actions workflow, we have put the finishing touches on our end-to-end CI/CD pipeline. </span><a id="_idTextAnchor400"/><span class="koboSpan" id="kobo.1025.1">Our AWS-based solution will be up and running our VM cloud architecture in </span><span class="No-Break"><span class="koboSpan" id="kobo.1026.1">no time.</span></span></p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor401"/><span class="koboSpan" id="kobo.1027.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1028.1">In this chapter, we built a multi-tier cloud architecture using AWS and VMs, a fully operational GitFlow process, and an end-to-end CI/CD pipeline using </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">GitHub Actions.</span></span></p>
<p><span class="koboSpan" id="kobo.1030.1">In the next chapter, our fearless leader at Söze Enterprises will be throwing us into turmoil with some big new ideas, and we’ll have to respond to his call to action. </span><span class="koboSpan" id="kobo.1030.2">It turns out our CEO, Keyser, has been up late watching some YouTube videos about the next big thing – containers – and after talking with his pal Jeff on his superyacht, he has decided that we need to refactor our whole solution so that it can run on Docker and Kubernetes. </span><span class="koboSpan" id="kobo.1030.3">Luckily, the good people at Amazon have a service that might help us out: AWS </span><strong class="bold"><span class="koboSpan" id="kobo.1031.1">Elastic Kubernetes </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1032.1">Service</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1033.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1034.1">EKS</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1035.1">).</span></span></p>
</div>
</body></html>