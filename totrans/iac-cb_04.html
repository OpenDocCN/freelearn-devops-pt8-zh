<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Automating Complete Infrastructures with Terraform"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Automating Complete Infrastructures with Terraform</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Provisioning a complete CoreOS infrastructure on Digital Ocean with Terraform</li><li class="listitem" style="list-style-type: disc">Provisioning a three-tier infrastructure on Google Compute Engine</li><li class="listitem" style="list-style-type: disc">Provisioning a GitLab CE + CI runners on OpenStack</li><li class="listitem" style="list-style-type: disc">Managing Heroku Apps and Add-ons using Terraform</li><li class="listitem" style="list-style-type: disc">Creating a scalable Docker Swarm cluster on bare metal with Packet</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec52"/>Introduction</h1></div></div></div><p>In this chapter, we'll describe complete infrastructures using Terraform, how it looks when everything is tied together, with a real project in mind. Most examples from previous chapters on Terraform were on Amazon Web Services, so to try to be more diverse and complete, this chapter is dedicated to other infrastructure services, namely Digital Ocean, Google Cloud, Heroku, and Packet. On Digital Ocean, we'll build a fully working and monitored CoreOS cluster with DNS dynamically updated. On Google Cloud, we'll build a three-tier infrastructure with two HTTP nodes behind a load balancer and an isolated MySQL managed database. Using OpenStack, we'll deploy a GitLab CE and two GitLab CI runners, using different storage solutions. We'll see how we can integrate and automate a Heroku environment. We'll end this chapter with a powerful and scalable Docker Swarm cluster on bare metal using Packet, capable of scaling hundreds of containers.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>The Terraform version in use for this book is 0.7.4.</p></div></div></div></div>
<div class="section" title="Provisioning a complete CoreOS infrastructure on Digital Ocean with Terraform"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec53"/>Provisioning a complete CoreOS infrastructure on Digital Ocean with Terraform</h1></div></div></div><p>In <a id="id344" class="indexterm"/>this recipe, we'll <a id="id345" class="indexterm"/>build from <a id="id346" class="indexterm"/>scratch a fully working CoreOS cluster on Digital Ocean in their New York region, using Terraform and cloud-init. We'll add some latency monitoring as well with StatusCake, so we have a good foundation of using Terraform on Digital Ocean.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec124"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Terraform installation</li><li class="listitem" style="list-style-type: disc">A Digital Ocean account</li><li class="listitem" style="list-style-type: disc">A StatusCake account</li><li class="listitem" style="list-style-type: disc">An Internet connection</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec125"/>How to do it…</h2></div></div></div><p>Let's start by creating the <code class="literal">digitalocean</code> provider (it only requires an API token) in a file named <code class="literal">providers.tf</code>:</p><div class="informalexample"><pre class="programlisting">provider "digitalocean" {
  token = "${var.do_token}"
}</pre></div><p>Declare the <code class="literal">do_token</code> variable in a file named <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "do_token" {
  description = "Digital Ocean Token"
}</pre></div><p>Also, don't forget to set it in a private <code class="literal">terraform.tfvars</code> file:</p><div class="informalexample"><pre class="programlisting">do_token = "a1b2c3d4e5f6"</pre></div><div class="section" title="Handling the SSH key"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec68"/>Handling the SSH key</h3></div></div></div><p>We <a id="id347" class="indexterm"/>know that we'll need an SSH key to log into the cluster members. With <a id="id348" class="indexterm"/>Digital Ocean, the resource is named <code class="literal">digitalocean_ssh_key</code>. I propose that we name the SSH key file <code class="literal">iac_admin_sshkey</code> in the <code class="literal">keys</code> directory, but as you might prefer something else, let's use a variable for that as well. Let's write this in a <code class="literal">keys.tf</code> file:</p><div class="informalexample"><pre class="programlisting">resource "digitalocean_ssh_key" "default" {
  name       = "Digital Ocean SSH Key"
  public_key = "${file("${var.ssh_key_file}.pub")}"
}</pre></div><p>Create the related variable in <code class="literal">variables.tf</code>, with our suggested default:</p><div class="informalexample"><pre class="programlisting">variable "ssh_key_file" {
  default     = "keys/iac_admin_sshkey"
  description = "Default SSH Key file"
}</pre></div><p>It's <a id="id349" class="indexterm"/>now time to effectively override the <a id="id350" class="indexterm"/>value in the <code class="literal">terraform.tfvars</code> file if you feel like it:</p><div class="informalexample"><pre class="programlisting">ssh_key_file = "./keys/my_own_key"</pre></div></div><div class="section" title="Creating the CoreOS cluster members"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec69"/>Creating the CoreOS cluster members</h3></div></div></div><p>Here's <a id="id351" class="indexterm"/>the core of our infrastructure: three nodes running in the New York City data center NYC1, with private networking <a id="id352" class="indexterm"/>enabled, no backups activated (set it to <code class="literal">true</code> if you feel like it!), the SSH key we previously created, and a cloud-init file to initiate configuration. A virtual machine at Digital Ocean is named a <span class="emphasis"><em>droplet</em></span>, so the resource to launch a droplet is <code class="literal">digitalocean_droplet</code>. All variables' names relate to what we just enumerated:</p><div class="informalexample"><pre class="programlisting">resource "digitalocean_droplet" "coreos" {
  image              = "${var.coreos_channel}"
  count              = "${var.cluster_nodes}"
  name               = "coreos-${count.index+1}"
  region             = "${var.do_region}"
  size               = "${var.do_droplet_size}"
  ssh_keys           = ["${digitalocean_ssh_key.default.id}"]
  private_networking = true
  backups            = false
  user_data          = "${file("cloud-config.yml")}"
}</pre></div><p>Declare all the variables in the <code class="literal">variables.tf</code> file, with some good defaults (the smallest 512 MB droplet, a three-node cluster), and some defaults we'll want to override (AMS3 data center or the stable CoreOS channel):</p><div class="informalexample"><pre class="programlisting">variable "do_region" {
  default     = "ams3"
  description = "Digital Ocean Region"
}

variable "do_droplet_size" {
  default     = "512mb"
  description = "Droplet Size"
}

variable "coreos_channel" {
  default     = "coreos-stable"
  description = "CoreOS Channel"
}

variable "cluster_nodes" {
  default     = "3"
  description = "Number of nodes in the cluster"
}</pre></div><p>Here <a id="id353" class="indexterm"/>are our overridden <a id="id354" class="indexterm"/>values in <code class="literal">terraform.tfvars</code> (but feel free to put your own values, such as using another data center or CoreOS release):</p><div class="informalexample"><pre class="programlisting">do_region = "nyc1"
coreos_channel = "coreos-beta"</pre></div></div><div class="section" title="Adding useful output"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec70"/>Adding useful output</h3></div></div></div><p>It <a id="id355" class="indexterm"/>would be awesome to automatically have a few auto-documented lines on how to connect to our CoreOS cluster. As we can do that with the Terraform outputs, let's use this example for a start, in <code class="literal">outputs.tf</code>. This is constructing an SSH command line with dynamic information from Terraform that we'll be able to use easily (it's simply iterating over every <code class="literal">digitalocean_droplet.coreos.*</code> available):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>output "CoreOS Cluster Members" {</strong></span>
<span class="strong"><strong>  value = "${formatlist("ssh core@%v -i ${var.ssh_key_file}", digitalocean_droplet.coreos.*.ipv4_address)}"</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>The output will look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>CoreOS Cluster Members = [</strong></span>
<span class="strong"><strong>    ssh core@192.241.128.44 -i ./keys/iac_admin_sshkey,</strong></span>
<span class="strong"><strong>    ssh core@192.241.130.33 -i ./keys/iac_admin_sshkey,</strong></span>
<span class="strong"><strong>    ssh core@198.199.120.212 -i ./keys/iac_admin_sshkey</strong></span>
<span class="strong"><strong>]</strong></span>
</pre></div></div><div class="section" title="Dynamic DNS Integration"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec71"/>Dynamic DNS Integration</h3></div></div></div><p>One <a id="id356" class="indexterm"/>of the attractive features of Digital Ocean is the easy DNS integration. For example, if our domain is <code class="literal">infrastructure-as-code.org</code> and we launch a <span class="emphasis"><em>blog</em></span> droplet, we'll end up registering it automatically under the public DNS name <code class="literal">blog.infrastructure-as-code.org</code>. Pretty easy and dynamic! To <a id="id357" class="indexterm"/>give Digital Ocean power on our domain, we need to go to our registrar (where we bought our domain), and configure our domain to be managed by Digital Ocean, using their own nameservers, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">ns1.digitalocean.com</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ns2.digitalocean.com</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ns3.digitalocean.com</code></li></ul></div><p>This prerequisite being done, let's declare our domain in the <code class="literal">dns.tf</code> file using the <code class="literal">digitalocean_domain</code> resource, automatically using a <code class="literal">cluster_domainname</code> variable for the domain name, and an initial IP address matching, that we can either set to a value you already know or to an arbitrary droplet:</p><div class="informalexample"><pre class="programlisting">resource "digitalocean_domain" "cluster_domainname" {
  name       = "${var.cluster_domainname}"
  ip_address = "${digitalocean_droplet.coreos.0.ipv4_address}"
}</pre></div><p>Add the new variable in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "cluster_domainname" {
  default     = "infrastructure-as-code.org"
  description = "Domain to use"
}</pre></div><p>Don't forget to override it as necessary in <code class="literal">terraform.tfvars</code>.</p><p>The next step is to register automatically every droplet in the DNS. By iterating over each droplet, and extracting their <code class="literal">name</code> and <code class="literal">ipv4_address</code> attributes, we'll add this <code class="literal">digitalocean_record</code> resource into the mix:</p><div class="informalexample"><pre class="programlisting">resource "digitalocean_record" "ipv4" {
  count  = "${var.cluster_nodes}"
  domain = "${digitalocean_domain.cluster_domainname.name}"
  type   = "A"
  name   = "${element(digitalocean_droplet.coreos.*.name, count.index)}"
  value  = "${element(digitalocean_droplet.coreos.*.ipv4_address, count.index)}"
}</pre></div><p>This will automatically register every droplet under the name core-[1,2,3].mydomain.com, for easier access and reference.</p><p>If you like, you can access the <code class="literal">fqdn</code> attribute of this resource right from the outputs (<code class="literal">outputs.tf</code>):</p><div class="informalexample"><pre class="programlisting">output "CoreOS Cluster Members DNS" {
  value = "${formatlist("ssh core@%v -i ${var.ssh_key_file}", digitalocean_record.ipv4.*.fqdn)}"
}</pre></div></div><div class="section" title="Integrating cloud-init"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec72"/>Integrating cloud-init</h3></div></div></div><p>We <a id="id358" class="indexterm"/>need to build a fully working <code class="literal">cloud-config.yml</code> file for our CoreOS cluster. Refer to the cloud-init part of this book in <a class="link" href="ch05.html" title="Chapter 5. Provisioning the Last Mile with Cloud-Init">Chapter 5</a>, <span class="emphasis"><em>Provisioning the Last Mile with Cloud-Init</em></span> for more information on the <code class="literal">cloud-config.yml</code> file, and especially on configuring CoreOS with it.</p><p>What we need for a fully usable CoreOS cluster are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working etcd cluster on the local network interface (<code class="literal">$private_ipv4</code>)</li><li class="listitem" style="list-style-type: disc">A working fleet cluster on the local network interface (<code class="literal">$private_ipv4</code>)<p>Fleet is a distributed init system. You can think of it as systemd for a whole cluster</p></li></ul></div><p>To configure etcd, we first need to obtain a new token. This token is unique and can be distributed through different channels. It can be easily <a id="id359" class="indexterm"/>obtained through the <a class="ulink" href="https://coreos.com/os/docs/latest/cluster-discovery.html">https://coreos.com/os/docs/latest/cluster-discovery.html</a> etcd service. Then we'll start 2 units—etcd and fleet.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -w "\n" 'https://discovery.etcd.io/new?size=3'</strong></span>
<span class="strong"><strong>https://discovery.etcd.io/b04ddb7ff454503a66ead486b448afb7</strong></span>
</pre></div><p>Note this URL carefully and copy paste it in the following <code class="literal">cloud-config.yml</code> file:</p><div class="informalexample"><pre class="programlisting">#cloud-config
# https://coreos.com/validate/
coreos:
  etcd2:
    discovery: "https://discovery.etcd.io/b04ddb7ff454503a66ead486b448afb7"
    advertise-client-urls: "http://$private_ipv4:2379"
    initial-advertise-peer-urls: "http://$private_ipv4:2380"
    listen-client-urls: http://0.0.0.0:2379
    listen-peer-urls: http://$private_ipv4:2380
  units:
    - name: etcd2.service
      command: start
    - name: fleet.service
      command: start
  fleet:
    public-ip: "$public_ipv4"
    metadata: "region=ams,provider=digitalocean"</pre></div><p>This will be enough to start an etcd + fleet cluster on CoreOS. <a class="link" href="ch05.html" title="Chapter 5. Provisioning the Last Mile with Cloud-Init">Chapter 5</a>, <span class="emphasis"><em>Provisioning the Last</em></span>
</p><p>
<span class="emphasis"><em>Mile with Cloud-Init</em></span>, for in-depth details on cloud-init.</p></div><div class="section" title="Integrating dynamic StatusCake monitoring"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec73"/>Integrating dynamic StatusCake monitoring</h3></div></div></div><p>We <a id="id360" class="indexterm"/>can reuse our knowledge from previous chapters to easily integrate <a id="id361" class="indexterm"/>full latency <a id="id362" class="indexterm"/>monitoring to the hosts of our CoreOS cluster, using a free StatusCake account (<a class="ulink" href="https://statuscake.com">https://statuscake.com</a>).</p><p>Start by configuring the provider in <code class="literal">providers.tf</code>:</p><div class="informalexample"><pre class="programlisting">provider "statuscake" {
  username = "${var.statuscake_username}"
  apikey   = "${var.statuscake_apikey}"
}</pre></div><p>Declare the required variables in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "statuscake_username" {
  default     = "changeme"
  description = "StatusCake Account Username"
}

variable "statuscake_apikey" {
  default     = "hackme"
  description = "StatusCake Account API Key"
}</pre></div><p>Also, override with your own values in <code class="literal">terraform.tfvars</code>.</p><p>Now we can use the <code class="literal">statuscake_test</code> resource to activate immediate latency (ping) monitoring on every droplet by iterating over each <code class="literal">digitalocean_droplet.coreos.*</code> resource value:</p><div class="informalexample"><pre class="programlisting">resource "statuscake_test" "coreos_cluster" {
  count        = "${var.cluster_nodes}"
  website_name = "${element(digitalocean_droplet.coreos.*.name, count.index)}.${var.cluster_domainname}"
  website_url  = "${element(digitalocean_droplet.coreos.*.ipv4_address, count.index)}"
  test_type    = "PING"
  check_rate   = 300
  paused       = false
}</pre></div><p>It's time to <code class="literal">terraform apply</code> this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ terraform apply</strong></span>
<span class="strong"><strong>[...]</strong></span>

<span class="strong"><strong>CoreOS Cluster Members = [</strong></span>
<span class="strong"><strong>    ssh core@159.203.189.142 -i ./keys/iac_admin_sshkey,</strong></span>
<span class="strong"><strong>    ssh core@159.203.189.146 -i ./keys/iac_admin_sshkey,</strong></span>
<span class="strong"><strong>    ssh core@159.203.189.131 -i ./keys/iac_admin_sshkey</strong></span>
<span class="strong"><strong>]</strong></span>
<span class="strong"><strong>CoreOS Cluster Members DNS = [</strong></span>
<span class="strong"><strong>    ssh core@coreos-1.mydomain.com -i ./keys/iac_admin_sshkey,</strong></span>
<span class="strong"><strong>    ssh core@coreos-2.mydomain.com -i ./keys/iac_admin_sshkey,</strong></span>
<span class="strong"><strong>    ssh core@coreos-3.mydomain.com -i ./keys/iac_admin_sshkey</strong></span>
<span class="strong"><strong>]</strong></span>
</pre></div><p>Confirm <a id="id363" class="indexterm"/>that we can connect to a member <a id="id364" class="indexterm"/>using the command line from the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ssh core@159.203.189.142 -i ./keys/iac_admin_sshkey</strong></span>
</pre></div><p>Verify the etcd cluster health:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ core@coreos-1 ~ $ etcdctl cluster-health</strong></span>
<span class="strong"><strong>member 668f889d5f96b578 is healthy: got healthy result from http://10.136.24.178:2379</strong></span>
<span class="strong"><strong>member c8e8906e0f3f63be is healthy: got healthy result from http://10.136.24.176:2379</strong></span>
<span class="strong"><strong>member f3b53735aca3062e is healthy: got healthy result from http://10.136.24.177:2379</strong></span>
<span class="strong"><strong>cluster is healthy</strong></span>
</pre></div><p>Check that all fleet members are all right:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>core@coreos-1 ~ $ fleetctl list-machines</strong></span>
<span class="strong"><strong>MACHINE         IP              METADATA</strong></span>
<span class="strong"><strong>24762c02...     159.203.189.146 provider=digitalocean,region=ams</strong></span>
<span class="strong"><strong>3b4b0792...     159.203.189.142 provider=digitalocean,region=ams</strong></span>
<span class="strong"><strong>59e15b88...     159.203.189.131 provider=digitalocean,region=ams</strong></span>
</pre></div><p>Enjoy, in less than a minute, you're ready to use a CoreOS cluster with basic monitoring, using only fully automated Terraform code!</p></div></div></div>
<div class="section" title="Provisioning a three-tier infrastructure on Google Compute Engine"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec54"/>Provisioning a three-tier infrastructure on Google Compute Engine</h1></div></div></div><p>We'll <a id="id365" class="indexterm"/>provision a ready to use, three-tier, load-balanced web infrastructure on Google Compute <a id="id366" class="indexterm"/>Engine, using two CentOS 7.2 servers for the web and one master Google MySQL instance. The MySQL instance will allow connections only from the two web servers (with valid credentials), and all three instances (SQL and HTTP) will be accessible from a single <span class="emphasis"><em>corporate</em></span> network (our company's network). The topology looks like this:</p><div class="mediaobject"><img src="graphics/B05671_04_01.jpg" alt="Provisioning a three-tier infrastructure on Google Compute Engine"/></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec126"/>Getting ready</h2></div></div></div><p>To step <a id="id367" class="indexterm"/>through this recipe, you will need the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Terraform installation</li><li class="listitem" style="list-style-type: disc">A Google Compute Engine account with a project</li><li class="listitem" style="list-style-type: disc">An Internet connection</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec127"/>How to do it…</h2></div></div></div><p>The first thing we need to do is to get our credentials from the console.</p><div class="section" title="Generating API credentials for a Google project"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec74"/>Generating API credentials for a Google project</h3></div></div></div><p>Navigate <a id="id368" class="indexterm"/>to your Google Cloud project, and in the <span class="emphasis"><em>API Manager</em></span>, select <span class="strong"><strong>Credentials</strong></span> | <span class="strong"><strong>Create credentials</strong></span> | <span class="strong"><strong>Service Account Key</strong></span>. Now choose <span class="strong"><strong>Compute Engine default service account</strong></span> from the dropdown list, in the JSON format. Save this file as <code class="literal">account.json</code> at the root of the infrastructure repository.</p><p>Create the variables to define our credentials file in <code class="literal">variables.tf</code>, store the region we're running in, and the Google Compute project name:</p><div class="informalexample"><pre class="programlisting">variable "credentials_file" {
  default     = "account.json"
  description = "API credentials JSON file"
}
variable "region" {
  default     = "europe-west"
  description = "Region name"
}
variable "project_name" {
  default     = "default-project"
  description = "Project ID to use"
}</pre></div><p>Don't forget to override those values in <code class="literal">terraform.tfvars</code> if you want to:</p><div class="informalexample"><pre class="programlisting">project_name = "iac-book-infra"
region = "us-east1"</pre></div><p>Now, in a <code class="literal">providers.tf</code> file, add the <code class="literal">google</code> provider:</p><div class="informalexample"><pre class="programlisting">provider "google" {
  credentials = "${file("${var.credentials_file}")}"
  project     = "${var.project_name}"
  region      = "${var.region}"
}</pre></div><p>Our <code class="literal">google</code> provider is now configured!</p></div><div class="section" title="Creating Google Compute HTTP instances"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec75"/>Creating Google Compute HTTP instances</h3></div></div></div><p>Here's <a id="id369" class="indexterm"/>the checklist <a id="id370" class="indexterm"/>of our requirements for these HTTP hosts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We want two of them</li><li class="listitem" style="list-style-type: disc">Their type is <code class="literal">n1-standard-1</code> (3.75 GB of RAM, one vCPU)</li><li class="listitem" style="list-style-type: disc">Their region and zone is: us-east1-d</li><li class="listitem" style="list-style-type: disc">They are running CentOS 7.2 (official image is: centos-cloud/centos 7)</li><li class="listitem" style="list-style-type: disc">The default SSH username is <code class="literal">centos</code></li><li class="listitem" style="list-style-type: disc">The SSH key known to us is (<code class="literal">keys/admin_key</code>)</li><li class="listitem" style="list-style-type: disc">We want a fully updated system with Docker installed and running</li></ul></div><p>Let's <a id="id371" class="indexterm"/>define generic variables for all these requirements in a <code class="literal">variables.tf</code> file:</p><div class="informalexample"><pre class="programlisting">variable "machine_type" {
  default     = "f1-micro"
  description = "Machine type"
}

variable "zone" {
  default     = "c"
  description = "Region Zone"
}

variable "disk_image" {
  default     = "centos-cloud/centos-7"
  description = "Disk image"
}

variable "ssh_key" {
  default     = "keys/admin_key"
  description = "SSH key"
}

variable "ssh_username" {
  default     = "root"
  description = "The SSH username to use"
}

variable "www_servers" {
  default = "2"
  description = "Amount of www servers"
}</pre></div><p>Now <a id="id372" class="indexterm"/>let's override in <code class="literal">terraform.tfvars</code> the generic values we just set:</p><div class="informalexample"><pre class="programlisting">machine_type = "n1-standard-1"
zone = "d"
ssh_username = "centos"</pre></div><p>Google <a id="id373" class="indexterm"/>Cloud instances are called from Terraform using the resource <code class="literal">google_compute_instance</code>:</p><p>Let's add what we already know in this resource:</p><div class="informalexample"><pre class="programlisting">resource "google_compute_instance" "www" {
  count        = "${var.www_servers}"
  name         = "www-${count.index+1}"
  machine_type = "${var.machine_type}"
  zone         = "${var.region}-${var.zone}"

  disk {
    image = "${var.disk_image}"
  }

  metadata {
    ssh-keys = "${var.ssh_username}:${file("${var.ssh_key}.pub")}"
  }
}</pre></div><p>This <a id="id374" class="indexterm"/>could be enough, but we want to go much farther.</p><p>For example, we'll later add a firewall, whose rule will apply to a target defined by its tags. Let's add a tag right now, so we can use it later:</p><div class="informalexample"><pre class="programlisting">tags         = ["www"]</pre></div><p>We have to configure networking. It's necessary in our case to have a public IPv4, because we need to access the servers by SSH from outside. We might have chosen to not have publicly exposed servers and use a bastion host instead. To create a network interface in our default network, mapped behind a public IPv4, add the following to the <code class="literal">google_compute_instance</code> resource:</p><div class="informalexample"><pre class="programlisting">  network_interface {
    network = "default"

    access_config {
      nat_ip = ""
    }
  }</pre></div><p>Let's finish by connecting automatically to each instance and fully update it, then install, enable, and start Docker. We do this using the <code class="literal">remote-exec</code> provisioner, correctly configured with the right SSH username and private key:</p><div class="informalexample"><pre class="programlisting">provisioner "remote-exec" {
    connection {
      user        = "${var.ssh_username}"
      private_key = "${file("${var.ssh_key}")}"
    }

    inline = [
      "sudo yum update -y",
      "sudo yum install -y docker",
      "sudo systemctl enable docker",
      "sudo systemctl start docker",
    ]
  }</pre></div><p>We're <a id="id375" class="indexterm"/>finally done, with <a id="id376" class="indexterm"/>our two instances automatically provisioned!</p></div><div class="section" title="Creating a Google Compute Firewall rule"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec76"/>Creating a Google Compute Firewall rule</h3></div></div></div><p>Our <a id="id377" class="indexterm"/>goal is simple: we want to allow anyone (0.0.0.0/0) to access using HTTP (TCP port <code class="literal">80</code>) any instance with the tag <code class="literal">www</code> in the <a id="id378" class="indexterm"/>default network. To do this, let's use the <code class="literal">google_compute_firewall</code> resource:</p><div class="informalexample"><pre class="programlisting">resource "google_compute_firewall" "fw" {
  name    = "www-firewall"
  network = "default"

  allow {
    protocol = "tcp"
    ports    = ["80"]
  }

  source_ranges = ["0.0.0.0/0"]
  target_tags   = ["www"]
}</pre></div></div><div class="section" title="Load balancing Google Compute instances"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec77"/>Load balancing Google Compute instances</h3></div></div></div><p>To <a id="id379" class="indexterm"/>load balance <a id="id380" class="indexterm"/>requests across our two instances, we'll need to create a <span class="emphasis"><em>pool</em></span> of hosts, where membership will be handled by a simple health check: an HTTP <code class="literal">GET</code> on / every second, with an immediate timeout (<code class="literal">1</code> second), and removal after <code class="literal">3</code> errors. We can do this in a file named <code class="literal">pool.tf</code> with the <code class="literal">google_compute_http_health_check</code> resource:</p><div class="informalexample"><pre class="programlisting">resource "google_compute_http_health_check" "www" {
  name                = "http"
  request_path        = "/"
  check_interval_sec  = 1
  healthy_threshold   = 1
  unhealthy_threshold = 3
  timeout_sec         = 1
}</pre></div><p>Feel <a id="id381" class="indexterm"/>free to transform <a id="id382" class="indexterm"/>those values into variables for better tuning on your end!</p><p>Now, let's define the pool, which is defined by the results of the health checks and instances inclusion. This is done using the <code class="literal">google_compute_target_pool</code> resource:</p><div class="informalexample"><pre class="programlisting">resource "google_compute_target_pool" "www" {
  name          = "www-pool"
  instances     = ["${google_compute_instance.www.*.self_link}"]
  health_checks = ["${google_compute_http_health_check.www.name}"]
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>The <code class="literal">self_link</code> attribute returns the URI of the resource.</p></div></div><p>Now we have our pool of hosts with health checks, let's create the load balancer itself. It's done using the <code class="literal">google_compute_forwarding_rule</code> resource, simply pointing to the pool of hosts we created earlier. Add the following in a <code class="literal">loadbalancer.tf</code> file:</p><div class="informalexample"><pre class="programlisting">resource "google_compute_forwarding_rule" "http" {
  name       = "http-lb"
  target     = "${google_compute_target_pool.www.self_link}"
  port_range = "80"
}</pre></div></div><div class="section" title="Creating a Google MySQL database instance"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec78"/>Creating a Google MySQL database instance</h3></div></div></div><p>Our <a id="id383" class="indexterm"/>typical target <a id="id384" class="indexterm"/>application needs a database to store and access data. We won't get into database replication here, but it can also be done quite simply with Terraform on Google Cloud.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note26"/>Note</h3><p>Double-check <a id="id385" class="indexterm"/>you have the SQL API activated in the Google Cloud Console: <a class="ulink" href="https://console.cloud.google.com/apis/library">https://console.cloud.google.com/apis/library</a>. By default, it isn't.</p></div></div><p>Here's a checklist of what we know about our MySQL database:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It's running on us-east1 region</li><li class="listitem" style="list-style-type: disc">It's running MySQL 5.6</li><li class="listitem" style="list-style-type: disc">It's type is <span class="emphasis"><em>D2</em></span> (1 GB of RAM)</li><li class="listitem" style="list-style-type: disc">Our own network and both HTTP servers can access it</li><li class="listitem" style="list-style-type: disc">We want a database named <code class="literal">app_db</code></li><li class="listitem" style="list-style-type: disc">We want a user with a password to be allowed to connect from the HTTP servers</li></ul></div><p>Let's <a id="id386" class="indexterm"/>put all these variables in the <code class="literal">variables.tf</code> file:</p><div class="informalexample"><pre class="programlisting">variable "db_type" {
  default     = "D0"
  description = "Google SQL DB type"
}

variable "db_authorized_network" {
  default     = "0.0.0.0/0"
  description = "A corporate network authorized to access the DB"
}

variable "db_username" {
  default     = "dbadmin"
  description = "A MySQL username"
}

variable "db_password" {
  default     = "changeme"
  description = "A MySQL password"
}

variable "db_name" {
  default     = "db_name"
  description = "MySQL database name"
}</pre></div><p>Don't <a id="id387" class="indexterm"/>forget to override each generic value in the <code class="literal">terraform.tfvars</code>:</p><div class="informalexample"><pre class="programlisting">db_authorized_network = "163.172.161.158/32"
db_username = "sqladmin"
db_password = "pwd1970"
db_name = "app_db"
db_type = "D2"</pre></div><p>Now <a id="id388" class="indexterm"/>we can build our <a id="id389" class="indexterm"/>database using the <code class="literal">google_sql_database_instance</code> resource in a <code class="literal">db.tf</code> file:</p><div class="informalexample"><pre class="programlisting">resource "google_sql_database_instance" "master" {
  name             = "mysql-mastr-1"
  region           = "${var.region}"
  database_version = "MYSQL_5_6"

  settings = {
    tier              = "${var.db_type}"
    activation_policy = "ALWAYS"         // vs "ON_DEMAND"
    pricing_plan      = "PER_USE"        // vs "PACKAGE"

    ip_configuration {
      ipv4_enabled = true

      authorized_networks {
        name  = "authorized_network"
        value = "${var.db_authorized_network}"
      }

      authorized_networks {
        name  = "${google_compute_instance.www.0.name}"
        value = "${google_compute_instance.www.0.network_interface.0.access_config.0.assigned_nat_ip}"
      }

      authorized_networks {
        name  = "${google_compute_instance.www.1.name}"
        value = "${google_compute_instance.www.1.network_interface.0.access_config.0.assigned_nat_ip}"
      }
    }
  }
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note27"/>Note</h3><p>The <code class="literal">pricing_plan</code> <code class="literal">"PACKAGE"</code> is more interesting for a long-lasting database. Also, the <code class="literal">authorized_network</code> block doesn't currently support a <code class="literal">count</code> value, so we can't iterate dynamically over every HTTP host. For now, we have to duplicate the block, but that may very well change in a newer Terraform version.</p></div></div><p>Let's now create a database, using a <code class="literal">google_sql_database</code> resource:</p><div class="informalexample"><pre class="programlisting">resource "google_sql_database" "db" {
  name     = "${var.db_name}"
  instance = "${google_sql_database_instance.master.name}"
}</pre></div><p>Finish <a id="id390" class="indexterm"/>by creating <a id="id391" class="indexterm"/>the SQL user with host restriction. Like the <code class="literal">authorized_network</code> block, the <code class="literal">google_sql_user</code> resource doesn't support a count value yet, so we have to duplicate the code for each HTTP server for now:</p><div class="informalexample"><pre class="programlisting">resource "google_sql_user" "user_www_1" {
  name     = "${var.db_username}"
  password = "${var.db_password}"
  instance = "${google_sql_database_instance.master.name}"
  host     = "${google_compute_instance.www.0.network_interface.0.access_config.0.assigned_nat_ip}"
}

resource "google_sql_user" "user_www_2" {
  name     = "${var.db_username}"
  password = "${var.db_password}"
  instance = "${google_sql_database_instance.master.name}"
  host     = "${google_compute_instance.www.1.network_interface.0.access_config.0.assigned_nat_ip}"
}</pre></div></div><div class="section" title="Adding some useful outputs"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec79"/>Adding some useful outputs</h3></div></div></div><p>It would <a id="id392" class="indexterm"/>be awesome to have some useful information such as IPs for all our instances and services and usernames and passwords. Let's add some outputs in <code class="literal">outputs.tf</code>:</p><div class="informalexample"><pre class="programlisting">output "HTTP Servers" {
  value = "${join(" ", google_compute_instance.www.*.network_interface.0.access_config.0.assigned_nat_ip)}"
}

output "MySQL DB IP" {
  value = "${google_sql_database_instance.master.ip_address.0.ip_address}"
}

output "Load Balancer Public IPv4" {
  value = "${google_compute_forwarding_rule.http.ip_address}"
}

output "DB Credentials" {
  value = "Username=${var.db_username} Password=${var.db_password}"
}</pre></div><p>Here <a id="id393" class="indexterm"/>we are!</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ terraform apply</strong></span>
<span class="strong"><strong>[...]</strong></span>
<span class="strong"><strong>Outputs:</strong></span>

<span class="strong"><strong>DB Credentials = Username=sqladmin Password=pwd1970</strong></span>
<span class="strong"><strong>HTTP Servers = 104.196.180.192 104.196.157.246</strong></span>
<span class="strong"><strong>Load Balancer Public IPv4 = 104.196.45.46</strong></span>
<span class="strong"><strong>MySQL DB IP = 173.194.111.120</strong></span>
</pre></div><p>Simply deploy our application on the HTTP servers and we're done! To test drive the load balancer and the HTTP instances, you can simply deploy the NGINX container on each server and see the traffic flow:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo docker run -it --rm -p 80:80 --name web nginx</strong></span>
</pre></div></div></div></div>
<div class="section" title="Provisioning a GitLab CE + CI runners on OpenStack"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec55"/>Provisioning a GitLab CE + CI runners on OpenStack</h1></div></div></div><p>OpenStack <a id="id394" class="indexterm"/>is a very popular <a id="id395" class="indexterm"/>open source cloud computing solution. Many providers are based on it, and you can roll your own in your data center. In this example, we'll use the public OpenStack by OVH, located in Montreal, QC (Canada), but we can use any other OpenStack. There're differences in implementation for every custom deployment, but we'll stick with very stable features.</p><p>We'll launch one compute instance running Ubuntu LTS 16.04 for GitLab, with a dedicated block device for Docker, and two other compute instances for GitLab CI runners. Security will allow HTTP for everyone, but SSH only for a known IP from our corporate network. To store our builds or releases, we'll create a <span class="emphasis"><em>container</em></span>, which is in OpenStack terminology—an object storage. The equivalent with AWS S3 is a <span class="emphasis"><em>bucket</em></span>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec128"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Terraform installation.</li><li class="listitem" style="list-style-type: disc">An <a id="id396" class="indexterm"/>OpenStack account on any OpenStack provider (public or private). This <a id="id397" class="indexterm"/>recipe uses an account on OVH's public OpenStack (<a class="ulink" href="https://www.ovh.com/us/">https://www.ovh.com/us/</a>).</li><li class="listitem" style="list-style-type: disc">An Internet connection.</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec129"/>How to do it…</h2></div></div></div><p>We'll <a id="id398" class="indexterm"/>create:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Three compute instances (virtual machines)</li><li class="listitem" style="list-style-type: disc">One keypair</li><li class="listitem" style="list-style-type: disc">One block storage device</li><li class="listitem" style="list-style-type: disc">One security group</li><li class="listitem" style="list-style-type: disc">One object storage bucket</li></ul></div><div class="section" title="Configuring the OpenStack provider"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec80"/>Configuring the OpenStack provider</h3></div></div></div><p>Let's <a id="id399" class="indexterm"/>start by configuring the OpenStack provider. We need four pieces of information: a username, a password, an OpenStack tenant name, and an OpenStack authentication endpoint URL. To make the code very dynamic, let's create variables for those in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "user_name" {
  default     = "changeme"
  description = "OpenStack username"
}

variable "password" {
  default     = "hackme"
  description = "OpenStack password"
}

variable "tenant_name" {
  default     = "123456"
  description = "OpenStack Tenant name"
}

variable "auth_url" {
  default     = "https://openstack.url/v2.0"
  description = "OpenStack Authentication Endpoint"
}</pre></div><p>Don't forget to override the default values with your own in the <code class="literal">terraform.tfvars</code> file!</p><div class="informalexample"><pre class="programlisting">user_name   = "***"
tenant_name = "***"
password    = "***"
auth_url    = "https://auth.cloud.ovh.net/v2.0/"</pre></div><p>Now <a id="id400" class="indexterm"/>we're good to go.</p></div><div class="section" title="Creating a key pair on OpenStack"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec81"/>Creating a key pair on OpenStack</h3></div></div></div><p>To <a id="id401" class="indexterm"/>authenticate ourselves on the instances, we need to provide <a id="id402" class="indexterm"/>the public part of the key pair to OpenStack. This is done using the <code class="literal">openstack_compute_keypair_v2</code> resource, specifying in which region we want the key, and where the key is. Let's add both variables in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "region" {
  default     = "GRA1"
  description = "OpenStack Region"
}

variable "ssh_key_file" {
  default     = "keys/admin_key"
  description = "Default SSH key"
}</pre></div><p>Next, override them in the <code class="literal">terraform.tfvars</code> file:</p><div class="informalexample"><pre class="programlisting">region      = "BHS1"</pre></div><p>Now we can build our resource in the <code class="literal">keys.tf</code> file:</p><div class="informalexample"><pre class="programlisting">resource "openstack_compute_keypair_v2" "ssh" {
  name       = "Admin SSH Public Key"
  region     = "${var.region}"
  public_key = "${file("${var.ssh_key_file}.pub")}"
}</pre></div></div><div class="section" title="Creating a security group on OpenStack"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec82"/>Creating a security group on OpenStack</h3></div></div></div><p>We know <a id="id403" class="indexterm"/>our requirements are to allow HTTP (TCP/80) from <a id="id404" class="indexterm"/>anywhere, but SSH (TCP/22) only from one corporate network. Add it right now in <code class="literal">variables.tf</code> so we can use it:</p><div class="informalexample"><pre class="programlisting">variable "allowed_network" {
  default = "1.2.3.4/32"
  description = "The Whitelisted Corporate Network"
}</pre></div><p>Don't forget to override with your own network in <code class="literal">terraform.tfvars</code>.</p><p>Let's create a <a id="id405" class="indexterm"/>first security group allowing HTTP for everyone in our region, using the <code class="literal">openstack_compute_secgroup_v2</code> resource in a <code class="literal">security.tf</code> file:</p><div class="informalexample"><pre class="programlisting"> resource "openstack_compute_secgroup_v2" "http-sg" {
  name        = "http-sg"
  description = "HTTP Security Group"
  region      = "${var.region}"

  rule {
    from_port   = 80
    to_port     = 80
    ip_protocol = "tcp"
    cidr        = "0.0.0.0/0"
  }
}</pre></div><p>Following the <a id="id406" class="indexterm"/>same pattern, create another security group to allow SSH only from our corporate network:</p><div class="informalexample"><pre class="programlisting">resource "openstack_compute_secgroup_v2" "base-sg" {
  name        = "base-sg"
  description = "Base Security Group"
  region      = "${var.region}"

  rule {
    from_port   = 22
    to_port     = 22
    ip_protocol = "tcp"
    cidr        = "${var.allowed_network}"
  }
}</pre></div></div><div class="section" title="Creating block storage volumes on OpenStack"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec83"/>Creating block storage volumes on OpenStack</h3></div></div></div><p>In our <a id="id407" class="indexterm"/>requirements, we want a dedicated <a id="id408" class="indexterm"/>volume to be available to our GitLab instance, for Docker. We decide this one will be <code class="literal">10</code> GB in size. This volume will be mounted by the compute instance under a dedicated device (likely <code class="literal">/dev/vdb</code>). The whole thing is done using the <code class="literal">openstack_blockstorage_volume_v2</code> resource:</p><div class="informalexample"><pre class="programlisting">resource "openstack_blockstorage_volume_v2" "docker" {
  region      = "${var.region}"
  name        = "docker-vol"
  description = "Docker volume"
  size        = 10
}</pre></div><p>Add a <a id="id409" class="indexterm"/>simple output in <code class="literal">outputs.tf</code> so we <a id="id410" class="indexterm"/>know the volume description, name, and size:</p><div class="informalexample"><pre class="programlisting">output "Block Storage" {
  value = "${openstack_blockstorage_volume_v2.docker.description}: ${openstack_blockstorage_volume_v2.docker.name}, ${openstack_blockstorage_volume_v2.docker.size}GB"
}</pre></div><p>We now have every requirement to launch our compute instances.</p></div><div class="section" title="Creating compute instances on OpenStack"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec84"/>Creating compute instances on OpenStack</h3></div></div></div><p>It's now <a id="id411" class="indexterm"/>time to create the instances. We <a id="id412" class="indexterm"/>know they have to be Ubuntu 16.04, and we decide on a flavor name: a flavor is the type of the machine. It varies from every other OpenStack installation. In our case, it's named <code class="literal">vps-ssd-1</code>. Let's define some defaults in the <code class="literal">variables.tf</code> file:</p><div class="informalexample"><pre class="programlisting">variable "image_name" {
  default     = "CentOS"
  description = "Default OpenStack image to boot"
}

variable "flavor_name" {
  default     = "some_flavor"
  description = "OpenStack instance flavor"
}</pre></div><p>Also, override them with good values in <code class="literal">terraform.tfvars</code>:</p><div class="informalexample"><pre class="programlisting">image_name  = "Ubuntu 16.04"
flavor_name = "vps-ssd-1"</pre></div><p>To create a compute instance, we use a resource named <code class="literal">openstack_compute_instance_v2</code>. This resource takes all the parameters we previously declared (name, image, flavor, SSH key, and security groups). Let's try this in <code class="literal">instances.tf</code>:</p><div class="informalexample"><pre class="programlisting">resource "openstack_compute_instance_v2" "gitlab" {
  name            = "gitlab"
  region          = "${var.region}"
  image_name      = "${var.image_name}"
  flavor_name     = "${var.flavor_name}"
  key_pair        = "${openstack_compute_keypair_v2.ssh.name}"
  security_groups = ["${openstack_compute_secgroup_v2.base-sg.name}", "${openstack_compute_secgroup_v2.http-sg.name}"]
}</pre></div><p>To attach <a id="id413" class="indexterm"/>the block storage volume we created, we need to add a <code class="literal">volume {}</code> block inside the resource:</p><div class="informalexample"><pre class="programlisting">  volume {
    volume_id = "${openstack_blockstorage_volume_v2.docker.id}"
    device    = "/dev/vdb"
  }</pre></div><p>Now, an <a id="id414" class="indexterm"/>optional but fun part is that the commands needed to format the volume, mount it at the right place, fully update the system, install Docker, and run the GitLab CE container. This is done using the <code class="literal">remote-exec</code> provisioner and requires a SSH username. Let's set it as <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "ssh_username" {
  default     = "ubuntu"
  description = "SSH username"
}</pre></div><p>Now we can just type in all the commands to be executed when the instance is ready:</p><div class="informalexample"><pre class="programlisting">  provisioner "remote-exec" {
    connection {
      user        = "${var.ssh_username}"
      private_key = "${file("${var.ssh_key_file}")}"
    }

    inline = [
      "sudo mkfs.ext4 /dev/vdb",
      "sudo mkdir /var/lib/docker",
      "sudo su -c \"echo '/dev/vdb /var/lib/docker ext4 defaults 0 0' &gt;&gt; /etc/fstab\"",
      "sudo mount -a",
      "sudo apt update -y",
      "sudo apt upgrade -y",
      "sudo apt install -y docker.io",
      "sudo systemctl enable docker",
      "sudo systemctl start docker",
      "sudo docker run -d -p 80:80 --name gitlab gitlab/gitlab-ce:latest",
    ]
  }</pre></div><p>Add a simple output in the <code class="literal">outputs.tf</code> file, so we easily know the GitLab instance public IP:</p><div class="informalexample"><pre class="programlisting">output "GitLab Instance" {
  value = "gitlab: http://${openstack_compute_instance_v2.gitlab.access_ip_v4}"
}</pre></div><p>The runner <a id="id415" class="indexterm"/>instances are the same, but a <a id="id416" class="indexterm"/>little simpler, as they don't need a local volume. However, we need to set the amount of runners we want in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "num_runners" {
  default     = "1"
  description = "Number of GitLab CI runners"
}</pre></div><p>Override the value to have more runners in <code class="literal">terraform.tfvars</code>:</p><div class="informalexample"><pre class="programlisting">num_runners = "2"</pre></div><p>Now we can create our runner instances using the <code class="literal">openstack_compute_instance_v2</code> resource:</p><div class="informalexample"><pre class="programlisting">resource "openstack_compute_instance_v2" "runner" {
  count           = "${var.num_runners}"
  name            = "gitlab-runner-${count.index+1}"
  region          = "${var.region}"
  image_name      = "${var.image_name}"
  flavor_name     = "${var.flavor_name}"
  key_pair        = "${openstack_compute_keypair_v2.ssh.name}"
  security_groups = ["${openstack_compute_secgroup_v2.base-sg.name}", "${openstack_compute_secgroup_v2.http-sg.name}"]

  provisioner "remote-exec" {
    connection {
      user        = "${var.ssh_username}"
      private_key = "${file("${var.ssh_key_file}")}"
    }

    inline = [
      "sudo apt update -y",
      "sudo apt upgrade -y",
      "sudo apt install -y docker.io",
      "sudo systemctl enable docker",
      "sudo systemctl start docker",
      "sudo docker run -d --name gitlab-runner -v /var/run/docker.sock:/var/run/docker.sock gitlab/gitlab-runner:latest",
    ]
  }
}</pre></div><p>This <a id="id417" class="indexterm"/>will launch a GitLab CI runner, so <a id="id418" class="indexterm"/>builds can be triggered by GitLab! (there's one last step of configuration, though. It's out of the scope of this book, but we need to register each runner to the main GitLab instance by executing <code class="literal">docker exec -it gitlab-runner gitlab-runner register</code> and answering the questions).</p><p>Add the following output to <code class="literal">outputs.tf</code> so we know all the IP addresses of our runners:</p><div class="informalexample"><pre class="programlisting">output "GitLab Runner Instances" {
  value = "${join(" ", openstack_compute_instance_v2.runner.*.access_ip_v4)}"
}</pre></div></div><div class="section" title="Creating an object storage container on OpenStack"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec85"/>Creating an object storage container on OpenStack</h3></div></div></div><p>This <a id="id419" class="indexterm"/>one is very simple: it only requires <a id="id420" class="indexterm"/>a name and a region. As it's to store releases, let's call it <code class="literal">releases</code>, using the <code class="literal">openstack_objectstorage_container_v1</code> resource, in an <code class="literal">objectstorage.tf</code> file:</p><div class="informalexample"><pre class="programlisting">resource "openstack_objectstorage_container_v1" "releases" {
  region = "${var.region}"
  name   = "releases"
}</pre></div><p>Add a simple output in <code class="literal">outputs.tf</code> so we remember the <code class="literal">Object Storage</code> container name:</p><div class="informalexample"><pre class="programlisting">output "Object Storage" {
  value = "Container name: ${openstack_objectstorage_container_v1.releases.name}"
}</pre></div></div><div class="section" title="Applying"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec86"/>Applying</h3></div></div></div><p>In <a id="id421" class="indexterm"/>the end, do a <code class="literal">terraform apply</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ terraform apply</strong></span>
<span class="strong"><strong>[...]</strong></span>

<span class="strong"><strong>Outputs:</strong></span>

<span class="strong"><strong>Block Storage = Docker volume: docker-vol, 10GB</strong></span>
<span class="strong"><strong>GitLab Instance = gitlab: http://158.69.95.202</strong></span>
<span class="strong"><strong>GitLab Runner Instances = 158.69.95.200 158.69.95.201</strong></span>
<span class="strong"><strong>Object Storage = Container name: releases</strong></span>
</pre></div><p>Connect to the GitLab instance and enjoy the runners (after GitLab token registration)!</p></div></div></div>
<div class="section" title="Managing Heroku apps and add-ons using Terraform"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec56"/>Managing Heroku apps and add-ons using Terraform</h1></div></div></div><p>Heroku <a id="id422" class="indexterm"/>is a popular <span class="strong"><strong>Platform-as-a-Service</strong></span> (<span class="strong"><strong>PaaS</strong></span>), where you <a id="id423" class="indexterm"/>have absolutely no control over the infrastructure. But <a id="id424" class="indexterm"/>even for such platforms, Terraform <a id="id425" class="indexterm"/>can automate and manage <a id="id426" class="indexterm"/>things for you, so Heroku can do the rest. We'll create an app (a simple GitHub Hubot: <a class="ulink" href="http://hubot.github.com/">http://hubot.github.com/</a>), but feel free <a id="id427" class="indexterm"/>to use your own. On top of this app, we'll <a id="id428" class="indexterm"/>automatically plug a Heroku add-on (redis) and deploy everything.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec130"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Terraform installation</li><li class="listitem" style="list-style-type: disc">A <a id="id429" class="indexterm"/>Heroku account (<a class="ulink" href="https://www.heroku.com/">https://www.heroku.com/</a>)</li><li class="listitem" style="list-style-type: disc">An optional Slack Token</li><li class="listitem" style="list-style-type: disc">An Internet connection</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec131"/>How to do it…</h2></div></div></div><p>First things first: we need to define the Heroku provider. It consists of an e-mail address and an API key. Let's create generic variables for that in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "heroku_email" {
  default     = "user@mail.com"
  description = "Heroku account email"
}

variable "heroku_api_key" {
  default     = "12345"
  description = "Heroku account API key"
}</pre></div><p>Don't forget to override them in <code class="literal">terraform.tfvars</code>:</p><div class="informalexample"><pre class="programlisting">heroku_email = "me@gmail.com"
heroku_api_key = "52eef461-5e34-47d8-8191-ede7ef6cf9bg"</pre></div><p>Now we can create the Heroku provider with the information we have:</p><div class="informalexample"><pre class="programlisting">provider "heroku" {
  email   = "${var.heroku_email}"
  api_key = "${var.heroku_api_key}"
}</pre></div><div class="section" title="Creating a Heroku application with Terraform"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec87"/>Creating a Heroku application with Terraform</h3></div></div></div><p>Instead <a id="id430" class="indexterm"/>of clicking through Heroku to <a id="id431" class="indexterm"/>create an application, let's do it right from Terraform. We want to run our app in Europe and we want Hubot to connect to Slack, so we need to provide a Slack token as well. Let's start by creating default values in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "heroku_region" {
  default = "us"
  description = "Heroku region"
}

variable "slack_token" {
  default = "xoxb-1234-5678-1234-5678"
  description = "Slack Token"
}</pre></div><p>Now we can create our first Heroku app with its variables using the <code class="literal">heroku_app</code> resource, in <code class="literal">heroku.tf</code>:</p><div class="informalexample"><pre class="programlisting">resource "heroku_app" "hubot" {
  name   = "iac-book-hubot"
  region = "${var.heroku_region}"

  config_vars {
    HUBOT_SLACK_TOKEN = "${var.slack_token}"
  }
}</pre></div><p>That's it! As simple as it seems.</p><p>Add some output in <code class="literal">outputs.tf</code> so we have better information about our app, like the Heroku app URL and environment variables:</p><div class="informalexample"><pre class="programlisting">output "heroku URL" {
  value = "${heroku_app.hubot.web_url}"
}

output "heroku_vars" {
  value = "${heroku_app.hubot.all_config_vars}"
}

output "heroku Git URL" {
  value = "${heroku_app.hubot.git_url}"
}</pre></div></div><div class="section" title="Adding Heroku add-ons using Terraform"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec88"/>Adding Heroku add-ons using Terraform</h3></div></div></div><p>Some <a id="id432" class="indexterm"/>add-ons need Redis to store data. Instead <a id="id433" class="indexterm"/>of going through the web application and enabling add-ons, let's instead use the <code class="literal">heroku_addon</code> resource. It takes a reference to the app to link the add-on to, and a plan (<code class="literal">hobby-dev</code> is free, so let's use that):</p><div class="informalexample"><pre class="programlisting">resource "heroku_addon" "redis" {
  app  = "${heroku_app.hubot.name}"
  plan = "heroku-redis:hobby-dev"
}</pre></div></div><div class="section" title="Using Heroku with Terraform"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec89"/>Using Heroku with Terraform</h3></div></div></div><p>It's <a id="id434" class="indexterm"/>out of the scope of this book to show Heroku usage, but let's <a id="id435" class="indexterm"/>apply this terraform code:</p><div class="informalexample"><pre class="programlisting">$ terraform apply
[...]
Outputs:

heroku Git URL = https://git.heroku.com/iac-book-hubot.git
heroku URL = https://iac-book-hubot.herokuapp.com/
heroku_vars = {
  HUBOT_SLACK_TOKEN = xoxb-1234-5678-91011-00e4dd
}</pre></div><p>If you don't have an application ready to ship on Heroku, let's try to deploy GitHub's chat robot <span class="emphasis"><em>Hubot</em></span>. It's an easy application ready to use on Heroku. Quickly reading through the Hubot documentation, let's install the Hubot generator:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ npm install -g yo generator-hubot</strong></span>
</pre></div><p>Create a new <code class="literal">hubot</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir src; cd src</strong></span>
<span class="strong"><strong>$ yo hubot</strong></span>
</pre></div><p>Answer the questions and when you're done, using the usual <code class="literal">heroku</code> command, add the Heroku git remote for our Heroku app:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ heroku git:remote --app iac-book-hubot</strong></span>
</pre></div><p>Now you can <code class="literal">git push heroku</code> and see your application being deployed, all using Terraform.</p></div></div></div>
<div class="section" title="Creating a scalable Docker Swarm cluster on bare metal with Packet"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec57"/>Creating a scalable Docker Swarm cluster on bare metal with Packet</h1></div></div></div><p>IaaS <a id="id436" class="indexterm"/>clouds have been popularized <a id="id437" class="indexterm"/>through heavy usage of virtual machines. Recent initiatives are targeting bare metal servers with an API, so we get the best of both worlds—on-demand servers through an API and incredible performance through direct access to the hardware. <a class="ulink" href="https://www.packet.net/">https://www.packet.net/</a> is a bare <a id="id438" class="indexterm"/>metal IaaS provider (<a class="ulink" href="https://www.scaleway.com/">https://www.scaleway.com/</a> is another) very well <a id="id439" class="indexterm"/>supported by Terraform with an awesome global network. Within minutes we have new hardware ready and connected to the network.</p><p>We'll build a fully automated and scalable Docker Swarm cluster, so we can operate highly scalable and performant workloads on bare metal: this setup can scale thousands of containers in just a few minutes. This cluster is composed of <span class="emphasis"><em>Type 0</em></span> machines (4 cores and 8 GB RAM), for one manager and 2 nodes, totaling 12 cores and 24 GB of RAM, but we can use more performant machines if we want: the same cluster with <span class="emphasis"><em>Type 2</em></span> machines will have 72 cores and 768 GB of RAM (though the price will adapt accordingly).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec132"/>Getting ready</h2></div></div></div><p>To step through this recipe, you will need the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A working Terraform installation</li><li class="listitem" style="list-style-type: disc">A Packet.net account with an API key</li><li class="listitem" style="list-style-type: disc">An Internet connection</li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec133"/>How to do it…</h2></div></div></div><p>Let's start by creating the <code class="literal">packet</code> provider, using the API key (an authentication token). Create the variable in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "auth_token" {
  default     = "1234"
  description = "API Key Auth Token"
}</pre></div><p>Also, be sure to override the value in <code class="literal">terraform.tfvars</code> with the real token:</p><div class="informalexample"><pre class="programlisting">auth_token = "JnN7e6tPMpWNtGcyPGT93AkLuguKw2eN"</pre></div><div class="section" title="Creating a Packet project using Terraform"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec90"/>Creating a Packet project using Terraform</h3></div></div></div><p>Packet, like <a id="id440" class="indexterm"/>some other IaaS providers, uses <a id="id441" class="indexterm"/>the notion of <span class="emphasis"><em>project</em></span> to group machines. Let's create a project named <code class="literal">Docker Swarm Bare Metal Infrastructure</code>, since that's what we want to do, in a <code class="literal">projects.tf</code> file:</p><div class="informalexample"><pre class="programlisting">resource "packet_project" "swarm" {
  name = "Docker Swarm Bare Metal Infrastructure"
}</pre></div><p>This way, if you happen to manage multiple projects or customers, you can split them all into their own projects.</p></div><div class="section" title="Handling Packet SSH keys using Terraform"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec91"/>Handling Packet SSH keys using Terraform</h3></div></div></div><p>To <a id="id442" class="indexterm"/>connect to the machines using SSH, we <a id="id443" class="indexterm"/>need at least one public key uploaded to our Packet account. Let's create a variable to store it in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "ssh_key" {
  default     = "keys/admin_key"
  description = "Path to SSH key"
}</pre></div><p>Don't forget to override the value in <code class="literal">terraform.tfvars</code> if you use another name for the key.</p><p>Let's use the <code class="literal">packet_ssh_key</code> resource to create the SSH key on our Packet account:</p><div class="informalexample"><pre class="programlisting">resource "packet_ssh_key" "admin" {
  name       = "admin_key"
  public_key = "${file("${var.ssh_key}.pub")}"
}</pre></div></div><div class="section" title="Bootstraping a Docker Swarm manager on Packet using Terraform"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec92"/>Bootstraping a Docker Swarm manager on Packet using Terraform</h3></div></div></div><p>We'll create <a id="id444" class="indexterm"/>two types of servers for this Docker Swarm cluster: managers <a id="id445" class="indexterm"/>and nodes. Managers are controlling what's executed on the nodes. We'll <a id="id446" class="indexterm"/>start by bootstrapping the Docker Swarm manager server, using the Packet service (more alternatives are available from Packet API):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We want the cheapest server (<code class="literal">baremetal_0</code>)</li><li class="listitem" style="list-style-type: disc">We want the servers in Amsterdam (<code class="literal">ams1</code>)</li><li class="listitem" style="list-style-type: disc">We want the servers to run Ubuntu 16.04 (<code class="literal">ubuntu_16_04_image</code>)</li><li class="listitem" style="list-style-type: disc">Default SSH user is <code class="literal">root</code></li><li class="listitem" style="list-style-type: disc">Billing will be <code class="literal">hourly</code>, but that can be <code class="literal">monthly</code> as well</li></ul></div><p>Let's put <a id="id447" class="indexterm"/>generic information in <code class="literal">variables.tf</code> so <a id="id448" class="indexterm"/>we can manipulate<a id="id449" class="indexterm"/> them:</p><div class="informalexample"><pre class="programlisting">variable "facility" {
  default     = "ewr1"
  description = "Packet facility (us-east=ewr1, us-west=sjc1, eu-west=ams1)"
}

variable "plan" {
  default     = "baremetal_0"
  description = "Packet machine type"
}

variable "operating_system" {
  default     = "coreos_stable"
  description = "Packet operating_system"
}

variable "ssh_username" {
  default     = "root"
  description = "Default host username"
}</pre></div><p>Also, override them in <code class="literal">terraform.tfvars</code> to match our values:</p><div class="informalexample"><pre class="programlisting">facility = "ams1"
operating_system = "ubuntu_16_04_image"</pre></div><p>To create a server with Packet, let's use the <code class="literal">packet_device</code> resource, specifying the chosen plan, facility, operating system, billing, and the project in which it will run:</p><div class="informalexample"><pre class="programlisting">resource "packet_device" "swarm_master" {
  hostname         = "swarm-master"
  plan             = "${var.plan}"
  facility         = "${var.facility}"
  operating_system = "${var.operating_system}"
  billing_cycle    = "hourly"
  project_id       = "${packet_project.swarm.id}"
}</pre></div><p>Now, let's create two scripts that will execute when the server is ready. The first one will update Ubuntu (<code class="literal">update_os.sh</code>) while the second will install Docker (<code class="literal">install_docker.sh</code>). </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/usr/bin/env bash</strong></span>
<span class="strong"><strong># file: ./scripts/update_os.sh</strong></span>
<span class="strong"><strong>sudo apt update -yqq</strong></span>
<span class="strong"><strong>sudo apt upgrade -yqq</strong></span>
</pre></div><p>This script will install and start Docker:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/usr/bin/env bash</strong></span>
<span class="strong"><strong># file: ./scripts/install_docker.sh</strong></span>
<span class="strong"><strong>curl -sSL https://get.docker.com/ | sh</strong></span>
<span class="strong"><strong>sudo systemctl enable docker</strong></span>
<span class="strong"><strong>sudo systemctl start docker</strong></span>
</pre></div><p>We <a id="id450" class="indexterm"/>can now call those <a id="id451" class="indexterm"/>scripts as a <code class="literal">remote-exec</code> provisioner <a id="id452" class="indexterm"/>inside the <code class="literal">packet_device</code> resource:</p><div class="informalexample"><pre class="programlisting">  provisioner "remote-exec" {
    connection {
      user        = "${var.ssh_username}"
      private_key = "${file("${var.ssh_key}")}"
    }

    scripts = [
      "scripts/update_os.sh",
      "scripts/install_docker.sh",
    ]
  }</pre></div><p>At this point, the system is fully provisioned and functional, with Docker running.</p><p>To initialize a Docker Swarm cluster, starting with Docker 1.12, we can just issue the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker swarm init --advertise-addr docker.manager.local.ip</strong></span>
</pre></div><p>A server at Packet has one interface sharing both public and private IP addresses. The private IP is the second one, and is available through the following exported attribute: <code class="literal">${packet_device.swarm_master.network.2.address}</code>. Let's create another <code class="literal">remote-exec</code> provisioner, so the Swarm manager is initialized automatically, right after bootstrap:</p><div class="informalexample"><pre class="programlisting">  provisioner "remote-exec" {
    connection {
      user        = "${var.ssh_username}"
      private_key = "${file("${var.ssh_key}")}"
    }

    inline = [
      "docker swarm init --advertise-addr ${packet_device.swarm_master.network.2.address}",
    ]
  }</pre></div><p>At <a id="id453" class="indexterm"/>this point, we have a Docker cluster <a id="id454" class="indexterm"/>running, with only one node—the <a id="id455" class="indexterm"/>manager itself.</p><p>The last step is to store the Swarm token, so the nodes can join. The token can be obtained with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker swarm join-token worker -q</strong></span>
</pre></div><p>We'll store this token in a simple file in our infrastructure repository (<code class="literal">worker.token</code>), so we can access it and version it. Let's create a variable to store our token in a file in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "worker_token_file" {
  default     = "worker.token"
  description = "Worker token file"
}</pre></div><p>We will execute the previous <code class="literal">docker swarm</code> command through SSH when everything else is done, using a <code class="literal">local-exec</code> provisioner. As we can't interact with the process, let's skip the host key checking and other initial SSH checks:</p><div class="informalexample"><pre class="programlisting">  provisioner "local-exec" {
    command = "ssh -t -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i ${var.ssh_key} ${var.ssh_username}@${packet_device.swarm_master.network.0.address} \"docker swarm join-token worker -q\" &gt; ${var.worker_token_file}"
  }</pre></div><p>We're now done with the Docker Swarm manager!</p></div><div class="section" title="Bootstraping Docker Swarm nodes on Packet using Terraform"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec93"/>Bootstraping Docker Swarm nodes on Packet using Terraform</h3></div></div></div><p>We <a id="id456" class="indexterm"/>need nodes to join the swarm, so the <a id="id457" class="indexterm"/>workload can be spread. For <a id="id458" class="indexterm"/>convenience, the machine specs for the nodes will be the same as that of the master. Here's what will happen:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Two nodes are created</li><li class="listitem" style="list-style-type: disc">The token file is sent to each node</li><li class="listitem" style="list-style-type: disc">The operating system is updated, and Docker is installed</li><li class="listitem" style="list-style-type: disc">The node joins the swarm</li></ul></div><p>Let's start by creating a variable for the number of nodes we want, in <code class="literal">variables.tf</code>:</p><div class="informalexample"><pre class="programlisting">variable "num_nodes" {
  default     = "1"
  description = "Number of Docker Swarm nodes"
}</pre></div><p>Override that value as the cluster grows in <code class="literal">terraform.tfvars</code>:</p><div class="informalexample"><pre class="programlisting">num_nodes = "2"</pre></div><p>Create the nodes using the same <code class="literal">packet_device</code> resource we used for the master:</p><div class="informalexample"><pre class="programlisting">resource "packet_device" "swarm_node" {
  count            = "${var.num_nodes}"
  hostname         = "swarm-node-${count.index+1}"
  plan             = "${var.plan}"
  facility         = "${var.facility}"
  operating_system = "${var.operating_system}"
  billing_cycle    = "hourly"
  project_id       = "${packet_project.swarm.id}"
}</pre></div><p>Add a <code class="literal">file</code> provisioner to copy the token file:</p><div class="informalexample"><pre class="programlisting">  provisioner "file" {
    source      = "${var.worker_token_file}"
    destination = "${var.worker_token_file}"
  }</pre></div><p>Using the same update and Docker installation scripts as the master, create the same <code class="literal">remote-exec</code> provisioner:</p><div class="informalexample"><pre class="programlisting">  provisioner "remote-exec" {
    connection {
      user        = "${var.ssh_username}"
      private_key = "${file("${var.ssh_key}")}"
    }

    scripts = [
      "scripts/update_os.sh",
      "scripts/install_docker.sh",
    ]
  }</pre></div><p>The operating system is now fully updated and Docker is running.</p><p>Now we want to join the Docker Swarm cluster. To do this, we need two pieces of information: the token and the local IP of the master. We already have the token in a file locally, and Terraform knows the local IP of the swarm manager. So a trick is to create a simple <a id="id459" class="indexterm"/>script (I suggest you write a more robust one!), that <a id="id460" class="indexterm"/>reads the local token, and <a id="id461" class="indexterm"/>takes the local manager IP address as an argument. In a file named <code class="literal">scripts/join_swarm.sh</code>, enter the following lines:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env bash
# file: scripts/join_swarm.sh
MASTER=$1
SWARM_TOKEN=$(cat worker.token)
docker swarm join --token ${SWARM_TOKEN} ${MASTER}:2377</pre></div><p>Now we just have to send this file to the nodes using the <code class="literal">file</code> provisioner:</p><div class="informalexample"><pre class="programlisting">  provisioner "file" {
    source      = "scripts/join_swarm.sh"
    destination = "join_swarm.sh"
  }</pre></div><p>Use it as a last step through a <code class="literal">remote-exec</code> provisioner, sending the local Docker master IP (<code class="literal">${packet_device.swarm_master.network.2.address}"</code>) as an argument to the script:</p><div class="informalexample"><pre class="programlisting">  provisioner "remote-exec" {
    connection {
      user        = "${var.ssh_username}"
      private_key = "${file("${var.ssh_key}")}"
    }

    inline = [
      "chmod +x join_swarm.sh",
      "./join_swarm.sh ${packet_device.swarm_master.network.2.address}",
    ]
  }//.</pre></div><p>Launch the whole infrastructure:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ terraform apply</strong></span>
<span class="strong"><strong>Outputs:</strong></span>

<span class="strong"><strong>Swarm Master Private IP = 10.80.86.129</strong></span>
<span class="strong"><strong>Swarm Master Public IP = 147.75.100.19</strong></span>
<span class="strong"><strong>Swarm Nodes = Public: 147.75.100.23,147.75.100.3, Private: 10.80.86.135,10.80.86.133</strong></span>
</pre></div><p>Our cluster is running.</p></div><div class="section" title="Using the Docker Swarm cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec94"/>Using the Docker Swarm cluster</h3></div></div></div><p>Using our <a id="id462" class="indexterm"/>Docker Swarm cluster is out of the scope of this book, but now we have it, let's take a quick look to scale a container to the thousands!</p><p>Verify we have our 3 nodes:</p><div class="informalexample"><pre class="programlisting"># docker node ls
ID                           HOSTNAME                STATUS  AVAILABILITY  MANAGER STATUS
9sxqi2f1pywmofgf63l84n7ps *  swarm-master.local.lan  Ready   Active        Leader
ag07nh1wzsbsvnef98sqf5agy    swarm-node-1.local.lan  Ready   Active
cppk5ja4spysu6opdov9f3x8h    swarm-node-2.local.lan  Ready   Active</pre></div><p>We want a common network for our containers, and we want to scale to the thousands. So a typical /24 network won't be enough (that's the <code class="literal">docker network</code> default). Let's create a /16 overlay network, so we have room for scale!</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># docker network create -d overlay --subnet 172.16.0.0/16 nginx-network</strong></span>
</pre></div><p>Create a Docker service that will simply launch an nginx container on this new overlay network, with 3 replicas (3 instances of the container running at the same time):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># docker service create --name nginx --network nginx-network --replicas 3 -p 80:80/tcp nginx</strong></span>
</pre></div><p>Verify if it's working:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># docker service ls</strong></span>
<span class="strong"><strong>ID            NAME   REPLICAS  IMAGE  COMMAND</strong></span>
<span class="strong"><strong>aeq9lspl0mpg  nginx  3/3       nginx</strong></span>
</pre></div><p>Now, accessing by HTTP any of the public IPs of the cluster, any container of any node can answer: we can make an HTTP request to node-1, and it can be a container on node-2 responding. Nice!</p><p>Let's scale our service now, from 3 replicas to 100:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># docker service scale nginx=100</strong></span>
<span class="strong"><strong>nginx scaled to 100</strong></span>
<span class="strong"><strong># docker service ls</strong></span>
<span class="strong"><strong>ID            NAME   REPLICAS  IMAGE  COMMAND</strong></span>
<span class="strong"><strong>aeq9lspl0mpg  nginx  100/100   nginx</strong></span>
</pre></div><p>We just scaled to a hundred containers in a few seconds and split them on all 3 bare metal machines.</p><p>Now, you <a id="id463" class="indexterm"/>know you can scale, and with such a configuration you can push the <code class="literal">nginx</code> service to 500, 1000, or maybe more!</p></div></div></div></body></html>