<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer132">
<h1 class="chapter-number" id="_idParaDest-227"><a id="_idTextAnchor244"/>11</h1>
<h1 id="_idParaDest-228"><a id="_idTextAnchor245"/>Publishing Applications</h1>
<p>Running applications on Kubernetes adds resilience to all of an application’s components by running its processes as containers. This helps us to provide stability and update these components without impacting our users. Although Kubernetes provides a lot of resources to simplify the cluster-wide management of the applications, we do need to understand how using these resources will affect the way our applications are reached by <span class="No-Break">our users.</span></p>
<p>In this chapter, we will learn how to publish our applications to make them accessible to our users. This will involve publishing certain Pods or containers to provide services, but sometimes, we may also need to debug our applications to fix issues <span class="No-Break">that arise.</span></p>
<p>By the end of this chapter, we will have learned how <strong class="bold">NetworkPolicy resources</strong> help us to isolate the workloads deployed in our cluster, and we will have reviewed the use of <strong class="bold">service mesh</strong> solutions to improve the overall security between our <span class="No-Break">applications’ components.</span></p>
<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
<ul>
<li>Understanding Kubernetes features for publishing <span class="No-Break">applications cluster-wide</span></li>
<li>Proxying and forwarding applications <span class="No-Break">for debugging</span></li>
<li>Using the host network namespace for <span class="No-Break">publishing applications</span></li>
<li>Publishing applications with Kubernetes’ <span class="No-Break">NodePort feature</span></li>
<li>Providing access to your Services with <span class="No-Break">LoadBalancer Services</span></li>
<li>Understanding <span class="No-Break">Ingress Controllers</span></li>
<li>Improving our <span class="No-Break">applications’ security</span></li>
</ul>
<p>We will start this chapter by reviewing the different options we have with Kubernetes out of the box for delivering our applications to <span class="No-Break">our users.</span></p>
<h1 id="_idParaDest-229"><a id="_idTextAnchor246"/>Technical requirements</h1>
<p>You can find the labs for this chapter at <a href="https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter11">https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter11</a>, where you will find some extended explanations, omitted in the chapter’s content to make it easier to follow. The <em class="italic">Code In Action</em> video for this chapter can be found <span class="No-Break">at </span><a href="https://packt.link/JdOIY"><span class="No-Break">https://packt.link/JdOIY</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-230"><a id="_idTextAnchor247"/>Understanding Kubernetes features for publishing applications cluster-wide</h1>
<p>Kubernetes is a <a id="_idIndexMarker1215"/>container orchestrator that allows users to run their applications’ workloads cluster-wide. We reviewed in <a href="B19845_09.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>,<em class="italic"> Implementing Architecture Patterns</em>, the different patterns we can use to deploy our applications using different Kubernetes resources. Pods are the minimum deployment unit for our applications and have <a id="_idIndexMarker1216"/>dynamic IP addresses, thus we can’t use them for publishing our applications. Dynamism affects the <a id="_idIndexMarker1217"/>exposure of all the components internally and externally – while Kubernetes successfully makes the creation and removal of containers simple, the IP addresses used will continuously change. Therefore, we need an intermediate component, the Service resource, to manage the interaction of any kind of client with the Pods (running on the backend) associated with an application component. We can also have Service resources pointing to external resources (for example, the <strong class="source-inline">ExternalName</strong> <span class="No-Break">Service type).</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">It is crucial to understand that not all an application’s components need to be accessible outside of the cluster or even namespace scopes. In this chapter, we are going to learn different options and mechanisms for publishing applications for access both inside and outside of the Kubernetes cluster. You as a developer must know and understand which of the application’s components will act as frontends for your application and thus must be accessible and which should act as backends and be reachable, and employ the appropriate mechanisms in <span class="No-Break">each case.</span></p>
<p>We will use Kubernetes Service resources to publish the application’s Pods internally or externally as required. We will never connect to Pods’ published ports directly. Pods’ ports will be associated with a Service resource using labels. An intermediate resource is created to associate Services with Pods, EndpointSlices, and Endpoint resources. These resources are created automatically for you when you create a Service and the associated Pods are located. The EndpointSlices point to Endpoint resources, which are updated when the backend Pods (or external <span class="No-Break">Services) change.</span></p>
<p>Let’s see <a id="_idIndexMarker1218"/>how this works with<a id="_idIndexMarker1219"/> an example. We will create a Service resource before its actual Pods. The following code snippet shows an example of a Service <span class="No-Break">resource manifest:</span></p>
<pre class="source-code">
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  selector:
   myapp: test
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080</pre> <p>If we create the Service resource using the preceding YAML manifest and retrieve the created endpoints, we will see which Pods (with their IP addresses) are associated as backends. Let’s see the currently <span class="No-Break">associated endpoints:</span></p>
<pre class="console">
$ kubectl get endpoints myservice
NAME        ENDPOINTS   AGE
myservice   &lt;none&gt;      29s</pre> <p>The list of endpoints is empty because we don’t have any associated backend Pod (with the <strong class="source-inline">myapp=test</strong> label). Let’s create a simple Pod with this label using <span class="No-Break"><strong class="source-inline">kubectl run</strong></span><span class="No-Break">:</span></p>
<pre class="console">
$ kubectl run mypod1 --labels myapp=test \
--image=nginx:alpine
pod/mypod1 created</pre> <p>We now review the associated <span class="No-Break">Pods again:</span></p>
<pre class="console">
$ kubectl get endpoints myservice
NAME        ENDPOINTS         AGE
myservice   10.1.0.113:8080   5m58s</pre> <p>Notice<a id="_idIndexMarker1220"/> that we didn’t specify any port <a id="_idIndexMarker1221"/>for the Pod, hence the association may be wrong (in fact, the <strong class="source-inline">docker.io/nginx:alpine</strong> image defines port <strong class="source-inline">80</strong> for the process). Kubernetes does not verify this information; it just creates the required links <span class="No-Break">between resources.</span></p>
<p>EndpointSlice resources are managed by Kubernetes and are dynamically updated whenever a new Pod is created or an old one fails (in fact, the backend Endpoint resources change and the update is propagated). If you are having problems with a Service not responding but your Pods are running, this is something you may need <span class="No-Break">to check.</span></p>
<p>This is just an example of creating a <a id="_idIndexMarker1222"/>simple <strong class="bold">ClusterIP</strong> Service, which is the default option. We already learned the different Service resource types in <a href="B19845_09.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Implementing Architecture Patterns</em>, but it may be important to quickly review those types that allow us to <span class="No-Break">publish applications:</span></p>
<ul>
<li><strong class="bold">ClusterIP</strong>: This is the default type, used to publish a Service internally. An FQDN is created in Kubernetes’ internal DNS (CoreDNS component) associated with the IP address of the Service resource (assigned by the internal IPAM from the <span class="No-Break">Services’ pool).</span></li>
<li><strong class="bold">Headless</strong>: These<a id="_idIndexMarker1223"/> Services don’t have an associated IP address, although they also have an FQDN. In this case, all IP addresses of the Pods associated with the Endpoint resource will <span class="No-Break">be resolved.</span></li>
<li><strong class="bold">NodePort</strong>: This<a id="_idIndexMarker1224"/> Service type allows us to publish an application’s component externally, outside of the cluster network. It will create a NAT association between a port in all the cluster hosts and the port of the Service. When external network packets reach the host’s associated port, the overlay network routes the traffic to the appropriate Pods. Services are a logical resource; they don’t really have a physical device or associated software component. They just <a id="_idIndexMarker1225"/>link the Service’s IP address with the backend endpoints via DNS. The ports used by the NodePort Service resource can be fixed when we create it, or we can let Kubernetes choose a random one for us from the <strong class="source-inline">30000</strong>-<strong class="source-inline">32767</strong> port range. It is important to understand that NodePort Services have a ClusterIP address, associated via the internal <span class="No-Break">Service’s FQDN.</span></li>
<li><strong class="bold">LoadBalancer</strong>: This <a id="_idIndexMarker1226"/>type of Service resource integrates with external cloud or on-premise software or hardware load balancers (or it creates them in your cloud infrastructure) from the underlying infrastructure to route user traffic to an application’s Pods. In this case, a NodePort is created (along with its associated ClusterIP) to route the traffic from the external load balancer to the backend Endpoint resources. Kubernetes will use its own integration with the cloud infrastructure to create the required load balancers or apply specific configurations pointing to the associated NodePorts whenever a LoadBalancer Service resource <span class="No-Break">is created.</span></li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">We employ ClusterIP and Headless Services for internal use and NodePort and LoadBalancer Services whenever we are going to expose our applications. But this is not strictly true, as we can also use <strong class="bold">Ingress Controllers</strong> to publish applications without using either NodePort or LoadBalancer resources. This helps you to abstract your applications from the <span class="No-Break">underlying infrastructure.</span></p>
<p>Let’s <a id="_idIndexMarker1227"/>continue exploring the <a id="_idIndexMarker1228"/>different options provided by the Kubernetes platform for publishing applications by introducing the <strong class="bold">Ingress Controller</strong> concept. An<a id="_idIndexMarker1229"/> Ingress Controller is a Kubernetes controller that we can add to our cluster to implement reverse proxy functionalities. This will allow us to use ClusterIP Service resources to expose our applications because the traffic coming from our users will be routed entirely internally from this proxy component to the Service and then reach the associated Pods. This proxy is configured dynamically by using <strong class="bold">Ingress</strong> resources. These<a id="_idIndexMarker1230"/> resources allow us to define our applications’ host headers and link them to our Service resources. Your work as a developer involves creating appropriate Ingress resources for your frontend <span class="No-Break">application’s components.</span></p>
<p>Finally, let’s<a id="_idIndexMarker1231"/> introduce the Kubernetes <strong class="bold">proxy</strong> publishing feature <a id="_idIndexMarker1232"/>that directly proxifies the cluster API on a defined port. This will allow us to directly access the API and all the resources included in the Kubernetes cluster. For example, we can get all the Pods defined in the default namespace using the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">api/v1/namespaces/default/pods</strong></span><span class="No-Break"> path.</span></p>
<p>For <a id="_idIndexMarker1233"/>debugging purposes, we can also use <strong class="source-inline">kubectl port-forward</strong>, which proxies specific Services to our desktop computer client. Note that neither method, <strong class="source-inline">proxy</strong> or <strong class="source-inline">port-forward</strong>, should be permitted in production because they directly expose important resources, bypassing our Kubernetes and load balancer <span class="No-Break">infrastructure security.</span></p>
<p>In the next section, we will use the <strong class="source-inline">kubectl proxy</strong> feature to access a Service resource and reach <span class="No-Break">our application.</span></p>
<h1 id="_idParaDest-231"><a id="_idTextAnchor248"/>Proxying and forwarding applications for debugging</h1>
<p>In this section, we <a id="_idIndexMarker1234"/>will learn how to publish the Kubernetes API directly on our <a id="_idIndexMarker1235"/>desktop computer and reach any Service created in the cluster (if we have the appropriate permissions), and how to forward a Service directly to our client computer using the <span class="No-Break"><strong class="source-inline">port-forward</strong></span><span class="No-Break"> feature.</span></p>
<h2 id="_idParaDest-232"><a id="_idTextAnchor249"/>Kubernetes client proxy feature</h2>
<p>We use <strong class="source-inline">kubectl proxy</strong> to enable<a id="_idIndexMarker1236"/> the Kubernetes proxy feature. Some important options help us manage how and where the Kubernetes API will be accessible. We use the following options to define where the Kubernetes API will <span class="No-Break">be published:</span></p>
<ul>
<li><strong class="source-inline">--address</strong>: This option allows us to define the IP address of our client host used for publishing the Kubernetes API. By default, <strong class="source-inline">127.0.0.1</strong> <span class="No-Break">is used.</span></li>
<li><strong class="source-inline">--port</strong> or <strong class="source-inline">-p</strong>: This option is used to set the specific port where the Kubernetes API will be available. The default value is <strong class="source-inline">8001</strong>, and although we can let Kubernetes use a random port by using <strong class="source-inline">-p=0</strong>, it is recommended to always define a <span class="No-Break">specific port.</span></li>
<li><strong class="source-inline">--unix-socket</strong> or <strong class="source-inline">-u</strong>: This option is used to define a Unix socket instead of a TCP port, which is more secure if you limit access to the socket at the <span class="No-Break">filesystem level.</span></li>
</ul>
<p>The following <a id="_idIndexMarker1237"/>options are used to secure the Kubernetes <span class="No-Break">API access:</span></p>
<ul>
<li><strong class="source-inline">--accept-hosts</strong> and <strong class="source-inline">--accept-paths</strong>: These options allow us to ensure that only specific host headers and API paths will be allowed. For example, we can ensure local access only using the following regex pattern, <strong class="source-inline">'^localhost$,^127\.0\.0\.1$,^\[::1\]$'</strong>, with the <strong class="source-inline">--</strong><span class="No-Break"><strong class="source-inline">accept-hosts</strong></span><span class="No-Break"> argument.</span></li>
<li><strong class="source-inline">--reject-methods</strong>: We can block specific API methods by rejecting them. For example, we can disable the patching of any Kubernetes resource by using <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">proxy --reject-methods='PATCH'</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">--reject-paths</strong>: We can specify certain paths to be denied by using this option. We can, for example, disable the attachment of a new process to a Pod resource (<strong class="source-inline">kubectl exec</strong> equivalent) by <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">–-reject-paths='^/api/.*/pods/.*/exec,'</strong></span><span class="No-Break">.</span></li>
</ul>
<p>It is important to understand that, although we have seen some options for ensuring security, the Kubernetes proxy feature shouldn’t be used in production environments because it may be possible to bypass the RBAC system if someone gets access to the API via the proxied port. The user authentication used for creating the proxy will allow access to anyone via the <span class="No-Break">exposed API.</span></p>
<p>This method should only be used for debugging in either your own Docker Desktop, Rancher Desktop, or Minikube for exposing a Kubernetes remote development environment. Your Kubernetes administrators must enable this method for you if you are not using your own Kubernetes environment. You must ensure that your operating system allows access to the specified port by reviewing your firewall settings if you still aren’t able to reach the proxied <span class="No-Break">Kubernetes API.</span></p>
<p>Now we have<a id="_idIndexMarker1238"/> reviewed how we can employ this method to publish Kubernetes APIs, let’s use it to access a created Service resource with a <span class="No-Break">quick example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer110">
<img alt="Figure 11.1 – Creating a simple webserver Service with NGINX and exposing the Kubernetes API" height="743" src="image/B19845_11_1.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Creating a simple webserver Service with NGINX and exposing the Kubernetes API</p>
<p class="callout-heading">Important note</p>
<p class="callout">Notice that we executed <strong class="source-inline">kubectl proxy</strong> in the background using <strong class="source-inline">&amp;</strong>. We did this to be able to continue in the current terminal. The <strong class="source-inline">kubectl proxy</strong> action runs in the foreground, and it will keep running until we issue <em class="italic">Ctrl</em> + <em class="italic">C</em> to terminate the process. To end the background execution, we can use the <span class="No-Break">following steps:</span></p>
<p class="callout"><strong class="source-inline">$ </strong><span class="No-Break"><strong class="source-inline">jobs</strong></span></p>
<p class="callout"><strong class="source-inline">[1]+  Running                 kubectl </strong><span class="No-Break"><strong class="source-inline">proxy &amp;</strong></span></p>
<p class="callout"><strong class="source-inline">$ </strong><span class="No-Break"><strong class="source-inline">kill %1</strong></span></p>
<p>Now that we have access to Kubernetes API, we can access the ClusterIP Service resource directly using the proxied port, but first, let’s review the <span class="No-Break">Service resource:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<img alt="Figure 11.2 – Accessing the webserver Service resource using the Kubernetes proxy" height="504" src="image/B19845_11_2.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Accessing the webserver Service resource using the Kubernetes proxy</p>
<p>We <a id="_idIndexMarker1239"/>configured port <strong class="source-inline">8080</strong> for the <strong class="source-inline">webserver</strong> Service resource. The Kubernetes proxy will publish the Service resources using the following URI format (<span class="No-Break">Kubernetes API):</span></p>
<p><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">api/v1/namespaces/&lt;NAMESPACE&gt;/services/&lt;SERVICE_NAME&gt;:&lt;SERVICE_PORT&gt;/proxy/</strong></span></p>
<p>Therefore, the <strong class="source-inline">webserver</strong> Service is accessible in <strong class="source-inline">/api/v1/namespaces/default/services/webserver:8080/proxy/</strong>, and we can reach NGINX’s default <strong class="source-inline">index.xhtml</strong> page, as we can see in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<img alt="Figure 11.3 – Accessing the webserver Service using the kubectl proxy feature" height="834" src="image/B19845_11_3.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Accessing the webserver Service using the kubectl proxy feature</p>
<p>The Service is accessible and we reached the <strong class="source-inline">webserver</strong> Service’s default page. Let’s now review how we can forward the Service’s port to our desktop computer without having to implement a complex <span class="No-Break">routing infrastructure.</span></p>
<h2 id="_idParaDest-233"><a id="_idTextAnchor250"/>Kubernetes client port-forward feature</h2>
<p>Instead of <a id="_idIndexMarker1240"/>accessing the full Kubernetes API, we can use <strong class="source-inline">kubectl port-forward</strong> to forward ports from a Service, Deployment, ReplicaSet, StatefulSet, or even a Pod resource directly. In this case, a transparent NAT is used to forward a backend port to a port defined on our desktop computer by executing the <strong class="source-inline">kubectl</strong> <span class="No-Break">command-line client.</span></p>
<p>Let’s see how this works, using the <strong class="source-inline">webserver</strong> Service defined in the previous section as <span class="No-Break">an example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer113">
<img alt="Figure 11.4 – Using port-forward to publish the webserver Service resource example" height="813" src="image/B19845_11_4.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Using port-forward to publish the webserver Service resource example</p>
<p>As you can see in this example, it is quite simple to forward any application’s Kubernetes resource listening on a defined port. We can specify the port attached to our application  in our local client by using <strong class="source-inline">[LOCAL_PORT:]RESOURCE_PORT</strong>. Note that it is important to choose the local IP address when working on a multihomed host with multiple IP addresses using the <strong class="source-inline">--address</strong> argument. This will improve the overall security by attaching an interface if we define the appropriate firewall rules to only allow our host. By default, <strong class="source-inline">localhost</strong> is used, which means that it will remain secure as long as we are the <a id="_idIndexMarker1241"/>only user with access to our <span class="No-Break">desktop computer.</span></p>
<p>In the next section, we will discuss the direct use of the host’s kernel network namespace for publishing <span class="No-Break">Pod resources.</span></p>
<h1 id="_idParaDest-234"><a id="_idTextAnchor251"/>Using the host network namespace for publishing applications</h1>
<p>So far, we <a id="_idIndexMarker1242"/>have seen different methods for accessing ClusterIP Service resources or Pods (created using different workload types) by <a id="_idIndexMarker1243"/>either proxying or forwarding their ports to our desktop computers. Sometimes, however, the applications require a direct connection to the host’s interfaces, without the bridge interface created by the container runtime. In this case, the containers in the Pod will use the network namespace of the host, which allows the processes inside to control the host because they will have access to all the host’s interfaces and network traffic. This can be dangerous and must only be used to manage and monitor the <span class="No-Break">host’s interfaces.</span></p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor252"/>Using the hostNetwork key</h2>
<p>To use the<a id="_idIndexMarker1244"/> host’s network namespace, we set the <strong class="source-inline">hostNetwork</strong> key to <strong class="source-inline">true</strong>. The Pod will now get all the IP addresses associated with the host, including those of all the virtual interfaces associated with the containers running in that host. But what is particularly important in terms of publishing our applications is that they will be accessible through any of the host’s IP addresses, waiting for requests on the ports defined by the <strong class="source-inline">ports</strong> keys in the Pod <strong class="source-inline">spec</strong> section. Let’s see how this works by executing an NGINX Pod with the aforementioned <strong class="source-inline">hostNetwork</strong> key. We will use <strong class="source-inline">cat</strong> (redirected to <strong class="source-inline">kubectl</strong>) to create a Pod resource on the fly using the <strong class="source-inline">nginx/nginx-unprivileged:stable-alpine3.18</strong> image (which uses the unprivileged <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">8080</strong></span><span class="No-Break">):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<img alt="Figure 11.5 – Exposing a Pod using hostNetwork" height="862" src="image/B19845_11_5.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Exposing a Pod using hostNetwork</p>
<p>This <a id="_idIndexMarker1245"/>way, your NGINX web server will be accessible in the host’s IP address where it is running (in this example, on the IP address <strong class="source-inline">192.168.65.4</strong>, which is the address of our Docker Desktop worker and master host). The following code snippet shows the creation of the <strong class="source-inline">webserver</strong> application using the host’s interfaces, and how we get the content of the <span class="No-Break">NGINX process:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="Figure 11.6 – Accessing the webserver application using the host’s IP address" height="635" src="image/B19845_11_6.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Accessing the webserver application using the host’s IP address</p>
<p>Notice that we executed the <strong class="source-inline">curl</strong> binary inside the <strong class="source-inline">webserver</strong> Pod. In this example, we are using <a id="_idIndexMarker1246"/>Docker Desktop with <strong class="bold">Windows Subsystem for Linux 2</strong> (<strong class="bold">WSL 2</strong>), which doesn’t have direct routing from another WSL client; but the important thing here is the accessibility to the NGINX process using the host’s IP address. We can use another Pod, running the <strong class="source-inline">frjaraur/nettools</strong> image (developed and maintained by me), to verify that the application <span class="No-Break">is accessible.</span></p>
<p>In this case, we<a id="_idIndexMarker1247"/> are using just one port on our Pod; in fact, we didn’t even declare the ports on our Pod’s container, hence all the ports defined in the container image will be used. Using <strong class="source-inline">hostNetwork</strong>, all the ports defined in the image will be exposed, which may be a problem if you don’t want to expose some specific Pods externally (for example, if your application has an internal API or administration interface that you will not be able to access). If you manage the platform yourself, you can manage access by modifying the host’s firewall, but this can be tricky. In such situations, we can use the <strong class="source-inline">hostPort</strong> key at container level, instead of using <strong class="source-inline">hostNetwork</strong> at the Pod resource level. Let’s explore this in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-236"><a id="_idTextAnchor253"/>Using hostPort</h2>
<p>The <strong class="source-inline">hostPort</strong> key is <a id="_idIndexMarker1248"/>used inside the <strong class="source-inline">containers</strong> section of the Pod, where we define the ports to be exposed either internally or externally. With <strong class="source-inline">hostPort</strong>, we can expose only those ports that are required, while the remainder can stay internal. Let’s see an example involving defining two containers within the <span class="No-Break"><strong class="source-inline">webserver</strong></span><span class="No-Break"> Pod:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 11.7 – Example with two containers but only one exposed at the host level" height="847" src="image/B19845_11_7.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Example with two containers but only one exposed at the host level</p>
<p>In the <a id="_idIndexMarker1249"/>preceding screenshot, we have two containers. Let’s verify whether they are reachable using the <strong class="source-inline">frjaraur/nettools</strong> image again, trying to access both ports <span class="No-Break">via </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<img alt="Figure 11.8 – Access to the webserver Service’s ports 8080 and 80" height="938" src="image/B19845_11_8.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Access to the webserver Service’s ports 8080 and 80</p>
<p>In the <a id="_idIndexMarker1250"/>preceding screenshot, we can see that only port <strong class="source-inline">8080</strong> is accessible on the host’s IP address. Port <strong class="source-inline">80</strong> is accessible locally within the <span class="No-Break">Kubernetes cluster.</span></p>
<p>Neither <strong class="source-inline">hostNetwork</strong> nor <strong class="source-inline">hostPort</strong> should be used without a Kubernetes administrator’s supervision. Both represent a security breach and should be avoided unless strictly necessary for our application. They are commonly used for monitoring or administrative workloads when we need to manage or monitor the hosts’ <span class="No-Break">IP addresses.</span></p>
<p>Now that we have learned the different options we have at the host level, let’s continue reviewing the NodePort mechanism associated with <span class="No-Break">Service resources.</span></p>
<h1 id="_idParaDest-237"><a id="_idTextAnchor254"/>Publishing applications with Kubernetes’ NodePort feature</h1>
<p>As we<a id="_idIndexMarker1251"/> mentioned at the beginning<a id="_idIndexMarker1252"/> of this chapter, in the <em class="italic">Understanding the Kubernetes features for publishing applications cluster-wide</em> section, every NodePort Service resource has an associated ClusterIP IP address. This IP address is used to internally load balance all the client requests (from the Kubernetes cluster, internal, and external clients). Kubernetes provides this internal load to all available Pod replicas. All replicas will have the same weight, hence <a id="_idIndexMarker1253"/>they will receive the same amount <a id="_idIndexMarker1254"/>of requests. The ClusterIP IP address makes the applications running within Pods accessible internally. To make them available externally, the NodePort Service type attaches the defined port on all cluster nodes using NAT. The following schema represents the route taken by a request to an application running inside a <span class="No-Break">Kubernetes cluster:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<img alt="Figure 11.9 – NodePort simplified communications schema" height="403" src="image/B19845_11_9.jpg" width="754"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – NodePort simplified communications schema</p>
<p>The Endpoint resource is used to map the Pods’ backends with the Service’s ClusterIP. This resource is dynamically configured using label selectors in the Service’s YAML manifest. Here is a <span class="No-Break">simple example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="Figure 11.10 – Simple Pod and NodePort YAML manifests" height="611" src="image/B19845_11_10.jpg" width="1022"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Simple Pod and NodePort YAML manifests</p>
<p>The <a id="_idIndexMarker1255"/>preceding screenshot shows the most common Service resource usage. With this manifest, an EndpointSlice<a id="_idIndexMarker1256"/> resource will be created, associating the application’s Pods with the Service by using the labels defined in the selector section. Notice that using these label selectors will create these EndpointSlice resources pointing to backend Pod resources running in the same namespace. But we can create Service resources without dynamic Pods attachment. This scenario could be useful, for example, to link external Services running outside of Kubernetes with an internal Service resource (this is how the <strong class="source-inline">ExternalName</strong> Service resource type works), or to access a Service from another namespace as if it were deployed on your current namespace. The internal Pods are made accessible thanks to the kube-proxy component, which will inject the traffic to the Pod’s containers. This only happens in those nodes where the actual Pods are running, although the Service is <span class="No-Break">accessible cluster-wide.</span></p>
<p>EndpointSlice resources using label selectors will create Endpoint resources, and thus, their status updates are propagated. Failed Pod resources will be deprecated from the actual Service and requests will not be routed to those backends, hence Kubernetes will only route to healthy Pods. This is the most popular and recommended method for using Service resources because this way, your resources are <span class="No-Break">infrastructure agnostic.</span></p>
<p>Let’s see a quick <a id="_idIndexMarker1257"/>example of how Endpoint resource creation works by creating a <strong class="source-inline">webserver</strong> Pod and publishing the web process in NodePort mode by using <span class="No-Break"><strong class="source-inline">kubectl expose</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 11.11 – Exposing a Pod using the imperative format" height="643" src="image/B19845_11_11.jpg" width="1596"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Exposing a Pod using the imperative format</p>
<p>In the preceding <a id="_idIndexMarker1258"/>example, we created a Pod and then exposed it by <a id="_idIndexMarker1259"/>using <strong class="bold">imperative mode</strong>. We didn’t specify a NodePort port, hence Kubernetes assigned one from the <strong class="source-inline">30000</strong>-<strong class="source-inline">32767</strong> range to the port of the Service resource. We also retrieved a list of endpoints and the dynamic configurations created. We used the <strong class="source-inline">kubectl expose &lt;WORKLOAD_TYPE&gt; &lt;WORKLOAD_NAME&gt;</strong> format syntax to create the Service. This uses label selectors for the creation of the Service resource, taking the labels from the actual workload, hence an EndpointSlice resource was created to attach the available Pods to <span class="No-Break">the Service.</span></p>
<p>In this example, the <strong class="source-inline">webserver</strong> application will be accessible using the <strong class="source-inline">docker-desktop</strong> node IP address, which may require additional configuration if you use WSL2 for execution. This is because in this infrastructure we will need to declare an NAT IP address to forward to your desktop computer. This will not be required if Hyper-V or Minikube are used as a Kubernetes environment on your PC. In a remote Kubernetes cluster, you must ensure that the IP addresses of the hosts and the ports are reachable from <span class="No-Break">your computer.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Because the NodePort Services use the host’s ports, these ports must be allowed in each node’s firewall. Your Kubernetes administrator may have configured multiple interfaces on your Kubernetes platform nodes and should inform you about which IP addresses to use to make your <span class="No-Break">applications accessible.</span></p>
<p>If your <a id="_idIndexMarker1260"/>workloads run on a cloud<a id="_idIndexMarker1261"/> infrastructure, additional steps may be required to allow access to your Service resources, and thus this is often not a good option for publishing <span class="No-Break">your applications.</span></p>
<p>In the next section, we will review the LoadBalancer Service type, which was created specifically for cloud environments but is now also available for on-premises infrastructure thanks to software load balancers such <span class="No-Break">as MetalLB.</span></p>
<h1 id="_idParaDest-238"><a id="_idTextAnchor255"/>Providing access to your Services with LoadBalancer Services</h1>
<p>A <a id="_idIndexMarker1262"/>LoadBalancer-type Service requires an external device to integrate your application. This type of <a id="_idIndexMarker1263"/>Service resource includes a NodePort and its ClusterIP IP address. The external device provides a LoadBalancer IP address that will be load-balanced to the IP addresses of the cluster nodes and associated NodePorts. This configuration is completely managed for you by Kubernetes, but you must define an appropriate <strong class="source-inline">spec</strong> section for your infrastructure. This type of resource depends on the actual infrastructure because it will use the APIs from software-defined networking infrastructure to route and publish the applications’ Services. Loadbalancer Service resources were prepared primarily for Kubernetes cloud platforms but are now more commonly encountered in modern local data centers with software-defined networks and API-managed devices, although they require a good knowledge of the underlying platform to work. As mentioned before, each LoadBalancer Service resource is assigned an IP address dynamically, which may require additional management on your cloud infrastructure and even <span class="No-Break">additional costs.</span></p>
<p>The cloud provider decides how the Service is to be load-balanced. Depending on the cloud platform used, the NodePort part can sometimes be omitted as direct routing may be available if defined by the <span class="No-Break">platform vendor.</span></p>
<p>On-premises<a id="_idIndexMarker1264"/> virtual cloud infrastructures such as OpenStack can be integrated into our Kubernetes platforms to manage this type of Service resource because they are also part of the Kubernetes core. But if you are not using OpenStack or any other on-premise virtual cloud infrastructure, there are solutions such as MetalLB (<a href="https://metallb.org/">https://metallb.org/</a>) that make it possible to run a Kubernetes-compatible and dynamically configurable load balancer on any <span class="No-Break">bare-metal infrastructure.</span></p>
<p>This <a id="_idIndexMarker1265"/>type of Service resource is not recommended if you are looking for maximum compatibility and want to avoid vendor-specific resources. It really has a lot of dependencies on the underlying infrastructure and may require additional configurations to be done on <span class="No-Break">the platform.</span></p>
<p>If you as a developer have to implement a Service of type LoadBalancer (or you’re simply curious about their definition), you can use Minikube as it implements this functionality on your desktop computer without any external requirements to negotiate. Docker Desktop will report the LoadBalancer IP address as <strong class="source-inline">localhost</strong>, hence you will be able to connect to the given Services directly using the <strong class="source-inline">127.0.0.1</strong> <span class="No-Break">IP address.</span></p>
<p>Let’s see how this works with a simple example. We will first start a new Minikube cluster environment (ensure your Docker Desktop or Rancher Desktop instances are stopped before starting Minikube), and then we will <a id="_idIndexMarker1266"/>create a <strong class="bold">Minikube tunnel</strong>, which is a Minikube feature that will create a tunnel between your host and the Minikube node. We will use an administrator console for executing the <strong class="source-inline">minikube start</strong> and <strong class="source-inline">minikube </strong><span class="No-Break"><strong class="source-inline">tunnel</strong></span><span class="No-Break"> commands:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 11.12 – Execution of a Minikube cluster from an administrator console" height="810" src="image/B19845_11_12.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Execution of a Minikube cluster from an administrator console</p>
<p>We will<a id="_idIndexMarker1267"/> open another console, but <a id="_idIndexMarker1268"/>this time we will connect to the Kubernetes cluster, so we don’t need to execute the commands as an administrator. We create a Pod and then expose it using imperative mode with the <span class="No-Break">LoadBalancer type:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 11.13 – Creating a LoadBalancer Service in Minikube" height="940" src="image/B19845_11_13.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Creating a LoadBalancer Service in Minikube</p>
<p>Notice <a id="_idIndexMarker1269"/>that we have a new column with the external IP addresses. In this case, it is an emulation of a real <a id="_idIndexMarker1270"/>external load balancer device that provides a specific IP address for the new Service manifest. Minikube, in fact, creates a tunnel from the Kubernetes node to your desktop computer, making the Pod accessible over the assigned load-balanced IP address <strong class="source-inline">(EXTERNAL-IP)</strong> and the Service’s port. In this case, we will reach the NGINX web server at <strong class="source-inline">http://10.98.19.87:8080</strong>. We then test the accessibility of the application with <strong class="source-inline">curl</strong> (which is an alias for Windows PowerShell’s <span class="No-Break"><strong class="source-inline">Invoke-WebRequest</strong></span><span class="No-Break"> command).</span></p>
<p>The dependency of the LoadBalancer Service type on the platform infrastructure makes this type too specific for day-to-day usage and may not be available in all Kubernetes clusters. Therefore, the best solution for compatibility is to use Ingress Controllers, as we will learn in the <span class="No-Break">following section.</span></p>
<h1 id="_idParaDest-239"><a id="_idTextAnchor256"/>Understanding Ingress Controllers</h1>
<p>An <strong class="bold">Ingress Controller</strong> is a <a id="_idIndexMarker1271"/>piece of software that provides load balancing, SSL termination, and host-based virtual routing. It is a reverse proxy that runs in the Kubernetes cluster, which manages a reverse proxy network component that can run inside the Kubernetes cluster or externally, just like any other network infrastructure device. An Ingress Controller acts just like any other controller deployed in a Kubernetes cluster, although it is not managed by the cluster itself. We must deploy this controller manually as it is not part of the Kubernetes core. If required, we can deploy multiple Ingress Controllers in a cluster and define which one is to be used by default if none <span class="No-Break">is specified.</span></p>
<p>Ingress Controllers work very well with HTTP/HTTPS applications (OSI Layer 7, the application layer), but we can publish TCP and UDP applications too (OSI Layer 4, the transport layer), although this does require more configuration and may not be the best option. In such cases, it may be better to use an external load balancer and route traffic to NodePort Service resources because TCP and UDP Ingress resources will need additional ports to distribute <span class="No-Break">incoming traffic.</span></p>
<p>Kubernetes administrators<a id="_idIndexMarker1272"/> use <strong class="bold">IngressClass resources</strong> to declare the different Ingress Controllers available on a platform. Each Ingress Controller is associated with an IngressClass resource. You as the developer must create Ingress resources, which are the definitions required for reverse-proxying your <span class="No-Break">application’s workloads.</span></p>
<p>There are multiple <a id="_idIndexMarker1273"/>options for deploying an Ingress Controller: cloud providers and many software vendors have developed their own solutions, and you can include any of them in your own Kubernetes setup, but you must understand their specific features and particularities. You can review the available solutions in the Kubernetes documentation at <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers</a> and ask your Kubernetes administrator about the Ingress Controllers available on your platform before preparing your applications. Small tweaks may be necessary on your side in your Ingress resources. In the following section, we will examine the <a id="_idIndexMarker1274"/>most frequently encountered option, the <strong class="bold">Kubernetes NGINX Ingress Controller</strong>, included by default in some <span class="No-Break">Kubernetes solutions.</span></p>
<h2 id="_idParaDest-240"><a id="_idTextAnchor257"/>Deploying an Ingress Controller</h2>
<p>To deploy<a id="_idIndexMarker1275"/> an Ingress Controller, we simply follow the specific instructions for the chosen solution. There may be different approaches for installing the given software in your cluster, but we will follow the easiest one: deploying a YAML file containing all the required resources in one file (<a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml">https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml</a>). Make sure to check the latest available release before using the URL just provided and consult the specific instructions provided for it. At the time of writing this book, NGINX Controller release <strong class="source-inline">1.8.1</strong> was the latest release available. In this example, we use the cloud YAML file, although you can use the bare-metal option if you have a fully functional Kubernetes environment installed (this version uses NodePort instead of the LoadBalancer type). Let’s work through our <span class="No-Break">simple example:</span></p>
<ol>
<li>We start by running <strong class="source-inline">kubectl apply</strong> on a Docker <span class="No-Break">Desktop environment:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 11.14 – Deployment of the popular Kubernetes NGINX Ingress Controller" height="614" src="image/B19845_11_14.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Deployment of the popular Kubernetes NGINX Ingress Controller</p>
<p class="list-inset">As <a id="_idIndexMarker1276"/>you can see in the previous screenshot, many resources are created for the Ingress Controller to work. A new namespace was created, <strong class="source-inline">ingress-nginx</strong>, and some Pods are now <span class="No-Break">running there:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 11.15 – Deployment, Pods, and IngressClass resources created" height="457" src="image/B19845_11_15.jpg" width="1459"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – Deployment, Pods, and IngressClass resources created</p>
<p class="list-inset">In the preceding screenshot, we can see an <strong class="source-inline">IngressClass</strong> resource created. We may need to configure it <span class="No-Break">as default.</span></p>
<ol>
<li value="2">Let’s check the deployed Ingress Controller. We first check the Service resource created to reach the Deployment resource using <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">get svc</strong></span><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="Figure 11.16 – Verification of the deployed Ingress Controller" height="780" src="image/B19845_11_16.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Verification of the deployed Ingress Controller</p>
<p class="list-inset">Note in<a id="_idIndexMarker1277"/> the preceding screenshot how the Service resource was created as a <strong class="source-inline">LoadBalancer</strong> type. It acquired the <strong class="source-inline">localhost</strong> IP address (we are using Docker Desktop in this example), which means that we should be able to reach the NGINX Ingress Controller backend directly with <strong class="source-inline">curl</strong> using <strong class="source-inline">localhost</strong>. The Service is listening on ports <strong class="source-inline">80</strong> and <strong class="source-inline">443</strong>, and we were able to reach both (we passed the <strong class="source-inline">-k</strong> argument to <strong class="source-inline">curl</strong> to avoid having to verify the associated auto-signed and untrusted <span class="No-Break">SSL certificate).</span></p>
<p>The use of Ingress Controllers improves security when we add SSL certificates to implement SSL tunnels between our applications’ exposed components and our users, or even between different components that use the Ingress URL associated with the <span class="No-Break">Service resource.</span></p>
<p>Let’s go ahead now and learn how to manage the behavior of our applications using <span class="No-Break">Ingress resources.</span></p>
<h2 id="_idParaDest-241"><a id="_idTextAnchor258"/>Ingress resources</h2>
<p>As with <a id="_idIndexMarker1278"/>any other resource, we need to define <strong class="source-inline">apiVersion</strong>, <strong class="source-inline">kind</strong>, <strong class="source-inline">metadata</strong>, and <strong class="source-inline">spec</strong> keys and sections. The most important section is <strong class="source-inline">.spec.rules</strong>, which defines a list of host rules that dynamically configure the reverse proxy deployed by the Ingress Controller. Let’s see a <span class="No-Break">basic example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 11.17 – Ingress resource example" height="589" src="image/B19845_11_17.jpg" width="818"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Ingress resource example</p>
<p>In the<a id="_idIndexMarker1279"/> preceding screenshot, we can see the <strong class="source-inline">ingressClassName</strong> key, which indicates the Ingress Controller to be used. The <strong class="source-inline">rules</strong> section defines a list of host headers and the paths associated with the different backends. In our example, the <a href="http://www.webserver.com">www.webserver.com</a> host header is required; if requests do not include it, they will be redirected to the default backend (if defined) or be shown a <strong class="source-inline">404</strong> error (page not found). The <strong class="source-inline">backend</strong> section describes the Service resource that will receive the <span class="No-Break">application’s requests.</span></p>
<p>Let’s run a quick example using the <strong class="source-inline">webserver</strong> Service resource created in the previous section. It will listen on port <strong class="source-inline">8080</strong>, hence we create an Ingress resource with a fake hostname and validate its accessibility with <strong class="source-inline">curl -H "host: &lt;FAKE_HOST&gt;" http://localhost</strong> (we use <strong class="source-inline">localhost</strong> because its IP address is the one associated with the <span class="No-Break"><strong class="source-inline">LoadBalancer</strong></span><span class="No-Break"> Service):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 11.18 – Ingress webserver resource for the webserver Service example" height="1030" src="image/B19845_11_18.jpg" width="1626"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – Ingress webserver resource for the webserver Service example</p>
<p>Security <a id="_idIndexMarker1280"/>features are implemented in the <strong class="source-inline">.spec.tls</strong> section, where we link the hosts with their keys and certificates, integrated into a Secret resource. This Secret must be included in the namespace in which you defined the Ingress resource, and it is of the <strong class="source-inline">tls</strong> type. The <strong class="source-inline">data</strong> sections in these Secrets must include the key for the generation of the certificate along with the generated certificate itself. We will learn how to create this via an example in the <span class="No-Break"><em class="italic">Labs</em></span><span class="No-Break"> section.</span></p>
<p>We can have an Ingress resource with rules for multiple hosts and each host with multiple paths, although it is more common to separate each host on a different Ingress resource for easier management and include multiple paths for reaching different backend Service resources. This combination represents a typical microservice architecture where each application functionality is provided by different <span class="No-Break">backend Services.</span></p>
<p>The <strong class="source-inline">annotations</strong> section can be used to instruct the Ingress Controller regarding special configurations. Here is a list of some of the most important configurations we can manage via annotations for the Kubernetes NGINX <span class="No-Break">Ingress Controller:</span></p>
<ul>
<li><strong class="source-inline">nginx.ingress.kubernetes.io/rewrite-target</strong>: It is usual to integrate some rewrite rules in our Ingress resource for rewriting the application’s URI paths. There are also options for <span class="No-Break">redirecting URLs.</span></li>
<li><strong class="source-inline">nginx.ingress.kubernetes.io/auth-type</strong> and <strong class="source-inline">nginx.ingress.kubernetes.io/auth-secret</strong>: These will allow us to use basic authentication at the Ingress level for <span class="No-Break">our applications.</span></li>
<li><strong class="source-inline">nginx.ingress.kubernetes.io/proxy-ssl-verify</strong>: If our Service resource backends use TLS, there are many annotations available to manage how NGINX connects <span class="No-Break">to them.</span></li>
<li><strong class="source-inline">nginx.ingress.kubernetes.io/enable-cors</strong>: We may need to enable <strong class="bold">Cross-Origin Resource Sharing</strong> (<strong class="bold">CORS</strong>) in our application to allow some external<a id="_idIndexMarker1281"/> routes and URLs. There are also other interesting options here for managing and securing <span class="No-Break">CORS behavior.</span></li>
<li><strong class="source-inline">nginx.ingress.kubernetes.io/client-body-buffer-size</strong>: It’s quite common to limit the size of client requests to avoid overall performance issues, but your application may require <span class="No-Break">larger responses.</span></li>
</ul>
<p>There are many <a id="_idIndexMarker1282"/>options available beyond these, and you may need to ask your Kubernetes and infrastructure administrators for advice. The range on offer includes integrating external authentication backends, limiting the rate of requests to <a id="_idIndexMarker1283"/>avoid <strong class="bold">distributed denial-of-service</strong> (<strong class="bold">DDoS</strong>) attacks, redirecting and rewriting URLs, enabling SSL passthrough, and even managing canary application deployments, routing some requests to a newer release of your workload backends. Some of the options can be defined at the Ingress Controller level, which will affect all Ingress resources at once. For a full list of available annotations, please refer to the following <span class="No-Break">page: </span><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/"><span class="No-Break">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/</span></a><span class="No-Break">.</span></p>
<p>It is very important to understand that the options mentioned here may differ from those available for other Ingress Controllers (at least, they will use other annotation keys, for sure). Some Ingress Controllers such as Kong also implement API management for your backend Services, which can be very useful if they are involved in many interactions. Ask your Kubernetes administrators about the Ingress Controllers deployed on <span class="No-Break">your platform.</span></p>
<p>We covered the <a id="_idIndexMarker1284"/>basics here, but as always, note that your Ingress resource may need some small tweaks to fully implement your platform requirements. In OpenShift, for example, Ingress Controllers can be enabled, but by default, Kubernetes will use OpenShift Route, which is the Red Hat implementation of a L7 reverse proxy for publishing applications in Kubernetes. Ingress Controllers and OpenShift Route are quite similar (even their resources look alike) but you should review further specific information about it if your application needs to run on an OpenShift cluster. The following link may help you decide which one to use if both implementations are available for your <span class="No-Break">application: </span><a href="https://cloud.redhat.com/blog/kubernetes-ingress-vs-openshift-route"><span class="No-Break">https://cloud.redhat.com/blog/kubernetes-ingress-vs-openshift-route</span></a><span class="No-Break">.</span></p>
<p>By default, Kubernetes implements a flat network, without any access boundaries between applications. This applies no restrictions to any lateral movement (East-West traffic), a configuration that could cause critical security issues. In the next section, we will review some security improvements to help us publish our <span class="No-Break">applications securely.</span></p>
<h1 id="_idParaDest-242"><a id="_idTextAnchor259"/>Improving our applications’ security</h1>
<p>In Kubernetes, application<a id="_idIndexMarker1285"/> traffic flows freely by default. A flat network is deployed to cover Pod-to-Pod and Service-to-Pod communications – remember that containers within a Pod have a common, shared IP address. Pods running within a Kubernetes cluster will see each other, and it will require some extra work to protect one application from another, even if they run on different nodes and in <span class="No-Break">different namespaces.</span></p>
<p>It may be strange to hear, but applications running in different namespaces can see each other. In fact, if they have an associated Service resource, it would be easy to use the internal DNS to resolve its associated IP address and access <span class="No-Break">its processes.</span></p>
<p>In the next section, we will learn how NetworkPolicy resources can be used to define our applications’ communications and have Kubernetes block any unwanted connectivity <span class="No-Break">for us.</span></p>
<h2 id="_idParaDest-243"><a id="_idTextAnchor260"/>Network policies</h2>
<p>NetworkPolicy<a id="_idIndexMarker1286"/> resources (also referred to as <strong class="source-inline">netpol</strong>) allow us <a id="_idIndexMarker1287"/>to manage OSI Layer 3 and 4 communications (IP and port access, respectively). Kubernetes provides the NetworkPolicy resource as part of its core, but its implementation depends on the CNI deployed in your cluster. Therefore, it is vital to use a CNI (such as Calico, Canal, or Cilium, among others) that implements <span class="No-Break">this feature.</span></p>
<p>NetworkPolicy resources define all aspects of Pod communications: egress (output traffic) and ingress (input traffic). As we will see in a few moments, NetworkPolicies are applied to specific sets of Pods by using the <strong class="source-inline">.spec.podSelector</strong> section. NetworkPolicy resources are namespaced, hence <strong class="source-inline">podSelector</strong> allows us to decide which Pods are to be affected by our rule definitions. Multiple rules can be applied to a Pod, and your Kubernetes administrators may have included some <strong class="source-inline">GlobalNetworkPolicy</strong> resources that affect the entire cluster, thus you should inquire whether any cluster default rules require allowing some egress or ingress traffic. It is quite common to allow only DNS traffic by default, disallowing all additional egress traffic. If this is the case in your cluster, you will need to declare all egress (as well as ingress) communications in your applications’ manifests. Let’s see a quick example of a NetworkPolicy in which we declare both <a id="_idIndexMarker1288"/>ingress and <span class="No-Break">egress communications:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 11.19 – NetworkPolicy resource manifest with both egress and ingress rules" height="586" src="image/B19845_11_19.jpg" width="938"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – NetworkPolicy resource manifest with both egress and ingress rules</p>
<p>Let’s<a id="_idIndexMarker1289"/> review some of the most important keys and sections available<a id="_idIndexMarker1290"/> in the preceding code snippet. The <strong class="source-inline">.spec.podSelector</strong> section declares which Pods in the current namespace (if none are declared under the <strong class="source-inline">metadata</strong> section) will be affected by this policy. Under the <strong class="source-inline">policyTypes</strong> key, we can see a list of policy types defined. We should clarify here that egress communications are those initiated from a Pod, while ingress communications are those that go into the Pod. If you declare both types, egress and ingress, and then only declare a section for one of them (either an <strong class="source-inline">egress</strong> or <strong class="source-inline">ingress</strong> section, as in the preceding example), the omitted one is declared as empty, meaning that that type of communication will not be allowed <em class="italic">at all</em>. The <strong class="source-inline">egress</strong> section in this example is a list of rules to be applied. Let’s have a closer look <span class="No-Break">at this:</span></p>
<ul>
<li>The first rule allows egress communications from selected Pods (those with the <strong class="source-inline">app=myapp</strong> label) to port <strong class="source-inline">5978</strong> on any host in the <span class="No-Break"><strong class="source-inline">10.0.0.0/24</strong></span><span class="No-Break"> subnet</span></li>
<li>The second rule allows egress communications to UDP port <strong class="source-inline">53</strong> on any host (Kubernetes internal and <span class="No-Break">external DNS)</span></li>
</ul>
<p>In the <strong class="source-inline">ingress</strong> section, two rules are <span class="No-Break">also declared:</span></p>
<ul>
<li>The first rule allows access to port <strong class="source-inline">6379</strong> on the selected Pods (those containing the <strong class="source-inline">app=myapp</strong> label) for any communication coming from the <strong class="source-inline">172.17.0.0/16</strong> subnet (except those hosted on the <strong class="source-inline">172.17.1.0/24</strong> subnet), from Pods running in namespaces with the <strong class="source-inline">project=myproject</strong> label, and from Pods in the current namespace with the <strong class="source-inline">role=frontend</strong> label. We can say that <em class="italic">Kubernetes is all </em><span class="No-Break"><em class="italic">about labels</em></span><span class="No-Break">.</span></li>
<li>The second ingress rule allows access to selected Pods on port <strong class="source-inline">80</strong> from hosts on the <span class="No-Break"><strong class="source-inline">192.168.200.0/24</strong></span><span class="No-Break"> subnet.</span></li>
</ul>
<p>These rules may <a id="_idIndexMarker1291"/>seem complex but are quite easy to implement <span class="No-Break">in practice.</span></p>
<p>If you are <a id="_idIndexMarker1292"/>planning to deploy all your application’s components in a specific namespace, it could be worthwhile to allow all egress and ingress communications between your Pods. This isn’t a good idea for production because only attackers’ lateral movements to other namespaces will be blocked, but a security issue in one of your Pods could affect others in the same namespace. While preparing your NetworkPolicy resources or debugging your application, allowing all namespace East-West traffic may also be necessary. The following YAML manifests allow all internal communication and expose only the frontend <span class="No-Break">component externally:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 11.20 – Example manifests for allowing all namespaced communications as well as access to port 80 on a specific Pod" height="437" src="image/B19845_11_20.jpg" width="938"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Example manifests for allowing all namespaced communications as well as access to port 80 on a specific Pod</p>
<p>The preceding screenshot shows <span class="No-Break">two manifests:</span></p>
<ul>
<li>The one on the left declares a NetworkPolicy resource that allows all communications between all Pods deployed in the current namespace. The rule applies to all Pods in the namespace because <strong class="source-inline">podSelector</strong> <span class="No-Break">is empty.</span></li>
<li>The manifest on the right allows access to the Pod with the <strong class="source-inline">appcomponent=frontend</strong> label (<strong class="source-inline">podSelector</strong> applies on this Pod) on port <strong class="source-inline">80</strong>, but only from hosts on the <span class="No-Break"><strong class="source-inline">192.168.200.0/24</strong></span><span class="No-Break"> subnet.</span></li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">NetworkPolicy resources apply at the connection level and don’t leave any connectivity traces by default, which may be inconvenient when trying to fix some connectivity issues between components. Some CNI plugins such as Calico enable you to log connections between Pods. This requires additional permissions on your Kubernetes environment. Ask your Kubernetes administrators if they can provide some connectivity traces if required for debugging your applications. In some cases, it is best to start with a NetworkPolicy that allows and logs all the connections made in <span class="No-Break">the namespace.</span></p>
<p>You as a <a id="_idIndexMarker1293"/>developer are responsible for creating and maintaining your <a id="_idIndexMarker1294"/>application’s resource manifests and thus, the NetworkPolicy resources required by your application. It is up to you how to organize them, but it’s recommended to use descriptive names and group multiple rules in a manifest per each application component. This way, you will be able to fine-tune each component’s configuration. In the <em class="italic">Labs</em> section, we have prepared for you a specific exercise where you will protect an application by allowing only <span class="No-Break">trusted access.</span></p>
<p>NetworkPolicies allow you to thoroughly isolate all your application’s components, and although they may be hard to implement, this solution does provide great granularity and does not depend on the underlying infrastructure. You only require a Kubernetes CNI that supports <span class="No-Break">this feature.</span></p>
<p>In the next section, we will review how service mesh solutions can provide more complex security functionality by injecting small, lightweight proxies on all your <span class="No-Break">application’s Pods.</span></p>
<h2 id="_idParaDest-244"><a id="_idTextAnchor261"/>Service mesh</h2>
<p>By <a id="_idIndexMarker1295"/>implementing NetworkPolicies, we enforce some firewall-like <a id="_idIndexMarker1296"/>connectivity rules between our applications’ workloads, but this may not be enough. A service mesh is considered an infrastructure layer that interconnects services and manages how they will interact with each other. A service mesh is used to manage East-West and North-South traffic to background Services, in some cases even substituting the Ingress Controller if the service mesh solution is deployed <span class="No-Break">in Kubernetes.</span></p>
<p>The most popular service mesh <a id="_idIndexMarker1297"/>solution is <strong class="bold">Istio</strong> (an open source solution that is part <a id="_idIndexMarker1298"/>of the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>)), although other options worth mentioning include Linkerd, Consul Connect, and Traefik Mesh. If you are running a cloud Kubernetes platform, you may have your own cloud provider <span class="No-Break">solution available.</span></p>
<p>Service mesh solutions are capable of adding TLS communications, traffic management, and observability to your applications without having to modify their code. If you are looking for a transparent security and management layer, using a service mesh solution may be perfect for you, but it also adds a high level of complexity and some <span class="No-Break">platform overhead.</span></p>
<p>Service mesh solutions deploy a small proxy on all your application workloads. These proxies intercept all your application’s network traffic and apply rules to allow or disallow your application <span class="No-Break">processes’ communications.</span></p>
<p>The implementation and use of service meshes are out of the scope of this book but it is worth investigating whether your Kubernetes administrators have deployed a service mesh solution on your platform that may necessitate the implementation of service <span class="No-Break">mesh-specific resources.</span></p>
<p>As mentioned earlier in this section, NetworkPolicy resources isolate your application’s workloads by disabling unauthorized communications, which may provide sufficient security for a production environment. These resources are highly configurable, and you are responsible for defining the required communications between your application’s components and preparing the required YAML manifests to fully implement all your application communications. In the following <em class="italic">Labs</em> section, we will see some of the content learned in this chapter in action as we try publishing the <strong class="source-inline">simplestlab</strong> application used in <span class="No-Break">previous chapters.</span></p>
<h1 id="_idParaDest-245"><a id="_idTextAnchor262"/>Labs</h1>
<p>In this section, we show you how to work through the implementation of the Ingress resources for the <strong class="source-inline">simplestlab</strong> Tier-3 application, prepared for Kubernetes in <a href="B19845_09.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Implementing Architecture Patterns</em>, and improved upon in <a href="B19845_10.xhtml#_idTextAnchor231"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Leveraging Application Data Management in Kubernetes</em>. Manifests for all the resources have been prepared for you in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git">https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git</a> and can be found in the <strong class="source-inline">Chapter11</strong> folder. Ensure you have the latest revision available by simply executing <strong class="source-inline">git clone</strong> to download all its content, or use <strong class="source-inline">git pull</strong> if you have already downloaded the repository before. All the manifests and steps required for running the <strong class="source-inline">simplestlab</strong> application are located inside the <strong class="source-inline">Containers-for-Developers-Handbook/Chapter11/simplestlab</strong> directory, while all the manifests for Ingress and NetworkPolicy resources can be found directly in the <span class="No-Break"><strong class="source-inline">Chapter11</strong></span><span class="No-Break"> folder.</span></p>
<p>These labs will help you learn and understand how Ingress and NetworkPolicy resources work in Kubernetes. You will deploy an Ingress Controller, publish the <strong class="source-inline">simplestlab</strong> example application using the HTTP and HTTPS protocols, and create some NetworkPolicy resources to allow only appropriate connectivity. The Ingress Controller lab will work on Docker Desktop, Minikube, and Rancher, but for the NetworkPolicy resources part, you will need to use an appropriate Kubernetes CNI with support for such resources, such as Calico. Each Kubernetes desktop or platform implementation manages and presents its own networking infrastructure to users in a <span class="No-Break">different way.</span></p>
<p>These are the tasks you will find in this chapter’s <span class="No-Break">GitHub repository:</span></p>
<ol>
<li>We will first deploy the Kubernetes NGINX Ingress Controller (if you don’t have your own Ingress Controller in your <span class="No-Break">labs platform).</span></li>
<li>We will deploy all the manifests prepared for the <strong class="source-inline">simplestlab</strong> application, located inside the <strong class="source-inline">simplestlab</strong> folder. We will use <strong class="source-inline">kubectl create -</strong><span class="No-Break"><strong class="source-inline">f simplestlab</strong></span><span class="No-Break">.</span></li>
<li>Once all the components are ready, we will create an Ingress resource using the manifest prepared for <span class="No-Break">this task.</span></li>
<li>In the GitHub repository, you will find instructions for deploying a more advanced Ingress manifest with a self-signed certificate and encrypting the <span class="No-Break">client communications.</span></li>
<li>There is also a NetworkPolicy lab in the GitHub repository that will help you understand how to secure your applications using this feature with a compatible <span class="No-Break">CNI (Calico).</span></li>
</ol>
<p>In the first task, we will deploy our own <span class="No-Break">Ingress Controller.</span></p>
<h2 id="_idParaDest-246"><a id="_idTextAnchor263"/>Improving application access by deploying your own Ingress Controller</h2>
<p>For this<a id="_idIndexMarker1299"/> task, we will use Docker Desktop, which provides a good LoadBalancer service implementation. These Service resources will attach the localhost IP address, which will make it easy to connect to the published services. We will use the cloud deployment of Kubernetes NGINX Ingress Controller (<a href="https://kubernetes.github.io">https://kubernetes.github.io</a>) based on the LoadBalancer Service type, described in the following manifest: <a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml">https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml</a>. If you are using a completely bare-metal infrastructure, you can use the bare-metal YAML (<a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml">https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml</a>) and follow the additional instructions at <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/</a> for <span class="No-Break">NodePort routing.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Local copies of both YAML files are provided in the repository as <strong class="source-inline">kubernetes-nginx-ingress-controller-full-install-cloud.yaml</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">kubernetes-nginx-ingress-controller-full-install-baremetal.yaml</strong></span><span class="No-Break">.</span></p>
<p>Once this is done, follow <span class="No-Break">these steps:</span></p>
<ol>
<li>We will just deploy the cloud version, provided in the YAML as a series of concatenated <a id="_idIndexMarker1300"/>manifests. We just use <strong class="source-inline">kubectl apply</strong> to deploy <span class="No-Break">the controller:</span><pre class="source-code">
<strong class="bold">Chapter11$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-</strong>
<strong class="bold">nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml</strong>
<strong class="bold">namespace/ingress-nginx created</strong>
<strong class="bold">serviceaccount/ingress-nginx created</strong>
<strong class="bold">serviceaccount/ingress-nginx-admission created</strong>
<strong class="bold">role.rbac.authorization.k8s.io/ingress-nginx created</strong>
<strong class="bold">role.rbac.authorization.k8s.io/ingress-nginx-admission created</strong>
<strong class="bold">clusterrole.rbac.authorization.k8s.io/ingress-nginx created</strong>
<strong class="bold">clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created</strong>
<strong class="bold">rolebinding.rbac.authorization.k8s.io/ingress-nginx created</strong>
<strong class="bold">rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created</strong>
<strong class="bold">clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created</strong>
<strong class="bold">clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created</strong>
<strong class="bold">configmap/ingress-nginx-controller created</strong>
<strong class="bold">service/ingress-nginx-controller created</strong>
<strong class="bold">service/ingress-nginx-controller-admission created</strong>
<strong class="bold">deployment.apps/ingress-nginx-controller created</strong>
<strong class="bold">job.batch/ingress-nginx-admission-create created</strong>
<strong class="bold">job.batch/ingress-nginx-admission-patch created</strong>
<strong class="bold">ingressclass.networking.k8s.io/nginx created</strong>
<strong class="bold">validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created</strong></pre></li> <li>We can <a id="_idIndexMarker1301"/>review the workload <span class="No-Break">resources created:</span><pre class="source-code">
<strong class="bold">Chapter11$ kubectl get all -n ingress-nginx</strong>
<strong class="bold">NAME                                            READY   STATUS       RESTARTS   AGE</strong>
<strong class="bold">pod/ingress-nginx-admission-create-9cpnb        0/1     Completed   0          13m</strong>
<strong class="bold">pod/ingress-nginx-admission-patch-6gq2c         0/1     Completed   1          13m</strong>
<strong class="bold">pod/ingress-nginx-controller-74469fd44c-h6nlc   1/1     Running     0          13m</strong>
<strong class="bold">NAME                                         TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE</strong>
<strong class="bold">service/ingress-nginx-controller             LoadBalancer   10.100.162.170   localhost      80:31901/TCP,443:30080/TCP   13m</strong>
<strong class="bold">service/ingress-nginx-controller-admission   ClusterIP      10.100.197.210   &lt;none&gt;        443/TCP                      13m</strong>
<strong class="bold">NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE</strong>
<strong class="bold">deployment.apps/ingress-nginx-controller   1/1     1            1           13m</strong>
<strong class="bold">NAME                                                  DESIRED    CURRENT   READY   AGE</strong>
<strong class="bold">replicaset.apps/ingress-nginx-controller-74469fd44c   1         1         1       13m</strong>
<strong class="bold">NAME                                       COMPLETIONS   DURATION   AGE</strong>
<strong class="bold">job.batch/ingress-nginx-admission-create   1/1           7s          13m</strong>
<strong class="bold">job.batch/ingress-nginx-admission-patch    1/1           8s          13m</strong></pre></li> <li>The <strong class="source-inline">ingress-nginx-controller</strong> Service is attached to the <strong class="source-inline">localhost</strong> IP<a id="_idIndexMarker1302"/> address, so we can check its availability at <strong class="source-inline">http://localhost:80</strong> and <strong class="source-inline">https://localhost:443</strong> (<span class="No-Break">exposed ports):</span><pre class="source-code">
<strong class="bold">Chapter11$ curl http://localhost</strong>
<strong class="bold">&lt;html&gt;</strong>
<strong class="bold">&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;</strong>
<strong class="bold">&lt;body&gt;</strong>
<strong class="bold">&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;</strong>
<strong class="bold">&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;</strong>
<strong class="bold">&lt;/body&gt;</strong>
<strong class="bold">&lt;/html&gt;</strong></pre><p class="list-inset">It also works with HTTPS (we add the <strong class="source-inline">–k</strong> argument to avoid <span class="No-Break">certificate validation):</span></p><pre class="source-code"><strong class="bold">Chapter11$ curl https://localhost</strong>
<strong class="bold">curl: (60) SSL certificate problem: self-signed certificate</strong>
<strong class="bold">More details here: https://curl.se/docs/sslcerts.xhtml</strong>
<strong class="bold">curl failed to verify the legitimacy of the server and therefore could not</strong>
<strong class="bold">establish a secure connection to it. To learn more about this situation and</strong>
<strong class="bold">how to fix it, please visit the web page mentioned above.</strong>
<strong class="bold">Chapter11$ curl -k https://localhost</strong>
<strong class="bold">&lt;html&gt;</strong>
<strong class="bold">&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;</strong>
<strong class="bold">&lt;body&gt;</strong>
<strong class="bold">&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;</strong>
<strong class="bold">&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;</strong>
<strong class="bold">&lt;/body&gt;</strong>
<strong class="bold">&lt;/html&gt;</strong></pre></li> </ol>
<p>The <a id="_idIndexMarker1303"/>Ingress Controller is now deployed and listening, and the <strong class="source-inline">404</strong> error indicates that there isn’t an associated Ingress resource with the <strong class="source-inline">localhost</strong> host (in fact there isn’t even a default one configured, but the Ingress Controller <span class="No-Break">responds correctly).</span></p>
<h2 id="_idParaDest-247"><a id="_idTextAnchor264"/>Publishing the simplestlab application on Kubernetes using an Ingress Controller</h2>
<p>In this lab, we<a id="_idIndexMarker1304"/> will deploy <strong class="source-inline">simplestlab</strong>, a very simplified tier-3 application, located in the <strong class="source-inline">simplestlab</strong> directory, and we’ll publish its frontend, the <strong class="source-inline">lb</strong> component, without TLS encryption. You can follow <span class="No-Break">these steps:</span></p>
<ol>
<li>The manifests for the application are already written for you; we will just have to use <strong class="source-inline">kubectl</strong> to create an appropriate namespace for the application and then deploy all <span class="No-Break">its resources:</span><pre class="source-code">
<strong class="bold">Chapter11$ kubectl create ns simplestlab</strong>
<strong class="bold">namespace/simplestlab created</strong>
<strong class="bold">Chapter11$ kubectl create -n simplestlab \</strong>
<strong class="bold">-f simplestlab/</strong>
<strong class="bold">deployment.apps/app created</strong>
<strong class="bold">service/app created</strong>
<strong class="bold">secret/appcredentials created</strong>
<strong class="bold">service/db created</strong>
<strong class="bold">statefulset.apps/db created</strong>
<strong class="bold">secret/dbcredentials created</strong>
<strong class="bold">secret/initdb created</strong>
<strong class="bold">configmap/lb-config created</strong>
<strong class="bold">daemonset.apps/lb created</strong>
<strong class="bold">service/lb created</strong></pre></li> <li>We <a id="_idIndexMarker1305"/>can now verify the resources created in the <span class="No-Break"><strong class="source-inline">simplestlab</strong></span><span class="No-Break"> namespace:</span><pre class="source-code">
<strong class="bold">Chapter11$ kubectl get all -n simplestlab</strong>
<strong class="bold">NAME                       READY   STATUS    RESTARTS   AGE</strong>
<strong class="bold">pod/app-5f9797d755-5t4nz   1/1     Running   0          81s</strong>
<strong class="bold">pod/app-5f9797d755-9rzlh   1/1     Running   0          81s</strong>
<strong class="bold">pod/app-5f9797d755-nv58j   1/1     Running   0          81s</strong>
<strong class="bold">pod/db-0                   1/1     Running   0          80s</strong>
<strong class="bold">pod/lb-5wl7c               1/1     Running   0          80s</strong>
<strong class="bold">NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</strong>
<strong class="bold">service/app   ClusterIP   10.99.29.167    &lt;none&gt;        3000/TCP   81s</strong>
<strong class="bold">service/db    ClusterIP   None            &lt;none&gt;        5432/TCP   81s</strong>
<strong class="bold">service/lb    ClusterIP   10.105.219.69   &lt;none&gt;        80/TCP     80s</strong>
<strong class="bold">NAME                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</strong>
<strong class="bold">daemonset.apps/lb   1         1         1       1            1            &lt;none&gt;          80s</strong>
<strong class="bold">NAME                  READY   UP-TO-DATE   AVAILABLE   AGE</strong>
<strong class="bold">deployment.apps/app   3/3     3            3           81s</strong>
<strong class="bold">NAME                             DESIRED   CURRENT   READY   AGE</strong>
<strong class="bold">replicaset.apps/app-5f9797d755   3         3         3       81s</strong>
<strong class="bold">NAME                  READY   AGE</strong>
<strong class="bold">statefulset.apps/db   1/1     80s</strong></pre><p class="list-inset">Our<a id="_idIndexMarker1306"/> application is ready but inaccessible; the <strong class="source-inline">lb</strong> component isn’t exposed. It is listening on port <strong class="source-inline">80</strong>, but <strong class="source-inline">ClusterIP</strong> is used, hence the Service is only available <span class="No-Break">internally, cluster-wide.</span></p></li> <li>We will now create an Ingress resource. There are two manifests in the <strong class="source-inline">ingress</strong> directory. We will use <strong class="source-inline">simplestlab.ingress.yaml</strong>, which will be deployed <a id="_idIndexMarker1307"/>without custom <span class="No-Break">TLS encryption:</span><pre class="source-code">
<strong class="bold">Chapter11$ cat ingress/simplestlab.ingress.yaml</strong>
<strong class="bold">apiVersion: networking.k8s.io/v1</strong>
<strong class="bold">kind: Ingress</strong>
<strong class="bold">metadata:</strong>
<strong class="bold">  name: simplestlab</strong>
<strong class="bold">  annotations:</strong>
<strong class="bold">    # nginx.ingress.kubernetes.io/rewrite-target: /</strong>
<strong class="bold">spec:</strong>
<strong class="bold">  ingressClassName: nginx</strong>
<strong class="bold">  rules:</strong>
<strong class="bold">  - host: simplestlab.local.lab</strong>
<strong class="bold">    http:</strong>
<strong class="bold">      paths:</strong>
<strong class="bold">      - path: /</strong>
<strong class="bold">        pathType: Prefix</strong>
<strong class="bold">        backend:</strong>
<strong class="bold">          service:</strong>
<strong class="bold">            name: lb</strong>
<strong class="bold">            port:</strong>
<strong class="bold">              number: 80</strong></pre></li> <li>We will just deploy the previously <span class="No-Break">created manifest:</span><pre class="source-code">
<strong class="bold">Chapter11$ kubectl create \</strong>
<strong class="bold">-f ingress/simplestlab.ingress.yaml -n simplestlab</strong>
<strong class="bold">ingress.networking.k8s.io/simplestlab created</strong>
<strong class="bold">Chapter11$ kubectl get ingress -n simplestlab</strong>
<strong class="bold">NAME CLASS HOSTS ADDRESS PORTS AGE</strong>
<strong class="bold">simplestlab nginx simplestlab.local.lab 80 16s</strong></pre></li> <li>We <a id="_idIndexMarker1308"/>can check the defined host URL <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">Chapter11$ curl -H "host: simplestlab.local.lab" http://localhost/</strong>
<strong class="bold">&lt;!DOCTYPE html&gt;</strong>
<strong class="bold">&lt;html&gt;</strong>
<strong class="bold">&lt;head&gt;</strong>
<strong class="bold"> …</strong>
<strong class="bold">&lt;/head&gt;</strong>
<strong class="bold">&lt;body&gt;</strong>
<strong class="bold">…</strong>
<strong class="bold">&lt;/body&gt;</strong>
<strong class="bold">&lt;/html&gt;</strong>
<strong class="bold">&lt;/body&gt;</strong>
<strong class="bold">&lt;/html&gt;</strong></pre><p class="list-inset">The <strong class="source-inline">simplestlab</strong> application is now available <span class="No-Break">and accessible.</span></p></li> <li>We can change our <strong class="source-inline">/etc/hosts</strong> file (or equivalent MS Windows <strong class="source-inline">c:\system32\drivers\etc\hosts</strong> file). Add the following line and open the web browser to access the <span class="No-Break"><strong class="source-inline">simplestlab</strong></span><span class="No-Break"> application:</span><pre class="source-code">
127.0.0.1 simplestlab.local.lab</pre><p class="list-inset">This requires root or Administrator access, hence it may be more interesting to use <strong class="source-inline">curl</strong> with the <strong class="source-inline">-H</strong> or <strong class="source-inline">--header</strong> arguments to check <span class="No-Break">the application.</span></p></li> </ol>
<p class="callout-heading">Important note</p>
<p class="callout">You can use an extension on your web browser that allows you to modify the headers of your requests or an FQDN including <a href="http://nip.io">nip.io</a>, which will be used in <a href="B19845_13.xhtml#_idTextAnchor287"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Managing the Application Life Cycle</em>. For example, you can simply add the <strong class="source-inline">simple-modify-headers</strong> extension if you are using MS Edge (you will find equivalent ones for other web browsers and operating systems). Additional information for configuring this extension is discussed in the GitHub <strong class="source-inline">Readme.md</strong> file for <span class="No-Break">this chapter.</span></p>
<p class="list-inset">The<a id="_idIndexMarker1309"/> application will be available at <a href="http://localhost">http://localhost</a> (notice that we defined the URL pattern as <strong class="source-inline">http://locahost/*</strong> in the <strong class="source-inline">simple-modify-headers</strong> <span class="No-Break">extension configuration):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt=" Figure 11.21 – SIMPLE MODIFY HEADERS Edge extension configuration" height="447" src="image/B19845_11_21.jpg" width="1213"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 11.21 – SIMPLE MODIFY HEADERS Edge extension configuration</p>
<ol>
<li value="7">Once the extension is configured, we can reach the <strong class="source-inline">simplestlab</strong> application <span class="No-Break">using </span><a href="http://localhost"><span class="No-Break">http://localhost</span></a><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 11.22 – The simplestlab application is accessible thanks to the Ingress Controller" height="623" src="image/B19845_11_22.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.22 – The simplestlab application is accessible thanks to the Ingress Controller</p>
<p class="list-inset">In the GitHub repository, you will find instructions to add TLS to the Ingress resource to improve our application security and how t<a id="_idTextAnchor265"/>o implement NetworkPolicy resources using Calico as a CNI <span class="No-Break">with Minikube.</span></p>
<p>These labs<a id="_idIndexMarker1310"/> have helped you understand how to improve the security of your applications by isolating their components and exposing and publishing only those required by the users and other <span class="No-Break">applications’ components.</span></p>
<h1 id="_idParaDest-248"><a id="_idTextAnchor266"/>Summary</h1>
<p>In this chapter, we learned how to publish our applications in Kubernetes for our users and for other components deployed either internally in the same cluster or externally. Different mechanisms for this were examined, but ultimately, it is up to you to determine which of your applications’ components should be exposed <span class="No-Break">and accessible.</span></p>
<p>Throughout this chapter, we reviewed some quick solutions for debugging and publishing Service resources directly on our desktop computers with the <strong class="source-inline">kubectl</strong> client. We also examined different Service types that could be useful for locally accessing our remote applications on remote Kubernetes development clusters. We discussed how LoadBalancer Services are part of the Kubernetes core and were prepared for cloud platforms, due to which they may be difficult to implement on-premises, and this is why the recommended option for delivering applications is to create your own Ingress resource manifest. Ingress Controllers will help you to publish applications on any Kubernetes platform. You will use Ingress resources to define how applications will be published, and you may need to tweak their syntax according to the Ingress Controller deployed in your <span class="No-Break">Kubernetes platform.</span></p>
<p>Toward the end of the chapter, we introduced the NetworkPolicy resource and the service mesh concept, which offers the means to improve the security of our applications by dropping any untrusted and undefined communications. This was followed by some labs to test what <span class="No-Break">we learned.</span></p>
<p>In the next chapter, we will review some useful mechanisms and tools for monitoring and gathering performance data from <span class="No-Break">our applications.</span></p>
</div>
</div></body></html>