<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer098">
<h1 class="chapter-number" id="_idParaDest-185"><a id="_idTextAnchor202"/>9</h1>
<h1 id="_idParaDest-186"><a id="_idTextAnchor203"/>Implementing Architecture Patterns</h1>
<p>Kubernetes is the most popular container orchestrator in production. This platform provides different resources that allow us to deploy our applications with high resilience and with their components distributed cluster-wide while the platform itself runs with high availability. In this chapter, we will learn how these resources can provide different application architecture patterns, along with use cases and best practices to implement them. We will also review different options for managing application data and learn how to manage the health of our applications to make them respond to possible health and performance issues in the most effective way. At the end of this chapter, we will review how Kubernetes provides security patterns that improve application security. This chapter will give you a good overview of which Kubernetes resources will fit your applications’ requirements most accurately. The following topics will be covered in <span class="No-Break">this chapter:</span></p>
<ul>
<li>Applying Kubernetes resources to common <span class="No-Break">application patterns</span></li>
<li>Understanding advanced Pod <span class="No-Break">application patterns</span></li>
<li>Verifying <span class="No-Break">application health</span></li>
<li>Resource management <span class="No-Break">and scalability</span></li>
<li>Improving application security <span class="No-Break">with Pods</span></li>
</ul>
<h1 id="_idParaDest-187"><a id="_idTextAnchor204"/>Technical requirements</h1>
<p>You can find the labs for this chapter at <a href="https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9">https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter9</a>, where you will find some extended explanations that have been omitted in this chapter’s content to make it easier to follow. The <em class="italic">Code In Action</em> video for this chapter can be found <span class="No-Break">at </span><a href="https://packt.link/JdOIY"><span class="No-Break">https://packt.link/JdOIY</span></a><span class="No-Break">.</span></p>
<p>We will start this chapter by reviewing some of the resource types that were presented in <a href="B19845_08.xhtml#_idTextAnchor170"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Deploying Applications with the Kubernetes Orchestrator</em>, and present some common <span class="No-Break">use cases.</span></p>
<h1 id="_idParaDest-188"><a id="_idTextAnchor205"/>Applying Kubernetes resources to common application patterns</h1>
<p>The Kubernetes <a id="_idIndexMarker1040"/>container orchestrator is based on resources that are managed by different controllers. By default, our applications can use one of the following to run our processes <span class="No-Break">as containers:</span></p>
<ul>
<li><span class="No-Break">Pods</span></li>
<li><span class="No-Break">ReplicaSets</span></li>
<li><span class="No-Break">ReplicaControllers</span></li>
<li><span class="No-Break">Deployments</span></li>
<li><span class="No-Break">StatefulSets</span></li>
<li><span class="No-Break">DaemonSets</span></li>
<li><span class="No-Break">Jobs</span></li>
<li><span class="No-Break">CronJobs</span></li>
</ul>
<p>This list shows the standard or default resources that are allowed on every Kubernetes installation, but we can create custom resources to implement any non-standard or more specific application behavior. In this section, we are going to learn about these standard resources so that we can decide which one will fit our <span class="No-Break">application’s needs.</span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor206"/>Pods</h2>
<p>Pods are the<a id="_idIndexMarker1041"/> minimal unit for deploying workloads on a Kubernetes cluster. A Pod can contain multiple containers, and we have different mechanisms for <span class="No-Break">doing this.</span></p>
<p>The following applies to all the containers running inside a Pod <span class="No-Break">by default:</span></p>
<ul>
<li>They all share the network namespace, so they all refer to the same localhost and run with the same <span class="No-Break">IP address.</span></li>
<li>They are all scheduled together in the <span class="No-Break">same host.</span></li>
<li>They all share the namespaces that can be defined at the container level. We will define resource limits (using cgroups) for each container, although we can also define resource limits at the Pod level. Pod resource limits will be applied for all <span class="No-Break">the containers.</span></li>
<li>Volumes attached to a Pod are available to all containers <span class="No-Break">running inside.</span></li>
</ul>
<p>So, we can see that Pods are groups of containers running together that share kernel namespaces and <span class="No-Break">compute resources.</span></p>
<p>You will find a lot of Pods running a single container, and this is completely fine. The distribution of containers inside different Pods depends on your application’s components distribution. You need to ask yourself whether or not some processes must run together. For example, you can put together two containers that need to communicate fast, or you may need to keep track of files created by one of them without sharing a remote data volume. But it is very important to understand that all the containers running in a Pod <a id="_idIndexMarker1042"/>scale and replicate together. This means that multiple replicas of a Pod will execute the same number of replicas of their containers, and thus your application’s behavior can be impacted because the same type of process will run multiple times, and it will also access your files at the same time. This, for example, will break the data in your database or may lead to data inconsistency. Therefore, you need to decide wisely whether or not your application should distribute your application’s containers into different Pods or run <span class="No-Break">them together.</span></p>
<p>Kubernetes will keep track of the status of each container running inside a Pod by executing the Pod’s defined probes. Each container should have its own probes (different types of probes exist, as we will learn in the <em class="italic">Verifying application health</em> section). But at this point, we have to understand that the health of all the containers inside a Pod controls the overall behavior of the Pod. If one of the containers dies, the entire Pod is set as unhealthy, triggering the defined Kubernetes event and resource’s behavior. Thus, we can execute multiple Service containers in parallel or prepare our application by executing some pre-processes that can, for example, populate some minimal filesystem resources, binaries, permissions, and so on. These types of containers, which run before the actual application<a id="_idIndexMarker1043"/> processes, are called <strong class="bold">initial containers</strong>. The <strong class="bold">main containers</strong> (those <a id="_idIndexMarker1044"/>running in parallel as part of the Pod) will start after the complete execution of all the initial containers and only if all of them end correctly. The kubelet will run <strong class="bold">initial containers</strong> (or <strong class="source-inline">initContainers</strong>, which is the key that’s used to define them) sequentially before any other container; therefore, if any of these initial containers fail, the Pod will not run <span class="No-Break">as expected.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">The Kubernetes 1.25 release<a id="_idIndexMarker1045"/> introduced <strong class="bold">ephemeral containers</strong>, which is a new and different concept. While Pods are intended to run applications’ processes, ephemeral containers will be created to provide debugging facilities to the actual Pod containers. They run by using the <strong class="source-inline">kubectl debug</strong> action followed by the name of the Pod to which your terminal should attach (shared kernel namespaces). We will provide a quick example of this in the <span class="No-Break"><em class="italic">Labs</em></span><span class="No-Break"> section.</span></p>
<p>Let’s review how <a id="_idIndexMarker1046"/>we write the manifest required for creating <span class="No-Break">a Pod:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 9.1 – Pod manifest" height="252" src="image/B19845_09_1.jpg" width="355"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Pod manifest</p>
<p>All the manifests for Kubernetes have at least the <strong class="source-inline">apiVersion</strong>, <strong class="source-inline">kind</strong>, and <strong class="source-inline">metadata</strong> keys, and these are used to define which API version will be used to reach the associated API server path, which type of resource we are defining, and information that uniquely describes the resource within the Kubernetes cluster, respectively. We can access all the resource manifest information via the Kubernetes API by using the JSON or YAML keys hierarchy; for example, to retrieve a Pod’s name, we can use <strong class="source-inline">.metadata.name</strong> to access its key. The properties of the resource should usually be written in the <strong class="source-inline">spec</strong> or <strong class="source-inline">data</strong> section. Kubernetes roles, role bindings (in cluster and namespaces scopes), Service accounts, and other resources do not include either <strong class="source-inline">data</strong> or <strong class="source-inline">spec</strong> keys for declaring their functionality. And we can even create custom resources, with custom definitions for declaring their properties. In the default workload resources, we will always use the <strong class="source-inline">spec</strong> section to define the behavior of <span class="No-Break">our resource.</span></p>
<p>Notice that in the previous code snippet, the <strong class="source-inline">containers</strong> key is an array. This allows us to define multiple containers, as we already mentioned, and the same happens with initial containers; we will define a list of containers in both cases and we will need at least the image that the container runtime must use and the name for <span class="No-Break">the container.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">We can use <strong class="source-inline">kubectl explain pod.spec.containers --recursive</strong> to retrieve all the existent keys under the <strong class="source-inline">spec</strong> section for a defined resource. The <strong class="source-inline">explain</strong> action allows you to retrieve all the keys for each resource directly from the Kubernetes cluster; this is important as it doesn’t depend on your <strong class="source-inline">kubectl</strong> binary version. The output of this action also shows which keys can be changed at runtime, once the resource has been created in <span class="No-Break">the cluster.</span></p>
<p>It is important to <a id="_idIndexMarker1047"/>mention here that Pods by themselves don’t have cluster-wide auto-healing. This means that when you run a Pod and it is considered unhealthy for whatever reason (any of the containers is considered unhealthy) in a host within the cluster, it will not execute on another host. Pods include the <strong class="source-inline">restartPolicy</strong> property to manage the behavior of Pods when they die. We can set this property to <strong class="source-inline">Always</strong> (always restart the container’s Pods), <strong class="source-inline">OnFailure</strong> (only restart containers when they fail), or <strong class="source-inline">Never</strong>. A new Pod will never be recreated on another cluster host. We will need more advanced resources for managing the containers’ life cycle cluster-wide; these will be discussed in the <span class="No-Break">subsequent sections.</span></p>
<p>Pods are used to run a test for an application or one of its components, but we never use them to run actual Services because Kubernetes just keeps them running; it doesn’t manage their updates or reconfiguration. Let’s review how ReplicaSets solve these situations when we need to keep our application’s containers up <span class="No-Break">and running.</span></p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor207"/>ReplicaSets</h2>
<p>A ReplicaSet is a set of<a id="_idIndexMarker1048"/> Pods that should be running at the same time for an application’s components (or for the application itself if it just has one component). To define a ReplicaSet resource, we need to write <span class="No-Break">the following:</span></p>
<ul>
<li>A <strong class="source-inline">selector</strong> section in which we define which Pods are part of <span class="No-Break">the resource</span></li>
<li>The number of replicas required to keep the <span class="No-Break">resource healthy</span></li>
<li>A Pod template in which we define how new Pods should be created when one of the Pods in the <span class="No-Break">set dies</span></li>
</ul>
<p>Let’s review the<a id="_idIndexMarker1049"/> syntax of <span class="No-Break">these resources:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 9.2 – ReplicaSet manifest" height="486" src="image/B19845_09_2.jpg" width="878"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – ReplicaSet manifest</p>
<p>As you can see, the <strong class="source-inline">template</strong> section describes a Pod definition inside the ReplicaSet’s <strong class="source-inline">spec</strong> section. This <strong class="source-inline">spec</strong> section also includes the <strong class="source-inline">selector</strong> section, which defines what Pods will be included. We can use <strong class="source-inline">matchLabels</strong> to include exact label-key pairs from Pods, and <strong class="source-inline">matchExpressions</strong> to include advanced rules such as the existence of a defined label or its value included in a list <span class="No-Break">of strings.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Selectors from ReplicaSet resources apply to running Pods too. This means that you have to take care of the labels that uniquely identify your application’s components. ReplicaSet resources are namespaced, so we can use <strong class="source-inline">kubectl get pods -–show-labels</strong> before actually creating a ReplicaSet to ensure that the right Pods will be included in <span class="No-Break">the set.</span></p>
<p>In the Pod template, we will define the volumes to be attached to the different containers created by the ReplicaSet, but it is important to understand that these volumes are common to all the replicas. Therefore, all container replicas will attach the same volumes (in fact, the hosts where they run mount the volumes and the kubelet makes them available to the Pods’ containers), which may generate issues if your application does not allow such a situation. For example, if you are deploying a database, running more than one replica that’s attaching the same volume will probably break your data files. We should ensure that our application can run more than one replicated process at a time, and if not, ensure we apply the appropriate <strong class="source-inline">ReadWriteOnce</strong> mode flag in the <strong class="source-inline">accessMode</strong> key. We <a id="_idIndexMarker1050"/>will deep dive into this key, its importance, and its meaning for our workloads in <span class="No-Break"><em class="italic">Chapter 10</em></span>, <em class="italic">Leveraging Application Data Management </em><span class="No-Break"><em class="italic">in Kubernetes</em></span><span class="No-Break">.</span></p>
<p>The most important key in ReplicaSets is the <strong class="source-inline">replicas</strong> key, which defines the number of active healthy Pods that should be running. This allows us to scale up or down the number of instances for our application’s processes. The names of the Pods associated with a ReplicaSet will follow <strong class="source-inline">&lt;REPLICASET_NAME&gt;-&lt;POD_RANDOM_UNIQUE_GENERATED_ID&gt;</strong>. This helps us understand which ReplicaSet generated them. We also can review the ReplicaSet creator by using <strong class="source-inline">kubectl get pod –o yaml</strong>. The <strong class="source-inline">metadata.OwnerReferences</strong> key shows the ReplicaSet that finally created each <span class="No-Break">Pod resource.</span></p>
<p>We can modify the number of replicas of a running ReplicaSet resource using any of the <span class="No-Break">following methods:</span></p>
<ul>
<li>Editing the running ReplicaSet resource directly in Kubernetes using <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">edit &lt;REPLICASET_NAME&gt;</strong></span></li>
<li>Patching the current ReplicaSet resource using <span class="No-Break"><strong class="source-inline">kubectl patch</strong></span></li>
<li>Using the <strong class="source-inline">scale</strong> action with <strong class="source-inline">kubectl</strong>, setting the number of replicas: <strong class="source-inline">kubectl scale rs --replicas &lt;</strong><span class="No-Break"><strong class="source-inline">NUMBER_OF_REPLICAS&gt; &lt;REPLICASET_NAME&gt;</strong></span></li>
</ul>
<p>Although changing the number of replicas works automatically, other changes don’t work so well. In the Pod template, if we change the image to use for creating containers, the resource will show this change, but the current associated Pods will not change. This is because ReplicaSets do not manage their changes; we need to work with Deployment resources, which are more advanced. To make any change available in a ReplicaSet, we need to recreate the Pods manually by just removing the current Pods (using <strong class="source-inline">kubectl delete pod &lt;REPLICASET_POD_NAMES&gt;</strong>) or scaling the replicas down to zero and scaling up after all are deleted. Any of these methods will create fresh new replicas, using the new <span class="No-Break">ReplicaSet definition.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">You can use <strong class="source-inline">kubectl delete pod --selector &lt;LABEL_SELECTOR&gt;</strong>, with the current ReplicaSet selectors that were used to create them, to delete all associated <span class="No-Break">Pod resources.</span></p>
<p>ReplicaSets, by <a id="_idIndexMarker1051"/>default, don’t publish any Service; we need to create a Service resource to consume the deployed containers. When we create a Service associated with a ReplicaSet (using the Service’s label selector with the appropriate ReplicaSet’s labels), all the ReplicaSet instances will be accessible by using the Service’s <strong class="source-inline">ClusterIP</strong> address (default Service mode). All replicas get the same number of requests because the internal load balancing provides <span class="No-Break">round-robin access.</span></p>
<p>We will probably not use ReplicaSets as standalone resources in production as we have seen that any change in their definition requires additional interaction from our side, and that’s not ideal in dynamic environments such <span class="No-Break">as Kubernetes.</span></p>
<p>Before we look at Deployments, which are advanced resources for deploying ReplicaSets, we will quickly review ReplicationControllers, which are quite similar <span class="No-Break">to ReplicaSets.</span></p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor208"/>ReplicationControllers</h2>
<p>The ReplicationController was the<a id="_idIndexMarker1052"/> original method for Pod replication in Kubernetes, but now, it has been almost completely replaced by ReplicaSet resources. We can consider a ReplicationController as a less configurable ReplicaSet. Nowadays, we don’t directly create ReplicationControllers as we usually create Deployments for deploying application’s components running on Kubernetes. We learned that ReplicaSets have two options for selecting associated labels. The <strong class="source-inline">labelSelector</strong> key can be either a simple label search (<strong class="source-inline">matchLabels</strong>) or a more advanced rule that uses <strong class="source-inline">matchExpressions</strong>. ReplicationController manifests can only look for specific labels in Pods, which makes them simpler to use. The Pod template section looks similar in both ReplicaSets and ReplicaControllers. However, there is also a fundamental difference between ReplicationControllers and ReplicaSets. We can execute application upgrades by using rolling-update actions. These are not available for ReplicaSets but upgrades are <a id="_idIndexMarker1053"/>provided in such resources thanks to the use <span class="No-Break">of Deployments.</span></p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor209"/>Deployments</h2>
<p>We can say that a<a id="_idIndexMarker1054"/> Deployment is an advanced ReplicaSet. It adds the life cycle management part we missed by allowing us to upgrade the specifications of the Pods by creating a new ReplicaSet resource. This is the most used workload management resource in production. A Deployment resource creates and manages different ReplicaSets resources. When a Deployment resource is created, an associated ReplicaSet is also dynamically created, following the <strong class="source-inline">&lt;DEPLOYMENT_NAME&gt;-&lt;RS_RANDOM_UNIQUE_GENERATED_ID&gt;</strong> nomenclature. This dynamically created ReplicaSet will create associated Pods that follow the described nomenclature, so we will see Pod names such as <strong class="source-inline">&lt;DEPLOYMENT_NAME&gt;-&lt;RS_RANDOM_UNIQUE_GENERATED_ID&gt;-&lt;POD_ RANDOM_UNIQUE_GENERATED_ID&gt;</strong> in the defined namespace. This will help us follow which Deployment generates which Pod resources. Deployment resources manage the complete ReplicaSet life cycle. To do this, whenever we change any Deployment template specification key, a new ReplicaSet resource is created and this triggers the creation of new associated Pods. The Deployment resource keeps track of all associated ReplicaSets, which makes it easy to roll back to a previous release, without the latest resource modifications. This is very useful for releasing new application updates. Whenever an issue occurs with the updated resource, we can go back to any previous version in a few seconds thanks to Deployment resources – in fact, we can go back to any previous existing ReplicaSet resource. We will deep dive into rolling updates in <a href="B19845_13.xhtml#_idTextAnchor287"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Managing the Application </em><span class="No-Break"><em class="italic">Life Cycle</em></span><span class="No-Break">.</span></p>
<p>The following code snippet shows the syntax for <span class="No-Break">these resources:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 9.3 – Deployment manifest" height="496" src="image/B19845_09_3.jpg" width="887"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Deployment manifest</p>
<p>The <strong class="source-inline">strategy</strong> key allows us to decide whether our new containers try to start before the old ones die (the <strong class="source-inline">RollingUpdate</strong> value, which is used by default) or completely recreate the associated ReplicaSet (the <strong class="source-inline">Recreate</strong> value), which is needed when only one container can<a id="_idIndexMarker1055"/> access attached volumes in write mode at <span class="No-Break">the time.</span></p>
<p>We will use Deployments to deploy stateless or stateful application workloads in which we don’t require any special storage attachment and all replicas can be treated in the same way (all replicas are the same). Deployments work very well for deploying web Services with static content and dynamic ones when session persistence is managed in a different application component. We can’t use Deployment resources to deploy our application containers when each replica has to attach its own specific data volume or when we need to execute processes <span class="No-Break">in order.</span></p>
<p>We will now learn how StatefulSet resources help us solve these <span class="No-Break">specific situations.</span></p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor210"/>StatefulSets</h2>
<p>StatefulSet resources <a id="_idIndexMarker1056"/>are designed to manage stateful application components – those where persistent data must be unique between replicas. These resources also allow us to provide an order to different replicas when processes are executed. Each replica will receive a unique ordered identifier (an ordinal number starting from 0) and it will be used to scale the number of replicas up <span class="No-Break">or down.</span></p>
<p>The following code snippet shows an example of a <span class="No-Break">StatefulSet resource:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 9.4 – StatefulSet manifest" height="387" src="image/B19845_09_4.jpg" width="616"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – StatefulSet manifest</p>
<p>The preceding code<a id="_idIndexMarker1057"/> snippet shows <strong class="source-inline">template</strong> sections for both the Pod resources and the <span class="No-Break">volume resources.</span></p>
<p>The names for each Pod will follow <strong class="source-inline">&lt;STATEFULSET_NAME&gt;-&lt;REPLICA_NUMBER&gt;</strong>. For example, if we create a <strong class="source-inline">database</strong> StatefulSet resource with three replicas, the associated Pods will be <strong class="source-inline">database-0</strong>, <strong class="source-inline">database-1</strong>, and <strong class="source-inline">database-2</strong>. This name structure is also applied to the volumes defined in the StatefulSet’s <strong class="source-inline">volumeClaimTemplates</strong> <span class="No-Break">template section.</span></p>
<p>Notice that we also included the <strong class="source-inline">serviceName</strong> key in the previous code snippet. A headless Service (without <strong class="source-inline">ClusterIP</strong>) should be created to reference the ReplicaSet’s Pods in the Kubernetes internal DNS, but this key tells Kubernetes to create the required DNS entries. For the example presented, the first replica will be announced to the cluster DNS as <strong class="source-inline">database-0.database.NAMESPACE.svc.&lt;CLUSTER_NAME&gt;</strong>, and all other replicas will follow the same name schema. These names can be integrated into our application to create an application cluster or even configure advanced load-balancing mechanisms other than the default (used for ReplicaSets <span class="No-Break">and Deployments).</span></p>
<p>When we use StatefulSet resources, Pods will be created in order, which may introduce extra complexity when we need to remove some replicas. We will need to guarantee the correct execution of processes that may resolve dependencies between replicas; therefore, if we need to remove a StatefulSet replica, it will be safer to scale down the number of replicas instead of directly removing it. Remember, we have to prepare our application to manage unique replicas completely, and this may need some application process to remove an <a id="_idIndexMarker1058"/>application’s cluster component, for example. This situation is typical when you run distributed databases with multiple instances and decommissioning one instance requires database changes, but this also applies to any ReplicaSet manifest updates. You have to ensure that the changes are applied in the right order and, usually, it is preferred to scale down to zero and then scale up to the required number <span class="No-Break">of replicas.</span></p>
<p>In the StatefulSet example presented in the preceding code snippet, we specified a <strong class="source-inline">volumeClaimTemplate</strong> section, which defines the properties that are required for a dynamically provisioned volume. We will learn how dynamic storage provisioning works in <a href="B19845_10.xhtml#_idTextAnchor231"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Leveraging Application Data Management in Kubernetes</em>, but it is important to understand that this <strong class="source-inline">template</strong> section will inform the Kubernetes API that every replica requires its own ordered volume. This requirement for dynamic provisioning will usually be associated with the use of <span class="No-Break"><strong class="source-inline">StorageClass</strong></span><span class="No-Break"> resources.</span></p>
<p>Once these volumes (associated with each replica) are provisioned and used, deleting a replica (either directly by using <strong class="source-inline">kubectl delete pod</strong> or by scaling down the number of replicas) will never remove the associated volume. You can be sure that a database deployed via a ReplicaSet will never lose <span class="No-Break">its data.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">The ReplicaSet’s associated volumes will not be automatically removed, which makes these resources interesting for any workload if you need to ensure that data will not be deleted if you remove <span class="No-Break">the resource.</span></p>
<p>We can use StatefulSet to ensure that a replicated Service is managed uniquely. Software such as Hashicorp’s Consul runs clusterized on several predefined Nodes; we can deploy it on top of Kubernetes using containers, but Pods will need to be deployed in order and with their specific storage as if they were completely different hosts. A similar approach has to be applied in database Services because the replication of their processes may lead to data corruption. In these cases, we can use StatefulSet replicated resources, but the application should manage the integration between the different deployed replicas <a id="_idIndexMarker1059"/>and the scaling up and down procedure. Kubernetes just provides the underlying architecture that guarantees the data’s uniqueness and the replica <span class="No-Break">execution order.</span></p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor211"/>DaemonSets</h2>
<p>A DaemonSet resource <a id="_idIndexMarker1060"/>will execute exactly one associated Pod in each Kubernetes cluster Node. This ensures that any newly joined Node will get its own <span class="No-Break">replica automatically.</span></p>
<p>The following code snippet shows a DaemonSet <span class="No-Break">manifest example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 9.5 – DaemonSet manifest" height="408" src="image/B19845_09_5.jpg" width="703"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – DaemonSet manifest</p>
<p>As you may have noticed, we use label selectors to match and associate Pods. In the preceding example, we also<a id="_idIndexMarker1061"/> introduced the <strong class="source-inline">tolerations</strong> key. Let’s quickly introduce how <strong class="bold">taints</strong> and <strong class="bold">tolerations</strong> work with <a id="_idIndexMarker1062"/>any kind of workload manifest. They are used to avoid the execution of Pods on inappropriate Nodes. While taints are always associated with Nodes, tolerations are defined for Pods. A Pod must include all the taints associated with a Node as tolerations to be able to run on that particular Node. Node taints can produce three different effects: <strong class="source-inline">NoSchedule</strong> (only Pods with appropriate tolerations for the Node are allowed), <strong class="source-inline">PreferNoSchedule</strong> (Pods will not run on the Node unless no other one is available), or <strong class="source-inline">NoExecute</strong> (Pods will be evicted from the Node if they don’t have the appropriate tolerations). Taints and tolerations must match, and this allows us to<a id="_idIndexMarker1063"/> dedicate Nodes for certain tasks and avoid the execution of any other workloads on them. The kubelet will use dynamic taints to evict Pods when issues are found on a cluster Node – for example, when too much memory is in use or the disk is getting full. In our example, we add a toleration to execute the DaemonSet Pods on Nodes with the <span class="No-Break"><strong class="source-inline">node-role.kubernetes.io/control-plane=NoSchedule</strong></span><span class="No-Break"> taint.</span></p>
<p>DaemonSets are often used to deploy applications that should run on all Nodes, such as those running as software agents for monitoring or <span class="No-Break">logging purposes.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Although it isn’t too common, it is possible to use static Pods to run Node-specific processes. This is the mechanism that’s used by Kubernetes kubeadm-based Deployments. Static Pods are Pods associated with a Node, executed directly by the kubelet, and thus, they are not managed by Kubernetes. You can identify these Pods by their name because they include the host’s name. Manifests for executing static Pods are located in the <strong class="source-inline">/etc/kubernetes/manifests</strong> directory in <span class="No-Break">kubeadm clusters.</span></p>
<p>At this point, we have to mention that none of the workload management resources presented so far provide a mechanism to run a task that shouldn’t be maintained during its execution time. We will now review Job resources, which are specifically created for <span class="No-Break">this purpose.</span></p>
<h2 id="_idParaDest-195"><a id="_idTextAnchor212"/>Jobs</h2>
<p>A Job resource is in charge of executing a Pod until we get a successful termination. The Job resource also <a id="_idIndexMarker1064"/>tracks the execution of a set of Pods using template selectors. We configure a required number of successful executions and the Job resource is considered <em class="italic">Completed</em> when all the required Pod executions are <span class="No-Break">successfully finished.</span></p>
<p>In a Job resource, we can configure parallelism for executing more than one Pod at a time and being able to reach the required number of successful executions faster. Pods related to a Job will remain in our Kubernetes cluster until we delete the associated Job or remove <span class="No-Break">them manually.</span></p>
<p>A Job can be suspended, which will delete currently active Pods (in execution) until we resume <span class="No-Break">it again.</span></p>
<p>We can use Jobs to execute one-time tasks, but they are usually associated with periodic executions thanks to <strong class="source-inline">CronJob</strong> resources. Another common use case is the execution of certain one-time<a id="_idIndexMarker1065"/> tasks from applications directly in the Kubernetes cluster. In these cases, your application needs to be able to reach the Kubernetes API internally (the <strong class="source-inline">kubernetes</strong> Service in the <strong class="source-inline">default</strong> namespace) and the appropriate permissions for creating Jobs. This is usually achieved by associating a namespaced <strong class="source-inline">Role</strong>, which allows such actions, with the <strong class="source-inline">ServiceAccount</strong> resource that executes your application’s Pod. This association is established using a <span class="No-Break">namespaced </span><span class="No-Break"><strong class="source-inline">RoleBinding</strong></span><span class="No-Break">.</span></p>
<p>The following code snippet shows a <strong class="source-inline">Job</strong> <span class="No-Break">manifest example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="Figure 9.6 – Job manifest" height="349" src="image/B19845_09_6.jpg" width="591"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Job manifest</p>
<p>Here, we defined the number of successful completitions and the number of failures that will set the Job as failed by setting the <strong class="source-inline">completions</strong> and <strong class="source-inline">backoffLimit</strong> keys. At least three Pods must exit successfully before the limit of four failures is reached. Multiple Pods can be executed in parallel to speed up the completion by setting the <strong class="source-inline">parallelism</strong> key, which defaults <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
<p>The <em class="italic">TTL-after-finished</em> controller<a id="_idIndexMarker1066"/> provides a <strong class="bold">time-to-live</strong> (<strong class="bold">TTL</strong>) mechanism to limit the lifetime of completed Job resources. This will clean up finished jobs to remove old executions and maintain a clear view of current executions. This is very important when we continuously execute a lot of tasks. The controller will review completed Jobs and remove them<a id="_idIndexMarker1067"/> when the time since their completion is greater than the value of the <strong class="source-inline">ttlSecondsAfterFinished</strong> key. Since this key is based on a date-time reference, it is key to maintain our clusters’ time according to our <span class="No-Break">time zone.</span></p>
<p>Jobs are commonly used within CronJobs to define tasks that should be executed at certain periods – for example, for executing backups. Let’s learn how to implement CronJobs so that we can schedule <span class="No-Break">Jobs periodically.</span></p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor213"/>CronJobs</h2>
<p>CronJob resources<a id="_idIndexMarker1068"/> are used to schedule Jobs at specific times. The following code snippet shows a <strong class="source-inline">CronJob</strong> <span class="No-Break">manifest example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 9.7 – CronJob manifest" height="336" src="image/B19845_09_7.jpg" width="731"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – CronJob manifest</p>
<p>To be able to review logs from executed Pods (associated with the Jobs created), we can set <strong class="source-inline">failedJobsHistoryLimit</strong> and <strong class="source-inline">successfulJobsHistoryLimit</strong> to the desired number of Jobs to keep to be able to review the Pods’ logs. Notice that we planned the example Job daily, at 00:00, using the common <em class="italic">Unix Crontab</em> format, as shown in the <span class="No-Break">following schema:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 9.8 – Unix Crontab format" height="242" src="image/B19845_09_8.jpg" width="761"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Unix Crontab format</p>
<p>The <strong class="source-inline">schedule</strong> key <a id="_idIndexMarker1069"/>defines when the Job will be created and associated Pods will run. Remember to always quote your value to <span class="No-Break">avoid problems.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">CronJob resources use the Unix Crontab format, hence values such as <strong class="source-inline">@hourly</strong>, <strong class="source-inline">@daily</strong>, <strong class="source-inline">@monthly</strong>, or <strong class="source-inline">@yearly</strong> can <span class="No-Break">be used.</span></p>
<p>CronJobs can be suspended, which will affect any new Job creation if we change the value of the <strong class="source-inline">suspend</strong> key to <strong class="source-inline">true</strong>. To enable the CronJob again, we need to change this key to <strong class="source-inline">false</strong>, which will continue with the normal scheduling for creating <span class="No-Break">new Jobs.</span></p>
<p>A common use case for CronJobs is the execution of backup tasks for applications deployed on Kubernetes. With this solution, we avoid opening internal applications externally if user access <span class="No-Break">isn’t required.</span></p>
<p>Now that we understand the different resources we can use to deploy our workloads, let’s quickly review how they will help us provide resilience and high availability to <span class="No-Break">our applications.</span></p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor214"/>Ensuring resilience and high availability with Kubernetes resources</h2>
<p>Pod resources provide resilience out of the box as we can configure them to always restart if their processes fail. We can use the <strong class="source-inline">spec.restartPolicy</strong> key to define when they<a id="_idIndexMarker1070"/> should restart. It is important to understand that this option is limited to the host’s scope, so a Pod will just try to restart on the host on which it was previously running. Pod resources do not provide high availability or <span class="No-Break">resilience cluster-wide.</span></p>
<p>Deployments, and therefore ReplicaSets, and StatefulSets are prepared for applying resilience cluster-wide because resilience doesn’t depend on hosts. A Pod will still try to restart on the Node where it was previously running, but if it is not possible to run it, it will be scheduled to a new available one. This will allow Kubernetes administrators to perform maintenance tasks on Nodes moving workloads from one host to another, but this may impact your applications if they are not ready for such movements. In other words, if you only have one replica of your processes, they will go down for seconds (or minutes, depending on the size of your image and the time required by your processes to start), and this will impact your application. The solution is simple: deploy more than one replica of your application’s Pods. However, it is important to understand that your application needs to be prepared for multiple replicated processes working <span class="No-Break">in parallel.</span></p>
<p>StatefulSets’ replicas will never use the same volume, but this isn’t true for Deployments. All the replicas will share the volumes, and you must be aware of that. Sharing static content will work like a charm, but if multiple processes are trying to write the same file at the same time, you may encounter problems if your code doesn’t <span class="No-Break">manage concurrency.</span></p>
<p>DaemonSets work differently and we don’t have to manage any replication; just one Pod will run on each Node, but they will share volumes too. Because of the nature of such resources, it is not common to include shared volumes in <span class="No-Break">these cases.</span></p>
<p>But even if our application runs in a  replicated manner, we can’t ensure that all the replicas die at the same time without<a id="_idIndexMarker1071"/> configuring a <strong class="bold">Pod disruption policy</strong>. We can configure a minimum number of Pods to be available at the same time, ensuring not only resilience but also high availability. Our application will have some impact, but it will continue serving requests (<span class="No-Break">high availability).</span></p>
<p>To configure a disruption policy, we must use <strong class="source-inline">PodDisruptionBudget</strong> resources to provide the logic we need for our application. We will be able to set up the number of Pods that are required for our application workload under all circumstances by configuring the <strong class="source-inline">minAvailable</strong> or <strong class="source-inline">maxUnavailable</strong> keys. We can use integers (the number of Pods) or a percentage of the configured replicas. <strong class="source-inline">PodDisruptionBudget</strong> resources use selectors to choose between the Pods in the namespace (which we already use to create <a id="_idIndexMarker1072"/>Deployments, ReplicaSets, and more). The following code snippet shows <span class="No-Break">an example:</span></p>
<pre class="source-code">
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webserver-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: webserver</pre> <p>In this example, a minimum of two Pods with the <strong class="source-inline">app=webserver</strong> label are being monitored. We will define the number of replicas in our Deployment, but the <strong class="source-inline">PodDisruptionBudget</strong> resource will not allow us to scale down below two replicas. Therefore, two replicas will be running even if we decide to execute <strong class="source-inline">kubectl drain node1</strong> (assuming, in this example, that the <strong class="source-inline">webserver</strong> Deployment matches the <strong class="source-inline">app=webserver</strong> Pod’s labels and <strong class="source-inline">node1</strong> and <strong class="source-inline">node2</strong> have one replica each). <strong class="source-inline">PodDisruptionBudget</strong> resources are namespaced, so we can show all these resources in the namespace by executing <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">get poddisruptionbudgets</strong></span><span class="No-Break">.</span></p>
<p>In the following section, we will review some interesting ideas for solving common application architecture patterns using <span class="No-Break">Pod features.</span></p>
<h1 id="_idParaDest-198"><a id="_idTextAnchor215"/>Understanding advanced Pod application patterns</h1>
<p>In this section, we are<a id="_idIndexMarker1073"/> going to discuss some interesting patterns using simple Pods. All the patterns we are going to review are based on the special mechanisms offered by Kubernetes for sharing kernel namespaces in a Pod, which allow containers running inside to mount the same volumes and interconnect <span class="No-Break">via localhost.</span></p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor216"/>Init containers</h2>
<p>More than one Pod can run inside a container. Pods allow us to isolate different application processes that<a id="_idIndexMarker1074"/> we want to maintain separately<a id="_idIndexMarker1075"/> in different containers. This helps us, for example, to maintain different images that can be represented by separated code repositories and <span class="No-Break">build workflows.</span></p>
<p>Init containers run before the main application container (or containers, if we run more in parallel). These init containers can be used to set permissions on shared filesystems presented as volumes, create database schemas, or any other procedure that helps initialize our application. We can even use them to check dependencies before a process starts or even provision required files by retrieving them from an <span class="No-Break">external source.</span></p>
<p>We can define many init containers, and they will be executed in order, one by one, and all of them must end successfully before the actual application containers start. If any of the init containers fails, the Pod fails, although these containers don’t have associated probes for verifying their state. The processes executed by them must include verification if something <span class="No-Break">goes wrong.</span></p>
<p>It is important to understand that the total CPU and memory resources consumed by a Pod are calculated from the initialization of the Pod, hence init containers are checked. You keep the resource usage between the defined limits for your Pod (which includes the usage of all the containers running <span class="No-Break">in parallel).</span></p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor217"/>Sidecar containers</h2>
<p><strong class="bold">Sidecar containers</strong> run in <a id="_idIndexMarker1076"/>parallel with the main containers <a id="_idIndexMarker1077"/>without modifying their main behavior. These containers may change some of the features of the main containers without impacting their processes. We can use them, for example, to include a new container for monitoring purposes, present a component using a web server, sync content between application components, or even include some libraries in them to debug or instrument your processes. The new container can be injected into a running Deployment, triggering its Pods to recreate with this new item included, adding the desired feature. We can do this by patching (using <strong class="source-inline">kubectl patch</strong>) to modify the running Deployment <span class="No-Break">resource manifest.</span></p>
<p>Some modern monitoring applications, designed to integrate into Kubernetes, also use sidecar containers to deploy an application-specific monitoring component, which retrieves application <a id="_idIndexMarker1078"/>metrics and exposes them as a <span class="No-Break">new </span><span class="No-Break"><a id="_idIndexMarker1079"/></span><span class="No-Break">Service.</span></p>
<p>The next few patterns we are going to review are based on this sidecar <span class="No-Break">container concept.</span></p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor218"/>Ambassador containers</h2>
<p>The <strong class="bold">Ambassador</strong> applications pattern is <a id="_idIndexMarker1080"/>designed to offload common client connectivity tasks, helping<a id="_idIndexMarker1081"/> legacy applications implement more advanced features without changing any of their old code. With this design, we can improve the application’s routing, communications security, and resilience by adding additional load balancing, API gateways, and <span class="No-Break">SSL encryption.</span></p>
<p>We can deploy this pattern within Pods by adding special containers, designed for delivering light reverse-proxy features. In this way, Ambassador containers are used for deploying service mesh solutions, intercepting application process communications, and securing the interconnection with other application components by enforcing encrypted communications and managing application routes, among <span class="No-Break">other features.</span></p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor219"/>Adaptor containers</h2>
<p>The <strong class="bold">Adaptor</strong> container pattern<a id="_idIndexMarker1082"/> is used, for example, when <a id="_idIndexMarker1083"/>we want to include monitoring or retrieve logs from legacy applications without changing their code. To avoid this circumstance, we can include a second container in our application’s Pod to get the metrics or the logs from our application without modifying any of its original code. This also allows us to homogenize the content of a log or send it to a remote server. Well-prepared containers will redirect processes’ standard and error output to the foreground, and this allows us to review their log, but sometimes, the application can’t redirect the log or more than one log is created. We can unify them in one log or redirect their content by adding a second process (the Adaptor container), which formats (adding some custom columns, date format, and so on) and redirects the result to the standard output or a remote logging component. This method does not require special access to the host’s resources<a id="_idIndexMarker1084"/> and it may be transparent for <span class="No-Break">the application.</span></p>
<p><strong class="bold">Prometheus</strong> is a very popular open source monitoring solution and is extended in Kubernetes environments. Its main component will poll agent-like components and retrieve metrics from them, and it’s<a id="_idIndexMarker1085"/> very common to use this Adaptor container pattern to present the application’s metrics without modifying its standard behavior. These metrics will be exposed in the<a id="_idIndexMarker1086"/> application Pod in a different port, and the Prometheus server will connect to it to obtain <span class="No-Break">its metrics.</span></p>
<p>Let’s learn how containers’ health is verified by Kubernetes to decide the <span class="No-Break">Pod’s status.</span></p>
<h1 id="_idParaDest-203"><a id="_idTextAnchor220"/>Verifying application health</h1>
<p>In this section, we <a id="_idIndexMarker1087"/>are going to review how application Pods are considered healthy. Pods always start in the <strong class="source-inline">Pending</strong> state and continue to the <strong class="source-inline">Running</strong> state once the main container is considered healthy. If the Pod executes a Service process, it will stay in this <strong class="source-inline">Running</strong> state. If the Pod is associated with a Job resource, it may end successfully (the <strong class="source-inline">Succeeded</strong> state) or fail (the <span class="No-Break"><strong class="source-inline">Failed</strong></span><span class="No-Break"> state).</span></p>
<p>If we remove a Pod resource, it will go to <strong class="source-inline">Terminating</strong> until it is completely removed <span class="No-Break">from Kubernetes.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">If Kubernetes cannot retrieve the Pod’s status, its state will be <strong class="source-inline">Unknown</strong>. This is usually due to communication issues between the hosts’ kubelet and the <span class="No-Break">API server.</span></p>
<p>Kubernetes reviews the state of the containers to set the Pod’s state, and containers can be either <strong class="source-inline">Waiting</strong>, <strong class="source-inline">Running</strong>, or <strong class="source-inline">Terminated</strong>. We can use <strong class="source-inline">kubectl describe pod &lt;POD_NAME&gt;</strong> to review the details of these phases. Let’s quickly review <span class="No-Break">these states:</span></p>
<ul>
<li><strong class="source-inline">Waiting</strong> represents the state before <strong class="source-inline">Running</strong>, where all the pre-container-execution processes appear. In this phase, the container image is pulled from the registry and different volume mounts are prepared. If the Pod can’t run, we can have a <strong class="source-inline">Pending</strong> state, which will indicate a problem with deploying <span class="No-Break">the workload.</span></li>
<li><strong class="source-inline">Running</strong> indicates that the containers are running correctly, without <span class="No-Break">any issues.</span></li>
<li>The <strong class="source-inline">Terminated</strong> state is considered when the containers <span class="No-Break">are stopped.</span></li>
</ul>
<p>If the Pod was configured with a <strong class="source-inline">restartPolicy</strong> property of the <strong class="source-inline">Always</strong> or <strong class="source-inline">OnFailure</strong> type, all the <a id="_idIndexMarker1088"/>containers will be restarted on the node where they stopped. That’s why a Pod resource does not provide either high availability or resilience if the node <span class="No-Break">goes down.</span></p>
<p>Let’s review how the <a id="_idIndexMarker1089"/>Pod’s status is evaluated in these phases thanks to the execution <span class="No-Break">of </span><span class="No-Break"><strong class="bold">probes</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor221"/>Understanding the execution of probes</h2>
<p>The kubelet will execute probes<a id="_idIndexMarker1090"/> periodically by executing some code inside the containers or by directly executing network requests. Different probe types are available depending on the type of check we need for our <span class="No-Break">application’s components:</span></p>
<ul>
<li><strong class="source-inline">exec</strong>: This executes a command inside the container and the kubelet verifies whether this command <span class="No-Break">exits correctly.</span></li>
<li><strong class="source-inline">httpGet</strong>: This method is probably the most common as modern applications expose Services via the REST API. This check’s response must return 2XX or 3XX (<span class="No-Break">redirects) codes.</span></li>
<li><strong class="source-inline">tcpSocket</strong>: This probe is used to check whether the application’s port <span class="No-Break">is available.</span></li>
<li><strong class="source-inline">grpc</strong>: If our application is<a id="_idIndexMarker1091"/> consumed via modern <strong class="bold">Google Remote Procedure Calls</strong> (<strong class="bold">gRPCs</strong>), we can use this method to verify the <span class="No-Break">container’s state.</span></li>
</ul>
<p>Probes must return a valid value to consider the container healthy. Different probes can be executed one after another through the different phases of their lives. Let’s consider the different options available to verify whether the container’s processes are starting or serving the <span class="No-Break">application itself.</span></p>
<h3>Startup probes</h3>
<p><strong class="bold">Startup probes</strong> are the first in<a id="_idIndexMarker1092"/> the execution list if they are defined. They are executed when the container is started, and all other probes will wait until this probe ends successfully before they execute. If the probe fails, the kubelet will restart the Pod if it has defined <strong class="source-inline">Always</strong> or <strong class="source-inline">OnFailure</strong> in its <span class="No-Break"><strong class="source-inline">restartPolicy</strong></span><span class="No-Break"> key.</span></p>
<p>We will set up these probes if our processes take a lot of time before they are ready – for example, when we start a database server and it must manage previous transactions in its data before it is ready, or when our processes already integrate some sequenced checks before the final execution of the <span class="No-Break">main process.</span></p>
<h3>Liveness probes</h3>
<p><strong class="bold">Liveness probes</strong> check whether the container is running. If they fail, the kubelet will follow the <strong class="source-inline">restartPolicy</strong> value. They are <a id="_idIndexMarker1093"/>used when it’s hard to manage the failure of your main process within the process itself. It may be easier to integrate an external check via the <strong class="source-inline">livenessProbe</strong> key, which verifies whether or not the main process <span class="No-Break">is healthy.</span></p>
<h3>Readiness probes</h3>
<p><strong class="bold">Readiness probes</strong> are the final <a id="_idIndexMarker1094"/>probes in the sequence because they indicate whether or not the container is ready to accept requests. The Services that match this Pod in their Pod <strong class="source-inline">selector</strong> section will not mark this Pod as ready for requests until this probe ends successfully. The same happens when the probe fails; it will be removed from the list of available endpoints for the <span class="No-Break">Service resource.</span></p>
<p>Readiness probes are key to managing traffic to the Pods because we can ensure that the application component will correctly manage requests. This probe should always be set up to improve our <span class="No-Break">microservices’ interactions.</span></p>
<p>There are common keys that can be used at the <strong class="source-inline">spec.containers</strong> level that will help us customize the behavior of the different probe types presented. For example, we can configure the number of failed checks required to consider the probe as failed (<strong class="source-inline">failureThreshold</strong>) or the period between the execution of a probe type (<strong class="source-inline">periodSeconds</strong>). We can also configure some delay before any of these probes start by setting the <strong class="source-inline">initialDelaySeconds</strong> key, although it is recommended to understand how the application works and adjust the probes to fit our initial sequence. In the <em class="italic">Labs</em> section of this <a id="_idIndexMarker1095"/>chapter, we will review some of the probes we’ve <span class="No-Break">just discussed.</span></p>
<p>Now that we know how Kubernetes (the kubelet component) verifies the health of the Pods starting or running in the cluster, we must understand the <em class="italic">stop</em> sequence when they are considered <strong class="source-inline">Completed</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">Failed</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor222"/>Termination of Pods</h2>
<p>We can use the <strong class="source-inline">terminationGracePeriodSeconds</strong> key to set how much time the kubelet will wait if the Pod’s processes take a long time to end. When a Pod is deleted, the kubelet sends it a <strong class="source-inline">SIGTERM</strong> signal, but if it takes too long, the kubelet will send a <strong class="source-inline">SIGKILL</strong> signal to all container processes that are still alive when the <strong class="source-inline">terminationGracePeriodSeconds</strong> configured time is reached. This time threshold can also be<a id="_idIndexMarker1096"/> configured at the <span class="No-Break">probe level.</span></p>
<p>To remove a Pod immediately, we can force and change this Pod-level defined grace period by using <strong class="source-inline">kubelet delete pod &lt;POD_NAME&gt; --force</strong> along with <strong class="source-inline">--grace-period=0</strong>. Forcing the deletion of a Pod may result in unexpected consequences for your applications if you don’t understand how it works. The kubectl client sends the <strong class="source-inline">SIGKILL</strong> signal and doesn’t wait for confirmation, informing the API server that the Pod is already terminated. When the Pods are part of a StatefulSet, this may be dangerous as the Kubernetes cluster will try to execute a new Pod without confirming whether it has already been terminated. To avoid these situations, it is better to scale down to the replicas and scale up to do a <span class="No-Break">full restart.</span></p>
<p>Our applications may need to execute some specific processes to manage the interactions between different components when we update some of them, or even if they fail with an error. We can include some triggers when our containers start or stop – for example, to reconfigure a new master process in a <span class="No-Break">clusterized application.</span></p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor223"/>Container life cycle hooks</h2>
<p>Containers within a<a id="_idIndexMarker1097"/> Pod can include a <strong class="bold">life cycle hook</strong> in their specifications. Two types <span class="No-Break">are available:</span></p>
<ul>
<li><strong class="bold">PostStart</strong> hooks can be<a id="_idIndexMarker1098"/> used to execute a process <em class="italic">after</em> a container <span class="No-Break">is created.</span></li>
<li><strong class="bold">PreStop</strong> hooks are<a id="_idIndexMarker1099"/> executed <em class="italic">before</em> the container is terminated. The grace period starts when the kubelet receives a stop action, so this hook may be affected if the defined process takes <span class="No-Break">too long.</span></li>
</ul>
<p>Pods can be scaled up or<a id="_idIndexMarker1100"/> down manually whenever our application needs it and it’s supported, but we can go further and manage replicas automatically. The following section will show us how to make <span class="No-Break">it possible.</span></p>
<h1 id="_idParaDest-207"><a id="_idTextAnchor224"/>Resource management and scalability</h1>
<p>By default, Pods run without compute resource limits. This is fine for learning how your application behaves, and it can help you define its requirements <span class="No-Break">and limits.</span></p>
<p>Kubernetes cluster administrators can also define quotas that can be configured at different levels. It is usual to<a id="_idIndexMarker1101"/> define them at the namespace level and your applications will be confined with limits for CPU and memory. But these quotas can also identify some special resources, such as GPUs, storage, or even the number of resources that can be deployed in a namespace. In this section, we will learn how to limit resources in our Pods and containers, but you should always ask your Kubernetes administrators if any quota is applied at the namespace level to prepare your deployments for such compliance. More information about resource quota configurations can be found in the Kubernetes official <span class="No-Break">documentation: </span><a href="https://kubernetes.io/docs/concepts/policy/resource-quotas"><span class="No-Break">https://kubernetes.io/docs/concepts/policy/resource-quotas</span></a><span class="No-Break">.</span></p>
<p>We will use the <strong class="source-inline">spec.resources</strong> section to define the limits and requests associated with a Pod. Let’s look at how <span class="No-Break">they work:</span></p>
<ul>
<li><strong class="bold">Resource requests</strong> are used to<a id="_idIndexMarker1102"/> ensure workloads will run on the cluster. By setting the memory and CPU requests (<strong class="source-inline">spec.resources.requests.memory</strong> and <strong class="source-inline">spec.resources.requests.cpu</strong>, respectively), we can define the minimum resources required in any cluster host to run <span class="No-Break">our Pod.</span></li>
<li><strong class="bold">Resource limits</strong>, on the other <a id="_idIndexMarker1103"/>hand, are used to define the maximum resources allowed for a Pod. We will use <strong class="source-inline">spec.resources.limits.memory</strong> and <strong class="source-inline">spec.resources.limits.cpu</strong> to configure the maximum memory and number of CPUs <span class="No-Break">allocable, respectively.</span></li>
</ul>
<p>Resources can be defined either at the Pod or container level and they must be compliant with each other. The sum of all the container resource limits must not exceed the Pod values. If we omit the Pod resources, the sum of the defined container resources will be used. If any<a id="_idIndexMarker1104"/> of the containers do not contain a resource definition, the Pod limits and requests will be used. The container’s equivalent key <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">spec.containers[].resources</strong></span><span class="No-Break">.</span></p>
<p>Memory limits and requests will be configured in bytes and we can use suffixes such as <strong class="source-inline">ki</strong>, <strong class="source-inline">Mi</strong>, <strong class="source-inline">Gi</strong>, and <strong class="source-inline">Ti</strong> for multiples of 1,000, or <strong class="source-inline">k</strong>, <strong class="source-inline">M</strong>, and <strong class="source-inline">T</strong> for multiples of 1,024. For example, to specify a limit of 100 MB of memory, we will use <strong class="source-inline">100M</strong>. When the limited memory allowed is reached, <strong class="source-inline">OOMKiller</strong> will be triggered in the execution host and the Pod or container will <span class="No-Break">be terminated.</span></p>
<p>For the CPU, we will define the number of CPUs (it doesn’t matter whether they are physical or virtual) to be allowed or requested, if we are defining a request limit. When the CPU limit is reached, the container or Pod will not get more CPU resources, which will probably make your Pod to be considered unhealthy because checks will fail. CPU resources must be configured in either integers or fractionals, and we can add <strong class="source-inline">m</strong> as a suffix to represent millicores; hence, 0.5 CPUs can also be written as <strong class="source-inline">500m</strong>, and 0.001 CPUs will be represented <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">1m</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">When we are using Linux nodes, we can request and limit huge page resources, which allows us to define the page size for memory blocks allocated by the kernel. Specific key names must be used; for example, <strong class="source-inline">spec.resources.limits.hugepages-2Mi</strong> allows us to define the limit of memory blocks allocated for 2 MiB <span class="No-Break">huge pages.</span></p>
<p>Your administrators can prepare for some <strong class="source-inline">LimitRange</strong> resources, which will define constraints for the limits and requests associated with your <span class="No-Break">Pod resources.</span></p>
<p>Now that we know how we can limit and ask for resources, we can vertically scale a workload by increasing its limits. Horizontal scaling, on the other hand, will require the replication of Pods. We can now continue and learn how to dynamically and horizontally scale Pods related to a <a id="_idIndexMarker1105"/><span class="No-Break">running workload.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout"><strong class="bold">Vertical Pod autoscaling</strong> is also <a id="_idIndexMarker1106"/>available as a project inside Kubernetes. It is less popular because vertical scaling impacts your current Deployments or StatefulSets as it requires scaling the number of resources on your running replicas up or down. This makes them hard to apply and it is better to fine-grain resources in your applications and use horizontal Pod autoscaling, which does not modify current <span class="No-Break">replica specifications.</span></p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor225"/>Horizontal Pod autoscaling</h2>
<p><strong class="bold">HorizontalPodAutoscaler</strong> works as a <a id="_idIndexMarker1107"/>controller. It scales Pods up or down when the load associated with the <a id="_idIndexMarker1108"/>workload is increased or decreased. Autoscaling is only available for Deployments (by scaling and modifying their ReplicaSets) and StatefulSets. To measure the consumption of resources associated with a specific workload, we have to include a tool such as <strong class="bold">Kubernetes Metrics Server</strong> in our<a id="_idIndexMarker1109"/> cluster. This server will be used to manage the standard metrics. This can be easily deployed using its manifests at <a href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a>. It can also be executed as a pluggable add-on if you are using Minikube on your laptop or <span class="No-Break">desktop computer.</span></p>
<p>We will define a <strong class="source-inline">HorizontalPodAutoscaler</strong> (<strong class="source-inline">hpa</strong>) resource; the controller will retrieve and analyze the metrics for a workload resource specified in the <span class="No-Break"><strong class="source-inline">hpa</strong></span><span class="No-Break"> definition.</span></p>
<p>Different types of metrics can be used for the <strong class="source-inline">hpa</strong> resource, although the most common is the Pod’s <span class="No-Break">CPU consumption.</span></p>
<p>Metrics related to Pods can be defined and thus the controller checks their metrics and analyzes them using an algorithm that combines these metrics with cluster available resources and Pod states (<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details</a>) and then decides whether or not the associated resource (Deployment or StatefulSet) should <span class="No-Break">be scaled.</span></p>
<p>To define an <strong class="source-inline">hpa</strong> resource, we will set up a metric to analyze and a range of replicas to use (max and min replicas). When this value is reached, the controller reviews the current replicas, and if there’s still room for a new one, it will be created. <strong class="source-inline">hpa</strong> resources can be defined in either imperative or declarative format. For example, to manage a minimum of two Pods and a maximum of 10 when more than 50% of the CPU consumption is reached for the current Pods, we can use the <span class="No-Break">following syntax:</span></p>
<pre class="console">
kubectl autoscale &lt;RESOURCE_TYPE&gt; &lt;RESOURCE_NAME&gt; --cpu-percent=50 --min=2 --max=10</pre> <p>When a resource’s CPU consumption is more than 50%, then a replica is created, while one replica is decreased <a id="_idIndexMarker1110"/>when this metric is below that value; however, we will never execute more than 10 replicas or less <span class="No-Break">than two.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">We can review the manifest that’s used for creating any resource by adding <strong class="source-inline">–o yaml</strong>. The manifest will be presented and we will be able to verify its values. As an example, we can use <strong class="source-inline">kubectl autoscale deploy webserver --cpu-percent=50 --min=2 --max=10  -</strong><span class="No-Break"><strong class="source-inline">o yaml</strong></span><span class="No-Break">.</span></p>
<p class="callout">If we want to review values before creating the resource, we can add the <strong class="source-inline">--dry-run=client</strong> argument to only show the manifest, without actually creating <span class="No-Break">the resource.</span></p>
<p>As <strong class="source-inline">hpa</strong> resources are namespaced, we can get all the already deployed <strong class="source-inline">hpa</strong> resources by executing <strong class="source-inline">kubectl get </strong><span class="No-Break"><strong class="source-inline">hpa -A</strong></span><span class="No-Break">.</span></p>
<p>With that, we have seen how Kubernetes provides resilience, high availability, and autoscaling facilities out of the box by using specific resources. In the next section, we will learn how it also provides some interesting security features that will help us improve our <span class="No-Break">application security.</span></p>
<h1 id="_idParaDest-209"><a id="_idTextAnchor226"/>Improving application security with Pods</h1>
<p>In a Kubernetes cluster, we<a id="_idIndexMarker1111"/> can categorize the applications’ workloads<a id="_idIndexMarker1112"/> distributed cluster-wide as either privileged or unprivileged. Privileged workloads should always be avoided for normal applications unless they are strictly necessary. In this section, we will help you define the security of your applications by declaring your requirements in your <span class="No-Break">workload manifests.</span></p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor227"/>Security contexts</h2>
<p>In a security context, we <a id="_idIndexMarker1113"/>define the privileges and security configuration required for a Pod or the containers included in it. Security contexts allow us to configure the following <span class="No-Break">security features:</span></p>
<ul>
<li><strong class="source-inline">runAsUser</strong>/<strong class="source-inline">runAsGroup</strong>: These options manage the <strong class="source-inline">userID</strong> and <strong class="source-inline">groupID</strong> properties that run the main process with containers. We can add more groups by using the <span class="No-Break"><strong class="source-inline">supplementalGroups</strong></span><span class="No-Break"> key.</span></li>
<li><strong class="source-inline">runAsNonRoot</strong>: This key can control whether we allow the process to run <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">root</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">fsGroup</strong>/<strong class="source-inline">fsGroupChangePolicy</strong>: These options manage the permissions of the volumes included within a Pod. The <strong class="source-inline">fsGroup</strong> key will set the owner of the filesystems mounted as volumes and the owner of any new file. We can use <strong class="source-inline">fsGroupChangePolicy</strong> to only apply the ownership change if the permissions don’t match the <span class="No-Break">configured </span><span class="No-Break"><strong class="source-inline">fsGroup</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">seLinuxOptions</strong>/<strong class="source-inline">seccompProfile</strong>: These options allow us to overwrite default SELinux and <strong class="source-inline">seccomp</strong> settings by configuring special SELinux labels and a special <span class="No-Break"><strong class="source-inline">seccomp</strong></span><span class="No-Break"> profile.</span></li>
<li><strong class="source-inline">capabilities</strong>: Kernel capabilities can be added or removed (<strong class="source-inline">drop</strong>) to only allow specific kernel interactions (containers share the host’s kernel). You should avoid unnecessary capabilities in <span class="No-Break">your applications.</span></li>
<li><strong class="source-inline">privileged</strong>/<strong class="source-inline">AllowPrivilegeEscalation</strong>: We can allow processes inside a container to be executed as <strong class="source-inline">privileged</strong> (with all the capabilities) by setting the <strong class="source-inline">privileged</strong> key to <strong class="source-inline">true</strong> or to be able to gain privileges, even if this key was set to <strong class="source-inline">false</strong>, by setting <strong class="source-inline">AllowPrivilegeEscalation</strong> to <strong class="source-inline">true</strong>. In this case, container processes do not have all capabilities but they will allow internal processes to run as if they had the <span class="No-Break"><strong class="source-inline">CAP_SYS_ADMIN</strong></span><span class="No-Break"> capability.</span></li>
<li><strong class="source-inline">readOnlyRootFilesystem</strong>: It is always a very good idea to run your containers with their root filesystem in read-only mode. This won’t allow processes to make any changes in the container. If you understand the requirements of<a id="_idIndexMarker1114"/> your application, you will be able to identify any directory that may be changed and add an appropriate volume to run your processes correctly. It is quite usual, for example, to add <strong class="source-inline">/tmp</strong> as a separate temporal <span class="No-Break">filesystem (</span><span class="No-Break"><strong class="source-inline">emptyDir</strong></span><span class="No-Break">).</span></li>
</ul>
<p>Some of these keys are available at the container or Pod level or both. Use <strong class="source-inline">kubectl explain pod.spec.securityContext</strong> or <strong class="source-inline">kubectl explain pod.spec.containers.securityContext</strong> to retrieve a detailed list of the options available in each scope. You have to be aware of the scope that’s used because Pod specifications apply to all containers unless the same key exists under the container scope – in which case, its value will <span class="No-Break">be used.</span></p>
<p>Let’s review the best settings we can prepare to improve our <span class="No-Break">application security.</span></p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor228"/>Security best practices</h2>
<p>The following list shows some<a id="_idIndexMarker1115"/> of the most used settings for improving security. You, as a developer, can improve your application security if you ensure the following security measures can be enabled for <span class="No-Break">your Pods:</span></p>
<ul>
<li><strong class="source-inline">runAsNonRoot</strong> must always be set to <strong class="source-inline">true</strong> to avoid the use of <strong class="source-inline">root</strong> on your containers. Ensure you also configure <strong class="source-inline">runAsUser</strong> and <strong class="source-inline">runAsGroup</strong> to IDs greater than <strong class="source-inline">1000</strong>. Your Kubernetes administrators can suggest some IDs for your application. This will help control application <span class="No-Break">IDs cluster-wide.</span></li>
<li>Always drop all capabilities and enable only those required by <span class="No-Break">your application.</span></li>
<li>Never use privileged containers for your applications unless it is strictly necessary. Usually, only monitoring- or kernel-related applications require <span class="No-Break">special privileges.</span></li>
<li>Identify the filesystem’s requirement for your application and always set <strong class="source-inline">readOnlyRootFilesystem</strong> to <strong class="source-inline">true</strong>. This simple setting improves security, disabling any unexpected changes. Required filesystems can be mounted as volumes (many options are available, as we will learn in <a href="B19845_10.xhtml#_idTextAnchor231"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Leveraging Application Data Management </em><span class="No-Break"><em class="italic">in Kubernetes</em></span><span class="No-Break">).</span></li>
<li>Ask your Kubernetes administrators whether there are some SELinux settings you should consider to apply them on your Pods. This also applies to <strong class="source-inline">seccomp</strong> profiles. Your administrators may have configured a default profile. Ask your administrators about this situation to avoid any system <span class="No-Break">call issues.</span></li>
<li>Your administrators<a id="_idIndexMarker1116"/> may have been using tools such as Kyverno or OPA Gatekeeper to improve cluster security. In these cases, they can enforce <a id="_idIndexMarker1117"/>security context settings by using <strong class="bold">admission controllers</strong> in the Kubernetes cluster. The use of these features is outside the scope of this book but you may ask your administrators about the compliance rules required to execute applications in your <span class="No-Break">Kubernetes platform.</span></li>
</ul>
<p>In the next section, we will review how to implement some of the Kubernetes features we learned about in this chapter by preparing the multi-component application we used in previous chapters (<a href="B19845_05.xhtml#_idTextAnchor118"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Creating Multi-Container Applications</em>, and <a href="B19845_07.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Orchestrating with Swarm</em>) to run <span class="No-Break">on Kubernetes.</span></p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor229"/>Labs</h1>
<p>This section will show you how to deploy the <strong class="source-inline">simplestlab</strong> three-tier application in Kubernetes. Manifests for all its components have been prepared for you while following the techniques and <a id="_idIndexMarker1118"/>Kubernetes resources explained in this chapter. You will be able to verify the usage of the different options and you will able to play with them to review the content and best practices described in <span class="No-Break">this chapter.</span></p>
<p>The code for these labs is available in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git">https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git</a>. Ensure you have the latest revision available by simply executing <strong class="source-inline">git clone </strong><a href="https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git">https://github.com/PacktPublishing/Containers-for-Developers-Handbook.git</a> to download all its content, or <strong class="source-inline">git pull</strong> if you’ve already downloaded the repository before. All the manifests and the steps required for running <strong class="source-inline">simplestlab</strong> are located inside the <span class="No-Break"><strong class="source-inline">Containers-for-Developers-Handbook/Chapter9</strong></span><span class="No-Break"> directory.</span></p>
<p>In the labs in GitHub, we will deploy the <strong class="source-inline">simplestlab</strong> application, which is used in previous chapters, on Kubernetes by defining appropriate <span class="No-Break">resource manifests:</span></p>
<ul>
<li>The <strong class="bold">database</strong> component will be deployed using a <span class="No-Break">StatefulSet resource</span></li>
<li>The <strong class="bold">application backend</strong> component will be deployed using a <span class="No-Break">Deployment resource</span></li>
<li>The <strong class="bold">load balancer</strong> (or <strong class="bold">presenter</strong>) component will be deployed using a <span class="No-Break">DaemonSet resource</span></li>
</ul>
<p>In their manifests, we have<a id="_idIndexMarker1119"/> included some of the mechanisms we learned about in this chapter for checking the component’s health, replicating their processes, and improving their security by disallowing their execution as the root user, among other features. Let’s start by reviewing and deploying the <span class="No-Break">database component:</span></p>
<ol>
<li>We will use a StatefulSet to ensure that replicating its processes (scaling up) will never represent a problem to our data. It is important to understand that a new replica starts empty, without data, and joins the pool of available endpoints for the Service, which will probably be a problem. This means that in these conditions, the Postgres database isn’t scalable, so this component is deployed as a StatefulSet to preserve its data even in the case of a manual replication. This example only provides resilience, so do not scale this component. If you need to deploy a database with high availability, you will need a distributed database such as MongoDB. The full manifest for the database manifest can be found in <strong class="source-inline">Chapter9/db.satatefulset.yaml</strong>. Here is a small extract from <span class="No-Break">this file:</span><pre class="source-code">
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: db
  labels:
    component: db
    app: simplestlab
spec:
  replicas: 1
  selector:
    matchLabels:
      component: db
      app: simplestlab
  template:
    metadata:
      labels:
        component: db
        app: simplestlab
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 10000
        runAsGroup: 10000
        fsGroup: 10000
...
  volumeClaimTemplates:
  - metadata:
      name: postgresdata
    spec:
      accessModes: [ "ReadWriteOnce" ]
      #storageClassName: "csi-hostpath-sc"
      resources:
        requests:
          storage: 1Gi</pre><p class="list-inset">Here, we defined a template for the Pods to create and a separate template for the VolumeClaims (we will talk about them in <a href="B19845_10.xhtml#_idTextAnchor231"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>). This ensures that each Pod will get its<a id="_idIndexMarker1120"/> own volume. The volume that’s created will be mounted in the database container as the <strong class="source-inline">/data</strong> filesystem and its size will be 1,000 MB (1 Gi). No other container is created. The <strong class="source-inline">POSTGRES_PASSWORD</strong> and <strong class="source-inline">PGDATA</strong> environment variables are set and passed to the container. They will be used to create the password for the Postgres user and the patch for the database data. The image that’s used for the container is <strong class="source-inline">docker.io/frjaraur/simplestdb:1.0</strong> and port <strong class="source-inline">5432</strong> will be used to expose its Service. Pods only expose their Services internally, in the Kubernetes network, so you will never be able to reach these Services from remote clients. We specified one replica and the controller will associate the pods with this StatefulSet by searching for Pods with <strong class="source-inline">component=db</strong> and <strong class="source-inline">app=simplestlab</strong> labels. We simplified the database’s probes by just checking a TCP connection to port <strong class="source-inline">5432</strong>. We defined a security context at the Pod’s level, which will apply to all the containers <span class="No-Break">by default:</span></p><pre class="source-code">     securityContext:
        runAsNonRoot: true
        runAsUser: 10000
        runAsGroup: 10000
        fsGroup: 10000
        fsGroupChangePolicy: OnRootMismatch</pre></li> <li>The database processes will run as <strong class="source-inline">10000:10000</strong> <strong class="source-inline">user:group</strong>, hence they are secure (no root is required). We could have gone further if we set the container as read-only but in this case, we didn’t as Docker’s official Postgres image; however, it would have been better to use a full <span class="No-Break">read-only filesystem.</span><p class="list-inset">The Pod will get an IP address, though this may change if the Pod is recreated for any reason, which makes Pods’ IP addresses impossible to use in such dynamic environments. We will use a Service to associate a <em class="italic">fixed</em> IP address with a Service and then<a id="_idIndexMarker1121"/> with the endpoints of the Pods related to <span class="No-Break">the Service.</span></p></li>
<li>The following is an extract from the Service manifest (you will find it <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">Chapter9/db.service.yaml</strong></span><span class="No-Break">):</span><pre class="source-code">
apiVersion: v1
kind: Service
metadata:
  name: db
spec:
  clusterIP: None
  selector:
    component: db
    app: simplestlab
 ...</pre><p class="list-inset">This Service is associated with the Pods by using a selector (the <strong class="source-inline">components=db</strong> and <strong class="source-inline">app=simplestlab</strong> labels) and Kubernetes will route the traffic to the appropriate Pods. When a TCP packet reaches the Service’s port, <strong class="source-inline">5432</strong>, it is load balanced to all the available Pod’s endpoints (in this case, we will just have one replica) in port <strong class="source-inline">5432</strong>. In both cases, we used port <strong class="source-inline">5432</strong>, but you must understand that <strong class="source-inline">targetPort</strong> refers to the container port, while the port key refers to the Service’s port, and they can be completely different. We are using a headless Service because it works very well with StatefulSets and their resolution in <span class="No-Break">round-robin mode.</span></p></li> <li>With the StatefulSet definition and the Service, we can deploy the <span class="No-Break">database component:</span><pre class="source-code">
<strong class="bold">PS Chapter9&gt; kubectl create -f .\db.statefulset.yaml</strong>
<strong class="bold">statefulset.apps/db created</strong>
<strong class="bold">PS Chapter9&gt; kubectl create -f .\db.service.yaml</strong>
<strong class="bold">service/db created</strong>
<strong class="bold">PS Chapter9&gt; kubectl get pods</strong>
<strong class="bold">NAME   READY   STATUS    RESTARTS   AGE</strong>
<strong class="bold">db-0   1/1     Running   0          17s</strong></pre><p class="list-inset">We can now continue<a id="_idIndexMarker1122"/> and review the <span class="No-Break"><strong class="source-inline">app</strong></span><span class="No-Break"> component.</span></p></li> <li>The application (backend component) is deployed as a <strong class="source-inline">Deployment</strong> workload. Let’s see an extract of <span class="No-Break">its manifest:</span><pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  labels:
    component: app
    app: simplestlab
spec:
  replicas: 3
  selector:
    matchLabels:
      component: app
      app: simplestlab
  template:
    metadata:
      labels:
        component: app
        app: simplestlab
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001</pre><p class="list-inset">You can find the complete manifest in the <span class="No-Break"><strong class="source-inline">Chapter9/app.deployment.yaml</strong></span><span class="No-Break"> file.</span></p></li> <li>For this component, we<a id="_idIndexMarker1123"/> defined three replicas, so three Pods will be deployed cluster-wide. In this component, we are using the <strong class="source-inline">docker.io/frjaraur/simplestapp:1.0</strong> image. We’ve configured two security contexts, one at the <span class="No-Break">Pod’s level:</span><pre class="source-code">
     securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001</pre><p class="list-inset">The second is for enforcing the use of a read-only filesystem for <span class="No-Break">the container:</span></p><pre class="source-code">       securityContext:
          readOnlyRootFilesystem: true</pre></li> <li>Here, we prepared <strong class="source-inline">readinessProbe</strong> using <strong class="source-inline">httpGet</strong> but we still keep <strong class="source-inline">tcpSocket</strong> for <strong class="source-inline">livenessProbe</strong>. We coded <strong class="source-inline">/healthz</strong> as the application’s health endpoint for checking <span class="No-Break">its healthiness.</span></li>
<li>In this component, we<a id="_idIndexMarker1124"/> added a resource section for the <span class="No-Break">app container:</span><pre class="source-code">
       resources:
          requests:
            cpu: 10m
            memory: 20M
          limits:
            cpu: 20m
            memory: 30Mi</pre><p class="list-inset">In this case, we asked Kubernetes for at least 10 millicores of CPU and 20M of memory. The <strong class="source-inline">limits</strong> section describes the maximum CPU (20 millicores) and memory (30Mi). If the memory limit is reached, Kubelet will trigger the OOM-Killer procedure and it will kill the container. When the CPU limit is reached, the kernel does not provide more CPU cycles to the container, which may lead the probes to fail and hence the container will die. This component is stateless and it is running completely in <span class="No-Break">read-only mode.</span></p></li> </ol>
<p class="callout-heading">Important note</p>
<p class="callout">In the full YAML file manifest, you will see that we are using the environment variables for passing sensitive data. Always avoid passing sensitive data in environment variables as anyone with access to your manifest files will be able to read it. We will learn how to include sensitive data in <a href="B19845_10.xhtml#_idTextAnchor231"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Leveraging Application Data Management </em><span class="No-Break"><em class="italic">in Kubernetes</em></span><span class="No-Break">.</span></p>
<ol>
<li value="9">We will also add a Service for accessing the <strong class="source-inline">app</strong> <span class="No-Break"><strong class="source-inline">Deployment</strong></span><span class="No-Break"> workload:</span><pre class="source-code">
apiVersion: v1
kind: Service
metadata:
  name: app
spec:
  selector:
    app: simplestlab
    component: app
  ports:
    - protocol: TCP
      port: 3000
      targetPort: 3000</pre></li> <li>We create both<a id="_idIndexMarker1125"/> <span class="No-Break">Kubernetes resources:</span><pre class="source-code">
<strong class="bold">PS Chapter9&gt; kubectl create -f .\app.deployment.yaml `</strong>
<strong class="bold">-f .\app.service.yaml</strong>
<strong class="bold">deployment.apps/app created</strong>
<strong class="bold">service/app created</strong>
<strong class="bold">PS Chapter9&gt; kubectl get pods</strong>
<strong class="bold">NAME                  READY   STATUS    RESTARTS   AGE</strong>
<strong class="bold">db-0                  1/1     Running   0          96s</strong>
<strong class="bold">app-585f8bb87-r8dqh   1/1     Running   0          41s</strong>
<strong class="bold">app-585f8bb87-wsfm7   1/1     Running   0          41s</strong>
<strong class="bold">app-585f8bb87-t5gpx   1/1     Running   0          41s</strong></pre><p class="list-inset">We can now continue with the <span class="No-Break">frontend component.</span></p></li> <li>For the frontend component, we will deploy NGINX on each Kubernetes cluster node. In this case, we are using a DaemonSet to run the component cluster-wide; one replica will be deployed on each node. We will prepare this component to also run as a non-root user, so some special configurations are needed. Here, we prepared <strong class="source-inline">configMap</strong> with these special configurations for NGINX; you will find it as <strong class="source-inline">Chapter9/ lb.configmap.yaml</strong>. This configuration will allow us to run as user <strong class="source-inline">101</strong> (<strong class="source-inline">nginx</strong>). We created this configMap before<a id="_idIndexMarker1126"/> the actual DaemonSet; although it is possible to do the opposite, it is important to understand the requirements and prepare them before the workload deployment. This configuration allows us to run NGINX as non-root on a port greater than <strong class="source-inline">1024</strong> (system ports). We will use port <strong class="source-inline">8080</strong> to publish the <span class="No-Break"><strong class="source-inline">loadbalancer</strong></span><span class="No-Break"> component.</span><p class="list-inset">Notice that we added a <strong class="source-inline">proxy_pass</strong> sentence to reroute the requests for <strong class="source-inline">/</strong> to <strong class="source-inline">http://app:3000</strong>, where <strong class="source-inline">app</strong> is the Service’s name, resolved via internal DNS. We will use <strong class="source-inline">/healthz</strong> to check the <span class="No-Break">container’s healthiness.</span></p></li>
<li>Let’s see an extract from the <span class="No-Break">DaemonSet manifest:</span><pre class="source-code">
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: lb
  labels:
    component: lb
    app: simplestlab
        image: docker.io/nginx:alpine
        ports:
        - containerPort: 8080
        securityContext:
          readOnlyRootFilesystem: true
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: tmp
          mountPath: /tmp/nginx
        - name: conf
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
...
      volumes:
      - name: cache
        emptyDir: {}
      - name: tmp
        emptyDir: {}
      - name: conf
        configMap:
          name: lb-config</pre><p class="list-inset">Notice that we added <strong class="source-inline">/var/cache/nginx</strong> and <strong class="source-inline">/tmp</strong> as <strong class="source-inline">emptyDir </strong>volumes, as mentioned<a id="_idIndexMarker1127"/> previously. This component will be also stateless and run in read-only mode, but some temporal directories must be created as <strong class="source-inline">emptyDir</strong> volumes so that they can be written to without allowing the full <span class="No-Break">container’s filesystem.</span></p></li> <li>The following security contexts <span class="No-Break">are created:</span><ul><li>At the <span class="No-Break">Pod level:</span><pre class="source-code">
     securityContext:
        runAsNonRoot: true
        runAsUser: 101
        runAsGroup: 101</pre></li><li>At the <span class="No-Break">container level:</span><pre class="source-code">       securityContext:
          readOnlyRootFilesystem: true</pre></li></ul></li> <li>Finally, we <a id="_idIndexMarker1128"/>have the <strong class="source-inline">Service</strong> definition, where we will use a <strong class="source-inline">NodePort</strong> type to quickly expose <span class="No-Break">our application:</span><pre class="source-code">
apiVersion: v1
kind: Service
metadata:
  name: lb
spec:
  type: NodePort
  selector:
    app: simplestlab
    component: lb
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
      nodePort: 32000</pre></li> <li>Now, let’s deploy all the <strong class="source-inline">lb</strong> component (<span class="No-Break">frontend) manifests:</span><pre class="source-code">
<strong class="bold">PS Chapter9&gt; kubectl create -f .\lb.configmap.yaml</strong>
<strong class="bold">configmap/lb-config created</strong>
<strong class="bold">PS Chapter9&gt; kubectl create -f .\lb.daemonset.yaml</strong>
<strong class="bold">daemonset.apps/lb created</strong>
<strong class="bold">PS Chapter9&gt; kubectl create -f .\lb.service.yaml</strong>
<strong class="bold">service/lb created</strong></pre><p class="list-inset">Now, we can reach our application in any Kubernetes cluster host’s port <strong class="source-inline">32000</strong>. Your browser should <a id="_idIndexMarker1129"/>access the application and show something like this (if you’re using Docker Desktop, you will need to <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">http://localhost:32000</strong></span><span class="No-Break">):</span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 9.9 – simplestlab application web GUI" height="781" src="image/B19845_09_9.jpg" width="1133"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – simplestlab application web GUI</p>
<p>You can find additional <a id="_idIndexMarker1130"/>steps for scaling up and down the application backend component in the <strong class="source-inline">Chapter9</strong> code repository. The labs included in this chapter will help you understand how to deploy an application using different <span class="No-Break">Kubernetes resources.</span></p>
<h1 id="_idParaDest-213"><a id="_idTextAnchor230"/>Summary</h1>
<p>In this chapter, we learned about the resources that can help us deploy application workloads in Kubernetes. We took a look at the different options for running replicated processes and verifying their health to provide resilience, high availability, and auto-scalability. We also learned about some of the Pod features that can help us implement advanced patterns and improve the overall application security. We are now ready to deploy our application using the best patterns and apply and customize the resources provided by Kubernetes, and we know how to implement appropriate health checks while limiting resource consumption in <span class="No-Break">our platform.</span></p>
<p>In the next chapter, we will deep dive into the options we have for managing data within Kubernetes and presenting it to <span class="No-Break">our applications.</span></p>
</div>
</div></body></html>