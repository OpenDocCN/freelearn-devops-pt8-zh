- en: Big Data Storage - Azure Data Lake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we have to store unlimited amounts of data. That scenario covers
    most big data platforms, where having even a soft limit for the maximum capacity
    could cause problems with the active development and maintenance of our application.
    Thanks to Azure Data Lake, we have limitless possibilities when it comes to storing
    both structured and unstructured data, all with an efficient security model and
    great performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Data Lake Store fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data in Azure Data Lake Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security features and concerns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for working with Azure Data Lake Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform the exercises in this chapter, you will need:'
  prefs: []
  type: TYPE_NORMAL
- en: Access to an Azure subscription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Azure Data Lake Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When considering your storage solution, you have to take into account the amount
    of data you want to store. Depending on your answer, you may choose a different
    option from services available in Azure—Azure Storage, Azure SQL, or Azure Cosmos
    DB. There is also a variety of databases available as images for VMs (such as
    Cassandra or MongoDB); the ecosystem is quite rich so everyone can find what they
    are looking for. The problem arises when you do not have an upper limit for the
    amount of data stored or, considering the characteristics of today's applications,
    that amount grows so rapidly that there is no possibility to declare a safe limit,
    which we will never hit. For those kinds of scenario, there is a separate kind
    of storage named Data Lakes. They allow you to store data in its natural format,
    so it does not imply any kind of structure over information stored. In Azure,
    a solution for that kind of problem is named Azure Data Lake Store; in this chapter,
    you will learn the basics of this service, which allows you to dive deeper into
    the service and adjust it to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Data Lake Store fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Azure Data Lake Store is called a hyper-scale repository for data for a reason—there
    is no limit when it comes to storing files. It can have any format, be any size,
    and store information structured differently. This is also a great model for big
    data analytics as you can store files in the way that is the best for your processing
    services (some prefer a small number of big files, some prefer many small files
    – choose what suits you the most). This is not possible for other storage solutions
    such as relational, NoSQL, or graph databases, as they always have some restrictions
    when it comes to saving unstructured data. Let''s check an example comparison
    between Azure Data Lake Store and Azure Storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **AZDS** | **Azure Storage** |'
  prefs: []
  type: TYPE_TB
- en: '| **Limits** | No file size/number of files limits | Maximum account capacity
    of 500 TBs, the maximum size of files |'
  prefs: []
  type: TYPE_TB
- en: '| **Redundancy** | LRS | LRS/ZRS/GRS/RA-GRS |'
  prefs: []
  type: TYPE_TB
- en: '| **API** | WebHDFS | Azure Blob Storage API |'
  prefs: []
  type: TYPE_TB
- en: 'The important thing here is the redundancy—for now, the only model which Azure
    Data Lake Store supports is LRS. That means that, in the event of a disaster,
    you may lose data stored inside a single data center. To avoid that, you will
    have to implement your own policy to copy data to a replica. In fact, you have
    available two models—synchronous replication, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e7bb730-f268-497e-afff-f01ac1910c0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or you have asynchronous, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4b4b501-74a0-4c00-9593-0513251b8e01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are some obvious pros and cons of both solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronous**: Ensures that a copy of data was saved to a replica, more difficult
    to handle when considering duplicates, and lower performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronous**: Data can be lost (because you will not move data to a replica
    before a disaster), better performance (because you just save without waiting
    for replication), and easier to handle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While replication may look better for Azure Storage, remember that the Azure
    Data Lake Store filesystem is based on HDFS—this allows for a seamless integration
    with many OSS tools, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Pig
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And many more...!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gives you a much better ecosystem, tool-wise. If you want to store data
    inside Azure Data Lake Store and prefer to use HDInsights to perform analysis
    and transformations over your files, instead of other Azure tools, you can easily
    connect to your instance and start working on them.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for now, ADLS supports HDInsight 3.2, 3.4, 3.5, and 3.6 distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to accessing files stored inside an instance of Azure Data Lake
    Store, it leverages the POSIX-style permissions model; you basically operate on
    three different permissions, which can be applied to a file or a folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Read (R)**: For reading data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write (W)**: For writing data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execute (E)**: Applicable to a folder, used to give read/write permissions
    in a folder context (such as creating children or listing files)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover more security concepts in the security section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Azure Data Lake Store instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create an Azure Data Lake Store instance, you will need to search for `Azure
    Data Lake`in the portal and fill in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfd77d09-ca4d-4a30-b11b-9895e7882c57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, you need to take into consideration the following facts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Location: Currently there are four different locations available—Central US,
    East US 2, North Europe, and West Europe. Do remember that transferring data between
    DCs costs you extra money, so if you plan to use this service, plan your architecture
    carefully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pricing package: There are two pricing models available—Pay-as-You-Go and fixed
    Monthly commitment. They have pros and cons (fixed pricing is in general cheaper
    but it is not that flexible when your application grows, it is difficult sometimes
    to plan required capacity ahead), so try to understand as best you can the characteristics
    of your applications using that service to choose whatever suits you the most.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encryption settings: By default, encryption of your data is Enabled for a new
    account. While it is possible to disable it, in most cases you will stay with
    the default settings. What is more, there are two models of encryption—either
    you let the service  manage encryption keys for you, or you provide your own keys
    (stored inside Azure Key Vault).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since it is a good idea to rotate encryption keys, you may face the issue when,
    due to a failure, even redundant copies of your data are inaccessible. While it
    is possible to recover from backup data, you will need an old key to decrypt it.
    Because of that, it is advisable to store a copy of old keys in case of unexpected
    outages.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you click on the Create button, your service will be provisioned—you can
    access it to see the overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/782a7941-5ce3-4aea-9220-ffdeb94d048a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since it is freshly created, we cannot see different metrics which describe
    how much data we are storing. What is more, the current cost is 0 USD—this is,
    of course, something we expected as no file was uploaded to the service. From
    the UI perspective, there is not much that we can do for now; some additional
    features such as Firewall will be described later in that chapter. Besides the
    portal, you can also easily access your instance of Azure Data Lake Store by using
    Microsoft Azure Storage Explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a023fcf9-5bae-4f31-b7f1-2f872bb76899.png)'
  prefs: []
  type: TYPE_IMG
- en: It makes things much easier when you have multiple files and folders and you
    try to navigate through them.
  prefs: []
  type: TYPE_NORMAL
- en: Storing data in Azure Data Lake Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because Azure Data Lake Store is all about storing data, in this section of
    the chapter you will see how you can store different files, use permissions to
    restrict access to them, and organize your instance. The important thing to remember
    here is the fact that you are not limited to using big data tools to store or
    access data stored within a service—if you manage to communicate with the Azure
    Data Lake Store protocol, you can easily operate on files using C#, JavaScript,
    or any other kind of programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Azure portal to navigate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started with working with files in the Azure portal, you will have to
    click on the Data explorerbutton:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14d96ccb-3be3-409b-a16d-10fb860d12af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click on it, you will see a new screen, where you are given many different
    options for creating a folder, uploading files, or changing access properties.
    While this tool is not the best way to manage thousands of files, it gives you
    some insight into what is stored and how:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc95ca29-aa07-461b-bd70-cf4fbaf0f1ca.png)'
  prefs: []
  type: TYPE_IMG
- en: The downside of the UI available in the portal is the fact that it has a tendency
    to hang, especially if you have hundreds of files. Some options (such as deleting
    a folder) also tend to fail if you have stored gigabytes of data. In that scenario,
    it is better to either use PowerShell or custom procedures to perform an operation.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will discuss options available on the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you click on the Filter button, you will see a screen that tells you what
    files you are interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c4d2f79-0835-4149-af31-6bc499ef594d.png)'
  prefs: []
  type: TYPE_IMG
- en: It is the easiest way to quickly limit files displayed within a folder, but
    of course it has some caveats—for example, you cannot use a wildcard to filter
    only a specific file extension.
  prefs: []
  type: TYPE_NORMAL
- en: To remove a filter, click on the Reset button on the Filter screen.
  prefs: []
  type: TYPE_NORMAL
- en: New folder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This simple option gives you the possibility to create a new folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15eee2c6-2190-4bd4-b9e3-7f94d401553b.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that by default a new folder can be accessed only by you—to make it visible
    to others (and to allow them to read it), you will have to assign a particular
    group of users explicitly to it.
  prefs: []
  type: TYPE_NORMAL
- en: Upload
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the Upload function, you can upload files directly from your local machine
    to the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e65e259-6129-43b9-a178-ee3f3b71cc19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The files you choose will be uploaded to the folder you are currently browsing.
    There is also the possibility of allowing overwriting existing files; if you decide
    not to do so and upload a duplicate, you will see the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c921855-8494-4b9f-b8eb-524a6fed15ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most important features of Azure Data Lake Store is the ability
    to fully declare access to a specific resource stored inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d36c0bb8-9efa-47c1-84c0-defca584e7e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By default, only you can access a file or a folder. To add a new user or a
    group, you can click on the + Addbutton:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71b4f4a8-fb81-43cc-8957-3b8f5fbb14ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are important things I would like to cover here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Permissions: Remember that to grant somebody access to list files inside a
    folder, you will have to assign two permissions: Read and Execute. The same applies
    to creating children inside a folder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add to: It is possible to propagate a particular set of permissions, not only
    to a single folder but also to all folders inside it. This is especially helpful
    when you can quickly allow somebody to list files and folders inside some parent
    directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add as: You can add a set of permissions, either as a default permission entry
    (which will be assigned by default to all other users of a folder) or as an access
    entry (which specifies how somebody can access it). You can also combine both
    to speed things up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Files and folders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next to each file and folder visible, you can see an icon, which displays a
    menu with additional options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/859cc1cb-fc6c-4335-8e2c-26204d145102.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, these are the same options we have just covered—they just apply to
    a specific folder and file.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure Storage Explorer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the preceding options can be performed using Microsoft Azure Storage
    Explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7acd5e9-c1d3-43e3-b1f6-64945ffec5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Unfortunately, it does not give you the possibility to assign permissions to
    files and folders. It is, however, a much better option for browsing stored data—what
    is more it automatically displays a set of required permissions assigned to an
    item.
  prefs: []
  type: TYPE_NORMAL
- en: Using SDKs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most flexible (and the most advanced) option to manage files and your Azure
    Data Lake Store instance is using an SDK for a language you are using. Currently,
    there are three different languages officially supported:'
  prefs: []
  type: TYPE_NORMAL
- en: .NET platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is also the possibility of using a REST API, so basically you can connect
    with it using any language you want. Here, The following code snippet allows you
    to connect to the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two options available when it comes to authenticating to connect
    to your service:'
  prefs: []
  type: TYPE_NORMAL
- en: End-user authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service-to-service authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both scenarios are described in detail in the documentation available in the *Further
    reading*section. Whichever option you choose, you will end up with using a generated
    OAuth 2.0 token. Here, you can find a simple provider to a service that leverages
    the described methods and allows you to easily create a new folder and append
    data to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can read more about writing such a provider in the blog post mentioned in
    the *Further reading*section.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing about using SDKs is the ability to abstract many operations
    and automate them—you can easily delete files recursively or dynamically create
    them. Such operations are unavailable when using UIs and most serious project
    developers would rather code stuff than rely on manual file management.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Data Lake Store offers a bit of a different security model than other
    storage options available for Azure. In fact, it offers you a complex solution
    that consists of authentication, authorization, network isolation, data protection,
    and auditing. As it is designed to be the very base of data-driven systems, it
    has to extend common capabilities when it comes to securing who (or what) and
    how to access information stored. In this section, we will cover different security
    features available and describe them in detail, so you are familiar with them
    and know how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and authorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To authenticate who or what can access data stored, Azure Data Lake Store uses Azure
    Active Directory to know what the current entity accessing data is. To authorize
    it, it leverages both **role-based access control**(**RBAC**), to secure the resource
    itself, and POSIX ACL to secure data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand the distinction between these two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authentication**: This determines who or what tries to access a particular
    resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authorization**: This secures a resource by limiting access to it to those
    who have been assigned a particular set of permissions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to remember that if you have multiple subscriptions hosting
    different resources that would like to access Azure Data Lake Store, you have
    to assign the same Azure AD instance to all of them—if you fail to do so, some
    will not be able to access data, as only users and services defined within a directory
    assigned to ADLS can be authenticated and given access to it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's check the difference between the RBAC and POSIX models.
  prefs: []
  type: TYPE_NORMAL
- en: RBAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RBAC controls who can access an Azure resource. It is a separate set of roles
    and permissions, that has nothing to do with the data stored. To check this feature,
    click on the Access control (IAM)blade:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30fc62be-cd45-4d9f-99c9-0c28c7902cd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the preceding screen, you can see that I have three different apps (services)
    and one user assigned to the resource. They also have different roles:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Owner: Full access including determining who can access the resource'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contributor: Full access excluding determining who can access the resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you click the Roles button, you will see a full list of possible roles for
    the ADLS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d075c409-8ef7-4988-8f16-4c41ba0116ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the Access Control (IAM)blade, you can easily control who can access your
    instance of Azure Data Lake Store and how—use it any time you want to change permissions
    or the set of users/services accessing it.
  prefs: []
  type: TYPE_NORMAL
- en: A good idea is to manage groups rather than individual entities—this allows
    you to add/remove a user or an entity in one place (Azure AD) instead of browsing
    resources and their RBAC.
  prefs: []
  type: TYPE_NORMAL
- en: POSIX ACL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As described previously, you can manage access to data stored within your instance
    of ADLS by providing a set of permissions defined as Read, Write,and Execute.
    They are the part of the POSIX **access control list** (**ACL**) that is a feature
    of Hadoop HDFS, which is the part of the engine of this Azure service. If you
    have used, for example FTP servers, you probably have worked with filesystem permissions;
    they were described as numbers or strings containing the letters `r`, `w`, `x`,
    and the character `-`. The following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-rwx------`is equal to `0700` and declares read, write, and execute permissions
    only for the owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-rwxrwxrwx`is equal to `0777` and declares read, write, and execute permissions
    for everyone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-rw-rw-rw-`is equal to `0666` and declares read and write permissions for
    everyone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find more about the POSIX ACL model in the *Further reading*section.
  prefs: []
  type: TYPE_NORMAL
- en: Network isolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I mentioned the Firewall blade earlier, but we skipped it so you could learn
    something about it once you are familiar with the service. When you click on the Firewall blade,
    you will see a screen that allows you to specify which IP address can access your
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1902c0b2-ee17-4201-8ae4-4d129fcd0dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: The important thing here is the ability to block other Azure services from accessing
    your data—this can be helpful if you have requirements that force you to disallow
    anyone from reading any information stored in ADLS. You can find out more about
    security features in the link provided in the *Further reading*section.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Data Lake Store is a bit different when it comes to accessing data stored
    and performing read and writes. As this service is designed for storing petabytes
    of data, it is important to know the best practices for doing so, to avoid problems
    such as the need to reorganize all files or slow reads/writes. This also includes
    security features (as discussed earlier), as this is an important part of the
    whole solution. In this section, we will focus on multiple advice regarding ADLS,
    so you will use it consciously and leverage the best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One important feature of many storage solutions is their performance. In general,
    we expect that our databases will work without a problem whether the load is low
    or high and a single record is big or small. When it comes to ADLS, you have to
    take into account the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelism**: As stated in the documentation, it is important to ensure,
    that you provide a certain level of parallelism when performing reads/writes.
    The ideal number of threads per one core is defined as 8-12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**File size**: While different data analytics solutions may work differently
    with different file sizes, it is also important to know that ADLS also has an
    optimal file size to work with. As it is based on HDFS and leverages the POSIX
    model for permissions, it promotes bigger files (several hundred megabytes) instead
    of smaller ones to avoid problems with replication, connections, and authentication
    checks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I/O limits**: While some hard limits when it comes to throughput are not
    enabled on Azure Data Lake Store, you still can face some problems when your jobs
    are very demanding, capacity-wise. It is important to remember that even in this
    service, you can still face some soft limits that can be removed after contacting
    Azure support. If you face a 429 error, throttling may be the case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batching**: As in many cases where you face high throughput, it may be beneficial
    to use batching to lower write operations. In ADLS, the optimal size for a batch
    is defined as 4 MBs – by performing writes of that size, you can lower the required
    IOPS and improve the overall performance of the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We discussed this topic a little previously, but here we summarize it. When
    using ADLS and considering its security features (such as authentication, authorization,
    and access to files), it is important to remember the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prefer groups over users/services**: While, initially, it is easier to assign
    an individual user to a resource or a folder, you will quickly face problems when
    the number of people interested in data starts to grow rapidly. This is why it
    is better to use Azure AD groups to both determine RBAC access to the resource
    itself and POSIX ACL for files and folders. It also improves the performance of
    the solution, as it is quicker to check whether an entity belongs to a group than
    to traverse through a long list of users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The minimum set of permissions**: As in other services, always start with
    a minimum set of permissions required by someone who accesses your instance of Azure
    Data Lake Store. Do not assign a Write permission to somebody who only reads data,
    or Execute to a service that reads only a single file in a folder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enable the firewall**: In general, you do not want to allow anyone to access
    data stored inside ADLS. To secure your solution, so that only a subset of IP
    addresses can access information, enable the firewall so anyone outside the list
    will be rejected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resiliency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is crucial to ensure, that your data is stored in a safe manner and will
    not be lost in the case of any issue inside the DC. As mentioned at the very beginning
    of this chapter, ADLS does not support geo-redundancy—you have to implement it
    on your own. To do so, you have to incorporate a tool that will allow you to replicate
    data in the way you need. There are three different tools mentioned in the documentation—Distcp,
    Azure Data Factory, and AdlsCopy, but of course, you can use any other tool that
    can connect to Azure Data Lake Store and integrate with the service.
  prefs: []
  type: TYPE_NORMAL
- en: When considering DR or HA for Azure Data Lake Store, take into consideration
    factors such as RPO, inconsistency, and complex data merging problems in the event
    of performing a failover. Sometimes, it is better to wait for a service to recover
    instead of switching to the secondary replica.
  prefs: []
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will choose a different data structure for different use scenarios—for
    IoT data it will be very granular:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, for storing user data, the structure may be completely different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It all depends on your current requirements. The data structure is extremely
    important when you plan to perform an analysis on the files stored—it directly
    affects the size of files and their number, which can further affect the possible
    toolset for your activities.
  prefs: []
  type: TYPE_NORMAL
- en: Another important thing here is the legal requirements—if you use any kind of
    sensitive data as a folder or a filename, you will have to be able to perform
    a clean up efficiently if a user tells you that he/she wants to be forgotten or
    asks for an account to be removed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned a bit about Azure Data Lake Store, an Azure
    service designed to store an almost unlimited amount of data without affecting
    its structure. We have covered things such as data structure, security features,
    and best practices, so you should be able to get started on your own and build
    your very first solution based on this particular Azure component. Bear in mind
    that what can easily replace Azure Storage for example—it all depends on your
    requirements and expectations. If you're looking for a more flexible security
    model, better performance, and better limits, ADLS is for you. This ends this
    part of the book, which included services for storing data, monitoring services,
    and performing communication between them. In the next chapter, you will learn
    more about scaling, performance, and maintainability in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which security model is better—managing security groups or individual entities,
    and why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between RBAC and POSIX ACL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the maximum size of a file in ADLS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which data structure is better—a single folder containing thousands of files
    or a hierarchy of folders containing several files each?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can Azure Data Lake Store be used with any programming language?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between ADLS and Azure Storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you ensure that your solution based on ADLS is geo-redundant?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: End-user authentication: [https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-end-user-authenticate-net-sdk](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-end-user-authenticate-net-sdk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service-to-service authentica**tion**: [https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-net-sdk](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-net-sdk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a Data Lake Store provider: [http://blog.codenova.pl/post/azure-functions-webjobs-and-data-lake-writing-a-custom-extension-2](http://blog.codenova.pl/post/azure-functions-webjobs-and-data-lake-writing-a-custom-extension-2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Lake Store operations .NET: [https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-net-sdk](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-net-sdk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POSIX ACL: [https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security overview: [https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-security-overview](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-security-overview)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices: [https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
