- en: Chapter 8. Internode Coordination
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 节点间协调
- en: '|   | *"Rest is not idleness, and to lie sometimes on the grass under trees
    on a summer''s day, listening to the murmur of the water, or watching the clouds
    float across the sky, is by no means a waste of time."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“休息不是懒散，有时在夏日的午后躺在树下，听着水流的潺潺声，或看着云朵飘过天空，绝不是浪费时间。”* |   |'
- en: '|   | --*John Lubbock* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*约翰·拉博克* |'
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Managing firewalls with iptables
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 iptables 管理防火墙
- en: Building high-availability services using Heartbeat
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Heartbeat 构建高可用服务
- en: Managing NFS servers and file shares
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理 NFS 服务器和文件共享
- en: Using HAProxy to load-balance multiple web servers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 HAProxy 对多个 Web 服务器进行负载均衡
- en: Managing Docker with Puppet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Puppet 管理 Docker
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: As powerful as Puppet is to manage the configuration of a single server, it's
    even more useful when coordinating many machines. In this chapter, we'll explore
    ways to use Puppet to help you create high-availability clusters, share files
    across your network, set up automated firewalls, and use load-balancing to get
    more out of the machines you have. We'll use exported resources as the communication
    between nodes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet 强大的功能可以管理单台服务器的配置，但当需要协调多台机器时，它的作用更加明显。在本章中，我们将探讨如何使用 Puppet 来帮助你创建高可用集群、在网络中共享文件、设置自动化防火墙，并通过负载均衡来最大化现有机器的效能。我们将使用导出的资源作为节点之间的通信手段。
- en: Managing firewalls with iptables
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 iptables 管理防火墙
- en: 'In this chapter, we will begin to configure services that require communication
    between hosts over the network. Most Linux distributions will default to running
    a host-based firewall, **iptables**. If you want your hosts to communicate with
    each other, you have two options: turn off iptables or configure iptables to allow
    the communication.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始配置需要通过网络进行主机间通信的服务。大多数 Linux 发行版默认会运行基于主机的防火墙 **iptables**。如果你希望主机之间可以互相通信，你有两个选择：关闭
    iptables 或者配置 iptables 允许通信。
- en: I prefer to leave iptables turned on and configure access. Keeping iptables
    is just another layer on your defense across the network. iptables isn't a magic
    bullet that will make your system secure, but it will block access to services
    you didn't intend to expose to the network.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我倾向于保持 iptables 开启并配置访问控制。启用 iptables 就是为你的网络防线增加了一层保护。iptables 并不是一剂能够让系统变得绝对安全的“魔法药丸”，但它会阻止你无意间暴露给网络的服务。
- en: Configuring iptables properly is a complicated task, which requires deep knowledge
    of networking. The example presented here is a simplification. If you are unfamiliar
    with iptables, I suggest you research iptables before continuing. More information
    can be found at [http://wiki.centos.org/HowTos/Network/IPTables](http://wiki.centos.org/HowTos/Network/IPTables)
    or [https://help.ubuntu.com/community/IptablesHowTo](https://help.ubuntu.com/community/IptablesHowTo).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正确配置 iptables 是一项复杂的任务，需要对网络有深入的了解。这里展示的示例是简化版。如果你不熟悉 iptables，建议在继续之前先研究一下
    iptables。更多信息可以参考 [http://wiki.centos.org/HowTos/Network/IPTables](http://wiki.centos.org/HowTos/Network/IPTables)
    或 [https://help.ubuntu.com/community/IptablesHowTo](https://help.ubuntu.com/community/IptablesHowTo)。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In the following examples, we''ll be using the Puppet Labs Firewall module
    to configure iptables. Prepare by installing the module into your Git repository
    with `puppet module install`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用 Puppet Labs 防火墙模块来配置 iptables。通过 `puppet module install` 将该模块安装到你的
    Git 仓库中进行准备：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'To configure the firewall module, we need to create a set of rules, which will
    be applied before all other rules. As a simple example, we''ll create the following
    rules:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置防火墙模块，我们需要创建一组规则，这些规则会在所有其他规则之前应用。作为一个简单的例子，我们将创建以下规则：
- en: Allow all traffic on the loopback (lo) interface
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许所有流量通过回环 (lo) 接口
- en: Allow all ICMP traffic
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许所有 ICMP 流量
- en: Allow all traffic that is part of an established connection (ESTABLISHED, RELATED)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许所有属于已建立连接的流量（ESTABLISHED, RELATED）
- en: Allow all TCP traffic to port 22 (ssh)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许所有 TCP 流量通过 22 端口（ssh）
- en: 'We will create a `myfw` (my firewall) class to configure the firewall module.
    We will then apply the `myfw` class to a node to have iptables configured on that
    node:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个 `myfw`（我的防火墙）类来配置防火墙模块。然后，我们将在节点上应用 `myfw` 类，来配置该节点上的 iptables：
- en: 'Create a class to contain these rules and call it `myfw::pre`:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个类来包含这些规则，并将其命名为 `myfw::pre`：
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When traffic doesn''t match any of the previous rules, we want a final rule
    that will drop the traffic. Create the class `myfw::post` to contain the default
    drop rule:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当流量不匹配任何先前的规则时，我们希望有一条最终的规则来丢弃流量。创建`myfw::post`类来包含默认的丢弃规则：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a `myfw` class, which will include `myfw::pre` and `myfw::post` to configure
    the firewall:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`myfw`类，其中包括`myfw::pre`和`myfw::post`来配置防火墙：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Attach the `myfw` class to a node definition; I''ll do this to my cookbook
    node:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`myfw`类附加到节点定义上；我将对我的食谱节点执行此操作：
- en: '[PRE4]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Run Puppet on cookbook to see whether the firewall rules have been applied:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在食谱上运行Puppet，以查看防火墙规则是否已应用：
- en: '[PRE5]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Verify the new rules with `iptables-save`:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`iptables-save`验证新规则：
- en: '[PRE6]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How it works...
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This is a great example of how to use metaparameters to achieve a complex ordering
    with little effort. Our `myfw` module achieves the following configuration:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的例子，展示了如何使用元参数以较少的努力实现复杂的顺序。我们的`myfw`模块实现了以下配置：
- en: '![How it works...](img/4882OS_08_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4882OS_08_01.jpg)'
- en: All the rules in the `myfw::pre` class are guaranteed to come before any other
    firewall rules we define. The rules in `myfw::post` are guaranteed to come after
    any other firewall rules. So, we have the rules in `myfw::pre` first, then any
    other rules, followed by the rules in `myfw::post`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`myfw::pre`类中的所有规则都确保在我们定义的任何其他防火墙规则之前执行。`myfw::post`中的规则则确保在任何其他防火墙规则之后执行。因此，我们首先执行`myfw::pre`中的规则，然后是其他规则，最后是`myfw::post`中的规则。'
- en: 'Our definition for the `myfw` class sets up this dependency with resource defaults:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`myfw`类的定义设置了这个依赖关系，使用资源默认值：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These defaults first tell Puppet that any firewall resource should be executed
    before anything in the `myfw::post` class. Second, they tell Puppet that any firewall
    resource should require that the resources in `myfw::pre` already be executed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些默认设置首先告诉Puppet，任何防火墙资源应该在`myfw::post`类中的任何内容之前执行。其次，它们告诉Puppet，任何防火墙资源应该要求`myfw::pre`中的资源已经执行过。
- en: 'When we defined the `myfw::pre` class, we removed the require statement in
    a resource default for Firewall resources. This ensures that the resources within
    the myfw::pre-class don''t require themselves before executing (Puppet will complain
    that we created a cyclic dependency otherwise):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义`myfw::pre`类时，我们在防火墙资源的资源默认值中移除了require语句。这样可以确保`myfw::pre`类中的资源在执行之前不会相互依赖（否则Puppet会抱怨我们创建了一个循环依赖）：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We use the same trick in our `myfw::post` definition. In this case, we only
    have a single rule in the post class, so we simply remove the `before` requirement:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`myfw::post`定义中使用了相同的技巧。在这种情况下，我们在post类中只有一条规则，因此我们简单地移除了`before`要求：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we include a rule to purge all the existing iptables rules on the
    system. We do this to ensure we have a consistent set of rules; only rules defined
    in Puppet will persist:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们加入一条规则，用于清除系统上所有现有的iptables规则。这样做是为了确保我们拥有一套一致的规则；只有Puppet中定义的规则会被保留：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There's more...
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'As we hinted, we can now define firewall resources in our manifests and have
    them applied to the iptables configuration after the initialization rules (`myfw::pre`)
    but before the final drop (`myfw::post`). For example, to allow http traffic on
    our cookbook machine, modify the node definition as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所提示的，我们现在可以在我们的清单中定义防火墙资源，并在初始化规则（`myfw::pre`）之后应用到iptables配置中，但在最终的丢弃规则（`myfw::post`）之前。例如，要允许我们的食谱机器上HTTP流量，请按照以下方式修改节点定义：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run Puppet on cookbook:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在食谱上运行Puppet：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Verify that the new rule has been added after the last myfw::pre rule (port
    22, ssh):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 验证新规则是否已添加到最后一条`myfw::pre`规则之后（端口22，ssh）：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The Puppet Labs Firewall module has a built-in notion of order, all our firewall
    resource titles begin with a number. This is a requirement. The module attempts
    to order resources based on the title. You should keep this in mind when naming
    your firewall resources.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet Labs的防火墙模块有一个内建的顺序概念，我们的所有防火墙资源标题都以数字开头。这是一个要求。该模块会尝试根据标题来排序资源。在命名防火墙资源时，您应该牢记这一点。
- en: In the next section, we'll use our firewall module to ensure that two nodes
    can communicate as required.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用我们的防火墙模块确保两个节点可以按要求进行通信。
- en: Building high-availability services using Heartbeat
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Heartbeat构建高可用性服务
- en: High-availability services are those that can survive the failure of an individual
    machine or network connection. The primary technique for high availability is
    redundancy, otherwise known as throwing hardware at the problem. Although the
    eventual failure of an individual server is certain, the simultaneous failure
    of two servers is unlikely enough that this provides a good level of redundancy
    for most applications.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用服务是那些能够在单台机器或网络连接故障的情况下仍能继续运行的服务。高可用性的主要技术是冗余，也就是通过增加硬件来解决问题。虽然单台服务器最终会发生故障是不可避免的，但两台服务器同时发生故障的可能性非常小，这为大多数应用提供了良好的冗余水平。
- en: One of the simplest ways to build a redundant pair of servers is to have them
    share an IP address using Heartbeat. Heartbeat is a daemon that runs on both machines
    and exchanges regular messages—heartbeats—between the two. One server is the primary
    one, and normally has the resource; in this case, an IP address (known as a virtual
    IP, or VIP). If the secondary server fails to detect a heartbeat from the primary
    server, it can take over the address, ensuring continuity of service. In real-world
    scenarios, you may want more machines involved in the VIP, but for this example,
    two machines works well enough.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 构建冗余服务器对的最简单方法之一是让它们共享一个IP地址，通过Heartbeat实现。Heartbeat是一个在两台机器上运行的守护进程，它们之间会定期交换信息——心跳。一个服务器是主服务器，通常拥有资源；在这种情况下，是IP地址（称为虚拟IP，或VIP）。如果从主服务器未能接收到心跳，辅助服务器可以接管该地址，从而确保服务的连续性。在实际场景中，你可能希望更多的机器参与VIP，但在这个示例中，使用两台机器已经足够。
- en: In this recipe, we'll set up two machines in this configuration using Puppet,
    and I'll explain how to use it to provide a high-availability service.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用Puppet设置这两台机器，并解释如何使用它提供高可用服务。
- en: Getting ready
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You'll need two machines, of course, and an extra IP address to use as the VIP.
    You can usually request this from your ISP, if necessary. In this example, I'll
    be using machines named `cookbook` and `cookbook2`, with `cookbook` being the
    primary. We'll add the hosts to the heartbeat configuration.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你需要两台机器，以及一个额外的IP地址来作为VIP。通常，你可以向ISP请求这个地址（如果需要的话）。在这个示例中，我将使用名为`cookbook`和`cookbook2`的两台机器，其中`cookbook`是主机。我们将把这些主机添加到heartbeat配置中。
- en: How to do it…
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Follow these steps to build the example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤构建示例：
- en: 'Create the file `modules/heartbeat/manifests/init.pp` with the following contents:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件`modules/heartbeat/manifests/init.pp`，内容如下：
- en: '[PRE14]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create the file `modules/heartbeat/manifests/vip.pp` with the following contents:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件`modules/heartbeat/manifests/vip.pp`，内容如下：
- en: '[PRE15]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create the file `modules/heartbeat/templates/vip.ha.cf.erb` with the following
    contents:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件`modules/heartbeat/templates/vip.ha.cf.erb`，内容如下：
- en: '[PRE16]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Modify your `site.pp` file as follows. Replace the `ip1` and `ip2` addresses
    with the primary IP addresses of your two nodes, `vip` with the virtual IP address
    you''ll be using, and `node1` and `node2` with the hostnames of the two nodes.
    (Heartbeat uses the fully-qualified domain name of a node to determine whether
    it''s a member of the cluster, so the values for `node1` and `node2` should match
    what''s given by `facter fqdn` on each machine.):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式修改你的`site.pp`文件。将`ip1`和`ip2`地址替换为你两台节点的主IP地址，将`vip`替换为你将使用的虚拟IP地址，并将`node1`和`node2`替换为两台节点的主机名。（Heartbeat使用节点的完全限定域名来确定它是否是集群的一部分，因此`node1`和`node2`的值应与每台机器上`facter
    fqdn`给出的值匹配。）
- en: '[PRE17]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Run Puppet on each of the two servers:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每台服务器上运行Puppet：
- en: '[PRE18]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Verify that the VIP is running on one of the nodes (it should be on cookbook
    at this point; note that you will need to use the `ip` command, `ifconfig` will
    not show the address):'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证VIP是否正在某个节点上运行（此时它应该在cookbook上；注意，你需要使用`ip`命令，`ifconfig`不会显示该地址）：
- en: '[PRE19]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we can see, cookbook has the `eth0:1` interface active. If you stop heartbeat
    on `cookbook`, `cookbook2` will create `eth0:1` and take over:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所见，cookbook上激活了`eth0:1`接口。如果你停止`cookbook`上的heartbeat，`cookbook2`将创建`eth0:1`并接管：
- en: '[PRE20]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works…
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'We need to install Heartbeat first of all, using the `heartbeat` class:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要安装Heartbeat，使用`heartbeat`类：
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we use the `heartbeat::vip` class to manage a specific virtual IP:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`heartbeat::vip`类来管理特定的虚拟IP：
- en: '[PRE22]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, the class includes an `interface` parameter; by default, the
    VIP will be configured on `eth0:1`, but if you need to use a different interface,
    you can pass it in using this parameter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该类包含一个`interface`参数；默认情况下，VIP将配置在`eth0:1`上，但如果你需要使用不同的接口，可以通过此参数传入。
- en: 'Each pair of servers that we configure with a virtual IP will use the `heartbeat::vip`
    class with the same parameters. These will be used to build the `haresources`
    file:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们配置的每一对服务器都将使用相同参数的 `heartbeat::vip` 类，这些参数将用于构建 `haresources` 文件：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This tells Heartbeat about the resource it should manage (that''s a Heartbeat
    resource, such as an IP address or a service, not a Puppet resource). The resulting
    `haresources` file might look as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉 Heartbeat 该管理的资源（这是一个 Heartbeat 资源，例如 IP 地址或服务，而不是 Puppet 资源）。生成的 `haresources`
    文件可能如下所示：
- en: '[PRE24]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The file is interpreted by Heartbeat as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件由 Heartbeat 按如下方式解释：
- en: '`cookbook.example.com`: This is the name of the primary node, which should
    be the default owner of the resource'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cookbook.example.com`：这是主节点的名称，应该是资源的默认所有者'
- en: '`IPaddr`: This is the type of resource to manage; in this case, an IP address'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IPaddr`：这是要管理的资源类型；在这种情况下，是一个 IP 地址'
- en: '`192.168.122.200/24`: This is the value for the IP address'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`192.168.122.200/24`：这是 IP 地址的值'
- en: '`eth0:1`: This is the virtual interface to configure with the managed IP address'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eth0:1`：这是要配置的虚拟接口，带有管理的 IP 地址'
- en: For more information on how heartbeat is configured, please visit the high-availability
    site at [http://linux-ha.org/wiki/Heartbeat](http://linux-ha.org/wiki/Heartbeat).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于如何配置 Heartbeat 的信息，请访问高可用性网站 [http://linux-ha.org/wiki/Heartbeat](http://linux-ha.org/wiki/Heartbeat)。
- en: 'We will also build the `ha.cf` file that tells Heartbeat how to communicate
    between cluster nodes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将构建 `ha.cf` 文件，以告诉 Heartbeat 如何在集群节点之间通信：
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To do this, we use the template file:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用模板文件：
- en: '[PRE26]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The interesting values here are the IP addresses of the two nodes (`ip1` and
    `ip2`), and the names of the two nodes (`node1` and `node2`).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键值是两个节点的 IP 地址（`ip1` 和 `ip2`）以及两个节点的名称（`node1` 和 `node2`）。
- en: 'Finally, we create an instance of `heartbeat::vip` on both machines and pass
    it an identical set of parameters as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在两台机器上创建 `heartbeat::vip` 实例，并传递相同的参数集，如下所示：
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There's more...
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: With Heartbeat set up as described in the example, the virtual IP address will
    be configured on `cookbook` by default. If something happens to interfere with
    this (for example, if you halt or reboot `cookbook`, or stop the `heartbeat` service,
    or the machine loses network connectivity), `cookbook2` will immediately take
    over the virtual IP.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 按照示例中所描述的设置 Heartbeat，虚拟 IP 地址默认将配置在 `cookbook` 上。如果发生干扰（例如，如果你停止或重启 `cookbook`，或者停止
    `heartbeat` 服务，或机器失去网络连接），`cookbook2` 将立即接管虚拟 IP。
- en: The `auto_failback` setting in `ha.cf` governs what happens next. If `auto_failback`
    is set to `on`, when `cookbook` becomes available once more, it will automatically
    take over the IP address. Without `auto_failback`, the IP will stay where it is
    until you manually fail it again (by stopping `heartbeart` on `cookbook2`, for
    example).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`ha.cf` 中的 `auto_failback` 设置决定了接下来的行为。如果 `auto_failback` 设置为 `on`，当 `cookbook`
    恢复可用时，它将自动接管 IP 地址。如果没有设置 `auto_failback`，IP 地址将保持当前位置，直到你手动使其故障（例如，通过停止 `cookbook2`
    上的 `heartbeat`）。'
- en: One common use for a Heartbeat-managed virtual IP is to provide a highly available
    website or service. To do this, you need to set the DNS name for the service (for
    example, `cat-pictures.com`) to point to the virtual IP. Requests for the service
    will be routed to whichever of the two servers currently has the virtual IP. If
    this server should go down, requests will go to the other, with no visible interruption
    in service to users.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Heartbeat 管理的虚拟 IP 的一个常见用途是提供高度可用的网站或服务。为此，您需要将服务的 DNS 名称（例如 `cat-pictures.com`）指向虚拟
    IP。对该服务的请求将被路由到当前拥有虚拟 IP 的服务器。如果该服务器出现故障，请求将转发到另一台服务器，用户不会察觉到服务中断。
- en: Heartbeat works great for the previous example but is not in widespread use
    in this form. Heartbeat only works in two node clusters; for n-node clusters,
    the newer pacemaker project should be used. More information on Heartbeat, pacemaker,
    corosync, and other clustering packages can be found at [http://www.linux-ha.org/wiki/Main_Page](http://www.linux-ha.org/wiki/Main_Page).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Heartbeat 在之前的示例中效果很好，但这种形式并不广泛使用。Heartbeat 仅在两节点集群中有效；对于 n 节点集群，应使用更新的 pacemaker
    项目。有关 Heartbeat、pacemaker、corosync 及其他集群软件包的更多信息，请访问 [http://www.linux-ha.org/wiki/Main_Page](http://www.linux-ha.org/wiki/Main_Page)。
- en: Managing cluster configuration is one area where exported resources are useful.
    Each node in a cluster would export information about itself, which could then
    be collected by the other members of the cluster. Using the puppetlabs-concat
    module, you can build up a configuration file using exported concat fragments
    from all the nodes in the cluster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 管理集群配置是导出资源有用的一方面。集群中的每个节点将导出有关自身的信息，然后其他集群成员可以收集这些信息。使用 puppetlabs-concat 模块，您可以使用来自集群中所有节点的导出
    concat 碎片构建配置文件。
- en: Remember to look at the Forge before starting your own module. If nothing else,
    you'll get some ideas that you can use in your own module. Corosync can be managed
    with the Puppet labs module at [https://forge.puppetlabs.com/puppetlabs/corosync](https://forge.puppetlabs.com/puppetlabs/corosync).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始自己的模块之前，记得查看 Forge。如果没有其他用途，你至少可以获得一些可以在自己模块中使用的想法。Corosync 可以通过 Puppet labs
    模块进行管理，网址为 [https://forge.puppetlabs.com/puppetlabs/corosync](https://forge.puppetlabs.com/puppetlabs/corosync)。
- en: Managing NFS servers and file shares
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理 NFS 服务器和文件共享
- en: '**NFS** (**Network File System**) is a protocol to mount a shared directory
    from a remote server. For example, a pool of web servers might all mount the same
    NFS share to serve static assets such as images and stylesheets. Although NFS
    is generally slower and less secure than local storage or a clustered filesystem,
    the ease with which it can be used makes it a common choice in the datacenter.
    We''ll use our `myfw` module from before to ensure the local firewall permits
    `nfs` communication. We''ll also use the Puppet labs-concat module to edit the
    list of exported filesystems on our `nfs` server.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**NFS**（**网络文件系统**）是一种从远程服务器挂载共享目录的协议。例如，一组 Web 服务器可能会挂载相同的 NFS 共享，用于提供静态资源，如图片和样式表。尽管
    NFS 通常比本地存储或集群文件系统更慢且不太安全，但它的易用性使其在数据中心中成为常见的选择。我们将使用之前的 `myfw` 模块来确保本地防火墙允许 `nfs`
    通信。我们还将使用 Puppet 的 labs-concat 模块来编辑 `nfs` 服务器上导出的文件系统列表。'
- en: How to do it...
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this example, we'll configure an `nfs` server to share (export) some filesystem
    via NFS.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将配置一个 `nfs` 服务器，通过 NFS 共享（导出）某些文件系统。
- en: 'Create an `nfs` module with the following `nfs::exports` class, which defines
    a concat resource:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `nfs` 模块，包含以下 `nfs::exports` 类，它定义了一个 concat 资源：
- en: '[PRE28]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create the `nfs::export` defined type, we''ll use this definition for any `nfs`
    exports we create:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `nfs::export` 定义类型，我们将使用这个定义来创建任何 `nfs` 导出：
- en: '[PRE29]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now create the `nfs::server` class, which will include the OS-specific configuration
    for the server:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建 `nfs::server` 类，其中将包括服务器的操作系统特定配置：
- en: '[PRE30]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, create the `nfs::server::redhat` class:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建 `nfs::server::redhat` 类：
- en: '[PRE31]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create the `/etc/sysconfig/nfs` support file for RedHat systems in the files
    directory of our `nfs` repo (`modules/nfs/files/nfs`):'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的 `nfs` 仓库的文件目录中为 RedHat 系统创建 `/etc/sysconfig/nfs` 支持文件（`modules/nfs/files/nfs`）：
- en: '[PRE32]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now create the support class for Debian systems, `nfs::server::debian`:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在为 Debian 系统创建支持类 `nfs::server::debian`：
- en: '[PRE33]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create the nfs-common configuration for Debian (which will be placed in `modules/nfs/files/nfs-common`):'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 Debian 的 nfs-common 配置（将放置在 `modules/nfs/files/nfs-common` 中）：
- en: '[PRE34]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Apply the `nfs::server` class to a node and then create an export on that node:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `nfs::server` 类应用到一个节点，然后在该节点上创建一个导出：
- en: '[PRE35]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create a collector for the exported resource created by the `nfs::server` class
    in the preceding code snippet:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为前面代码片段中创建的 `nfs::server` 类导出的资源创建一个收集器：
- en: '[PRE36]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, run Puppet on the node Debian to create the exported resource. Then,
    run Puppet on the cookbook node to mount that resource:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在 Debian 节点上运行 Puppet 来创建导出的资源。然后，在 cookbook 节点上运行 Puppet 来挂载该资源：
- en: '[PRE37]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Verify the mount with `mount`:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `mount` 验证挂载：
- en: '[PRE38]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How it works…
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The `nfs::exports` class defines an exec, which runs `''exportfs -a''`, to
    export all filesystems defined in `/etc/exports`. Next, we define a concat resource
    to contain `concat::fragments`, which we will define next in our `nfs::export`
    class. Concat resources specify the file that the fragments are to be placed into;
    `/etc/exports` in this case. Our `concat` resource has a notify for the previous
    exec. This has the effect that whenever `/etc/exports` is updated, we run `''exportfs
    -a''` again to export the new entries:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`nfs::exports` 类定义了一个 exec，它运行 `''exportfs -a''`，以导出 `/etc/exports` 中定义的所有文件系统。接下来，我们定义一个
    concat 资源来包含 `concat::fragments`，我们将在接下来的 `nfs::export` 类中定义它。Concat 资源指定碎片要放置的文件；在这个例子中是
    `/etc/exports`。我们的 `concat` 资源有一个通知，用于执行上一个 exec。这样，每次更新 `/etc/exports` 时，我们都会重新运行
    `''exportfs -a''` 来导出新的条目：'
- en: '[PRE39]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We then created an `nfs::export` defined type, which does all the work. The
    defined type adds an entry to `/etc/exports` via a `concat::fragment` resource:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了一个`nfs::export`定义类型，完成所有工作。该定义类型通过`concat::fragment`资源向`/etc/exports`添加了一条条目：
- en: '[PRE40]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the definition, we use the attribute `$where` to define what filesystem we
    are exporting. We use `$who` to specify who can mount the filesystem. The attribute
    `$options` contains the exporting options such as **rw** (**read-write**), **ro**
    (**read-only**). Next, we have the options that will be placed in `/etc/fstab`
    on the client machine, the mount options, stored in `$mount_options`. The `nfs::exports`
    class is included here so that `concat::fragment` has a concat target defined.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义中，我们使用`$where`属性来定义我们要导出的文件系统。我们使用`$who`来指定谁可以挂载该文件系统。`$options`属性包含导出选项，例如**rw**（**读写**）、**ro**（**只读**）。接下来，我们有将要放入客户端机器的`/etc/fstab`中的选项，挂载选项存储在`$mount_options`中。这里包括了`nfs::exports`类，以便`concat::fragment`有一个定义的拼接目标。
- en: 'Next, the exported mount resource is created; this is done on the server, so
    the `${::ipaddress}` variable holds the IP address of the server. We use this
    to define the device for the mount. The device is the IP address of the server,
    a colon, and then the filesystem being exported. In this example, it is `''192.168.122.148:/srv/home''`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建了导出的挂载资源；这是在服务器上完成的，因此`${::ipaddress}`变量保存了服务器的 IP 地址。我们用这个定义挂载的设备。设备由服务器的
    IP 地址、一个冒号，然后是被导出的文件系统组成。在这个例子中，它是`'192.168.122.148:/srv/home'`：
- en: '[PRE41]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We reuse our `myfw` module and include it in the `nfs::server` class. This
    class illustrates one of the things to consider when writing your modules. Not
    all Linux distributions are created equal. Debian and RedHat deal with NFS server
    configuration quite differently. The `nfs::server` module deals with this by including
    OS-specific subclasses:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重用了`myfw`模块并将其包含在`nfs::server`类中。这个类说明了编写模块时需要考虑的一个问题。并非所有 Linux 发行版都是相同的。Debian
    和 RedHat 在处理 NFS 服务器配置时有很大的不同。`nfs::server`模块通过包含特定操作系统的子类来处理这个问题：
- en: '[PRE42]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `nfs::server` module opens several firewall ports for NFS communication.
    NFS traffic is always carried over port 2049 but ancillary systems, such as locking,
    quota, and file status daemons, use ephemeral ports chosen by the portmapper,
    by default. The portmapper itself uses port 111\. So our module needs to allow
    2049, 111, and a few other ports. We attempt to configure the ancillary services
    to use ports 4000 through 4010.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`nfs::server`模块为 NFS 通信打开了多个防火墙端口。NFS 流量始终通过端口 2049 传输，但辅助系统，如锁定、配额和文件状态守护进程，默认情况下使用由端口映射器选择的临时端口。端口映射器本身使用端口
    111。所以我们的模块需要允许 2049、111 和其他几个端口。我们尝试将辅助服务配置为使用 4000 到 4010 之间的端口。'
- en: 'In the `nfs::server::redhat` class, we modify `/etc/sysconfig/nfs` to use the
    ports specified. Also, we install the nfs-utils package and start the nfs service:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在`nfs::server::redhat`类中，我们修改了`/etc/sysconfig/nfs`以使用指定的端口。我们还安装了nfs-utils包并启动了nfs服务：
- en: '[PRE43]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We do the same for Debian-based systems in the `nfs::server::debian` class.
    The packages and services have different names but overall the process is similar:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`nfs::server::debian`类中对 Debian 系统做了相同的操作。包和服务的名称不同，但总体过程类似：
- en: '[PRE44]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'With everything in place, we include the server class to configure the NFS
    server and then define an export:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪后，我们包括了服务器类来配置 NFS 服务器，然后定义一个导出：
- en: '[PRE45]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'What''s important here is that we defined the `tag` attribute, which will be
    used in the exported resource we collect in the following code snippet:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的是我们定义了`tag`属性，该属性将在我们在以下代码片段中收集的导出资源中使用：
- en: '[PRE46]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We use the spaceship syntax (`<<| |>>`) to collect all the exported mount resources
    that have the tag we defined earlier (`srv_home`). We then use a syntax called
    "override on collect" to modify the name attribute of the mount to specify where
    to mount the filesystem.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用飞船语法（`<<| |>>`）收集所有具有我们先前定义的标签（`srv_home`）的导出挂载资源。然后，我们使用一种叫做“收集时覆盖”的语法来修改挂载的名称属性，指定挂载文件系统的位置。
- en: Using this design pattern with exported resources, we can change the server
    exporting the filesystem and have any nodes that mount the resource updated automatically.
    We can have many different nodes collecting the exported mount resource.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种带有导出资源的设计模式，我们可以更改导出文件系统的服务器，并且任何挂载该资源的节点会自动更新。我们可以有多个不同的节点收集导出的挂载资源。
- en: Using HAProxy to load-balance multiple web servers
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 HAProxy 对多个 Web 服务器进行负载均衡
- en: Load balancers are used to spread a load among a number of servers. Hardware
    load balancers are still somewhat expensive, whereas software balancers can achieve
    most of the benefits of a hardware solution.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器用于在多个服务器之间分配负载。硬件负载均衡器仍然相对昂贵，而软件负载均衡器可以实现大部分硬件解决方案的优点。
- en: '**HAProxy** is the software load balancer of choice for most people: fast,
    powerful, and highly configurable.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**HAProxy**是大多数人首选的软件负载均衡器：快速、强大且高度可配置。'
- en: How to do it…
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: In this recipe, I'll show you how to build an HAProxy server to load-balance
    web requests across web servers. We'll use exported resources to build the `haproxy`
    configuration file just like we did for the NFS example.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我将向你展示如何构建一个HAProxy服务器，用于在多个Web服务器之间负载均衡Web请求。我们将使用导出的资源来构建`haproxy`配置文件，就像我们为NFS示例所做的那样。
- en: 'Create the file `modules/haproxy/manifests/master.pp` with the following contents:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件`modules/haproxy/manifests/master.pp`，并将以下内容写入其中：
- en: '[PRE47]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create the file `modules/haproxy/files/haproxy.cfg` with the following contents:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件`modules/haproxy/files/haproxy.cfg`，并将以下内容写入其中：
- en: '[PRE48]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Modify your `manifests/nodes.pp` file as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改你的`manifests/nodes.pp`文件，内容如下：
- en: '[PRE49]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Create the slave server configuration in the `haproxy::slave` class:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`haproxy::slave`类中创建从服务器配置：
- en: '[PRE50]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create the `concat` container resource in the `haproxy::config` class as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`haproxy::config`类中创建`concat`容器资源，如下所示：
- en: '[PRE51]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Modify `site.pp` to define the master and slave nodes:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`site.pp`以定义主节点和从节点：
- en: '[PRE52]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Run Puppet on each of the slave servers:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个从节点上运行Puppet：
- en: '[PRE53]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Run Puppet on the master node to configure and run `haproxy`:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主节点上运行Puppet来配置并运行`haproxy`：
- en: '[PRE54]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Check the HAProxy stats interface on master port `8080` in your web browser
    (`http://master.example.com:8080`) to make sure everything is okay (The username
    and password are in `haproxy.cfg`, `haproxy`, and `topsecret`). Try going to the
    proxied service as well. Notice that the page changes on each reload as the service
    is redirected from slave1 to slave2 (`http://master.example.com`).
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Web浏览器中检查主服务器端口`8080`上的HAProxy统计界面（`http://master.example.com:8080`），确保一切正常（用户名和密码分别为`haproxy.cfg`、`haproxy`和`topsecret`）。还可以尝试访问被代理的服务。请注意，每次重新加载页面时，页面内容会发生变化，因为服务会从slave1被重定向到slave2（`http://master.example.com`）。
- en: How it works…
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We built a complex configuration from various components of the previous sections.
    This type of deployment becomes easier the more you do it. At a top level, we
    configured the master to collect exported resources from slaves. The slaves exported
    their configuration information to allow haproxy to use them in the load balancer.
    As slaves are added to the system, they can export their resources and be added
    to the balancer automatically.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前面几个部分的各种组件构建了一个复杂的配置。做得越多，这种类型的部署就越容易。总体而言，我们配置了主节点来收集从节点导出的资源。从节点将其配置信息导出，以便`haproxy`可以在负载均衡器中使用它们。随着从节点的增加，它们可以导出资源并自动添加到负载均衡器中。
- en: We used our `myfw` module to configure the firewall on the slaves and the master
    to allow communication.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`myfw`模块配置了从节点和主节点上的防火墙，以允许通信。
- en: We used the Forge Apache module to configure the listening web server on the
    slaves. We were able to generate a fully functioning website with five lines of
    code (10 more to place `index.html` on the website).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了Forge Apache模块来配置从服务器上的Web服务器监听。我们只用五行代码就生成了一个完全功能的网站（再加上10行代码将`index.html`放到网站上）。
- en: There are several things going on here. We have the firewall configuration and
    the Apache configuration in addition to the `haproxy` configuration. We'll focus
    on how the exported resources and the `haproxy` configuration fit together.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里涉及几个方面。除了`haproxy`配置外，我们还进行了防火墙配置和Apache配置。我们将重点关注导出资源和`haproxy`配置是如何结合在一起的。
- en: 'In the `haproxy::config` class, we created the concat container for the `haproxy`
    configuration:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在`haproxy::config`类中，我们为`haproxy`配置创建了`concat`容器：
- en: '[PRE55]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We reference this in `haproxy::slave`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`haproxy::slave`中引用了这个：
- en: '[PRE56]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We are doing a little trick here with concat; we don''t define the target in
    the exported resource. If we did, the slaves would try and create a `/etc/haproxy/haproxy.cfg`
    file, but the slaves do not have `haproxy` installed so we would get catalog failures.
    What we do is modify the resource when we collect it in `haproxy::master`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了一个小技巧，使用`concat`时没有在导出资源中定义目标。如果我们定义了，所有从节点都会尝试创建一个`/etc/haproxy/haproxy.cfg`文件，但从节点没有安装`haproxy`，因此会导致目录失败。我们做的是在`haproxy::master`中收集资源时修改它：
- en: '[PRE57]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'In addition to adding the target when we collect the resource, we also add
    a notify so that the `haproxy` service is restarted when we add a new host to
    the configuration. Another important point here is that we set the order attribute
    of the slave configurations to 0010, when we define the header for the `haproxy.cfg`
    file; we use an order value of 0001 to ensure that the header is placed at the
    beginning of the file:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在收集资源时添加目标外，我们还添加了一个通知，确保在将新主机添加到配置时重新启动 `haproxy` 服务。另一个重要点是，我们将从属配置的顺序属性设置为
    0010，当我们定义 `haproxy.cfg` 文件的头部时，我们使用顺序值 0001 来确保头部位于文件的开头：
- en: '[PRE58]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The rest of the `haproxy::master` class is concerned with configuring the firewall
    as we did in previous examples.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的 `haproxy::master` 类负责配置防火墙，方法与之前的示例相同。
- en: There's more...
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: HAProxy has a vast range of configuration parameters, which you can explore;
    see the HAProxy website at [http://haproxy.1wt.eu/#docs](http://haproxy.1wt.eu/#docs).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy 有非常广泛的配置参数，你可以探索这些参数；请查看 HAProxy 网站 [http://haproxy.1wt.eu/#docs](http://haproxy.1wt.eu/#docs)。
- en: Although it's most often used as a web server, HAProxy can proxy a lot more
    than just HTTP. It can handle any kind of TCP traffic, so you can use it to balance
    the load of MySQL servers, SMTP, video servers, or anything you like.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 HAProxy 最常用于 Web 服务器，但它不仅可以代理 HTTP，还能代理更多类型的流量。它可以处理任何类型的 TCP 流量，因此你可以用它来平衡
    MySQL 服务器、SMTP、视频服务器或任何你想要的服务负载。
- en: You can use the design we showed to attack many problems of coordination of
    services between multiple servers. This type of interaction is very common; you
    can apply it to many configurations for load balancing or distributed systems.
    You can use the same workflow described previously to have nodes export firewall
    resources (`@@firewall`) to permit their own access.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们展示的设计来解决多个服务器之间服务协调的问题。这类交互非常常见；你可以将它应用于负载均衡或分布式系统的许多配置。你可以使用之前描述的相同工作流程，让节点导出防火墙资源（`@@firewall`）以允许其自身访问。
- en: Managing Docker with Puppet
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Puppet 管理 Docker
- en: '**Docker** is a platform for rapid deployment of containers. Containers are
    like a lightweight virtual machine that might only run a single process. The containers
    in Docker are called docks and are configured with files called Dockerfiles. Puppet
    can be used to configure a node to not only run Docker but also configure and
    start several docks. You can then use Puppet to ensure that your docks are running
    and are consistently configured.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker** 是一个用于快速部署容器的平台。容器类似于轻量级的虚拟机，可能只运行一个进程。Docker 中的容器称为 Dock，并且通过称为
    Dockerfile 的文件来配置。Puppet 可以用于配置一个节点，不仅使其运行 Docker，还可以配置和启动多个 Dock。然后，你可以使用 Puppet
    来确保你的 Dock 正在运行并且配置一致。'
- en: Getting ready
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Download and install the Puppet Docker module from the Forge ([https://forge.puppetlabs.com/garethr/docker](https://forge.puppetlabs.com/garethr/docker)):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Forge 下载并安装 Puppet Docker 模块（[https://forge.puppetlabs.com/garethr/docker](https://forge.puppetlabs.com/garethr/docker)）：
- en: '[PRE59]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Add these modules to your Puppet repository. The `stahnma-epel` module is required
    for Enterprise Linux-based distributions; it contains the Extra Packages for Enterprise
    Linux YUM repository.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些模块添加到你的 Puppet 仓库中。`stahnma-epel` 模块是企业版 Linux 发行版所必需的，它包含了企业版 Linux YUM
    仓库中的额外包。
- en: How to do it...
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to manage Docker with Puppet:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来使用 Puppet 管理 Docker：
- en: 'To install Docker on a node, we just need to include the `docker` class. We''ll
    do more than install Docker; we''ll also download an image and start an application
    on our test node. In this example, we''ll create a new machine called `shipyard.`
    Add the following node definition to `site.pp`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在节点上安装 Docker，我们只需包含 `docker` 类。我们做的不仅仅是安装 Docker，还会下载一个镜像并在我们的测试节点上启动一个应用程序。在本示例中，我们将创建一个名为
    `shipyard` 的新机器。将以下节点定义添加到 `site.pp`：
- en: '[PRE60]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Run Puppet on your shipyard node to install Docker. This will also download
    the `phusion/baseimage docker` image:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 Shipyard 节点上运行 Puppet 来安装 Docker。这也会下载 `phusion/baseimage docker` 镜像：
- en: '[PRE61]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Verify that your container is running on shipyard using `docker ps`:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `docker ps` 验证容器是否在 Shipyard 上运行：
- en: '[PRE62]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Verify that the dock is running netcat on port 8080 by connecting to the port
    listed previously (`49157`):'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `docker ps` 验证 Dock 是否在 8080 端口上运行 netcat，通过连接之前列出的端口（`49157`）：
- en: '[PRE63]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: How it works...
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We began by installing the docker module from the Forge. This module installs
    the `docker-io` package on our node, along with any required dependencies.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过 Forge 安装了 docker 模块。这个模块会在我们的节点上安装 `docker-io` 包，并处理所有必要的依赖关系。
- en: We then defined a `docker::image` resource. This instructs Puppet to ensure
    that the named image is downloaded and available to docker. On our first run,
    Puppet will make docker download the image. We used `phusion/baseimage` as our
    example because it is quite small, well-known, and includes the netcat daemon
    we used in the example. More information on `baseimage` can be found at [http://phusion.github.io/baseimage-docker/](http://phusion.github.io/baseimage-docker/).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了一个 `docker::image` 资源。这指示 Puppet 确保指定的镜像已被下载并且可以供 Docker 使用。在第一次运行时，Puppet
    会让 Docker 下载该镜像。我们使用了 `phusion/baseimage` 作为示例，因为它非常小巧、知名，并且包含了我们在示例中使用的 netcat
    守护进程。关于 `baseimage` 的更多信息，请访问 [http://phusion.github.io/baseimage-docker/](http://phusion.github.io/baseimage-docker/)。
- en: We then went on to define a `docker::run` resource. This example isn't terribly
    useful; it simply starts netcat in listen mode on port 8080\. We need to expose
    that port to our machine, so we define the expose attribute of our `docker::run`
    resource. There are many other options available for the `docker::run` resource.
    Refer to the source code for more details.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们定义了一个 `docker::run` 资源。这个示例并不是特别有用，它只是简单地在 8080 端口启动了 netcat 的监听模式。我们需要将该端口暴露到我们的机器上，因此我们定义了
    `docker::run` 资源的 expose 属性。`docker::run` 资源有很多其他可用选项，更多详情请参考源代码。
- en: We then used docker ps to list the running docks on our shipyard machine. We
    parsed out the listening port on our local machine and verified that netcat was
    listening.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们使用 docker ps 命令列出了我们船厂机器上运行的容器。我们提取了本地机器上监听的端口，并验证了 netcat 是否在监听。
- en: There's more...
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Docker is a great tool for rapid deployment and development. You can spin as
    many docks as you need on even the most modest hardware. One great use for docker
    is having docks act as test nodes for your modules. You can create a docker image,
    which includes Puppet, and then have Puppet run within the dock. For more information
    on docker, visit [http://www.docker.com/](http://www.docker.com/).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个非常适合快速部署和开发的工具。即使在最简单的硬件上，你也可以启动任意数量的容器。Docker 的一个很好的用途是让容器作为你模块的测试节点。你可以创建一个包含
    Puppet 的 Docker 镜像，然后让 Puppet 在容器内运行。欲了解更多关于 Docker 的信息，请访问 [http://www.docker.com/](http://www.docker.com/)。
