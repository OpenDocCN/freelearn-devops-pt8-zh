- en: Chapter 8. Internode Coordination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *"Rest is not idleness, and to lie sometimes on the grass under trees
    on a summer''s day, listening to the murmur of the water, or watching the clouds
    float across the sky, is by no means a waste of time."* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*John Lubbock* |'
  prefs: []
  type: TYPE_TB
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing firewalls with iptables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building high-availability services using Heartbeat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing NFS servers and file shares
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using HAProxy to load-balance multiple web servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Docker with Puppet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As powerful as Puppet is to manage the configuration of a single server, it's
    even more useful when coordinating many machines. In this chapter, we'll explore
    ways to use Puppet to help you create high-availability clusters, share files
    across your network, set up automated firewalls, and use load-balancing to get
    more out of the machines you have. We'll use exported resources as the communication
    between nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Managing firewalls with iptables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will begin to configure services that require communication
    between hosts over the network. Most Linux distributions will default to running
    a host-based firewall, **iptables**. If you want your hosts to communicate with
    each other, you have two options: turn off iptables or configure iptables to allow
    the communication.'
  prefs: []
  type: TYPE_NORMAL
- en: I prefer to leave iptables turned on and configure access. Keeping iptables
    is just another layer on your defense across the network. iptables isn't a magic
    bullet that will make your system secure, but it will block access to services
    you didn't intend to expose to the network.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring iptables properly is a complicated task, which requires deep knowledge
    of networking. The example presented here is a simplification. If you are unfamiliar
    with iptables, I suggest you research iptables before continuing. More information
    can be found at [http://wiki.centos.org/HowTos/Network/IPTables](http://wiki.centos.org/HowTos/Network/IPTables)
    or [https://help.ubuntu.com/community/IptablesHowTo](https://help.ubuntu.com/community/IptablesHowTo).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following examples, we''ll be using the Puppet Labs Firewall module
    to configure iptables. Prepare by installing the module into your Git repository
    with `puppet module install`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To configure the firewall module, we need to create a set of rules, which will
    be applied before all other rules. As a simple example, we''ll create the following
    rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Allow all traffic on the loopback (lo) interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow all ICMP traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow all traffic that is part of an established connection (ESTABLISHED, RELATED)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow all TCP traffic to port 22 (ssh)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will create a `myfw` (my firewall) class to configure the firewall module.
    We will then apply the `myfw` class to a node to have iptables configured on that
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a class to contain these rules and call it `myfw::pre`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When traffic doesn''t match any of the previous rules, we want a final rule
    that will drop the traffic. Create the class `myfw::post` to contain the default
    drop rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `myfw` class, which will include `myfw::pre` and `myfw::post` to configure
    the firewall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Attach the `myfw` class to a node definition; I''ll do this to my cookbook
    node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Puppet on cookbook to see whether the firewall rules have been applied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the new rules with `iptables-save`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a great example of how to use metaparameters to achieve a complex ordering
    with little effort. Our `myfw` module achieves the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4882OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All the rules in the `myfw::pre` class are guaranteed to come before any other
    firewall rules we define. The rules in `myfw::post` are guaranteed to come after
    any other firewall rules. So, we have the rules in `myfw::pre` first, then any
    other rules, followed by the rules in `myfw::post`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our definition for the `myfw` class sets up this dependency with resource defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These defaults first tell Puppet that any firewall resource should be executed
    before anything in the `myfw::post` class. Second, they tell Puppet that any firewall
    resource should require that the resources in `myfw::pre` already be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we defined the `myfw::pre` class, we removed the require statement in
    a resource default for Firewall resources. This ensures that the resources within
    the myfw::pre-class don''t require themselves before executing (Puppet will complain
    that we created a cyclic dependency otherwise):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the same trick in our `myfw::post` definition. In this case, we only
    have a single rule in the post class, so we simply remove the `before` requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we include a rule to purge all the existing iptables rules on the
    system. We do this to ensure we have a consistent set of rules; only rules defined
    in Puppet will persist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we hinted, we can now define firewall resources in our manifests and have
    them applied to the iptables configuration after the initialization rules (`myfw::pre`)
    but before the final drop (`myfw::post`). For example, to allow http traffic on
    our cookbook machine, modify the node definition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Run Puppet on cookbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that the new rule has been added after the last myfw::pre rule (port
    22, ssh):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Puppet Labs Firewall module has a built-in notion of order, all our firewall
    resource titles begin with a number. This is a requirement. The module attempts
    to order resources based on the title. You should keep this in mind when naming
    your firewall resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll use our firewall module to ensure that two nodes
    can communicate as required.
  prefs: []
  type: TYPE_NORMAL
- en: Building high-availability services using Heartbeat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: High-availability services are those that can survive the failure of an individual
    machine or network connection. The primary technique for high availability is
    redundancy, otherwise known as throwing hardware at the problem. Although the
    eventual failure of an individual server is certain, the simultaneous failure
    of two servers is unlikely enough that this provides a good level of redundancy
    for most applications.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest ways to build a redundant pair of servers is to have them
    share an IP address using Heartbeat. Heartbeat is a daemon that runs on both machines
    and exchanges regular messages—heartbeats—between the two. One server is the primary
    one, and normally has the resource; in this case, an IP address (known as a virtual
    IP, or VIP). If the secondary server fails to detect a heartbeat from the primary
    server, it can take over the address, ensuring continuity of service. In real-world
    scenarios, you may want more machines involved in the VIP, but for this example,
    two machines works well enough.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll set up two machines in this configuration using Puppet,
    and I'll explain how to use it to provide a high-availability service.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll need two machines, of course, and an extra IP address to use as the VIP.
    You can usually request this from your ISP, if necessary. In this example, I'll
    be using machines named `cookbook` and `cookbook2`, with `cookbook` being the
    primary. We'll add the hosts to the heartbeat configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to build the example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the file `modules/heartbeat/manifests/init.pp` with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the file `modules/heartbeat/manifests/vip.pp` with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the file `modules/heartbeat/templates/vip.ha.cf.erb` with the following
    contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify your `site.pp` file as follows. Replace the `ip1` and `ip2` addresses
    with the primary IP addresses of your two nodes, `vip` with the virtual IP address
    you''ll be using, and `node1` and `node2` with the hostnames of the two nodes.
    (Heartbeat uses the fully-qualified domain name of a node to determine whether
    it''s a member of the cluster, so the values for `node1` and `node2` should match
    what''s given by `facter fqdn` on each machine.):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Puppet on each of the two servers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the VIP is running on one of the nodes (it should be on cookbook
    at this point; note that you will need to use the `ip` command, `ifconfig` will
    not show the address):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see, cookbook has the `eth0:1` interface active. If you stop heartbeat
    on `cookbook`, `cookbook2` will create `eth0:1` and take over:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to install Heartbeat first of all, using the `heartbeat` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `heartbeat::vip` class to manage a specific virtual IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the class includes an `interface` parameter; by default, the
    VIP will be configured on `eth0:1`, but if you need to use a different interface,
    you can pass it in using this parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each pair of servers that we configure with a virtual IP will use the `heartbeat::vip`
    class with the same parameters. These will be used to build the `haresources`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells Heartbeat about the resource it should manage (that''s a Heartbeat
    resource, such as an IP address or a service, not a Puppet resource). The resulting
    `haresources` file might look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The file is interpreted by Heartbeat as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cookbook.example.com`: This is the name of the primary node, which should
    be the default owner of the resource'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPaddr`: This is the type of resource to manage; in this case, an IP address'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`192.168.122.200/24`: This is the value for the IP address'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eth0:1`: This is the virtual interface to configure with the managed IP address'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on how heartbeat is configured, please visit the high-availability
    site at [http://linux-ha.org/wiki/Heartbeat](http://linux-ha.org/wiki/Heartbeat).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also build the `ha.cf` file that tells Heartbeat how to communicate
    between cluster nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To do this, we use the template file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The interesting values here are the IP addresses of the two nodes (`ip1` and
    `ip2`), and the names of the two nodes (`node1` and `node2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create an instance of `heartbeat::vip` on both machines and pass
    it an identical set of parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Heartbeat set up as described in the example, the virtual IP address will
    be configured on `cookbook` by default. If something happens to interfere with
    this (for example, if you halt or reboot `cookbook`, or stop the `heartbeat` service,
    or the machine loses network connectivity), `cookbook2` will immediately take
    over the virtual IP.
  prefs: []
  type: TYPE_NORMAL
- en: The `auto_failback` setting in `ha.cf` governs what happens next. If `auto_failback`
    is set to `on`, when `cookbook` becomes available once more, it will automatically
    take over the IP address. Without `auto_failback`, the IP will stay where it is
    until you manually fail it again (by stopping `heartbeart` on `cookbook2`, for
    example).
  prefs: []
  type: TYPE_NORMAL
- en: One common use for a Heartbeat-managed virtual IP is to provide a highly available
    website or service. To do this, you need to set the DNS name for the service (for
    example, `cat-pictures.com`) to point to the virtual IP. Requests for the service
    will be routed to whichever of the two servers currently has the virtual IP. If
    this server should go down, requests will go to the other, with no visible interruption
    in service to users.
  prefs: []
  type: TYPE_NORMAL
- en: Heartbeat works great for the previous example but is not in widespread use
    in this form. Heartbeat only works in two node clusters; for n-node clusters,
    the newer pacemaker project should be used. More information on Heartbeat, pacemaker,
    corosync, and other clustering packages can be found at [http://www.linux-ha.org/wiki/Main_Page](http://www.linux-ha.org/wiki/Main_Page).
  prefs: []
  type: TYPE_NORMAL
- en: Managing cluster configuration is one area where exported resources are useful.
    Each node in a cluster would export information about itself, which could then
    be collected by the other members of the cluster. Using the puppetlabs-concat
    module, you can build up a configuration file using exported concat fragments
    from all the nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to look at the Forge before starting your own module. If nothing else,
    you'll get some ideas that you can use in your own module. Corosync can be managed
    with the Puppet labs module at [https://forge.puppetlabs.com/puppetlabs/corosync](https://forge.puppetlabs.com/puppetlabs/corosync).
  prefs: []
  type: TYPE_NORMAL
- en: Managing NFS servers and file shares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**NFS** (**Network File System**) is a protocol to mount a shared directory
    from a remote server. For example, a pool of web servers might all mount the same
    NFS share to serve static assets such as images and stylesheets. Although NFS
    is generally slower and less secure than local storage or a clustered filesystem,
    the ease with which it can be used makes it a common choice in the datacenter.
    We''ll use our `myfw` module from before to ensure the local firewall permits
    `nfs` communication. We''ll also use the Puppet labs-concat module to edit the
    list of exported filesystems on our `nfs` server.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we'll configure an `nfs` server to share (export) some filesystem
    via NFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an `nfs` module with the following `nfs::exports` class, which defines
    a concat resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `nfs::export` defined type, we''ll use this definition for any `nfs`
    exports we create:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now create the `nfs::server` class, which will include the OS-specific configuration
    for the server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create the `nfs::server::redhat` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `/etc/sysconfig/nfs` support file for RedHat systems in the files
    directory of our `nfs` repo (`modules/nfs/files/nfs`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now create the support class for Debian systems, `nfs::server::debian`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the nfs-common configuration for Debian (which will be placed in `modules/nfs/files/nfs-common`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the `nfs::server` class to a node and then create an export on that node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a collector for the exported resource created by the `nfs::server` class
    in the preceding code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, run Puppet on the node Debian to create the exported resource. Then,
    run Puppet on the cookbook node to mount that resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the mount with `mount`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `nfs::exports` class defines an exec, which runs `''exportfs -a''`, to
    export all filesystems defined in `/etc/exports`. Next, we define a concat resource
    to contain `concat::fragments`, which we will define next in our `nfs::export`
    class. Concat resources specify the file that the fragments are to be placed into;
    `/etc/exports` in this case. Our `concat` resource has a notify for the previous
    exec. This has the effect that whenever `/etc/exports` is updated, we run `''exportfs
    -a''` again to export the new entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We then created an `nfs::export` defined type, which does all the work. The
    defined type adds an entry to `/etc/exports` via a `concat::fragment` resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In the definition, we use the attribute `$where` to define what filesystem we
    are exporting. We use `$who` to specify who can mount the filesystem. The attribute
    `$options` contains the exporting options such as **rw** (**read-write**), **ro**
    (**read-only**). Next, we have the options that will be placed in `/etc/fstab`
    on the client machine, the mount options, stored in `$mount_options`. The `nfs::exports`
    class is included here so that `concat::fragment` has a concat target defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the exported mount resource is created; this is done on the server, so
    the `${::ipaddress}` variable holds the IP address of the server. We use this
    to define the device for the mount. The device is the IP address of the server,
    a colon, and then the filesystem being exported. In this example, it is `''192.168.122.148:/srv/home''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We reuse our `myfw` module and include it in the `nfs::server` class. This
    class illustrates one of the things to consider when writing your modules. Not
    all Linux distributions are created equal. Debian and RedHat deal with NFS server
    configuration quite differently. The `nfs::server` module deals with this by including
    OS-specific subclasses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `nfs::server` module opens several firewall ports for NFS communication.
    NFS traffic is always carried over port 2049 but ancillary systems, such as locking,
    quota, and file status daemons, use ephemeral ports chosen by the portmapper,
    by default. The portmapper itself uses port 111\. So our module needs to allow
    2049, 111, and a few other ports. We attempt to configure the ancillary services
    to use ports 4000 through 4010.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `nfs::server::redhat` class, we modify `/etc/sysconfig/nfs` to use the
    ports specified. Also, we install the nfs-utils package and start the nfs service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the same for Debian-based systems in the `nfs::server::debian` class.
    The packages and services have different names but overall the process is similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'With everything in place, we include the server class to configure the NFS
    server and then define an export:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'What''s important here is that we defined the `tag` attribute, which will be
    used in the exported resource we collect in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We use the spaceship syntax (`<<| |>>`) to collect all the exported mount resources
    that have the tag we defined earlier (`srv_home`). We then use a syntax called
    "override on collect" to modify the name attribute of the mount to specify where
    to mount the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Using this design pattern with exported resources, we can change the server
    exporting the filesystem and have any nodes that mount the resource updated automatically.
    We can have many different nodes collecting the exported mount resource.
  prefs: []
  type: TYPE_NORMAL
- en: Using HAProxy to load-balance multiple web servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load balancers are used to spread a load among a number of servers. Hardware
    load balancers are still somewhat expensive, whereas software balancers can achieve
    most of the benefits of a hardware solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**HAProxy** is the software load balancer of choice for most people: fast,
    powerful, and highly configurable.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, I'll show you how to build an HAProxy server to load-balance
    web requests across web servers. We'll use exported resources to build the `haproxy`
    configuration file just like we did for the NFS example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the file `modules/haproxy/manifests/master.pp` with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the file `modules/haproxy/files/haproxy.cfg` with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify your `manifests/nodes.pp` file as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the slave server configuration in the `haproxy::slave` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `concat` container resource in the `haproxy::config` class as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify `site.pp` to define the master and slave nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Puppet on each of the slave servers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Puppet on the master node to configure and run `haproxy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check the HAProxy stats interface on master port `8080` in your web browser
    (`http://master.example.com:8080`) to make sure everything is okay (The username
    and password are in `haproxy.cfg`, `haproxy`, and `topsecret`). Try going to the
    proxied service as well. Notice that the page changes on each reload as the service
    is redirected from slave1 to slave2 (`http://master.example.com`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We built a complex configuration from various components of the previous sections.
    This type of deployment becomes easier the more you do it. At a top level, we
    configured the master to collect exported resources from slaves. The slaves exported
    their configuration information to allow haproxy to use them in the load balancer.
    As slaves are added to the system, they can export their resources and be added
    to the balancer automatically.
  prefs: []
  type: TYPE_NORMAL
- en: We used our `myfw` module to configure the firewall on the slaves and the master
    to allow communication.
  prefs: []
  type: TYPE_NORMAL
- en: We used the Forge Apache module to configure the listening web server on the
    slaves. We were able to generate a fully functioning website with five lines of
    code (10 more to place `index.html` on the website).
  prefs: []
  type: TYPE_NORMAL
- en: There are several things going on here. We have the firewall configuration and
    the Apache configuration in addition to the `haproxy` configuration. We'll focus
    on how the exported resources and the `haproxy` configuration fit together.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `haproxy::config` class, we created the concat container for the `haproxy`
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We reference this in `haproxy::slave`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We are doing a little trick here with concat; we don''t define the target in
    the exported resource. If we did, the slaves would try and create a `/etc/haproxy/haproxy.cfg`
    file, but the slaves do not have `haproxy` installed so we would get catalog failures.
    What we do is modify the resource when we collect it in `haproxy::master`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to adding the target when we collect the resource, we also add
    a notify so that the `haproxy` service is restarted when we add a new host to
    the configuration. Another important point here is that we set the order attribute
    of the slave configurations to 0010, when we define the header for the `haproxy.cfg`
    file; we use an order value of 0001 to ensure that the header is placed at the
    beginning of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the `haproxy::master` class is concerned with configuring the firewall
    as we did in previous examples.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HAProxy has a vast range of configuration parameters, which you can explore;
    see the HAProxy website at [http://haproxy.1wt.eu/#docs](http://haproxy.1wt.eu/#docs).
  prefs: []
  type: TYPE_NORMAL
- en: Although it's most often used as a web server, HAProxy can proxy a lot more
    than just HTTP. It can handle any kind of TCP traffic, so you can use it to balance
    the load of MySQL servers, SMTP, video servers, or anything you like.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the design we showed to attack many problems of coordination of
    services between multiple servers. This type of interaction is very common; you
    can apply it to many configurations for load balancing or distributed systems.
    You can use the same workflow described previously to have nodes export firewall
    resources (`@@firewall`) to permit their own access.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Docker with Puppet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Docker** is a platform for rapid deployment of containers. Containers are
    like a lightweight virtual machine that might only run a single process. The containers
    in Docker are called docks and are configured with files called Dockerfiles. Puppet
    can be used to configure a node to not only run Docker but also configure and
    start several docks. You can then use Puppet to ensure that your docks are running
    and are consistently configured.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Download and install the Puppet Docker module from the Forge ([https://forge.puppetlabs.com/garethr/docker](https://forge.puppetlabs.com/garethr/docker)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Add these modules to your Puppet repository. The `stahnma-epel` module is required
    for Enterprise Linux-based distributions; it contains the Extra Packages for Enterprise
    Linux YUM repository.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to manage Docker with Puppet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Docker on a node, we just need to include the `docker` class. We''ll
    do more than install Docker; we''ll also download an image and start an application
    on our test node. In this example, we''ll create a new machine called `shipyard.`
    Add the following node definition to `site.pp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Puppet on your shipyard node to install Docker. This will also download
    the `phusion/baseimage docker` image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that your container is running on shipyard using `docker ps`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the dock is running netcat on port 8080 by connecting to the port
    listed previously (`49157`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We began by installing the docker module from the Forge. This module installs
    the `docker-io` package on our node, along with any required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: We then defined a `docker::image` resource. This instructs Puppet to ensure
    that the named image is downloaded and available to docker. On our first run,
    Puppet will make docker download the image. We used `phusion/baseimage` as our
    example because it is quite small, well-known, and includes the netcat daemon
    we used in the example. More information on `baseimage` can be found at [http://phusion.github.io/baseimage-docker/](http://phusion.github.io/baseimage-docker/).
  prefs: []
  type: TYPE_NORMAL
- en: We then went on to define a `docker::run` resource. This example isn't terribly
    useful; it simply starts netcat in listen mode on port 8080\. We need to expose
    that port to our machine, so we define the expose attribute of our `docker::run`
    resource. There are many other options available for the `docker::run` resource.
    Refer to the source code for more details.
  prefs: []
  type: TYPE_NORMAL
- en: We then used docker ps to list the running docks on our shipyard machine. We
    parsed out the listening port on our local machine and verified that netcat was
    listening.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker is a great tool for rapid deployment and development. You can spin as
    many docks as you need on even the most modest hardware. One great use for docker
    is having docks act as test nodes for your modules. You can create a docker image,
    which includes Puppet, and then have Puppet run within the dock. For more information
    on docker, visit [http://www.docker.com/](http://www.docker.com/).
  prefs: []
  type: TYPE_NORMAL
