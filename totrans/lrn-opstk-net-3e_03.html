<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Installing Neutron</h1>
                </header>
            
            <article>
                
<p>OpenStack Networking, also known as Neutron, provides a network infrastructure-as-a-service platform to users of the cloud. In the last chapter, we installed some of the base services of OpenStack, including the Identity, Image, and Compute services. In this chapter, I will guide you through the installation of Neutron networking services on top of the OpenStack environment that we installed in the previous chapter</p>
<p>The components to be installed include the following:</p>
<ul>
<li>Neutron API server</li>
<li>Modular Layer 2 (ML2) plugin</li>
<li>DHCP agent</li>
<li>Metadata agent</li>
</ul>
<p>By the end of this chapter, you will have a basic understanding of the function and operation of various Neutron plugins and agents, as well as a foundation on top of which a virtual switching infrastructure can be built.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic networking elements in Neutron</h1>
                </header>
            
            <article>
                
<p>Neutron constructs the virtual network using elements that are familiar to most system and network administrators, including networks, subnets, ports, routers, load balancers, and more.</p>
<p>Using version 2.0 of the core Neutron API, users can build a network foundation composed of the following entities:</p>
<ul>
<li><strong>Network</strong>: A network is an isolated Layer 2 broadcast domain. Typically, networks are reserved for the projects that created them, but they can be shared among projects if configured accordingly. The network is the core entity of the Neutron API. Subnets and ports must always be associated with a network.</li>
<li><strong>Subnet</strong>: A subnet is an IPv4 or IPv6 address block from which IP addresses can be assigned to virtual machine instances. Each subnet must have a CIDR and must be associated with a network. Multiple subnets can be associated with a single network and can be non-contiguous. A DHCP allocation range can be set for a subnet that limits the addresses provided to instances.</li>
<li><strong>Port</strong>: A port in Neutron is a logical representation of a virtual switch port. Virtual machine interfaces are mapped to Neutron ports, and these ports define both the MAC address and the IP address that is to be assigned to the interfaces plugged into them. Neutron port definitions are stored in the Neutron database, which is then used by the respective plugin agent to build and connect the virtual switching infrastructure.</li>
</ul>
<p>Cloud operators and users alike can configure network topologies by creating and configuring networks and subnets, and then instruct services like Nova to attach virtual devices to ports on these networks. Users can create multiple networks, subnets, and ports, but are limited to thresholds defined by per-project quotas set by the cloud administrator.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending functionality with plugins</h1>
                </header>
            
            <article>
                
<p>The OpenStack Networking project provides reference plugins and drivers that are developed and supported by the OpenStack community, and also supports third-party plugins and drivers that extend network functionality and implementation of the Neutron API. Plugins and drivers can be created that use a variety of software and hardware-based technologies to implement the network built by operators and users.</p>
<p>There are two major plugin types within the Neutron architecture:</p>
<ul>
<li>Core plugin</li>
<li>Service plugin</li>
</ul>
<p>A <strong>core plugin</strong> implements the core Neutron API, and is responsible for adapting the logical network described by networks, ports, and subnets into something that can be implemented by the L2 agent and IP address management system running on the host.</p>
<p>A <strong>service plugin</strong> provides additional network services such as routing, load balancing, firewalling, and more.</p>
<p>In this book, the following core plugin will be discussed:</p>
<ul>
<li>Modular Layer 2 Plugin</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following service plugins will be covered in later chapters:</p>
<ul>
<li>Router</li>
<li>Load balancer</li>
<li>Trunk</li>
</ul>
<div class="packt_infobox">The Neutron API provides a consistent experience to the user despite the chosen networking plugin. For more information on interacting with the Neutron API, please visit the following URL: <span class="URLPACKT"><a href="https://developer.openstack.org/api-ref/network/v2/index.html">https://developer.openstack.org/api-ref/network/v2/index.html</a>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modular Layer 2 plugin</h1>
                </header>
            
            <article>
                
<p>Prior to the inclusion of the <strong>Modular Layer 2 (ML2)</strong> plugin in the Havana release of OpenStack, Neutron was limited to using a single core plugin. This design resulted in homogenous network architectures that were not extensible. Operators were forced to make long-term decisions about the network stack that could not easily be changed in the future. The ML2 plugin, on the other hand, is extensible by design and supports heterogeneous network architectures that can leverage multiple technologies simultaneously. The ML2 plugin replaced two monolithic plugins in its reference implementation: the Linux bridge core plugin and the Open vSwitch core plugin.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drivers</h1>
                </header>
            
            <article>
                
<p>The ML2 plugin introduced the concept of TypeDrivers and Mechanism drivers to separate the types of networks being <em>implemented</em> and the mechanisms for <em>implementing</em> networks of those types.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TypeDrivers</h1>
                </header>
            
            <article>
                
<p>An ML2 <strong>TypeDriver</strong> maintains a type-specific network state, validates provider network attributes, and describes network segments using provider attributes. Provider attributes include network interface labels, segmentation IDs, and network types. Supported network types include <kbd>local</kbd>, <kbd>flat</kbd>, <kbd>vlan</kbd>, <kbd>gre</kbd>, <kbd>vxlan</kbd>, and <kbd>geneve</kbd>. The following table describes the differences between those network types:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 91.1528px">
<p><strong>Type</strong></p>
</td>
<td style="width: 763.847px">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 91.1528px">
<p>Local</p>
</td>
<td style="width: 763.847px">
<p>A <strong>local network</strong> is one that is isolated from other networks and nodes. Instances connected to a local network may communicate with other instances in the same network on the same <kbd>compute</kbd> node, but are unable to communicate with instances on another host. Because of this design limitation, local networks are recommended for testing purposes only.</p>
</td>
</tr>
<tr>
<td style="width: 91.1528px">
<p>Flat</p>
</td>
<td style="width: 763.847px">
<p>In a <strong>flat network</strong>, no 802.1q VLAN tagging or other network segregation takes place. In many environments, a flat network corresponds to an <em>access</em> VLAN or <em>native</em> VLAN on a trunk.</p>
</td>
</tr>
<tr>
<td style="width: 91.1528px">
<p>VLAN</p>
</td>
<td style="width: 763.847px">
<p><strong>VLAN networks</strong> are networks that utilize 802.1q tagging to segregate network traffic. Instances in the same VLAN are considered part of the same network and are in the same Layer 2 broadcast domain. Inter-VLAN routing, or routing between VLANs, is only possible through the use of a physical or virtual router.</p>
</td>
</tr>
<tr>
<td style="width: 91.1528px">GRE</td>
<td style="width: 763.847px">
<p><strong>GRE networks</strong> use the <strong>generic routing encapsulation</strong> tunneling protocol (IP protocol 47) to encapsulate packets and send them over point-to-point networks between nodes. The <kbd>KEY</kbd> field in the GRE header is used to segregate networks.</p>
</td>
</tr>
<tr>
<td style="width: 91.1528px">
<p>VXLAN</p>
</td>
<td style="width: 763.847px">
<p>A <strong>VXLAN network</strong> uses a unique segmentation ID, called a VXLAN Network Identifier (VNI), to differentiate traffic from other VXLAN networks. Traffic from one instance to another is encapsulated by the host using the VNI and sent over an existing Layer 3 network using UDP, where it is decapsulated and forwarded to the instance. The use of VXLAN to encapsulate packets over an existing network is meant to solve limitations of VLANs and physical switching infrastructure.</p>
</td>
</tr>
<tr>
<td style="width: 91.1528px">
<p>GENEVE</p>
</td>
<td style="width: 763.847px">
<p>A <strong>GENEVE network</strong> resembles a VXLAN network, in that it uses a unique segmentation ID, called a virtual network interface (<strong>VNI</strong>), to differentiate traffic from other GENEVE networks. Packets are encapsulated with a unique header and UDP is used as the transport mechanism. GENEVE leverages the benefits of multiple overlay technologies such as VXLAN, NVGRE, and STT and is primarily used by OVN at this time.</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mechanism drivers</h1>
                </header>
            
            <article>
                
<p>An ML2 <strong>Mechanism driver</strong> is responsible for taking information established by the type driver and ensuring that it is properly implemented. Multiple Mechanism drivers can be configured to operate simultaneously, and can be described using three types of models:</p>
<ul>
<li><strong>Agent-based:</strong> Includes Linux bridge, Open vSwitch, SR-IOV, and others</li>
<li><strong>Controller-based:</strong> Includes Juniper Contrail, Tungsten Fabric, OVN, Cisco ACI, VMWare NSX, and others</li>
<li><strong>Top-of-Rack</strong>: Includes Cisco Nexus, Arista, Mellanox, and others</li>
</ul>
<p>Mechanism drivers to be discussed in this book include the following:</p>
<ul>
<li>Linux bridge</li>
<li>Open vSwitch</li>
<li>L2 population</li>
</ul>
<p>The Linux bridge and Open vSwitch ML2 Mechanism drivers are used to configure their respective virtual switching technologies within nodes that host instances and network services. The Linux bridge driver supports <kbd>local</kbd>, <kbd>flat</kbd>, <kbd>vlan</kbd>, and <kbd>vxlan</kbd> network types, while the Open vSwitch driver supports all of those as well as the <kbd>gre</kbd> network type. Support for other type drivers, such as <kbd>geneve</kbd>, will vary based on the implemented Mechanism driver.</p>
<p>The L2 population driver is used to limit the amount of broadcast traffic that is forwarded across the overlay network fabric when VXLAN networks are used. Under normal circumstances, unknown unicast, multicast, and broadcast traffic may be flooded out from all tunnels to other <kbd>compute</kbd> nodes. This behavior can have a negative impact on the overlay network fabric, especially as the number of hosts in the cloud scales out.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As an authority on what instances and other network resources exist in the cloud, Neutron can pre-populate forwarding databases on all hosts to avoid a costly learning operation. ARP proxy, a feature of the L2 population driver, enables Neutron to pre-populate the ARP table on all hosts in a similar manner to avoid ARP traffic from being broadcast across the overlay fabric.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML2 architecture</h1>
                </header>
            
            <article>
                
<p>The following diagram demonstrates how the Neutron API service interacts with the various plugins and agents responsible for constructing the virtual and physical network at a high level:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/74a84179-b5ae-470a-8ae5-e0b84965baf7.png" style="width:45.17em;height:30.58em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Figure 3.1</div>
<p>The preceding diagram demonstrates the interaction between the Neutron API, Neutron plugins and drivers, and services such as the L2 and L3 agents. For more information on the Neutron ML2 plugin architecture, please refer to the following URL: <span class="URLPACKT"><a href="https://docs.openstack.org/neutron/pike/admin/config-ml2.html">https://docs.openstack.org/neutron/pike/admin/config-ml2.html</a></span></p>
<p><span class="URLPACKT"><a href="https://docs.openstack.org/neutron/pike/admin/config-ml2.html"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network namespaces</h1>
                </header>
            
            <article>
                
<p>OpenStack was designed with multi-tenancy in mind, and provides users with the ability to create and manage their own compute and network resources. Neutron supports each tenant having multiple private networks, routers, firewalls, load balancers, and other networking resources, and is able to isolate many of these objects through the use of network namespaces.</p>
<p>A <strong>network namespace</strong> is defined as a logical copy of the network stack with its own routes, firewall rules, and network interfaces. When using the open source reference plugins and drivers, every DHCP server, router, and load balancer that is created by a user is implemented in a network namespace. By using network namespaces, Neutron is able to provide isolated DHCP and routing services to each network, allowing users to create overlapping networks with other users in other projects and even other networks in the same project.</p>
<p>The following naming convention for network namespaces should be observed:</p>
<ul>
<li><strong>DHCP Namespace: </strong><kbd>qdhcp-&lt;network UUID&gt;</kbd></li>
<li><strong>Router Namespace: </strong><kbd>qrouter-&lt;router UUID&gt;</kbd></li>
<li><strong>Load Balancer Namespace: </strong><kbd>qlbaas-&lt;load balancer UUID&gt;</kbd></li>
</ul>
<p>A <kbd>qdhcp</kbd> namespace contains a DHCP service that provides IP addresses to instances using the DHCP protocol. In a reference implementation, <kbd>dnsmasq</kbd> is the process that services DHCP requests. The <kbd>qdhcp</kbd> namespace has an interface plugged into the virtual switch and is able to communicate with instances and other devices in the same network. A <kbd>qdhcp</kbd> namespace is created for every network where the associated subnet(s) have DHCP enabled.</p>
<p>A <kbd>qrouter</kbd> namespace represents a virtual router, and is responsible for routing traffic to and from instances in subnets it is connected to. Like the <kbd>qdhcp</kbd> namespace, the <kbd>qrouter</kbd> namespace is connected to one or more virtual switches depending on the configuration. In some cases, multiple namespaces may be used to plumb the virtual router infrastructure. These additional namespaces, known as <kbd>fip</kbd> and <kbd>snat</kbd>, are used for distributed virtual routers (DVR) and will be discussed later in this book.</p>
<p>A <kbd>qlbaas</kbd> namespace represents a virtual load balancer, and contains a service such as HAProxy that load balances traffic to instances. The <kbd>qlbaas</kbd> namespace is connected to a virtual switch and can communicate with instances and other devices in the same network.</p>
<div class="packt_infobox">Fun fact: The leading <kbd>q</kbd> in the name of the network namespaces stands for Quantum, the original name for the OpenStack Networking service.</div>
<p>Network namespaces of the aforementioned types will only be seen on nodes running the Neutron DHCP, L3, or LBaaS agents, respectively. These services are typically only configured on controllers or dedicated network nodes. When distributed virtual routers are configured, you may find router-related namespaces on <kbd>compute</kbd> nodes as well. The <kbd>ip netns list</kbd> command can be used to list available namespaces, and commands can be executed within the namespace using the following syntax:</p>
<pre>ip netns exec NAMESPACE_NAME &lt;command&gt; </pre>
<p>Commands that can be executed in the namespace include <kbd>ip</kbd>, <kbd>route</kbd>, <kbd>iptables</kbd>, and more. The output of these commands corresponds to data that's specific to the namespace they are executed in. Tools such as <kbd>tcpdump</kbd> can also be executed in a network namespace to assist in troubleshooting the virtual network infrastructure.</p>
<p>For more information on network namespaces, see the man page for <kbd>ip netns</kbd> at the following URL: <span class="URLPACKT"><a href="http://man7.org/linux/man-pages/man8/ip-netns.8.html">http://man7.org/linux/man-pages/man8/ip-netns.8.html</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing and configuring Neutron services</h1>
                </header>
            
            <article>
                
<p>In this installation, the various services that make up OpenStack Networking will be installed on the <kbd>controller</kbd> node rather than a dedicated networking node. The <kbd>compute</kbd> nodes will run L2 agents that interface with the <kbd>controller</kbd> node and provide virtual switch connections to instances.</p>
<div class="packt_infobox">Remember, the configuration settings recommended here and online at <span class="URLPACKT"><a href="http://docs.openstack.org">docs.openstack.org</a> </span>may not be appropriate for production environments.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the Neutron database</h1>
                </header>
            
            <article>
                
<p>Using the <kbd>mysql</kbd> client on the <kbd>controller</kbd> node, create the Neutron database and associated user:</p>
<pre>    <strong># mysql</strong></pre>
<p>Enter the following SQL statements at the <kbd>MariaDB [(none)] &gt;</kbd> prompt:</p>
<pre>CREATE DATABASE neutron; <br/>GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutron';<br/>GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutron'; <br/>quit; </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the Neutron user, role, and endpoint in Keystone</h1>
                </header>
            
            <article>
                
<p>To function properly, Neutron requires that a user, role, and endpoint be created in Keystone. When executed from the <kbd>controller</kbd> node, the following commands will create a user called <kbd>neutron</kbd> in Keystone, associate the <kbd>admin</kbd> role with the <kbd>neutron</kbd> user, and add the <kbd>neutron</kbd> user to the <kbd>service</kbd> project:</p>
<pre><strong>    # source ~/adminrc<br/>    </strong><strong># openstack user create --domain Default --password=neutron neutron<br/>    </strong><strong># openstack role add --project service --user neutron admin</strong></pre>
<p>Create a service in Keystone that describes the OpenStack Networking service by executing the following command on the <kbd>controller</kbd> node:</p>
<pre>    <strong># openstack service create --name neutron <br/>    </strong><strong>--description "OpenStack Networking" network</strong></pre>
<p>To create the endpoints, use the following <kbd>openstack endpoint create</kbd> commands:</p>
<pre><strong>    # openstack endpoint create --region RegionOne<br/></strong><strong>     network public http://controller01:9696<br/></strong><strong>    # openstack endpoint create --region RegionOne<br/></strong><strong>     network internal http://controller01:9696<br/></strong><strong>    # openstack endpoint create --region RegionOne<br/></strong><strong>     network admin http://controller01:9696</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Neutron packages</h1>
                </header>
            
            <article>
                
<p>To install the Neutron API server, the DHCP and metadata agents, and the ML2 plugin on the controller, issue the following command:</p>
<pre>    <strong># apt install neutron-server neutron-dhcp-agent <br/></strong><strong>    neutron-metadata-agent neutron-plugin-ml2 <br/>    </strong><strong>python-neutronclient</strong></pre>
<div class="packt_infobox">The Neutron DHCP and metadata agents may not be required by all Mechanism drivers but are used when implementing the <kbd>openvswitch</kbd> and <kbd>linuxbridge</kbd> drivers.</div>
<p>On all other hosts, only the ML2 plugin is required at this time:</p>
<pre><strong>    # apt install neutron-plugin-ml2</strong></pre>
<p>On all nodes, update the <kbd>[database]</kbd> section of the Neutron configuration file at <kbd>/etc/neutron/neutron.conf</kbd> to use the proper MySQL database connection string based on the preceding values rather than the default value:</p>
<pre>[database] <br/>... <br/>connection = mysql+pymysql://neutron:neutron@controller01/neutron </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Neutron to use Keystone</h1>
                </header>
            
            <article>
                
<p>The Neutron configuration file found at <kbd>/etc/neutron/neutron.conf</kbd> has dozens of settings that can be modified to meet the needs of the OpenStack cloud administrator. A handful of these settings must be changed from their defaults as part of this installation.</p>
<p>To specify Keystone as the authentication method for Neutron, update the <kbd>[DEFAULT]</kbd> section of the Neutron configuration file on all hosts with the following setting:</p>
<pre>[DEFAULT] <br/>... <br/>auth_strategy = keystone </pre>
<p>Neutron must also be configured with the appropriate Keystone authentication settings. The username and password for the <kbd>neutron</kbd> user in Keystone were set earlier in this chapter. Update the <kbd>[keystone_authtoken]</kbd> section of the Neutron configuration file on all hosts with the following settings:</p>
<pre>[keystone_authtoken] <br/>... <br/>auth_uri = http://controller01:5000 <br/>auth_url = http://controller01:35357 <br/>memcached_servers = controller01:11211 <br/>auth_type = password <br/>project_domain_name = default </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre>user_domain_name = default <br/>project_name = service <br/>username = neutron <br/>password = neutron </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Neutron to use a messaging service</h1>
                </header>
            
            <article>
                
<p>Neutron communicates with various OpenStack services on the AMQP messaging bus. Update the <kbd>[DEFAULT]</kbd> section of the Neutron configuration file on all hosts to specify RabbitMQ as the messaging broker:</p>
<pre>[DEFAULT] <br/>... <br/>transport_url = rabbit://openstack:rabbit@controller01 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Nova to utilize Neutron networking</h1>
                </header>
            
            <article>
                
<p>Before Neutron can be utilized as the network manager for OpenStack Compute services, the appropriate configuration options must be set in the Nova configuration file located at <kbd>/etc/nova/nova.conf</kbd> on certain hosts.</p>
<p>On the controller and <kbd>compute</kbd> nodes, update the <kbd>[neutron]</kbd> section with the following:</p>
<pre>[neutron]  <br/>... <br/>url= http://controller01:9696<br/>auth_url = http://controller01:35357<br/>auth_type = password <br/>project_domain_name = default <br/>user_domain_name = default <br/>region_name = RegionOne <br/>project_name = service <br/>username = neutron <br/>password = neutron </pre>
<p>Nova may require additional configuration once a Mechanism driver has been determined. The Linux bridge and Open vSwitch Mechanism drivers and their respective agents and Nova configuration changes will be discussed in further detail in upcoming chapters.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Neutron to notify Nova</h1>
                </header>
            
            <article>
                
<p>Neutron must be configured to notify Nova of network topology changes. On the <kbd>controller</kbd> node, update the <kbd>[nova]</kbd> section of the Neutron configuration file located at <kbd>/etc/neutron/neutron.conf</kbd> with the following settings:</p>
<pre>[nova] <br/>... <br/>auth_url = http://controller01:35357 <br/>auth_type = password <br/>project_domain_name = default <br/>user_domain_name = default <br/>region_name = RegionOne <br/>project_name = service <br/>username = nova <br/>password = nova </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Neutron services</h1>
                </header>
            
            <article>
                
<p>The <kbd>neutron-server</kbd> service exposes the Neutron API to users and passes all calls to the configured Neutron plugins for processing. By default, Neutron is configured to listen for API calls on all configured addresses, as seen by the default <kbd>bind_hosts</kbd> option in the Neutron configuration file:</p>
<pre>bind_host = 0.0.0.0 </pre>
<p>As an additional security measure, it is possible to expose the API on the management or API network. To change the default value, update the <kbd>bind_host</kbd> value in the <kbd>[DEFAULT]</kbd> section of the Neutron configuration located at <kbd>/etc/neutron/neutron.conf</kbd> with the management address of the <kbd>controller</kbd> node. The deployment explained in this book will retain the default value.</p>
<p>Other configuration options that may require tweaking include the following:</p>
<ul>
<li><kbd>core_plugin</kbd></li>
<li><kbd>service_plugins</kbd></li>
<li><kbd>dhcp_lease_duration</kbd></li>
<li><kbd>dns_domain</kbd></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Some of these settings apply to all nodes, while others only apply to the <kbd>network</kbd> or <kbd>controller</kbd> node. The <kbd>core_plugin</kbd> configuration option instructs Neutron to use the specified networking plugin. Beginning with the Icehouse release, the ML2 plugin supersedes both the Linux bridge and Open vSwitch monolithic plugins.</p>
<p>On all nodes, update the <kbd>core_plugin</kbd> value in the <kbd>[DEFAULT]</kbd> section of the Neutron configuration file located at <kbd>/etc/neutron/neutron.conf</kbd> and specify the ML2 plugin:</p>
<pre>[DEFAULT] <br/>... <br/>core_plugin = ml2 </pre>
<p>The <kbd>service_plugins</kbd> configuration option is used to define plugins that are loaded by Neutron for additional functionality. Examples of plugins include <kbd>router</kbd>, <kbd>firewall</kbd>, <kbd>lbaas</kbd>, <kbd>vpnaas</kbd> and <kbd>metering</kbd>. This option should only be configured on the <kbd>controller</kbd> node or any other node running the <kbd>neutron-server</kbd> service. Service plugins will be defined in later chapters.</p>
<p>The <kbd>dhcp_lease_duration</kbd> configuration option specifies the duration of an IP address lease by an instance. The default value is 86,400 seconds, or 24 hours. If the value is set too low, the network may be flooded with traffic due to short leases and frequent renewal attempts. The DHCP client on the instance itself is responsible for renewing the lease, and the frequency of this operation varies between operating systems. It is not uncommon for instances to attempt to renew their lease well before exceeding the lease duration. The value set for <kbd>dhcp_lease_duration</kbd> does not dictate how long an IP address stays associated with an instance, however. Once an IP address has been allocated to a port by Neutron, it remains associated with the port until the port or related instance is deleted.</p>
<p>The <kbd>dns_domain</kbd> configuration option specifies the DNS search domain that is provided to instances via DHCP when they obtain a lease. The default value is <kbd>openstacklocal</kbd>. This can be changed to whatever fits your organization. For the purpose of this installation, change the value from <kbd>openstacklocal</kbd> to <kbd>learningneutron.com.</kbd> On the <kbd>controller</kbd> node, update the <kbd>dns_domain</kbd> option in the Neutron configuration file located at <kbd>/etc/neutron/neutron.conf</kbd> to <kbd>learningneutron.com</kbd>:</p>
<pre>[DEFAULT] <br/>... <br/>dns_domain = learningneutron.com</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When instances obtain their address from the DHCP server, the domain is appended to the hostname, resulting in a fully-qualified domain name. Neutron does not support multiple domain names by default, instead relying on the project known as Designate to extend support for this functionality. More information on Designate can be found at the following URL: <span class="URLPACKT"><a href="https://docs.openstack.org/designate/latest/">https://docs.openstack.org/designate/latest/</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Starting neutron-server</h1>
                </header>
            
            <article>
                
<p>Before the <kbd>neutron-server</kbd> service can be started, the Neutron database must be updated based on the options we configured earlier in this chapter. Use the <kbd>neutron-db-manage</kbd> command on the <kbd>controller</kbd> node to update the database accordingly:</p>
<pre>    <strong># su -s /bin/sh -c "neutron-db-manage <br/>    </strong><strong>--config-file /etc/neutron/neutron.conf <br/>    </strong><strong>--config-file /etc/neutron/plugins/ml2/ml2_conf.ini <br/>    </strong><strong>upgrade head" neutron</strong></pre>
<p>Restart the Nova compute services on the <kbd>controller</kbd> node:</p>
<pre>    <strong># systemctl restart nova-api nova-scheduler nova-conductor</strong></pre>
<p>Restart the Nova compute service on the <kbd>compute</kbd> nodes:</p>
<pre>    <strong># systemctl restart nova-compute</strong></pre>
<p>Lastly, restart the <kbd>neutron-server</kbd> service on the <kbd>controller</kbd> node:</p>
<pre>    <strong># systemctl restart neutron-server</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the Neutron DHCP agent</h1>
                </header>
            
            <article>
                
<p>Neutron utilizes <kbd>dnsmasq</kbd>, a free and lightweight DNS forwarder and DHCP server, to provide DHCP services to networks. The <kbd>neutron-dhcp-agent</kbd> service is responsible for spawning and configuring <kbd>dnsmasq</kbd> and metadata processes for each network that leverages DHCP.</p>
<p>The DHCP driver is specified in the <kbd>/etc/neutron/dhcp_agent.ini</kbd> configuration file. The DHCP agent can be configured to use other drivers, but <kbd>dnsmasq</kbd> support is built-in and requires no additional setup. The default <kbd>dhcp_driver</kbd> value is <kbd>neutron.agent.linux.dhcp.Dnsmasq</kbd> and can be left unmodified.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Other notable configuration options found in the <kbd>dhcp_agent.ini</kbd> configuration file include the following:</p>
<ul>
<li><kbd>interface_driver</kbd></li>
<li><kbd>enable_isolated_metadata</kbd></li>
</ul>
<p>The <kbd>interface_driver</kbd> configuration option should be configured appropriately based on the Layer 2 agent chosen for your environment:</p>
<ul>
<li><strong>Linux bridge</strong>:   <kbd>neutron.agent.linux.interface.BridgeInterfaceDriver</kbd></li>
<li><strong>Open vSwitch</strong>: <kbd>neutron.agent.linux.interface.OVSInterfaceDriver</kbd></li>
</ul>
<p>Both the Linux bridge and Open vSwitch drivers will be discussed in further detail in upcoming chapters. For now, the default value of <kbd>&lt;none&gt;</kbd> will suffice.</p>
<div class="packt_infobox">Only one interface driver can be configured at a time per agent.</div>
<p>The <kbd>enable_isolated_metadata</kbd> configuration option is useful in cases where a physical network device such as a firewall or router serves as the default gateway for instances, but Neutron is still required to provide metadata services to those instances. When the L3 agent is used, an instance reaches the metadata service through the Neutron router that serves as its default gateway. An isolated network is assumed to be one in which a Neutron router is not serving as the gateway, but Neutron still handles DHCP requests for the instances. This is often the case when instances are leveraging flat or VLAN networks with physical gateway devices. The default value for <kbd>enable_isolated_metadata</kbd> is <kbd>False</kbd>. When set to <kbd>True</kbd>, Neutron can provide instances with a static route to the metadata service via DHCP in certain cases. More information on the use of metadata and this configuration can be found in <em><a href="dcaa0beb-6648-4d55-9ea7-f4789315539f.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a></em>, <em>Attaching Instances to Networks</em>. On the <kbd>controller</kbd> node, update the <kbd>enable_isolated_metadata</kbd> option in the DHCP agent configuration file located at <kbd>/etc/neutron/dhcp_agent.ini</kbd> to <kbd>True</kbd>:</p>
<pre>[DEFAULT] <br/>... <br/>enable_isolated_metadata = True </pre>
<p>Configuration options not mentioned here have sufficient default values and should not be changed unless your environment requires it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Restarting the Neutron DHCP agent</h1>
                </header>
            
            <article>
                
<p>Use the following commands to restart the <kbd>neutron-dhcp-agent</kbd> service on the <kbd>controller</kbd> node and check its status:</p>
<pre>    <strong># systemctl restart neutron-dhcp-agent<br/>    </strong><strong># systemctl status neutron-dhcp-agent</strong></pre>
<p>The output should resemble the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/f7befff5-2ea0-46bb-9bb3-c772d28b3334.png"/></div>
<p>The agent should be in an <kbd>active (running)</kbd> status. Use the <kbd>openstack network agent list</kbd> command to verify that the service has checked in:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a9aea419-11a2-4f30-a2d4-37228ee90a70.png"/></div>
<p>A smiley face under the <kbd>Alive</kbd> column means that the agent is properly communicating with the <kbd>neutron-server</kbd> service.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the Neutron metadata agent</h1>
                </header>
            
            <article>
                
<p>OpenStack Compute provides a metadata service that enables users to retrieve information about their instances that can be used to configure or manage the running instance. <strong>Metadata</strong> includes information such as the hostname, fixed and floating IPs, public keys, and more. In addition to metadata, users can access <strong>userdata</strong> such as scripts and other bootstrapping configurations that can be executed during the boot process or once the instance is active. OpenStack Networking implements a proxy that forwards metadata requests from instances to the metadata service provided by OpenStack Compute.</p>
<p>Instances typically access the metadata service over HTTP at <span class="URLPACKT"><kbd>http://169.254.169.254</kbd></span> during the boot process. This mechanism is provided by <kbd>cloud-init</kbd>, a utility found on most cloud-ready images and available at the following URL: <a href="https://launchpad.net/cloud-init"><span class="URLPACKT">https://launchpad.net/cloud-init</span></a>.</p>
<p>The following diagram provides a high-level overview of the retrieval of metadata from an instance when the <kbd>controller</kbd> node hosts networking services:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/6851a731-a7a9-44ae-81c5-ac8cc919fed7.png" style="width:43.67em;height:35.58em;"/></div>
<p>In the preceding diagram, the following actions take place when an instance makes a request to the metadata service:</p>
<ul>
<li>An instance sends a request for metadata to <kbd>169.254.269.254</kbd> via HTTP</li>
<li>The metadata request hits either the router or DHCP namespace depending on the route in the instance</li>
<li>The metadata proxy service in the namespace sends the request to the Neutron metadata agent service via a Unix socket</li>
<li>The Neutron metadata agent service forwards the request to the Nova metadata API service</li>
<li>The Nova metadata API service responds to the request and forwards the response to the Neutron metadata agent service</li>
<li>The Neutron metadata agent service sends the response back to the metadata proxy service in the namespace</li>
<li>The metadata proxy service forwards the HTTP response to the instance</li>
<li>The instance receives the metadata and/or the user data and continues the boot process</li>
</ul>
<p>For proper operation of metadata services, both Neutron and Nova must be configured to communicate together with a shared secret. Neutron uses this secret to sign the <kbd>Instance-ID</kbd> header of the metadata request to prevent spoofing. On the <kbd>controller</kbd> node, update the following metadata options in the <kbd>[neutron]</kbd> section of the Nova configuration file located at <kbd>/etc/nova/nova.conf</kbd>:</p>
<pre>[neutron] <br/>... <br/>service_metadata_proxy = true <br/>metadata_proxy_shared_secret = MetadataSecret123 </pre>
<p>Next, update the <kbd>[DEFAULT]</kbd> section of the metadata agent configuration file located at <kbd>/etc/neutron/metadata_agent.ini</kbd> with the Neutron authentication details and the metadata proxy shared secret:</p>
<pre>[DEFAULT] <br/>... <br/>nova_metadata_host = controller01 <br/>metadata_proxy_shared_secret = MetadataSecret123 </pre>
<p>Configuration options not mentioned here have sufficient default values and should not be changed unless your environment requires it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Restarting the Neutron metadata agent</h1>
                </header>
            
            <article>
                
<p>Use the following command to restart the <kbd>neutron-metadata-agent</kbd> and <kbd>nova-api</kbd> services on the <kbd>controller</kbd> node and to check the services' status:</p>
<pre><strong>    # systemctl restart nova-api neutron-metadata-agent<br/>    </strong><strong># systemctl status neutron-metadata-agent</strong></pre>
<p>The output should resemble the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1c5649e2-d2d1-451c-bf78-f570601b3cde.png"/></div>
<p>The agent should be in an <kbd>active (running)</kbd> status. Use the <kbd>openstack network agent list</kbd> command to verify that the service has checked in:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a2248299-e4a2-4223-8230-ebdbe09bde0e.png"/></div>
<p>A smiley face under the <kbd>Alive</kbd> column means that the agent is properly communicating with the <kbd>neutron-server</kbd> service.</p>
<p>If the services do not appear or have <kbd>XXX</kbd> under the <kbd>Alive</kbd> column, check the respective log files located at <kbd>/var/log/neutron</kbd> for assistance in troubleshooting. More information on the use of metadata can be found in <em><a href="dcaa0beb-6648-4d55-9ea7-f4789315539f.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a></em>, <em>Attaching Instances to Networks</em>, and later chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interfacing with OpenStack Networking</h1>
                </header>
            
            <article>
                
<p>The OpenStack Networking APIs can be accessed in a variety of ways, including via the Horizon dashboard, the <kbd>openstack</kbd> and <kbd>neutron</kbd> clients, the Python SDK, HTTP, and other methods. The following few sections will highlight the most common ways of interfacing with OpenStack Networking.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the OpenStack command-line interface</h1>
                </header>
            
            <article>
                
<p>Prior to the <kbd>openstack</kbd> command-line client coming on the scene, each project was responsible for maintaining its own client. Each client often used its own syntax for managing objects and the lack of consistency between clients made life for users and operators difficult. The <kbd>openstack</kbd> client provides a consistent naming structure for commands and arguments, along with a consistent output format with optional parsable formats such as csv, json, and others. Not all APIs and services are supported by the <kbd>openstack</kbd> client, however, which may mean that a project-specific client is required for certain actions.</p>
<p>To invoke the <kbd>openstack</kbd> client, issue the <kbd>openstack</kbd> command at the Linux command line:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1bf6ed78-d603-4915-9ff0-7a17577e9ea6.png" style="width:19.42em;height:2.92em;"/></div>
<p>The <kbd>openstack</kbd> shell provides commands that can be used to create, read, update, and delete the networking configuration within the OpenStack cloud. By typing a question mark or <kbd>help</kbd> within the <kbd>openstack</kbd> shell, a list of commands can be found. Additionally, running <kbd>openstack help</kbd> from the Linux command line provides a brief description of each command's functionality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Neutron command-line interface</h1>
                </header>
            
            <article>
                
<p>Neutron provides a command-line client for interfacing with its API. Neutron commands can be run directly from the Linux command line, or the Neutron shell can be invoked by issuing the <kbd>neutron</kbd> command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a551687e-ada2-47b4-905f-bb1d10a63d8a.png"/></div>
<div class="packt_tip">You must source the credentials file prior to invoking the <kbd>openstack</kbd> and <kbd>neutron</kbd> clients or an error will occur.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>neutron</kbd> shell provides commands that can be used to create, read, update, and delete the networking configuration within the OpenStack cloud. By typing a question mark or <kbd>help</kbd> within the Neutron shell, a list of commands can be found. Additionally, running <kbd>neutron help</kbd> from the Linux command line provides a brief description of each command's functionality.</p>
<p>The <kbd>neutron</kbd> client has been deprecated in favor of the <kbd>openstack</kbd> command-line client. However, certain functions, including LBaaS-related commands, are not yet available within the <kbd>openstack</kbd> client and must be managed using the <kbd>neutron</kbd> client. In future releases of OpenStack, the <kbd>neutron</kbd> client will no longer be available.</p>
<p>Many of the commands listed within the client's <kbd>help</kbd> listing will be covered in subsequent chapters of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the OpenStack Python SDK</h1>
                </header>
            
            <article>
                
<p>The OpenStack Python SDK is available on PyPI and can be installed with the following command:</p>
<pre>    $ pip install openstacksdk</pre>
<p>Documentation for the SDK is available at the following URL: <a href="https://developer.openstack.org/sdks/python/openstacksdk/users/index.html"><span class="URLPACKT">https://developer.openstack.org/sdks/python/openstacksdk/users/index.html</span></a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the cURL utility</h1>
                </header>
            
            <article>
                
<p>The OpenStack Networking API is REST-based and can be manipulated directly using HTTP. To make API calls using HTTP, you will need a token. Source the OpenStack credentials file and use the <kbd>openstack token issue</kbd> command shown here to retrieve a token:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/79cf5e5e-67d9-4d4a-a92f-e6f8d7bdab77.png"/></div>
<div class="packt_tip">The <kbd>--fit-width</kbd> argument is not necessary in normal operations, but helps make the token ID manageable for demonstration purposes.</div>
<p>To get a list of networks, the command should resemble the following:</p>
<pre>    <strong>$ curl -v -X GET -H 'X-Auth-Token: &lt;token id&gt;'<br/>      </strong><strong>http://controller01:9696/v2.0/networks</strong></pre>
<p>The output will resemble the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/31aba40b-482e-4d7a-bd35-62fa9adcdcbc.png" style="width:54.58em;height:34.08em;"/></div>
<p>In this example, the Neutron API returned a 200 OK response in json format. No networks currently exist, so an empty list was returned. Neutron returns HTTP status codes that can be used to determine if the command was successful.</p>
<p>The OpenStack Networking API is documented at the following URL: <span class="URLPACKT"><a href="https://developer.openstack.org/api-ref/network/v2/">https://developer.openstack.org/api-ref/network/v2/</a>.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>OpenStack Networking provides an extensible plugin architecture that makes implementing new network features possible. Neutron maintains the logical network architecture in its database, and network plugins and agents on each node are responsible for configuring virtual and physical network devices accordingly. Using the Modular Layer 2 (ML2) plugin, developers can spend less time implementing core Neutron API functionality and more time developing value-added features.</p>
<p>Now that OpenStack Networking services have been installed across all nodes in the environment, configuration of the Mechanism driver is all that remains before instances can be created. In the following two chapters, you will be guided through the configuration of the ML2 plugin and both the Linux bridge and Open vSwitch drivers and agents. We will also explore the differences between Linux bridge and Open vSwitch agents in terms of how they function and provide connectivity to instances.</p>


            </article>

            
        </section>
    </body></html>