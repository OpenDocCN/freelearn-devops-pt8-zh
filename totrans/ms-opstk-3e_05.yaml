- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: OpenStack Storage – Block, Object, and File Shares
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 存储 – 块存储、对象存储和文件共享
- en: “Perseverance, secret of all triumphs.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “毅力是所有胜利的秘诀。”
- en: – Victor Hugo
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 维克多·雨果
- en: The wide adoption of OpenStack services has increased user demand and, in turn,
    its array of capabilities, as demonstrated in the previous chapter with the compute
    service. This feature variation has also extended the OpenStack storage offering.
    Users running workloads on top of an OpenStack environment would require more
    than one storage type in addition to a robust and reliable storage solution out
    of the box. The spread of the **software-defined storage** ( **SDS** ) approach
    has enabled the OpenStack community to adopt more storage projects in which data
    storage is abstracted from physical storage. Storage systems can be built on top
    of commodity hardware, but that would require a good understanding of each storage
    project’s purpose and the architecture behind it before exposing the service to
    cloud users so that they can run workloads.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 服务的广泛采用增加了用户需求，从而也扩展了其功能，正如上一章中对计算服务的展示那样。这种功能的变化也扩展了 OpenStack 的存储服务。运行工作负载的用户除了需要一个强大可靠的开箱即用的存储解决方案外，还需要多种存储类型。**软件定义存储**（**SDS**）方法的推广使得
    OpenStack 社区能够采纳更多的存储项目，其中数据存储已从物理存储中抽象出来。存储系统可以建立在通用硬件之上，但这需要对每个存储项目的目的和背后的架构有充分的了解，然后才能将该服务暴露给云用户，以便他们运行工作负载。
- en: 'In this chapter, an overwhelming array of storage services in OpenStack will
    be discussed and enabled in our existing OpenStack environment. Throughout this
    chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论并启用在现有 OpenStack 环境中的大量存储服务。在本章中，我们将涵盖以下主题：
- en: Reviewing the Cinder Block Storage service in OpenStack
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾 OpenStack 中的 Cinder 块存储服务
- en: Expanding the Block Storage setup by configuring the three most used driver
    backends, using Kolla-Ansible for LVM, NFS, and Ceph
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过配置三个最常用的驱动后端，使用 Kolla-Ansible 来扩展块存储设置，包括 LVM、NFS 和 Ceph
- en: Uncovering the state of scheduling in Cinder and enabling a weighing mechanism
    in the existing cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭示 Cinder 中的调度状态并在现有集群中启用加权机制
- en: Discussing the Swift Object Storage service in OpenStack
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论 OpenStack 中的 Swift 对象存储服务
- en: Deploying an Object Storage cluster using kolla-ansible
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 kolla-ansible 部署对象存储集群
- en: Exploring the latest updates on the file-sharing service, Manila
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索最新的文件共享服务 Manila 更新
- en: Integrating the Manila service and deploying it using **kolla-ansible**
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成 Manila 服务并使用 **kolla-ansible** 部署它
- en: Defining Block Storage – Cinder
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义块存储 – Cinder
- en: Cinder provides persistent storage for instances. The Block Storage service
    has been extensively developed through each OpenStack release to support more
    features and vendors’ backend drivers, allowing for a variety of storage devices
    to be used. Most of the used storage backends include **logical volume management**
    ( **LVM** ), IBM storage drivers, NetApp, and Dell storage. Within the latest
    OpenStack releases, new backend drivers have been supported, such as Yadro, TOYOU,
    and Pure Storage FlashArray.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Cinder 为实例提供持久存储。块存储服务通过每个 OpenStack 版本的更新得到了广泛发展，以支持更多的功能和厂商的后端驱动程序，从而支持多种存储设备的使用。常用的存储后端包括
    **逻辑卷管理**（**LVM**）、IBM 存储驱动程序、NetApp 和 Dell 存储。在最新的 OpenStack 版本中，支持了新的后端驱动程序，如
    Yadro、TOYOU 和 Pure Storage FlashArray。
- en: The storage device is reached by Cinder via its management access API and grants
    direct access to the instances to access and attach volumes. A volume attached
    to an instance is shown as an additional hard drive can be partitioned and mounted
    onto the filesystem of a virtual machine. The volumes are generally accessed by
    the defined storage path, which could use iSCSI, NFS, and Fiber Channels to do
    so. It is important to bear in mind that Block Storage is persistent, which means
    it grants no data loss when an instance is terminated, unlike an ephemeral disk.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 存储设备通过 Cinder 的管理访问 API 访问，并提供对实例的直接访问，以便访问和附加卷。附加到实例的卷显示为一个额外的硬盘，可以分区并挂载到虚拟机的文件系统上。卷通常通过定义的存储路径进行访问，可以使用
    iSCSI、NFS 和光纤通道等方式。需要记住的是，块存储是持久化的，这意味着当实例终止时不会丢失数据，这与临时磁盘不同。
- en: 'In our multi-node environment layout, the latest novelty of the Cinder service
    can be illustrated as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的多节点环境布局中，Cinder 服务的最新新颖性可以如下所示：
- en: '![Figure 5.1 – Block Storage core architecture](img/B21716_05_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 块存储核心架构](img/B21716_05_01.jpg)'
- en: Figure 5.1 – Block Storage core architecture
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 块存储核心架构。
- en: 'The core Cinder components can be summarized as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 核心 Cinder 组件可以总结如下：
- en: '**cinder-api** : Processes block storage REST API requests and responses'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cinder-api**：处理块存储 REST API 请求和响应。'
- en: '**cinder-scheduler** : Like **nova-scheduler** in the Compute service, it redirects
    after filtering step requests to the appropriate **cinder-volume** server that
    will handle and provision the requested volume'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cinder-scheduler**：像计算服务中的**nova-scheduler**一样，它在过滤步骤后将请求重定向到适当的**cinder-volume**
    服务器，该服务器将处理并配置请求的卷。'
- en: '**cinder-volume** : Acts as a volume manager and runs in each storage node'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cinder-volume**：作为一个卷管理器，并在每个存储节点上运行。'
- en: '**cinder-backup** : Enables volume backups to different storage systems'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cinder-backup**：启用将卷备份到不同存储系统的功能。'
- en: 'The default OpenStack deployment would come with a Block Storage backend based
    on LVM. The other very common use case of the backend is the **Network File System**
    ( **NFS** ), which leverages existing shared storage. Here, the only thing that
    you need to do is configure the Cinder service so that it can use the NFS driver.
    It is important to keep up to date with the latest supported backend drivers by
    checking out the Cinder driver features matrix, available here: [https://docs.openstack.org/cinder/latest/reference/support-matrix.html](https://docs.openstack.org/cinder/latest/reference/support-matrix.html)
    .'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 OpenStack 部署将配备基于 LVM 的块存储后端。另一个非常常见的后端使用场景是**网络文件系统**（**NFS**），它利用现有的共享存储。在这种情况下，唯一需要做的就是配置
    Cinder 服务，使其能够使用 NFS 驱动程序。通过查看 Cinder 驱动程序功能矩阵，确保了解最新支持的后端驱动程序，具体内容请参见：[https://docs.openstack.org/cinder/latest/reference/support-matrix.html](https://docs.openstack.org/cinder/latest/reference/support-matrix.html)。
- en: The driver’s storage backends should support the minimum required volume functions,
    such as volume attachment, detachment, creation, deletion, extension, migration,
    and the creation of an image from a volume. The drivers should also provide snapshot
    management features such as creation, deletion, and volume creation from a snapshot
    or a cloned volume. A more extensive list of features can be considered optional
    within the latest OpenStack releases, depending on each backend driver, such as
    thin volume provisioning, live migration, multi-attach support, and **quality
    of service** ( **QoS** ). Make sure you scan the existing storage backends and
    reflect the features supported in the matrix before moving to a full-block storage
    deployment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序的存储后端应支持最基本的卷功能，如卷附加、分离、创建、删除、扩展、迁移以及从卷创建镜像。驱动程序还应提供快照管理功能，如创建、删除以及从快照或克隆卷创建卷。在最新的
    OpenStack 版本中，可以根据每个后端驱动程序考虑更广泛的功能，这些功能在最新版本中被视为可选，如精简卷配置、实时迁移、多附加支持和**服务质量**（**QoS**）。在进行完整块存储部署之前，请确保扫描现有存储后端并在矩阵中反映支持的功能。
- en: In addition to the aforementioned drivers’ capabilities, Cinder supports a variety
    of configurable drivers for different storage vendors, such as NetApp, IBM, VMware,
    Dell, and Synology. Bear in mind that not all backend products support all Cinder
    driver features. On the other hand, the aforementioned Cinder operations are considered
    *must-haves* when including a new driver storage backend in the Cinder code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述驱动程序的功能外，Cinder 还支持各种可配置的驱动程序，适用于不同的存储厂商，如 NetApp、IBM、VMware、Dell 和 Synology。请记住，并非所有后端产品都支持所有
    Cinder 驱动程序功能。另一方面，前述的 Cinder 操作被视为在将新驱动存储后端包括到 Cinder 代码中时的*必备项*。
- en: Security in Block Storage has been also tweaked by elaborating more on encryption
    in transit and at rest. The Antelope release supports most of the Cinder driver’s
    TLS, which was a blocker in old releases for security teams when they dealt with
    traffic initiated from different service endpoints, as well as end users. Encryption
    at rest has also reached a milestone, allowing data in volumes to be encrypted
    using the Cinder API.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 块存储的安全性也通过更详细的关于传输中和静态加密的内容进行了调整。Antelope 版本支持大多数 Cinder 驱动程序的 TLS，这是旧版本中安全团队处理来自不同服务端点以及终端用户发起的流量时的一个障碍。静态加密也达到了一个里程碑，允许使用
    Cinder API 对卷中的数据进行加密。
- en: Another recent Cinder addition is the Block Storage backup capability. It has
    become easier to simply fire a command line and instruct Cinder to either make
    a full backup of the volume or an incremental one if the backup exists.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个最近的 Cinder 增强功能是块存储备份功能。只需发出一个命令行，指示 Cinder 执行卷的完全备份或增量备份（如果备份已存在），变得更加简单。
- en: Expanding storage with multiple backends
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个后端扩展存储
- en: Our initial deployment highlighted the usage of a dedicated node to deal with
    storage and volume management in the OpenStack environment. In the next section,
    we will expand the initial deployment highlighted in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right Way (DevSecOps)* . Depending on
    which storage backends will be used, additional nodes will be dedicated to hosting
    Cinder volumes as part of the data plane. Cloud controller nodes host the Cinder
    API and scheduling services.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始部署突出了使用专用节点来处理 OpenStack 环境中的存储和卷管理。在接下来的部分中，我们将扩展初始部署，详细介绍 [*第2章*](B21716_02.xhtml#_idTextAnchor089)
    ，*以正确的方式启动 OpenStack 设置 – DevSecOps*。根据使用的存储后端，可能会有额外的节点专门用于托管 Cinder 卷，作为数据平面的一部分。云控制器节点托管
    Cinder API 和调度服务。
- en: Deploying with LVM
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 LVM 部署
- en: OpenStack’s default block storage backend is LVM. The **cinder-volume** service
    uses the iSCSI target to operate and manage access to the logical volumes in the
    storage node.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 的默认块存储后端是 LVM。**cinder-volume** 服务使用 iSCSI 目标来操作和管理存储节点中逻辑卷的访问。
- en: 'On the storage nodes, we will create an LVM physical volume group. In this
    example, we assume that **/dev/sdb** is an available block device on the target
    storage node:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储节点上，我们将创建一个 LVM 物理卷组。在本示例中，我们假设 **/dev/sdb** 是目标存储节点上可用的块设备：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Depending on the operating system, disk partitions might have different name
    listings. You can check these out by using the **fdisk** command line.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据操作系统的不同，磁盘分区的名称可能会有所不同。你可以通过使用 **fdisk** 命令行工具检查这些名称。
- en: 'Next, create a new volume group named **cinder-volumes** :'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个名为 **cinder-volumes** 的新卷组：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the **kolla-ansible** code repository, add the new storage hostname to the
    inventory file located under **/ansible/inventory** , named **multi_packtpub_prod**
    . In this example, we will use the **storage02.os** storage node:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **kolla-ansible** 代码库中，将新存储主机名添加到位于 **/ansible/inventory** 中的库存文件 **multi_packtpub_prod**。在此示例中，我们将使用
    **storage02.os** 存储节点：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Enable the LVM backend in the **/** **etc/kolla/globals.yml** file:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **/** **etc/kolla/globals.yml** 文件中启用 LVM 后端：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Important note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Once the LVM backend has been enabled, **kolla-ansible** activates the iSCSI
    module that comes with the Ubuntu operating system and is configured by default
    in the Cinder role, located in the **/kolla-ansible/ansible/roles/cinder/defaults/main.yml**
    file. For CentOS distributions, make sure that the **LioAdm iSCSI** target helper
    is installed beforehand.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 LVM 后端后，**kolla-ansible** 激活了 Ubuntu 操作系统自带的 iSCSI 模块，并且在 Cinder 角色中默认配置，该角色位于
    **/kolla-ansible/ansible/roles/cinder/defaults/main.yml** 文件中。对于 CentOS 发行版，确保提前安装了
    **LioAdm iSCSI** 目标助手。
- en: 'Configure the name of the Cinder volume group created previously in the **/**
    **etc/kolla/globals.yml** file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **/** **etc/kolla/globals.yml** 文件中配置之前创建的 Cinder 卷组的名称：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Run the job CI/CD pipeline in your staging environment before promoting the
    additional node to the production one. Once deployed, a new kolla container running
    **cinder-volume** should be running in the target storage node.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在将额外的节点提升为生产节点之前，首先在你的暂存环境中运行 CI/CD 流水线。一旦部署，目标存储节点应运行一个新的 kolla 容器，容器内运行 **cinder-volume**
    服务。
- en: Deploying with NFS
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 NFS 部署
- en: NFS is another commonly used storage backend. Within existing NFS storage, cloud
    operators can smoothly manage volumes via the NFS protocol. Created shares will
    be available and mounted by the **cinder-volume** service to compute nodes. The
    Cinder NFS driver enables access to file images on the shared NFS server and takes
    care of mapping the files to instances as block storage.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: NFS 是另一个常用的存储后端。在现有的 NFS 存储中，云操作员可以通过 NFS 协议平稳地管理卷。创建的共享将可用并由 **cinder-volume**
    服务挂载到计算节点上。Cinder NFS 驱动程序使得访问共享 NFS 服务器上的文件镜像成为可能，并将文件映射为块存储提供给实例。
- en: In the following example, an NFS server named **nfs-host-pp** with the **nfs-host-pp:/nfs/share/cinder**
    share file path is available.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，可用的 NFS 服务器名为 **nfs-host-pp**，共享文件路径为 **nfs-host-pp:/nfs/share/cinder**。
- en: 'Create a share file in the target storage node by running the following command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令，在目标存储节点上创建共享文件：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Make sure the NFS client is installed in both the compute and storage nodes.
    You can do this by using Ansible or the following command-line tool:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在计算节点和存储节点上都安装了 NFS 客户端。你可以使用 Ansible 或以下命令行工具来完成此操作：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Enable **cinder-volume** in the Block Storage node to access the share file
    by adjusting the file permissions:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在块存储节点中启用**cinder-volume**，通过调整文件权限访问共享文件：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the NFS backend, make sure you specify the path of the share directory on
    the storage node, the storage network, and the access modes, including share,
    read-write, and non-root remote user access in the **/** **etc/exports** file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NFS 后端中，确保在**/etc/exports**文件中指定存储节点上的共享目录路径、存储网络和访问模式，包括共享、读写和非 root 远程用户访问：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For custom NFS deployment settings, you can create a new configuration file
    under **/etc/kolla/config** that will take precedence when running **kolla-ansible**
    . The following example configuration file, named **nfs_shares** , will include
    a target storage node, **storage02.os** , with custom NFS-mounted file shares:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自定义 NFS 部署设置，您可以在**/etc/kolla/config**下创建一个新的配置文件，在运行**kolla-ansible**时会优先使用该文件。以下示例配置文件，名为**nfs_shares**，将包含一个目标存储节点**storage02.os**，并带有自定义的
    NFS 挂载文件共享：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Enable the NFS backend in the **/** **etc/kolla/globals.yml** file:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在**/etc/kolla/globals.yml**文件中启用 NFS 后端：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the job CI/CD pipeline with a newly configured NFS share will restart
    the **cinder-volume** service in the target storage node. Once deployed and running,
    the new NFS share directory should be mounted and visible in the storage node.
    On the storage node, run the following command line to validate the mapped NFS
    mount:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过新的 NFS 共享配置运行作业 CI/CD 流水线时，将会重启目标存储节点中的**cinder-volume**服务。部署并运行后，新的 NFS
    共享目录应已挂载并在存储节点中可见。在存储节点上，运行以下命令行验证已映射的 NFS 挂载：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Each NFS share uses a mount point under **/etc/cinder/nfs_share** with a hashed
    directory, **223af296419e436d9142928374d8e57e** , that will host any newly created
    volume. You can check this by running the following command line:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 NFS 共享使用**/etc/cinder/nfs_share**下的一个挂载点，并带有哈希目录**223af296419e436d9142928374d8e57e**，该目录将托管任何新创建的卷。您可以通过运行以下命令行来检查：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The new volume should be located under the hashed directory of the NFS mount
    point:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 新卷应位于 NFS 挂载点的哈希目录下：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: NFS storage is quite a popular storage backend in OpenStack. Another type of
    backend is based on SDS, which will be covered in the next section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NFS 存储在 OpenStack 中是一种非常流行的存储后端。另一种类型的后端基于 SDS，下一节将会介绍该内容。
- en: Deploying with Ceph
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Ceph 部署
- en: Ceph’s SDS system gained significant success since its early days. Ceph is designed
    to scale massively to exabytes of storage pools that can run on commodity x86
    hardware architecture. On top of that, Ceph interfaces support most storage types,
    including object, block, and file shares.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Ceph 的早期以来，其 SDS 系统取得了显著的成功。Ceph 旨在大规模扩展至可运行在普通 x86 硬件架构上的 exabyte 存储池。此外，Ceph
    接口支持大多数存储类型，包括对象存储、块存储和文件共享。
- en: The OpenStack Foundation has committed to ensuring the integration of Ceph,
    and it can be incorporated and utilized within an existing OpenStack environment.
    At its core is the **Reliable Autonomic Distributed Object Store** ( **RADOS**
    ), which deals with the distribution, replication, and management of objects within
    the Ceph storage cluster. Based on Ceph’s RADOS design, operators can define Ceph
    storage pools to be used as block storage and provide volumes to OpenStack instances.
    This can be achieved by another mature Cinder backend driver known as a **RADOS
    Block** **Device** ( **RBD** ).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 基金会承诺确保 Ceph 的集成，并且可以将其纳入现有的 OpenStack 环境中。其核心是**可靠的自适应分布式对象存储**（**RADOS**），用于处理
    Ceph 存储集群中对象的分发、复制和管理。基于 Ceph 的 RADOS 设计，操作员可以定义 Ceph 存储池，作为块存储并为 OpenStack 实例提供卷。这可以通过另一种成熟的
    Cinder 后端驱动程序实现，称为**RADOS 块** **设备**（**RBD**）。
- en: Important note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Ceph’s official documentation provides a complete description of the Ceph cluster
    setup using cephadm, available at [https://docs.ceph.com/en/latest/cephadm/install/](https://docs.ceph.com/en/latest/cephadm/install/)
    .
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph 的官方文档提供了使用 cephadm 设置 Ceph 集群的完整说明，地址为[https://docs.ceph.com/en/latest/cephadm/install/](https://docs.ceph.com/en/latest/cephadm/install/)。
- en: 'Ceph defines a set of core logical components:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph 定义了一组核心逻辑组件：
- en: '**Object Storage Devices** ( **OSDs** ): These correspond to the physical disks
    within a filesystem, such as **XFS** or **Btrfs** .'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象存储设备**（**OSDs**）：这些对应于文件系统中的物理磁盘，如**XFS**或**Btrfs**。'
- en: '**Monitor daemon server** ( **MON** ): Watches the state of data consistency
    and other metrics in each OSD node.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监视守护进程服务器**（**MON**）：监控每个 OSD 节点的数据一致性状态和其他度量指标。'
- en: '**Pool** : Provides mapping for stored objects in an OSD.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池**: 提供存储在 OSD 中的对象映射。'
- en: '**Placement groups** ( **PGs** ): Maps of each object stored and OSDs. Replicates
    objects across multiple OSDs within a pool.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**放置组** (**PGs**): 存储的每个对象和 OSD 的映射。将对象复制到池内多个 OSD 中。'
- en: 'If you have an existing Ceph cluster or are willing to deploy a new one, the
    following walk-through will guide you through the required steps for a basic Ceph
    integration in OpenStack. In the following example, a dedicated cluster of three
    Ceph nodes runs **ceph-osd** . Optionally, **ceph-mon** will be running in the
    Cloud Controller node, as shown in the following figure:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个现有的 Ceph 集群或愿意部署一个新的集群，下面的操作指南将引导你完成在 OpenStack 中进行基本 Ceph 集成所需的步骤。在以下示例中，三个
    Ceph 节点组成的专用集群运行 **ceph-osd**。可选地，**ceph-mon** 会在云控制节点上运行，如下图所示：
- en: '![Figure 5.2 – Ceph storage integration in OpenStack](img/B21716_05_02.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – OpenStack 中的 Ceph 存储集成](img/B21716_05_02.jpg)'
- en: Figure 5.2 – Ceph storage integration in OpenStack
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – OpenStack 中的 Ceph 存储集成
- en: 'Make sure you have the Ceph client and Python **rbd** library packages installed
    on each controller, compute, and storage node. Run the following command line
    if the Ceph client hasn’t been installed:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你在每个控制节点、计算节点和存储节点上安装了 Ceph 客户端和 Python **rbd** 库包。如果 Ceph 客户端尚未安装，可以运行以下命令行：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'On a Ceph node, create a storage pool to be used by Cinder:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ceph 节点上创建一个 Cinder 使用的存储池：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Initialize the created pool using the **rbd** command-line interface:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **rbd** 命令行界面初始化创建的池：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create a Ceph user and keyring for Cinder to access the created **cinder-volumes**
    pool:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Ceph 用户和密钥环，以便 Cinder 访问创建的 **cinder-volumes** 池：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Copy the created keyring to the OpenStack deployer, storage, and compute nodes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将创建的密钥环复制到 OpenStack 部署者、存储节点和计算节点：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Verify that the Cinder client can access the Ceph cluster by running the following
    command line:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令行，验证 Cinder 客户端是否能够访问 Ceph 集群：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Enable the Ceph backend in the **/** **etc/kolla/globals.yml** file:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **/** **etc/kolla/globals.yml** 文件中启用 Ceph 后端：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Additionally, make sure you specify the Ceph cinder user, the name of the pool,
    and the keyring in the **globals.yml** file:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，确保在 **globals.yml** 文件中指定 Ceph cinder 用户、池的名称和密钥环：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set the RBD Cinder UUID secret from the **/** **etc/kolla/passwords.yml** file:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从 **/** **etc/kolla/passwords.yml** 文件中设置 RBD Cinder UUID 密钥：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'From a Ceph node, copy the **ceph.conf** file to the custom **/etc/kolla/config/cinder**
    directory. An example of a Ceph configuration extract is shown here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Ceph 节点复制 **ceph.conf** 文件到自定义的 **/etc/kolla/config/cinder** 目录。这里展示了一个 Ceph
    配置摘录示例：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Roll the new changes by running the job CI/CD pipeline. The **kolla-ansible**
    playbook will restart the **cinder-volume** container with a new configuration.
    The storage node should have a Ceph RBD driver enabled and working access to the
    Ceph cluster. A new volume request should trigger **cinder-volume** to create
    the volume in the **cinder-volume** Ceph pool.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行作业 CI/CD 流水线来滚动更新新更改。**kolla-ansible** 剧本将重新启动 **cinder-volume** 容器，并应用新的配置。存储节点应启用
    Ceph RBD 驱动程序，并且能够正常访问 Ceph 集群。新的卷请求应触发 **cinder-volume** 在 **cinder-volume**
    Ceph 池中创建该卷。
- en: OpenStack operators could leverage the usage of multiple storage backends simultaneously.
    To enable such a capability efficiently, Cinder supports a special scheduling
    mechanism. We’ll explore this in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 运维人员可以同时利用多个存储后端。为了有效地启用这种能力，Cinder 支持一种特殊的调度机制。我们将在下一节中探讨此功能。
- en: Storage filtering and scheduling
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储过滤和调度
- en: 'As demonstrated in the previous section, a variety of storage backends are
    supported by Cinder. The block storage requests and their associated storage backend
    types are handled by **cinder-scheduler** . Similar to the Compute scheduler service,
    the Block Storage scheduler service takes care of assigning each volume to be
    created to a specific backend available in the storage pool. The Cinder scheduler
    uses filter policies to select the best backend fit for a new volume creation
    request. A few filtering criteria will be evaluated based on the storage information
    capabilities, such as the state, available space, and drive state. Afterward,
    **cinder-scheduler** uses a weighing mechanism that assigns each filtered backend
    a weight and sorts them out, as illustrated in the following figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所示，Cinder 支持多种存储后端。块存储请求及其相关的存储后端类型由**cinder-scheduler**处理。与计算调度服务类似，块存储调度服务负责将每个要创建的卷分配到存储池中可用的特定后端。Cinder
    调度器使用过滤策略来选择最适合新卷创建请求的后端。根据存储信息的能力，将评估几个过滤标准，例如状态、可用空间和磁盘状态。之后，**cinder-scheduler**
    使用加权机制，为每个过滤后的后端分配权重并进行排序，如下图所示：
- en: '![Figure 5.3 – Cinder’s scheduling and filtering mechanism](img/B21716_05_03.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – Cinder 的调度和过滤机制](img/B21716_05_03.jpg)'
- en: Figure 5.3 – Cinder’s scheduling and filtering mechanism
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – Cinder 的调度和过滤机制
- en: The scheduler periodically checks the state of the listed backends and continuously
    updates the storage backend candidate list.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器定期检查列出后端的状态，并持续更新存储后端候选列表。
- en: 'By default, Cinder uses **CapacityFilter** and, for weighing, **CapacityWeigher**
    , which filters the volume backends based on the storage size utilization. The
    latest Cinder scheduler filters and weighers, at least up until the Bobcat release,
    can be found at [https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-filters.html](https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-filters.html)
    and [https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-weights.html](https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-weights.html)
    , respectively. The following Cinder scheduler weighers list has changed over
    OpenStack releases, which lists the most adopted weighers in large deployments:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Cinder 使用 **CapacityFilter** 进行过滤，并使用 **CapacityWeigher** 进行加权，基于存储容量利用率对卷后端进行过滤。最新的
    Cinder 调度器过滤器和加权器，至少在 Bobcat 版本之前，可以在 [https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-filters.html](https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-filters.html)
    和 [https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-weights.html](https://docs.openstack.org/cinder/latest/configuration/block-storage/scheduler-weights.html)
    中找到。以下 Cinder 调度器加权器列表在 OpenStack 版本更新中发生了变化，列出了在大规模部署中最常用的加权器：
- en: '**CapacityWeigher** : Storage backend(s) are assigned the highest weight based
    on the most available and free storage capacity.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CapacityWeigher**：根据最可用和空闲的存储容量，为存储后端分配最高权重。'
- en: '**VolumeNumberWeigher** : Balances the volume allocation between different
    storage backends filtered by the scheduler. This is the best option to spread
    multiple volume creation requests evenly across different backends that share
    the same name.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VolumeNumberWeigher**：在调度器过滤的不同存储后端之间平衡卷分配。这是将多个卷创建请求均匀分配到共享相同名称的不同后端的最佳选择。'
- en: '**GoodnessWeigher** : A more fine-grained weigher that places volumes based
    on specific storage properties. It uses a formula that evaluates a storage backend,
    using a rating function, denoted as **goodness_function** , with the **"(property_rule
    )?challengeVal1 : challengeVal2"** format. The attribution of weights varies between
    **0** (the lowest) and **100** (the highest), based on the evaluation of **property_rule**
    , as follows:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GoodnessWeigher**：一种更精细的加权器，基于特定的存储属性分配卷。它使用一个公式评估存储后端，使用评分函数，表示为 **goodness_function**，格式为
    **"(property_rule )?challengeVal1 : challengeVal2"**。根据对 **property_rule** 的评估，权重的分配在
    **0**（最低）和 **100**（最高）之间变化，具体如下：'
- en: If **property_rule** returns **true** , then the storage backend is assigned
    the weight of the **challengeVal1** value
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 **property_rule** 返回 **true**，则存储后端将分配 **challengeVal1** 值的权重
- en: If **property_rule** returns **false** , then the storage backend is assigned
    the weight of the **challengeVal2** value
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 **property_rule** 返回 **false**，则存储后端将分配 **challengeVal2** 值的权重
- en: 'The following step-by-step configuration will demonstrate the usage of Cinder
    scheduling, by enabling default filters and using **GoodWeigher** to choose an
    ideal storage backend for each volume request. In this setup, two of the storage
    backends we configured in previous sections will be used for LVM and Ceph, respectively.
    We can continue to add a custom scheduling configuration by creating a new file,
    **/etc/kolla/config/cinder/cinder.conf** . Running **kolla-ansible** will merge
    any configuration file under the **/etc/kolla/config/cinder** directory with the
    one in the main template defined in the **/kolla-ansible/ansible/roles/cinder/templates/cinder.conf.j2**
    file. The new file will be configured with the following settings:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的逐步配置将演示如何使用Cinder调度，通过启用默认的过滤器并使用**GoodWeigher**为每个卷请求选择理想的存储后端。在此配置中，我们在前面的章节中配置的两个存储后端将分别用于LVM和Ceph。我们可以继续通过创建一个新文件**/etc/kolla/config/cinder/cinder.conf**来添加自定义调度配置。运行**kolla-ansible**将会把**/etc/kolla/config/cinder**目录下的任何配置文件与**/kolla-ansible/ansible/roles/cinder/templates/cinder.conf.j2**文件中的主模板配置合并。新文件将按以下设置进行配置：
- en: 'Enable the list of schedulers in a new section:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新部分中启用调度器列表：
- en: '[PRE24]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Add **GoodnessWeigher** as a main weigher mechanism, as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将**GoodnessWeigher**作为主要的加权机制添加，具体如下：
- en: '[PRE25]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For each backend section, add **goodness_function** to define a rating based
    on the storage provider’s capability. The next simple configuration evaluates
    the volume size and checks whether both backends’ utilization of **lvm-1** and
    **rbd** is less than 50%. If so, **50** and **80** weights will be assigned, respectively.
    In this case, **rbd** is the best candidate to place the volume. If the backend
    utilization on **lvm-1** is greater than 50% and less than 50% for **rbd** , the
    weigher will assign **80** to **rbd** and a weight of **30** to **lvm-1** . The
    last case of weight logic is when both backends’ utilization is more than 50%
    – in this case, **lvm-1** will be selected with **30** compared to **rbd** , which
    is assigned a weight of **20** :'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个后端部分，添加**goodness_function**来根据存储提供商的能力定义评分。下一个简单配置评估卷的大小，并检查两个后端**lvm-1**和**rbd**的利用率是否都低于50%。如果是，分别将分配**50**和**80**的权重。在这种情况下，**rbd**是放置卷的最佳候选者。如果**lvm-1**的后端利用率大于50%，而**rbd**小于50%，加权器将会为**rbd**分配**80**的权重，为**lvm-1**分配**30**的权重。最后一种加权逻辑是当两个后端的利用率都大于50%时——在这种情况下，将选择**lvm-1**，赋予其**30**的权重，相比之下**rbd**被赋予**20**的权重：
- en: '[PRE26]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Run the same job’s CI/CD pipeline, and make sure that **kolla-ansible** will
    restart the **cinder-volume** container with a new configuration. The **cinder.conf**
    file should have an extended configuration list of the configured schedulers and
    weighers.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行相同作业的CI/CD流水线，并确保**kolla-ansible**会使用新配置重新启动**cinder-volume**容器。**cinder.conf**文件应具有配置的调度器和加权器的扩展配置列表。
- en: 'Cinder provides constructs to request volumes with a specific storage backend,
    using volume types. The following example shows how to define two volume types
    for LVM and Ceph, based on each backend name to be used when creating volumes:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Cinder提供了构造来请求具有特定存储后端的卷，使用卷类型。以下示例显示了如何为LVM和Ceph定义两个卷类型，基于每个后端名称在创建卷时使用：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Users should be able to create volumes transparently based on storage types,
    as defined in the preceding command lines. The storage pool offer can be classified
    as large block storage with replication capabilities, defined as **rbd_large**
    , and a second standard storage capability for general purposes, defined as **lvm_standard**
    :'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 用户应该能够根据前面命令行中定义的存储类型透明地创建卷。存储池可以分为具有复制功能的大型块存储，定义为**rbd_large**，以及第二个用于一般用途的标准存储功能，定义为**lvm_standard**：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this scenario, the Cinder scheduler will assign each volume request to the
    designated backend storage – **lvm-1** and **rbd** , respectively. In other cases,
    two or more storage backends can share the same volume backend name, **volume_backend_name**
    – in this case, the scheduler will promote the storage backend type based on its
    scheduling and weighing filters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在此场景中，Cinder调度器将分别为每个卷请求分配指定的后端存储——**lvm-1**和**rbd**。在其他情况下，两个或多个存储后端可以共享相同的卷后端名称**volume_backend_name**——在这种情况下，调度器将根据其调度和加权过滤器推广存储后端类型。
- en: With the integration of various storage solutions and advanced scheduling mechanisms,
    cloud operators can take advantage of an array of features in each storage solution,
    managed through a single Cinder interface. Besides the Block Storage option, OpenStack
    offers another storage service focused on objects, code named Swift. We will cover
    this in the next section.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过集成各种存储解决方案和先进的调度机制，云运营商可以利用每个存储解决方案中的一系列功能，通过单一的 Cinder 界面进行管理。除了块存储选项外，OpenStack
    还提供了一个专注于对象的存储服务，代号为 Swift。我们将在下一节中讨论此内容。
- en: Revisiting object storage – Swift
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾对象存储 – Swift
- en: '**Swift** is a service tailored to store large volumes of unstructured object
    data, such as text or binary data. It enables OpenStack operators to build distributed
    object storage systems by using commodity storage. The hallmark of the Swift service
    is the way it is designed, which ensures data availability and durability. Several
    workloads that can run in an OpenStack cloud environment, such as web applications,
    can leverage object storage by enjoying the advantages of this simple service’s
    usage and management, as well as its transparent data replication and horizontal
    scaling.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**Swift** 是一种专门用于存储大规模非结构化对象数据（如文本或二进制数据）的服务。它使 OpenStack 操作员能够通过使用普通存储构建分布式对象存储系统。Swift
    服务的标志性特点在于它的设计方式，确保数据的可用性和持久性。可以在 OpenStack 云环境中运行的多个工作负载（如 Web 应用程序）可以通过享受这种简单服务的使用和管理优势，以及其透明的数据复制和水平扩展功能，从而利用对象存储。'
- en: Revisiting Swift
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾 Swift
- en: 'Swift’s logical core components consist mainly of the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Swift 的逻辑核心组件主要包括以下内容：
- en: '**Account server** : Represents a namespace for a list of containers associated
    with a Swift account.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**账户服务器**：表示与 Swift 账户关联的容器列表的命名空间。'
- en: '**Container server** : Refers to the user-defined storage area within a Swift
    account where it stores a list of objects.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器服务器**：指的是在 Swift 账户中用户定义的存储区域，用于存储对象列表。'
- en: '**Object server** : Manages an actual object within a container. Object storage
    defines where the actual data and its metadata are stored. Each object must belong
    to a container.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象服务器**：管理容器中的实际对象。对象存储定义了实际数据及其元数据的存储位置。每个对象必须属于一个容器。'
- en: '**Proxy server** : Handles different types of incoming object API or HTTP requests,
    such as container creation, object uploads, and deletion.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理服务器**：处理不同类型的传入对象 API 或 HTTP 请求，如容器创建、对象上传和删除。'
- en: '**Partition** : Manages the location of objects, containers, and account databases.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区**：管理对象、容器和账户数据库的位置。'
- en: '**Zones** : Separate objects within physical zone isolation to prevent wider
    data loss during a zonal failure in a cluster.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区域**：在物理区域隔离内分隔对象，以防止在集群中发生区域性故障时导致更广泛的数据丢失。'
- en: '**Rings** : Defines a logical mapping of objects, accounts, and containers
    to a physical location in a Swift cluster. Swift uses a ring per storage construct.
    It also uses **swift-ring-builder** to create an inventory of the cluster and
    create partitions.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环**：定义对象、账户和容器的逻辑映射，并将其映射到 Swift 集群中的物理位置。Swift 对每个存储构造使用一个环。它还使用 **swift-ring-builder**
    来创建集群的清单并创建分区。'
- en: 'As shown in the following figure, each partition will have three replicas that
    reflect three copies of the same object, container, or account to maintain high
    availability of data:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，每个分区将有三个副本，反映同一个对象、容器或账户的三个副本，以保持数据的高可用性：
- en: '![Figure 5.4 – Swift rings and replication in a three-zone layout](img/B21716_05_04.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 三区域布局中的 Swift 环和复制](img/B21716_05_04.jpg)'
- en: Figure 5.4 – Swift rings and replication in a three-zone layout
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 三区域布局中的 Swift 环和复制
- en: As well as the previous core components, Swift uses additional background daemons
    to manage objects’ indexing, updates, and replications.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的核心组件一样，Swift 使用额外的后台守护进程来管理对象的索引、更新和复制。
- en: Building a Swift cluster
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 Swift 集群
- en: 'There are a variety of parameters that should be considered when preparing
    an object storage cluster before integrating it with an existing OpenStack environment.
    The focus should be on the hardware requirements to achieve a highly available,
    standalone, and expandable cluster. The following is a starting point for deploying
    a Swift cluster:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在将对象存储集群与现有的 OpenStack 环境集成之前，应该考虑多种参数。重点应放在硬件需求上，以实现高可用、独立且可扩展的集群。以下是部署 Swift
    集群的起点：
- en: '**Object Storage capacity** : 100 TB'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象存储容量**：100 TB'
- en: '**Number of hard drive slots per** **chassis** : 50'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个** **机箱的硬盘插槽数量** ：50'
- en: '**Hard drive storage capacity** : 3 TB'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬盘存储容量** ：3 TB'
- en: '**Recommended cluster** **replicas** : 3'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐的集群** **副本** ：3'
- en: '**Supported** **filesystem** : XFS'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持的** **文件系统** ：XFS'
- en: As we intend to keep three replicas, as recommended, the total storage capacity
    should be three times the recommended size, giving us **3 * 100 =** **300 TB**
    .
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 根据推荐的三副本策略，总存储容量应为推荐大小的三倍，即 **3 * 100 =** **300 TB**。
- en: 'The usage of XFS as a filesystem requires a metadata overhead in storage with
    a factor of 1.0526. The rounded required storage space, in this case, will be
    calculated as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 XFS 作为文件系统需要在存储中增加 1.0526 倍的元数据开销。所需存储空间的四舍五入计算方式如下：
- en: '**300 * 1.0526 =** **316 TB**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**300 * 1.0526 =** **316 TB**'
- en: 'Using the total space required will help you find the number of hard drives
    required, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所需的总空间将帮助您计算所需的硬盘数量，如下所示：
- en: '**316/3 =** **106 drives**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**316 / 3 =** **106 个硬盘**'
- en: 'The number of storage nodes will be determined as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 存储节点的数量将根据以下方式确定：
- en: '**106 / 50 =** **3 nodes**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**106 / 50 =** **3 节点**'
- en: Our initial cluster will be composed of three storage nodes that will host containers,
    accounts, and objects. To manage the Swift requests, in our initial draft, we
    have assigned the **swift-proxy** server to run on the controller node. One of
    the caveats of keeping the **swift-proxy** role on the controller node is the
    performance risk that could overload the cloud controller resources. As a cloud
    operator, it is recommended to monitor the object API requests reaching the **swift-proxy**
    service and start to move it to dedicated hardware when the storage cluster continues
    to expand, in terms of size and usage.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始集群将由三个存储节点组成，这些节点将承载容器、账户和对象。为了管理 Swift 请求，在我们的初步草图中，我们将 **swift-proxy**
    服务器分配到控制节点上。将 **swift-proxy** 角色保持在控制节点上的一个问题是可能会导致云控制器资源的过载。作为云运营商，建议监控到达 **swift-proxy**
    服务的对象 API 请求，并在存储集群继续扩展时，根据大小和使用情况，开始将其迁移到专用硬件。
- en: 'Another critical consideration when dealing with an extended Swift storage
    system is the networking layout. In our initial design draft, a network interface
    is dedicated to handling storage. As per the Swift architecture, more interfaces
    can be assigned for different networking purposes. These are listed here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 处理扩展的 Swift 存储系统时，另一个关键的考虑因素是网络布局。在我们的初步设计草图中，网络接口专门用于处理存储。根据 Swift 架构，还可以分配更多接口用于不同的网络用途。具体列举如下：
- en: '**Storage interface** : A proxy server that interfaces the storage nodes running
    the object, container, and account servers. The associated configuration in **kolla-ansible**
    is defined in the **globals.yml** file as **swift_storage_interface** .'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储接口** ：一个代理服务器，用于接口存储节点，运行对象、容器和账户服务器。相关配置在 **kolla-ansible** 中定义，位于 **globals.yml**
    文件中，配置项为 **swift_storage_interface**。'
- en: '**Replication interface** : Optionally, an additional interface in each storage
    node can be dedicated to handling Swift storage replication between the storage
    nodes. The associated configuration in **kolla-ansible** is defined in the **globals.yml**
    file as **swift_replication_interface** . This option is not considered in the
    current setup and will use the storage interface to handle the replication traffic.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制接口** ：可选地，每个存储节点可以为处理存储节点之间的 Swift 存储复制而专门配置一个附加接口。相关配置在 **kolla-ansible**
    中定义，位于 **globals.yml** 文件中，配置项为 **swift_replication_interface**。在当前设置中，此选项未被考虑，复制流量将使用存储接口进行处理。'
- en: '**Internal API interface** : This can be defined to allow users access to the
    proxy server internally. The same interface can be used by HAProxy to establish
    load balancing between two or more swift-proxy services for high availability.
    The associated configuration in **kolla-ansible** is defined in the **globals.yml**
    file as **api_interface** . This option is not considered in the current setup.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部 API 接口** ：可以定义此接口以允许用户在内部访问代理服务器。相同的接口可以被 HAProxy 用来在两个或更多的 swift-proxy
    服务之间建立负载均衡，确保高可用性。相关配置在 **kolla-ansible** 中定义，位于 **globals.yml** 文件中，配置项为 **api_interface**。在当前设置中，此选项未被考虑。'
- en: '**External API interface** : Optionally, Swift can be accessed externally when
    workloads require public API access. The associated configuration in **kolla-ansible**
    is defined in the **globals.yml** file as **kolla_external_vip_interface** . This
    option is not considered in the current setup.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部 API 接口** ：可选地，当工作负载需要公共 API 访问时，Swift 可以在外部访问。相关的配置在 **kolla-ansible**
    中定义，在 **globals.yml** 文件中作为 **kolla_external_vip_interface** 。在当前设置中，此选项未考虑。'
- en: Swift can easily expand exponentially due to its common use cases; thus, cloud
    operators should expect possible growth of the underlying infrastructure. A common
    pain point is to get locked in a limited hardware configuration and not be prepared
    for it beforehand. Having additional network interfaces will prove a big advantage
    when operators realize there’s a degradation in network performance. Separating
    the traffic types in a dedicated interface will avoid the burden of operational
    overhead and firefighting.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其常见的使用案例，Swift 可以轻松地呈指数级扩展；因此，云运维人员应预期基础设施可能会增长。一个常见的痛点是被限制在有限的硬件配置中，而事先未为此做好准备。当运维人员发现网络性能下降时，拥有额外的网络接口将是一个巨大的优势。将不同类型的流量分配到专用接口将避免操作过载和应急处理的负担。
- en: Running Swift
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 Swift
- en: 'Before deploying the Swift cluster using **kolla-ansible** , we will need to
    prepare the storage node filesystem. The following shell script will create partitions
    on each available disk – **sdc** , **sdd** , and **sde** – that should run in
    each storage node:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 **kolla-ansible** 部署 Swift 集群之前，我们需要准备存储节点文件系统。以下 shell 脚本将会在每个可用磁盘上创建分区——**sdc**
    、**sdd** 和 **sde** ——这些分区应在每个存储节点中运行：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For each created partition, create an XFS file system using the following shell
    script:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个创建的分区，使用以下 shell 脚本创建一个 XFS 文件系统：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Important note
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The filesystem label is referred to as **KOLLA_SWIFT_DATA** , which should be
    visible in the storage node. The name of the device should match the configuration
    settings in the **globals** **.yml** file, defined by the **swift_devices_name**
    setting.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统标签被称为 **KOLLA_SWIFT_DATA** ，应在存储节点中可见。设备名称应与 **globals.yml** 文件中的配置设置相匹配，由
    **swift_devices_name** 设置定义。
- en: 'Next, generate the rings to prepare mapping objects in the Swift cluster. At
    the time of writing, the Object Storage playbook does not automate the ring generation
    process. Start by generating the object ring, and then run the following script
    from the deployer node. The first part of the script defines the cluster storage
    IP addresses, the Swift Kolla image, and a new directory for a custom Swift Kolla
    configuration path, respectively:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，生成环以准备 Swift 集群中的映射对象。撰写时，Object Storage 操作手册尚未自动化环生成过程。从生成对象环开始，然后从部署节点运行以下脚本。脚本的第一部分分别定义了集群存储
    IP 地址、Swift Kolla 镜像以及自定义 Swift Kolla 配置路径的新目录：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Swift uses the **swift-ring-builder** tool to generate an object ring. The
    generic format of the ring builder command is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Swift 使用 **swift-ring-builder** 工具来生成对象环。环构建命令的通用格式如下：
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here, we have the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '**<builder_file>** : This can be one of **account.builder** , **container.builder**
    , or **object.builder** .'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<builder_file>** ：这可以是 **account.builder** 、**container.builder** 或 **object.builder**
    中的一个。'
- en: '**<part_power>** : The number of partitions is approximated to the closest
    power of 2 to get the part power of the cluster. For example, for 50 hard disks,
    the recommended part power is 11, which gives us a value of 2,048 on average for
    all partitions. It is recommended that the approximation is rounded up.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<part_power>** ：分区数近似为 2 的最接近幂次，以获得集群的 part power。例如，对于 50 个硬盘，推荐的 part power
    为 11，这使得所有分区的平均值为 2,048。建议将近似值向上舍入。'
- en: '**<replicas>** : A choice of three replicas will be the recommended value.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<replicas>** ：推荐值是选择三个副本。'
- en: '**<min_part_hours>** : This determines the time in an hour during which only
    one replica of a partition can be moved.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<min_part_hours>** ：这决定了在一个小时内只能移动一个分区副本的时间。'
- en: Rackspace Lab provides an online easy tool to calculate the swift ring. It can
    be found at [https://rackerlabs.github.io/swift-ppc/](https://rackerlabs.github.io/swift-ppc/)
    .
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Rackspace Lab 提供了一个在线工具来计算 Swift 环。该工具可以在 [https://rackerlabs.github.io/swift-ppc/](https://rackerlabs.github.io/swift-ppc/)
    找到。
- en: 'The next part of the script will build the ring for the objects and iterate
    through each storage node by adding available devices to the ring on port **6000**
    :'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的下一部分将为对象构建环，并通过向端口 **6000** 上的环添加可用设备来迭代每个存储节点：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following section of the script will generate a ring file for the account
    and add available devices to the ring for each storage node on port **6001** :'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的以下部分将为账户生成一个ring文件，并为每个存储节点在端口**6001**上将可用设备添加到ring中：
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The next part of the script generates a ring for the container and adds available
    devices to the ring for each storage node on port **6002** :'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的下一部分为容器生成一个ring，并为每个存储节点在端口**6002**上将可用设备添加到ring中：
- en: '[PRE35]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Optionally, to distribute the partitions within the available drives in the
    ring, add the last section in the script by firing the rebalance option, using
    the same **swift-ring-builder** **rebalance** command-line tool:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，为了在ring内分配可用驱动器的分区，添加脚本中的最后一部分，通过触发rebalance选项，使用相同的**swift-ring-builder**
    **rebalance**命令行工具：
- en: '[PRE36]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This will iterate through each generated ring file and ensure each ring is
    balanced upon each partition move. After running the ring preparation script,
    make sure no errors are generated and that all three ring files are located under
    the **/etc/kolla/config/swift/** directory. The next step is to make sure the
    object storage hostnames are added to the inventory file, located under **/ansible/inventory**
    , as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这将遍历每个生成的ring文件，并确保在每次分区移动后每个ring都保持平衡。运行ring准备脚本后，确保没有错误生成，且所有三个ring文件都位于**/etc/kolla/config/swift/**目录下。下一步是确保对象存储主机名已添加到清单文件中，该文件位于**/ansible/inventory**，如下所示：
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We will keep the **swift-proxy** role on the cloud controller node:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在云控制节点上保持**swift-proxy**角色：
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Edit the **globals.yml** file by enabling the Swift service, as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过启用Swift服务编辑**globals.yml**文件，如下所示：
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If the storage partition label uses a different name than the default **KOLLA_SWIFT_DATA**
    defined in the disk partition preparation previously, make sure you adjust it
    accordingly by setting the following configuration line in the **globals.yml**
    file:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存储分区标签使用与先前在磁盘分区准备中定义的默认**KOLLA_SWIFT_DATA**不同的名称，请确保通过在**globals.yml**文件中设置以下配置行进行调整：
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Triggering the pipeline job should run each target node in the Kolla Swift
    containers. Create a container in one of the cluster nodes, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 触发管道作业应运行每个目标节点中的Kolla Swift容器。在集群节点之一中创建一个容器，如下所示：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create a simple object in the created container:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建的容器中创建一个简单的对象：
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You can show the container details, including the number of objects and the
    associated account:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以显示容器的详细信息，包括对象数量和关联的账户：
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As demonstrated in this section, the Swift architecture comes with a simple
    API that makes it easy to store unstructured data in redundant clusters with less
    operational burden. Swift is also often used for backup, archiving, and disaster
    recovery purposes. The last facet of storage options in OpenStack is the file
    share storage code known as Manila, which will be covered in the next section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节所示，Swift架构具有简单的API，使得将非结构化数据存储在冗余集群中变得容易，且运维负担较轻。Swift还经常用于备份、归档和灾难恢复。OpenStack中的最后一种存储选项是文件共享存储，称为Manila，将在下一节中介绍。
- en: Exploring file share services – Manila
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索文件共享服务 – Manila
- en: The next storage service in OpenStack is the file share code named Manila. The
    service offers simultaneous access to a storage-based file share for a variety
    of clients and storage backends. Multiple sharing protocols are supported by Manila,
    at least until the Bobcat release, including NFS, GlusterFS, CephFS, CIFS, HDFS,
    and, most recently, MapRFS. Note that the storage backends use drivers that must
    support one of the previously listed protocols.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack中的下一个存储服务是文件共享，代号为Manila。该服务为各种客户端和存储后端提供对存储基础文件共享的同时访问。Manila支持多种共享协议，至少在Bobcat版本之前，包括NFS、GlusterFS、CephFS、CIFS、HDFS，以及最近的MapRFS。请注意，存储后端使用的驱动程序必须支持其中一个先前列出的协议。
- en: Manila under the hood
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Manila的底层实现
- en: 'As illustrated in the following figure, the core components that orchestrate
    file shares and manage their life cycle in Manila are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，在Manila中协调文件共享并管理其生命周期的核心组件如下：
- en: '**Manila share server** : A unit of storage that hosts the shares'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Manila共享服务器** : 托管共享文件的存储单元'
- en: '**API server** : Exposes a REST API interface for client request handling'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API服务器** : 为客户端请求处理暴露REST API接口'
- en: '**Scheduler** : Selects the best share server to suit a file share request'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器** : 选择最合适的共享服务器以满足文件共享请求'
- en: '**Data service** : Handles data backups, recovery, and migration'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据服务** : 处理数据备份、恢复和迁移'
- en: 'Manila interacts with a few OpenStack core services, including the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Manila 与几个 OpenStack 核心服务交互，包括以下服务：
- en: '**Nova** : To create instances that run the share servers'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nova**：创建运行共享服务器的实例'
- en: '**Neutron** : To provide access to the file shares through the tenant instance
    network'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neutron**：通过租户实例网络提供文件共享访问'
- en: '**Cinder** : To create file shares in a block storage pool as volumes:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cinder**：在块存储池中创建文件共享作为卷：'
- en: '![Figure 5.5 – File share service core architecture](img/B21716_05_05.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 文件共享服务核心架构](img/B21716_05_05.jpg)'
- en: Figure 5.5 – File share service core architecture
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 文件共享服务核心架构
- en: Important note
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At the time of writing, Manila supports more than 25 share drivers’ backends,
    including CephFS, GlusterFS, LVM, and other vendors, such as EMC and Hitachi NAS.
    A fully updated list can be found at [https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html](https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html)
    .
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，Manila 支持超过 25 种共享驱动程序的后端，包括 CephFS、GlusterFS、LVM 以及其他厂商，如 EMC 和 Hitachi
    NAS。完整更新的列表可以在 [https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html](https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html)
    找到。
- en: The additional extended list of new features and drivers in Manila allows it
    to use multiple storage backends and handle them simultaneously. Similar to the
    Block Storage multi-backend setup, the latest versions of Manila introduced the
    filtering and weighing mechanisms to select a shared backend, based on specific
    properties. By default, **manila-scheduler** uses **DriverFilter** for filtering
    and **GoodnessWeigher** as a weigher mechanism.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Manila 新增的扩展功能和驱动程序列表使其能够同时使用多个存储后端并管理它们。类似于块存储多后端设置，Manila 的最新版本引入了筛选和加权机制，以根据特定属性选择共享后端。默认情况下，**manila-scheduler**
    使用 **DriverFilter** 进行筛选，使用 **GoodnessWeigher** 作为加权机制。
- en: Important note
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Filters and function weighers should be configured in the **manila.conf** file,
    similar to Cinder’s filtering function syntax. The filter function defines capability
    checks at both the share and host levels. The available share types of properties
    can be listed by running **manila extra-specs-list** on the command line. A full
    list of the latest properties supported by the filter function, up until the Antelope
    and Bobcat releases, is available at [https://docs.openstack.org/manila/latest/contributor/driver_filter_goodness_weigher.html](https://docs.openstack.org/manila/latest/contributor/driver_filter_goodness_weigher.html)
    .
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选器和功能加权器应在 **manila.conf** 文件中配置，类似于 Cinder 的筛选功能语法。筛选功能在共享和主机级别定义能力检查。可以通过在命令行运行
    **manila extra-specs-list** 来列出可用的共享类型属性。有关筛选功能支持的最新属性的完整列表（直到 Antelope 和 Bobcat
    版本），请参见 [https://docs.openstack.org/manila/latest/contributor/driver_filter_goodness_weigher.html](https://docs.openstack.org/manila/latest/contributor/driver_filter_goodness_weigher.html)。
- en: For the next deployment, the default scheduling configuration will be used,
    which filters the best backend based on the free available capacity.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个部署，将使用默认调度配置，该配置基于空闲可用容量筛选最佳后端。
- en: Running Manila
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 Manila
- en: Deploying the Manila service in our existing OpenStack environment is straightforward.
    In the following setup, we will use a generic backend that leverages Cinder to
    create a share as a volume. The attached volume to the instance can live within
    a Cinder pool storage backend that was configured in the *Expanding storage with
    multiple backends* section for LVM or Ceph, respectively.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的 OpenStack 环境中部署 Manila 服务是直接的。以下设置中，我们将使用一个通用后端，利用 Cinder 创建共享作为卷。实例附加的卷可以存在于在
    *使用多个后端扩展存储* 部分为 LVM 或 Ceph 配置的 Cinder 池存储后端中。
- en: 'Enabling the Cinder service in the **kolla-ansible** code is a requirement
    to run Manila with a generic storage backend. In the **globals.yml** file, enable
    the Manila service, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **kolla-ansible** 代码中启用 Cinder 服务是运行 Manila 与通用存储后端的必要条件。在 **globals.yml**
    文件中启用 Manila 服务，如下所示：
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The **globals.yml** file exposes another backend for Manila that includes NFS
    in Ceph and HNAS. For the generic backend, bear in mind that NFS and CIFS shares
    will be used. Proceed by enabling the generic share:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**globals.yml** 文件暴露了另一个后端给 Manila，包括 Ceph 中的 NFS 和 HNAS。对于通用后端，请记住将使用 NFS
    和 CIFS 共享。继续启用通用共享：'
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Important note
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At the time of writing, enabling the Manila service will automatically add the
    Horizon panel for Manila to manage file shares through the dashboard. If the Manila
    service is not visible in the dashboard after deployment, make sure the **enable_horizon_manila**
    setting is set to **true** .
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写时，启用Manila服务将自动添加Manila的Horizon面板，通过仪表板管理文件共享。如果部署后在仪表板中看不到Manila服务，请确保**enable_horizon_manila**设置为**true**。
- en: 'More Manila settings can be configured in the custom Manila configuration file,
    which is not controlled directly via the **globals.yml** file. Create a new configuration
    file, named **/etc/kolla/config/manila-share.conf** , to customize the Manila
    share settings. One considerable setting is the size of the instance share required.
    You can create the instance flavor in Nova in advance or use the default ones,
    which can be listed using the following command line:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在自定义Manila配置文件中配置更多Manila设置，该文件不通过**globals.yml**文件直接控制。创建一个新的配置文件，命名为**/etc/kolla/config/manila-share.conf**，用于自定义Manila共享设置。一个重要的设置是实例共享所需的大小。你可以提前在Nova中创建实例口味，或者使用默认的口味，可以通过以下命令行列出：
- en: '[PRE46]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'For example, we can instruct the file share instance to use the **m1.medium**
    flavor, referenced with an ID of **3** from the list:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以指示文件共享实例使用**m1.medium**口味，引用的ID为**3**，来自列表中：
- en: '[PRE47]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Other settings can be customized, such as the Manila shared backend name if
    you intend to have more than one backend. To reflect the name of the generic backend,
    add the following configuration setting:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 可以自定义其他设置，例如Manila共享后端名称，如果打算有多个后端。为了反映通用后端的名称，请添加以下配置设置：
- en: '[PRE48]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As it’s a Nova instance, the file share server will need a Glance image. By
    default, the Manila playbook uses **manila-service-image** as a reference name.
    If you intend to change it, make sure it has the same name as the one configured
    in the file.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是一个Nova实例，文件共享服务器需要一个Glance镜像。默认情况下，Manila剧本使用**manila-service-image**作为参考名称。如果打算更改它，请确保它与文件中配置的名称相同。
- en: 'The next part of preparing the Manila deployment is to make sure the Manila
    services are assigned to the cluster roles. Set the hostname, and ensure the Manila
    services are added to the inventory file located under **/ansible/inventory/multi_packtpub_prod**
    , as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 准备Manila部署的下一部分是确保Manila服务被分配到集群角色。设置主机名，并确保Manila服务已添加到位于**/ansible/inventory/multi_packtpub_prod**中的清单文件，如下所示：
- en: '[PRE49]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As highlighted in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack
    Control Plane – Shared Services* , the **manila-share** service uses the Neutron
    plugin to provide network access to the file share resources:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第3章*](B21716_03.xhtml#_idTextAnchor108)中所强调，*OpenStack控制平面 – 共享服务*，**manila-share**服务使用Neutron插件提供文件共享资源的网络访问：
- en: '[PRE50]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Launch the pipeline job so that you can deploy the Manila service in cross-target
    nodes. Manila service containers should be running in the cloud controller and
    network node. Make sure that the Manila tasks complete without failure, as shown
    here:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 启动管道作业，以便在跨目标节点中部署Manila服务。Manila服务容器应该在云控制器和网络节点中运行。确保Manila任务无误地完成，如下所示：
- en: '![Figure 5.6 – The kolla-ansible Manila deployment’s service output](img/B21716_05_06.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – kolla-ansible Manila部署的服务输出](img/B21716_05_06.jpg)'
- en: Figure 5.6 – The kolla-ansible Manila deployment’s service output
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – kolla-ansible Manila部署的服务输出
- en: 'Verify the Manila services in the cloud controller node:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 验证云控制器节点中的Manila服务：
- en: '[PRE51]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output of the previous command line shows that the **manila-scheduler**
    service has been enabled on the cloud controller host and that **manila-share**
    is running as part of the network node:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 上一命令行的输出显示，**manila-scheduler**服务已在云控制器主机上启用，且**manila-share**作为网络节点的一部分正在运行：
- en: '![Figure 5.7 – The OpenStack Manila services list](img/B21716_05_07.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – OpenStack Manila服务列表](img/B21716_05_07.jpg)'
- en: Figure 5.7 – The OpenStack Manila services list
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – OpenStack Manila服务列表
- en: The file-sharing service has seen great improvements in the latest OpenStack
    releases, including Bobcat, such as the introduction of file-share control access.
    This way, file shares can be locked to protect them against accidental deletion.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新的OpenStack版本（包括Bobcat）中，文件共享服务得到了极大改进，例如引入了文件共享控制访问。这样，文件共享可以被锁定，以防止意外删除。
- en: Important note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Starting from the Antelope release, the Manila CLI is planned to be deprecated,
    and it is highly recommended to use the OpenStack CLI to interact with the Manila
    REST API.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Antelope 版本开始，计划弃用 Manila CLI，强烈建议使用 OpenStack CLI 与 Manila REST API 进行交互。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter has explored the most trending OpenStack storage services. The
    expansion of each storage service feature and capability, including Swift, Cinder,
    and Manila, is almost *unstoppable* . This is confirmed by the vast array of supported
    backend drivers, either with the open source world ones, such as Ceph, or with
    a specific storage vendor. The latest OpenStack releases, including Antelope and
    Bobcat, have also introduced more stable advanced features, mainly for Cinder
    and Manila, through the scheduling and filtering mechanisms. Bear in mind that
    understanding each storage use case and its respective architecture are the keys
    to comfortably running a storage solution in an OpenStack environment, based on
    your requirements. The challenge of picking the right option to satisfy specific
    requirements is not limited to storage topics. It also applies to the networking
    paradigm in OpenStack. Connecting the different elements of the compute and storage
    services will be discussed in the next chapter of our OpenStack journey.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了最流行的 OpenStack 存储服务。每项存储服务功能和能力的扩展，包括 Swift、Cinder 和 Manila，几乎是*不可阻挡*的。这一点得到了广泛支持的后端驱动程序的验证，无论是开源世界中的
    Ceph，还是特定存储供应商的驱动程序。最新的 OpenStack 版本，包括 Antelope 和 Bobcat，也通过调度和过滤机制引入了更稳定的高级功能，主要针对
    Cinder 和 Manila。请记住，理解每个存储用例及其相应架构是根据需求在 OpenStack 环境中舒适运行存储解决方案的关键。选择满足特定需求的正确选项的挑战不仅限于存储问题，还适用于
    OpenStack 中的网络范式。计算和存储服务的不同元素的连接将在我们 OpenStack 旅程的下一章中讨论。
