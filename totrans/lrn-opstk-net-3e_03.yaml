- en: Installing Neutron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking, also known as Neutron, provides a network infrastructure-as-a-service
    platform to users of the cloud. In the last chapter, we installed some of the
    base services of OpenStack, including the Identity, Image, and Compute services.
    In this chapter, I will guide you through the installation of Neutron networking
    services on top of the OpenStack environment that we installed in the previous
    chapter
  prefs: []
  type: TYPE_NORMAL
- en: 'The components to be installed include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Neutron API server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modular Layer 2 (ML2) plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DHCP agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a basic understanding of the function
    and operation of various Neutron plugins and agents, as well as a foundation on
    top of which a virtual switching infrastructure can be built.
  prefs: []
  type: TYPE_NORMAL
- en: Basic networking elements in Neutron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neutron constructs the virtual network using elements that are familiar to most
    system and network administrators, including networks, subnets, ports, routers,
    load balancers, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using version 2.0 of the core Neutron API, users can build a network foundation
    composed of the following entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network**: A network is an isolated Layer 2 broadcast domain. Typically,
    networks are reserved for the projects that created them, but they can be shared
    among projects if configured accordingly. The network is the core entity of the
    Neutron API. Subnets and ports must always be associated with a network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subnet**: A subnet is an IPv4 or IPv6 address block from which IP addresses
    can be assigned to virtual machine instances. Each subnet must have a CIDR and
    must be associated with a network. Multiple subnets can be associated with a single
    network and can be non-contiguous. A DHCP allocation range can be set for a subnet
    that limits the addresses provided to instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Port**: A port in Neutron is a logical representation of a virtual switch
    port. Virtual machine interfaces are mapped to Neutron ports, and these ports
    define both the MAC address and the IP address that is to be assigned to the interfaces
    plugged into them. Neutron port definitions are stored in the Neutron database,
    which is then used by the respective plugin agent to build and connect the virtual
    switching infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud operators and users alike can configure network topologies by creating
    and configuring networks and subnets, and then instruct services like Nova to
    attach virtual devices to ports on these networks. Users can create multiple networks,
    subnets, and ports, but are limited to thresholds defined by per-project quotas
    set by the cloud administrator.
  prefs: []
  type: TYPE_NORMAL
- en: Extending functionality with plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenStack Networking project provides reference plugins and drivers that
    are developed and supported by the OpenStack community, and also supports third-party
    plugins and drivers that extend network functionality and implementation of the
    Neutron API. Plugins and drivers can be created that use a variety of software
    and hardware-based technologies to implement the network built by operators and
    users.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two major plugin types within the Neutron architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Core plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **core plugin** implements the core Neutron API, and is responsible for adapting
    the logical network described by networks, ports, and subnets into something that
    can be implemented by the L2 agent and IP address management system running on
    the host.
  prefs: []
  type: TYPE_NORMAL
- en: A **service plugin** provides additional network services such as routing, load
    balancing, firewalling, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, the following core plugin will be discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: Modular Layer 2 Plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following service plugins will be covered in later chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: Router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trunk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Neutron API provides a consistent experience to the user despite the chosen
    networking plugin. For more information on interacting with the Neutron API, please
    visit the following URL: [https://developer.openstack.org/api-ref/network/v2/index.html](https://developer.openstack.org/api-ref/network/v2/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Modular Layer 2 plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prior to the inclusion of the **Modular Layer 2 (ML2)** plugin in the Havana
    release of OpenStack, Neutron was limited to using a single core plugin. This
    design resulted in homogenous network architectures that were not extensible.
    Operators were forced to make long-term decisions about the network stack that
    could not easily be changed in the future. The ML2 plugin, on the other hand,
    is extensible by design and supports heterogeneous network architectures that
    can leverage multiple technologies simultaneously. The ML2 plugin replaced two
    monolithic plugins in its reference implementation: the Linux bridge core plugin
    and the Open vSwitch core plugin.'
  prefs: []
  type: TYPE_NORMAL
- en: Drivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML2 plugin introduced the concept of TypeDrivers and Mechanism drivers to
    separate the types of networks being *implemented* and the mechanisms for *implementing*
    networks of those types.
  prefs: []
  type: TYPE_NORMAL
- en: TypeDrivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An ML2 **TypeDriver** maintains a type-specific network state, validates provider
    network attributes, and describes network segments using provider attributes.
    Provider attributes include network interface labels, segmentation IDs, and network
    types. Supported network types include `local`, `flat`, `vlan`, `gre`, `vxlan`,
    and `geneve`. The following table describes the differences between those network
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Local | A **local network** is one that is isolated from other networks and
    nodes. Instances connected to a local network may communicate with other instances
    in the same network on the same `compute` node, but are unable to communicate
    with instances on another host. Because of this design limitation, local networks
    are recommended for testing purposes only. |'
  prefs: []
  type: TYPE_TB
- en: '| Flat | In a **flat network**, no 802.1q VLAN tagging or other network segregation
    takes place. In many environments, a flat network corresponds to an *access* VLAN
    or *native* VLAN on a trunk. |'
  prefs: []
  type: TYPE_TB
- en: '| VLAN | **VLAN networks** are networks that utilize 802.1q tagging to segregate
    network traffic. Instances in the same VLAN are considered part of the same network
    and are in the same Layer 2 broadcast domain. Inter-VLAN routing, or routing between
    VLANs, is only possible through the use of a physical or virtual router. |'
  prefs: []
  type: TYPE_TB
- en: '| GRE | **GRE networks** use the **generic routing encapsulation** tunneling
    protocol (IP protocol 47) to encapsulate packets and send them over point-to-point
    networks between nodes. The `KEY` field in the GRE header is used to segregate
    networks. |'
  prefs: []
  type: TYPE_TB
- en: '| VXLAN | A **VXLAN network** uses a unique segmentation ID, called a VXLAN
    Network Identifier (VNI), to differentiate traffic from other VXLAN networks.
    Traffic from one instance to another is encapsulated by the host using the VNI
    and sent over an existing Layer 3 network using UDP, where it is decapsulated
    and forwarded to the instance. The use of VXLAN to encapsulate packets over an
    existing network is meant to solve limitations of VLANs and physical switching
    infrastructure. |'
  prefs: []
  type: TYPE_TB
- en: '| GENEVE | A **GENEVE network** resembles a VXLAN network, in that it uses
    a unique segmentation ID, called a virtual network interface (**VNI**), to differentiate
    traffic from other GENEVE networks. Packets are encapsulated with a unique header
    and UDP is used as the transport mechanism. GENEVE leverages the benefits of multiple
    overlay technologies such as VXLAN, NVGRE, and STT and is primarily used by OVN
    at this time. |'
  prefs: []
  type: TYPE_TB
- en: Mechanism drivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An ML2 **Mechanism driver** is responsible for taking information established
    by the type driver and ensuring that it is properly implemented. Multiple Mechanism
    drivers can be configured to operate simultaneously, and can be described using
    three types of models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agent-based:** Includes Linux bridge, Open vSwitch, SR-IOV, and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller-based:** Includes Juniper Contrail, Tungsten Fabric, OVN, Cisco
    ACI, VMWare NSX, and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-of-Rack**: Includes Cisco Nexus, Arista, Mellanox, and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mechanism drivers to be discussed in this book include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linux bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open vSwitch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Linux bridge and Open vSwitch ML2 Mechanism drivers are used to configure
    their respective virtual switching technologies within nodes that host instances
    and network services. The Linux bridge driver supports `local`, `flat`, `vlan`,
    and `vxlan` network types, while the Open vSwitch driver supports all of those
    as well as the `gre` network type. Support for other type drivers, such as `geneve`,
    will vary based on the implemented Mechanism driver.
  prefs: []
  type: TYPE_NORMAL
- en: The L2 population driver is used to limit the amount of broadcast traffic that
    is forwarded across the overlay network fabric when VXLAN networks are used. Under
    normal circumstances, unknown unicast, multicast, and broadcast traffic may be
    flooded out from all tunnels to other `compute` nodes. This behavior can have
    a negative impact on the overlay network fabric, especially as the number of hosts
    in the cloud scales out.
  prefs: []
  type: TYPE_NORMAL
- en: As an authority on what instances and other network resources exist in the cloud,
    Neutron can pre-populate forwarding databases on all hosts to avoid a costly learning
    operation. ARP proxy, a feature of the L2 population driver, enables Neutron to
    pre-populate the ARP table on all hosts in a similar manner to avoid ARP traffic
    from being broadcast across the overlay fabric.
  prefs: []
  type: TYPE_NORMAL
- en: ML2 architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates how the Neutron API service interacts with
    the various plugins and agents responsible for constructing the virtual and physical
    network at a high level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74a84179-b5ae-470a-8ae5-e0b84965baf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram demonstrates the interaction between the Neutron API,
    Neutron plugins and drivers, and services such as the L2 and L3 agents. For more
    information on the Neutron ML2 plugin architecture, please refer to the following
    URL: [https://docs.openstack.org/neutron/pike/admin/config-ml2.html](https://docs.openstack.org/neutron/pike/admin/config-ml2.html)
  prefs: []
  type: TYPE_NORMAL
- en: Network namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack was designed with multi-tenancy in mind, and provides users with the
    ability to create and manage their own compute and network resources. Neutron
    supports each tenant having multiple private networks, routers, firewalls, load
    balancers, and other networking resources, and is able to isolate many of these
    objects through the use of network namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: A **network namespace** is defined as a logical copy of the network stack with
    its own routes, firewall rules, and network interfaces. When using the open source
    reference plugins and drivers, every DHCP server, router, and load balancer that
    is created by a user is implemented in a network namespace. By using network namespaces,
    Neutron is able to provide isolated DHCP and routing services to each network,
    allowing users to create overlapping networks with other users in other projects
    and even other networks in the same project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following naming convention for network namespaces should be observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DHCP Namespace: **`qdhcp-<network UUID>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Router Namespace: **`qrouter-<router UUID>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load Balancer Namespace: **`qlbaas-<load balancer UUID>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `qdhcp` namespace contains a DHCP service that provides IP addresses to instances
    using the DHCP protocol. In a reference implementation, `dnsmasq` is the process
    that services DHCP requests. The `qdhcp` namespace has an interface plugged into
    the virtual switch and is able to communicate with instances and other devices
    in the same network. A `qdhcp` namespace is created for every network where the
    associated subnet(s) have DHCP enabled.
  prefs: []
  type: TYPE_NORMAL
- en: A `qrouter` namespace represents a virtual router, and is responsible for routing
    traffic to and from instances in subnets it is connected to. Like the `qdhcp`
    namespace, the `qrouter` namespace is connected to one or more virtual switches
    depending on the configuration. In some cases, multiple namespaces may be used
    to plumb the virtual router infrastructure. These additional namespaces, known
    as `fip` and `snat`, are used for distributed virtual routers (DVR) and will be
    discussed later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: A `qlbaas` namespace represents a virtual load balancer, and contains a service
    such as HAProxy that load balances traffic to instances. The `qlbaas` namespace
    is connected to a virtual switch and can communicate with instances and other
    devices in the same network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fun fact: The leading `q` in the name of the network namespaces stands for
    Quantum, the original name for the OpenStack Networking service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Network namespaces of the aforementioned types will only be seen on nodes running
    the Neutron DHCP, L3, or LBaaS agents, respectively. These services are typically
    only configured on controllers or dedicated network nodes. When distributed virtual
    routers are configured, you may find router-related namespaces on `compute` nodes
    as well. The `ip netns list` command can be used to list available namespaces,
    and commands can be executed within the namespace using the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Commands that can be executed in the namespace include `ip`, `route`, `iptables`,
    and more. The output of these commands corresponds to data that's specific to
    the namespace they are executed in. Tools such as `tcpdump` can also be executed
    in a network namespace to assist in troubleshooting the virtual network infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on network namespaces, see the man page for `ip netns`
    at the following URL: [http://man7.org/linux/man-pages/man8/ip-netns.8.html](http://man7.org/linux/man-pages/man8/ip-netns.8.html).
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Neutron services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this installation, the various services that make up OpenStack Networking
    will be installed on the `controller` node rather than a dedicated networking
    node. The `compute` nodes will run L2 agents that interface with the `controller`
    node and provide virtual switch connections to instances.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the configuration settings recommended here and online at [docs.openstack.org](http://docs.openstack.org) may
    not be appropriate for production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Neutron database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the `mysql` client on the `controller` node, create the Neutron database
    and associated user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter the following SQL statements at the `MariaDB [(none)] >` prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Configuring the Neutron user, role, and endpoint in Keystone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To function properly, Neutron requires that a user, role, and endpoint be created
    in Keystone. When executed from the `controller` node, the following commands
    will create a user called `neutron` in Keystone, associate the `admin` role with
    the `neutron` user, and add the `neutron` user to the `service` project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a service in Keystone that describes the OpenStack Networking service
    by executing the following command on the `controller` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the endpoints, use the following `openstack endpoint create` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Installing Neutron packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install the Neutron API server, the DHCP and metadata agents, and the ML2
    plugin on the controller, issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The Neutron DHCP and metadata agents may not be required by all Mechanism drivers
    but are used when implementing the `openvswitch` and `linuxbridge` drivers.
  prefs: []
  type: TYPE_NORMAL
- en: 'On all other hosts, only the ML2 plugin is required at this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On all nodes, update the `[database]` section of the Neutron configuration
    file at `/etc/neutron/neutron.conf` to use the proper MySQL database connection
    string based on the preceding values rather than the default value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Configuring Neutron to use Keystone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Neutron configuration file found at `/etc/neutron/neutron.conf` has dozens
    of settings that can be modified to meet the needs of the OpenStack cloud administrator.
    A handful of these settings must be changed from their defaults as part of this
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify Keystone as the authentication method for Neutron, update the `[DEFAULT]`
    section of the Neutron configuration file on all hosts with the following setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Neutron must also be configured with the appropriate Keystone authentication
    settings. The username and password for the `neutron` user in Keystone were set
    earlier in this chapter. Update the `[keystone_authtoken]` section of the Neutron
    configuration file on all hosts with the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Configuring Neutron to use a messaging service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neutron communicates with various OpenStack services on the AMQP messaging
    bus. Update the `[DEFAULT]` section of the Neutron configuration file on all hosts
    to specify RabbitMQ as the messaging broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Configuring Nova to utilize Neutron networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before Neutron can be utilized as the network manager for OpenStack Compute
    services, the appropriate configuration options must be set in the Nova configuration
    file located at `/etc/nova/nova.conf` on certain hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the controller and `compute` nodes, update the `[neutron]` section with
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Nova may require additional configuration once a Mechanism driver has been determined.
    The Linux bridge and Open vSwitch Mechanism drivers and their respective agents
    and Nova configuration changes will be discussed in further detail in upcoming
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Neutron to notify Nova
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neutron must be configured to notify Nova of network topology changes. On the
    `controller` node, update the `[nova]` section of the Neutron configuration file
    located at `/etc/neutron/neutron.conf` with the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Configuring Neutron services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `neutron-server` service exposes the Neutron API to users and passes all
    calls to the configured Neutron plugins for processing. By default, Neutron is
    configured to listen for API calls on all configured addresses, as seen by the
    default `bind_hosts` option in the Neutron configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As an additional security measure, it is possible to expose the API on the management
    or API network. To change the default value, update the `bind_host` value in the
    `[DEFAULT]` section of the Neutron configuration located at `/etc/neutron/neutron.conf`
    with the management address of the `controller` node. The deployment explained
    in this book will retain the default value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other configuration options that may require tweaking include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`core_plugin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service_plugins`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dhcp_lease_duration`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dns_domain`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these settings apply to all nodes, while others only apply to the `network`
    or `controller` node. The `core_plugin` configuration option instructs Neutron
    to use the specified networking plugin. Beginning with the Icehouse release, the
    ML2 plugin supersedes both the Linux bridge and Open vSwitch monolithic plugins.
  prefs: []
  type: TYPE_NORMAL
- en: 'On all nodes, update the `core_plugin` value in the `[DEFAULT]` section of
    the Neutron configuration file located at `/etc/neutron/neutron.conf` and specify
    the ML2 plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `service_plugins` configuration option is used to define plugins that are
    loaded by Neutron for additional functionality. Examples of plugins include `router`, `firewall`, `lbaas`, `vpnaas`
    and `metering`. This option should only be configured on the `controller` node
    or any other node running the `neutron-server` service. Service plugins will be
    defined in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The `dhcp_lease_duration` configuration option specifies the duration of an
    IP address lease by an instance. The default value is 86,400 seconds, or 24 hours.
    If the value is set too low, the network may be flooded with traffic due to short
    leases and frequent renewal attempts. The DHCP client on the instance itself is
    responsible for renewing the lease, and the frequency of this operation varies
    between operating systems. It is not uncommon for instances to attempt to renew
    their lease well before exceeding the lease duration. The value set for `dhcp_lease_duration`
    does not dictate how long an IP address stays associated with an instance, however.
    Once an IP address has been allocated to a port by Neutron, it remains associated
    with the port until the port or related instance is deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dns_domain` configuration option specifies the DNS search domain that
    is provided to instances via DHCP when they obtain a lease. The default value
    is `openstacklocal`. This can be changed to whatever fits your organization. For
    the purpose of this installation, change the value from `openstacklocal` to `learningneutron.com.`
    On the `controller` node, update the `dns_domain` option in the Neutron configuration
    file located at `/etc/neutron/neutron.conf` to `learningneutron.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When instances obtain their address from the DHCP server, the domain is appended
    to the hostname, resulting in a fully-qualified domain name. Neutron does not
    support multiple domain names by default, instead relying on the project known
    as Designate to extend support for this functionality. More information on Designate
    can be found at the following URL: [https://docs.openstack.org/designate/latest/](https://docs.openstack.org/designate/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: Starting neutron-server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before the `neutron-server` service can be started, the Neutron database must
    be updated based on the options we configured earlier in this chapter. Use the
    `neutron-db-manage` command on the `controller` node to update the database accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the Nova compute services on the `controller` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the Nova compute service on the `compute` nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, restart the `neutron-server` service on the `controller` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Configuring the Neutron DHCP agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neutron utilizes `dnsmasq`, a free and lightweight DNS forwarder and DHCP server,
    to provide DHCP services to networks. The `neutron-dhcp-agent` service is responsible
    for spawning and configuring `dnsmasq` and metadata processes for each network
    that leverages DHCP.
  prefs: []
  type: TYPE_NORMAL
- en: The DHCP driver is specified in the `/etc/neutron/dhcp_agent.ini` configuration
    file. The DHCP agent can be configured to use other drivers, but `dnsmasq` support
    is built-in and requires no additional setup. The default `dhcp_driver` value
    is `neutron.agent.linux.dhcp.Dnsmasq` and can be left unmodified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other notable configuration options found in the `dhcp_agent.ini` configuration
    file include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`interface_driver`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_isolated_metadata`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `interface_driver` configuration option should be configured appropriately
    based on the Layer 2 agent chosen for your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linux bridge**:   `neutron.agent.linux.interface.BridgeInterfaceDriver`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open vSwitch**: `neutron.agent.linux.interface.OVSInterfaceDriver`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the Linux bridge and Open vSwitch drivers will be discussed in further
    detail in upcoming chapters. For now, the default value of `<none>` will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: Only one interface driver can be configured at a time per agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `enable_isolated_metadata` configuration option is useful in cases where
    a physical network device such as a firewall or router serves as the default gateway
    for instances, but Neutron is still required to provide metadata services to those
    instances. When the L3 agent is used, an instance reaches the metadata service
    through the Neutron router that serves as its default gateway. An isolated network
    is assumed to be one in which a Neutron router is not serving as the gateway,
    but Neutron still handles DHCP requests for the instances. This is often the case
    when instances are leveraging flat or VLAN networks with physical gateway devices.
    The default value for `enable_isolated_metadata` is `False`. When set to `True`,
    Neutron can provide instances with a static route to the metadata service via
    DHCP in certain cases. More information on the use of metadata and this configuration
    can be found in *[Chapter 7](dcaa0beb-6648-4d55-9ea7-f4789315539f.xhtml)*, *Attaching
    Instances to Networks*. On the `controller` node, update the `enable_isolated_metadata`
    option in the DHCP agent configuration file located at `/etc/neutron/dhcp_agent.ini`
    to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Configuration options not mentioned here have sufficient default values and
    should not be changed unless your environment requires it.
  prefs: []
  type: TYPE_NORMAL
- en: Restarting the Neutron DHCP agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use the following commands to restart the `neutron-dhcp-agent` service on the
    `controller` node and check its status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7befff5-2ea0-46bb-9bb3-c772d28b3334.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The agent should be in an `active (running)` status. Use the `openstack network
    agent list` command to verify that the service has checked in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9aea419-11a2-4f30-a2d4-37228ee90a70.png)'
  prefs: []
  type: TYPE_IMG
- en: A smiley face under the `Alive` column means that the agent is properly communicating
    with the `neutron-server` service.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Neutron metadata agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Compute provides a metadata service that enables users to retrieve
    information about their instances that can be used to configure or manage the
    running instance. **Metadata** includes information such as the hostname, fixed
    and floating IPs, public keys, and more. In addition to metadata, users can access
    **userdata** such as scripts and other bootstrapping configurations that can be
    executed during the boot process or once the instance is active. OpenStack Networking
    implements a proxy that forwards metadata requests from instances to the metadata
    service provided by OpenStack Compute.
  prefs: []
  type: TYPE_NORMAL
- en: Instances typically access the metadata service over HTTP at `http://169.254.169.254`
    during the boot process. This mechanism is provided by `cloud-init`, a utility
    found on most cloud-ready images and available at the following URL: [https://launchpad.net/cloud-init](https://launchpad.net/cloud-init).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram provides a high-level overview of the retrieval of metadata
    from an instance when the `controller` node hosts networking services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6851a731-a7a9-44ae-81c5-ac8cc919fed7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, the following actions take place when an instance
    makes a request to the metadata service:'
  prefs: []
  type: TYPE_NORMAL
- en: An instance sends a request for metadata to `169.254.269.254` via HTTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata request hits either the router or DHCP namespace depending on the
    route in the instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata proxy service in the namespace sends the request to the Neutron
    metadata agent service via a Unix socket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Neutron metadata agent service forwards the request to the Nova metadata
    API service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Nova metadata API service responds to the request and forwards the response
    to the Neutron metadata agent service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Neutron metadata agent service sends the response back to the metadata proxy
    service in the namespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata proxy service forwards the HTTP response to the instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The instance receives the metadata and/or the user data and continues the boot
    process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For proper operation of metadata services, both Neutron and Nova must be configured
    to communicate together with a shared secret. Neutron uses this secret to sign
    the `Instance-ID` header of the metadata request to prevent spoofing. On the `controller`
    node, update the following metadata options in the `[neutron]` section of the
    Nova configuration file located at `/etc/nova/nova.conf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, update the `[DEFAULT]` section of the metadata agent configuration file
    located at `/etc/neutron/metadata_agent.ini` with the Neutron authentication details
    and the metadata proxy shared secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Configuration options not mentioned here have sufficient default values and
    should not be changed unless your environment requires it.
  prefs: []
  type: TYPE_NORMAL
- en: Restarting the Neutron metadata agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use the following command to restart the `neutron-metadata-agent` and `nova-api`
    services on the `controller` node and to check the services'' status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c5649e2-d2d1-451c-bf78-f570601b3cde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The agent should be in an `active (running)` status. Use the `openstack network
    agent list` command to verify that the service has checked in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2248299-e4a2-4223-8230-ebdbe09bde0e.png)'
  prefs: []
  type: TYPE_IMG
- en: A smiley face under the `Alive` column means that the agent is properly communicating
    with the `neutron-server` service.
  prefs: []
  type: TYPE_NORMAL
- en: If the services do not appear or have `XXX` under the `Alive` column, check
    the respective log files located at `/var/log/neutron` for assistance in troubleshooting.
    More information on the use of metadata can be found in *[Chapter 7](dcaa0beb-6648-4d55-9ea7-f4789315539f.xhtml)*,
    *Attaching Instances to Networks*, and later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Interfacing with OpenStack Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenStack Networking APIs can be accessed in a variety of ways, including
    via the Horizon dashboard, the `openstack` and `neutron` clients, the Python SDK,
    HTTP, and other methods. The following few sections will highlight the most common
    ways of interfacing with OpenStack Networking.
  prefs: []
  type: TYPE_NORMAL
- en: Using the OpenStack command-line interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to the `openstack` command-line client coming on the scene, each project
    was responsible for maintaining its own client. Each client often used its own
    syntax for managing objects and the lack of consistency between clients made life
    for users and operators difficult. The `openstack` client provides a consistent
    naming structure for commands and arguments, along with a consistent output format
    with optional parsable formats such as csv, json, and others. Not all APIs and
    services are supported by the `openstack` client, however, which may mean that
    a project-specific client is required for certain actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To invoke the `openstack` client, issue the `openstack` command at the Linux
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bf6ed78-d603-4915-9ff0-7a17577e9ea6.png)'
  prefs: []
  type: TYPE_IMG
- en: The `openstack` shell provides commands that can be used to create, read, update,
    and delete the networking configuration within the OpenStack cloud. By typing
    a question mark or `help` within the `openstack` shell, a list of commands can
    be found. Additionally, running `openstack help` from the Linux command line provides
    a brief description of each command's functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Neutron command-line interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neutron provides a command-line client for interfacing with its API. Neutron
    commands can be run directly from the Linux command line, or the Neutron shell
    can be invoked by issuing the `neutron` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a551687e-ada2-47b4-905f-bb1d10a63d8a.png)'
  prefs: []
  type: TYPE_IMG
- en: You must source the credentials file prior to invoking the `openstack` and `neutron`
    clients or an error will occur.
  prefs: []
  type: TYPE_NORMAL
- en: The `neutron` shell provides commands that can be used to create, read, update,
    and delete the networking configuration within the OpenStack cloud. By typing
    a question mark or `help` within the Neutron shell, a list of commands can be
    found. Additionally, running `neutron help` from the Linux command line provides
    a brief description of each command's functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The `neutron` client has been deprecated in favor of the `openstack` command-line
    client. However, certain functions, including LBaaS-related commands, are not
    yet available within the `openstack` client and must be managed using the `neutron`
    client. In future releases of OpenStack, the `neutron` client will no longer be
    available.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the commands listed within the client's `help` listing will be covered
    in subsequent chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Using the OpenStack Python SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenStack Python SDK is available on PyPI and can be installed with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Documentation for the SDK is available at the following URL: [https://developer.openstack.org/sdks/python/openstacksdk/users/index.html](https://developer.openstack.org/sdks/python/openstacksdk/users/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Using the cURL utility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The OpenStack Networking API is REST-based and can be manipulated directly
    using HTTP. To make API calls using HTTP, you will need a token. Source the OpenStack
    credentials file and use the `openstack token issue` command shown here to retrieve
    a token:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79cf5e5e-67d9-4d4a-a92f-e6f8d7bdab77.png)'
  prefs: []
  type: TYPE_IMG
- en: The `--fit-width` argument is not necessary in normal operations, but helps
    make the token ID manageable for demonstration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a list of networks, the command should resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31aba40b-482e-4d7a-bd35-62fa9adcdcbc.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, the Neutron API returned a 200 OK response in json format.
    No networks currently exist, so an empty list was returned. Neutron returns HTTP
    status codes that can be used to determine if the command was successful.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenStack Networking API is documented at the following URL: [https://developer.openstack.org/api-ref/network/v2/](https://developer.openstack.org/api-ref/network/v2/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenStack Networking provides an extensible plugin architecture that makes implementing
    new network features possible. Neutron maintains the logical network architecture
    in its database, and network plugins and agents on each node are responsible for
    configuring virtual and physical network devices accordingly. Using the Modular
    Layer 2 (ML2) plugin, developers can spend less time implementing core Neutron
    API functionality and more time developing value-added features.
  prefs: []
  type: TYPE_NORMAL
- en: Now that OpenStack Networking services have been installed across all nodes
    in the environment, configuration of the Mechanism driver is all that remains
    before instances can be created. In the following two chapters, you will be guided
    through the configuration of the ML2 plugin and both the Linux bridge and Open
    vSwitch drivers and agents. We will also explore the differences between Linux
    bridge and Open vSwitch agents in terms of how they function and provide connectivity
    to instances.
  prefs: []
  type: TYPE_NORMAL
