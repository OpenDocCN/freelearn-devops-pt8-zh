<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer042">
<h1 class="chapter-number" id="_idParaDest-36"><a id="_idTextAnchor036"/>2</h1>
<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/>Building Docker Images</h1>
<p>Applications that have components running as software containers are quite a new development and a great way of avoiding problems with underlying infrastructure. As we learned in the previous chapter, containers are processes that are executed on hosts using their kernels, isolated using features present in these kernels (in some cases, for years), and encapsulated in their <span class="No-Break">own filesystems.</span></p>
<p>In this chapter, we will use container images, which are template-like objects, to create containers. Building these images is the first step to creating your own container-based applications. We will learn different procedures to create container images. These images will be our new application’s artifacts, and as such, we need to build them securely and be ready to run them on our laptops or computers, staging and production servers, or even <span class="No-Break">cloud-provisioned infrastructures.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Understanding how copy-on-write <span class="No-Break">filesystems work</span></li>
<li>Building <span class="No-Break">container images</span></li>
<li>Understanding common <span class="No-Break">Dockerfile keys</span></li>
<li>The command line for <span class="No-Break">creating images</span></li>
<li>Advanced image <span class="No-Break">creation techniques</span></li>
<li>Best practices for container <span class="No-Break">image creation</span></li>
</ul>
<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>Technical requirements</h1>
<p>In this chapter, we will teach you how to build container images and use them in your code-compiling workflow. We will use open source tools, as well as a few commercial ones that can run without licensing for non-professional use, to build images and verify their security. We have included some labs in this chapter to help you understand the content presented. These labs have been published at the following GitHub repository: <a href="https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter2">https://github.com/PacktPublishing/Containers-for-Developers-Handbook/tree/main/Chapter2</a>. Here, you will find some extended explanations that have been omitted from this book’s content to make the chapters easier to follow. The <em class="italic">Code In Action</em> video for this chapter can be found <span class="No-Break">at </span><a href="https://packt.link/JdOIY"><span class="No-Break">https://packt.link/JdOIY</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>Understanding how copy-on-write filesystems work</h1>
<p>Building a container<a id="_idIndexMarker186"/> image is the first step that’s required when you develop an application using containers. In this chapter, we will learn about different methods to build images. But first, it will be interesting to deep dive into how images can be created in terms <span class="No-Break">of filesystems.</span></p>
<p>Containers are processes that run isolated thanks to kernel features. They run on top of a host system with its own<a id="_idTextAnchor040"/> filesystem as if they were running completely independently within their own sub-system. Files included in this filesystem are grouped in different layers, one layer on top of another. Files that have to be modified from a lower layer are copied to the layer where the modification is going to be made, and these changes are then committed. New files are only created on the upper layer. This is the basis of <strong class="bold">copy-on-write</strong> (<span class="No-Break"><strong class="bold">CoW</strong></span><span class="No-Break">) filesystems.</span></p>
<p>As we can expect with this model, the container runtime will manage all these changes. Every file modification requires host resources to copy the file between layers and, thus, makes this mechanism a problem to create files continuously. Before creating a new file in the upper layer, all layers must be read to verify that the file isn’t present yet to copy its content to the <span class="No-Break">upper layer.</span></p>
<p>All these layers are <a id="_idIndexMarker187"/>presented in <strong class="bold">read-only</strong> mode to a container every time we create a container using a specific container image as a template, and a new layer is added on top of other<a id="_idIndexMarker188"/> layers in <strong class="bold">read-write</strong> mode. This new layer is the layer that will contain all the file changes since the container started. However, this behavior will occur in all containers running on your system. All containers based on the same container images share these read-only layers, which is very important in terms of disk usage. Only<a id="_idIndexMarker189"/> the <strong class="bold">container layer</strong> differs every time a new container <span class="No-Break">is executed.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">All data that should persist across different container executions must be declared and used outside the containers’ life cycle – for instance, by using <strong class="bold">volumes</strong>, as <a id="_idIndexMarker190"/>we will learn in <a href="B19845_04.xhtml#_idTextAnchor096"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Running Docker Containers</em>. We can declare volumes during the container-image-building process, which indicates that the content exists outside of the <span class="No-Break">image’s layers.</span></p>
<p>As we can see, using these templates speeds up container creation and reduces the size of all containers in our systems. If we compare this with virtual machines, it works like virtual machine templates or snapshots. Only changes are stored at the host level, although it is important to mention here that containers use very <span class="No-Break">little space.</span></p>
<p>However, performance is always affected when using CoW filesystems, which you should be aware <a id="_idIndexMarker191"/>of. Never store logs in a container layer as they may be lost if you remove the container, and it is very important to remember that due to the searching-copying-writing process for any file, your application performance may also be impacted. Therefore, we will never use a container layer to store logs, where processes are continuously writing files or monitoring data. You should write these files on remote backends or use the container volumes feature. This performance decrease applies when you write a lot of small files (thousands), the opposite (a few enormous files), or lots of directories with quite a deep tree structure. You, as a developer, must avoid any of these cases in your applications, and you should prepare your containers to <span class="No-Break">avoid them.</span></p>
<p>Now that we<a id="_idIndexMarker192"/> know the behavior of these CoW filesystems, applied to both container image creation and their execution, let’s learn how to <span class="No-Break">build images.</span></p>
<h1 id="_idParaDest-40"><a id="_idTextAnchor041"/>Creating container images</h1>
<p>In this section, we<a id="_idIndexMarker193"/> will review the different methods to build container images, along with their pros and cons and use cases, so that you can choose the right one, depending on <span class="No-Break">your needs.</span></p>
<p>There are three ways to create <span class="No-Break">container images:</span></p>
<ul>
<li>Using a base<a id="_idIndexMarker194"/> image within a Dockerfile, which is a recipe file that contains different automated steps that are executed to create <span class="No-Break">an image</span></li>
<li>Interactively and manually executing commands and storing the <span class="No-Break">resulting filesystem</span></li>
<li>From an empty filesystem, using a Dockerfile recipe file and copying only the binaries and libraries required for <span class="No-Break">our application</span></li>
</ul>
<p>It is easy to see that the last method is the best in terms of security, but this can be difficult to implement if your code has many dependencies and is very integrated with operating system files. Let’s explore these methods, starting with the <span class="No-Break">most common.</span></p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor042"/>Using Dockerfiles to create container images</h2>
<p>Before we <a id="_idIndexMarker195"/>describe this method, let’s learn <a id="_idIndexMarker196"/>what a <span class="No-Break"><strong class="bold">Dockerfile</strong></span><span class="No-Break"> is.</span></p>
<p>A Dockerfile <a id="_idIndexMarker197"/>is an <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OSI</strong>)-compliant file that works as a recipe, containing a step-by-step procedure to create a container image. It contains a set of key-value pairs that describe different executions and meta-information regarding the image’s behavior. We can use variables to expand arguments that are passed when building images, and it is perfect for automation. If a Dockerfile is well written, we can ensure <span class="No-Break">its reproducibility.</span></p>
<p>The following is an example of <span class="No-Break">a Dockerfile:</span></p>
<pre class="source-code">
FROM debian:stable-slim
RUN apt-get update -qq &amp;&amp; apt-get install -qq packag<a id="_idTextAnchor043"/>e1 package2
COPY . /myapp
RUN make /myapp
CMD python /myapp/app.py
EXPOSE 5000</pre> <p>As mentioned before, this file describes all the steps required to assemble an image. Let’s provide a quick overview of the steps taken in the <span class="No-Break">presented Dockerfile.</span></p>
<p>The first line, <strong class="source-inline">FROM debian:stable-slim</strong>, indicates that this container image will be taken as a base image; hence, all its layers will be used. The container runtime will download (<em class="italic">pull</em>) all these layers if they are not present in our host. If any of them are already in our host, they will be used. This layer could have come from any other image already in our host. Container image layers <span class="No-Break">are reused.</span></p>
<p>The second line, <strong class="source-inline">RUN apt-get update -qq &amp;&amp; apt-get install -qq package1 package2</strong>, executes all the content included as values. First, <strong class="source-inline">apt-get update –qq</strong> will be executed, and if it’s successful, <strong class="source-inline">apt-get install -qq package1 package2</strong> will be executed. This full step creates just one layer, on top of the previous one. This layer will automatically be enabled for any other image using the same execution, using<a id="_idIndexMarker198"/> the same <strong class="source-inline">debian:stable-slim</strong> <span class="No-Break">base </span><span class="No-Break"><a id="_idIndexMarker199"/></span><span class="No-Break">image.</span></p>
<p>Th<a id="_idTextAnchor044"/>e third line, <strong class="source-inline">COPY . /myapp</strong>, will copy all the files available in the current directory to a directory named <strong class="source-inline">/myapp</strong>, in a new layer. As mentioned in the second line, this also creates a reusable layer for any new image that contains the <span class="No-Break">same entry.</span></p>
<p>The fourth line, <strong class="source-inline">RUN make /myapp</strong>, executes the <strong class="source-inline">make /myapp</strong> command and creates a new line. Remember that this is an example. We added a <strong class="source-inline">make</strong> sentence to build our source code. In this step, for example, we run a compiler, previously installed in the image, and build our binary artifact. All executing layers (those that include a <strong class="source-inline">RUN</strong> key) should exit correctly. If this doesn’t happen, the image build process will break and be stopped. If this happens, all previous layers will remain in your system. The container runtime creates a layers cache, and all following executions will reuse them by default. This behavior can be avoided by recreating all previous images during the <span class="No-Break">build process.</span></p>
<p>The two final steps don’t add layers. The <strong class="source-inline">CMD</strong> key declares which command line will be executed (remember that a container runs a main process), and <strong class="source-inline">EXPOSE</strong> adds the meta-information regarding which port should be exposed (listening). This way, we explicitly declare in which port our application will listen to any kind <span class="No-Break">of communication.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">You should declare all relevant meta-information in your Dockerfiles, such as the <em class="italic">ports exposed</em>, the <em class="italic">volumes</em> for persistent data, the <em class="italic">username</em> (or <em class="italic">userid</em>) defined for your main process, and the command line that should run on startup. This information may be required by your container’s orchestrator administrators because it is very important to avoid security issues. They will probably force some security policies in the production platform that disallow your application’s execution. Ask them whether some security policies are applied to ensure you added the required information. Anyway, if you follow the security practices described in this book, you probably won’t have any problems <span class="No-Break">in production.</span></p>
<p>As you can see, this<a id="_idIndexMarker200"/> is a pretty reproducible <a id="_idIndexMarker201"/>process. This recipe will create the same image every time if we don’t change anything. This helps developers focus on their code. However, creating reproducible images is not that easy. If you take a closer look at the used <strong class="source-inline">FROM</strong> value, we use <strong class="source-inline">debian:stable-slim</strong>, which means that the default <a id="_idIndexMarker202"/>image <strong class="bold">registry</strong> will be used, <strong class="source-inline">docker.io</strong>. For now, you just have to know that a registry is a store for all container image layers. The value of the <strong class="source-inline">FROM</strong> key indicates that a <strong class="source-inline">debian</strong> image, with a specific tag of <strong class="source-inline">stable-slim</strong>, will be used, and thus, if Docker changes this image, all your image builds will also change. Tags are the way we identify images, but they are not uniquely identified. Each image and layer within images are uniquely<a id="_idIndexMarker203"/> identified by <strong class="bold">digest hashes</strong>, and these are the real relevant values that you should closely monitor. To get these values, we have to either pull the image or review the information in the defined registry. The easier method is to pull the image, which happens when you execute your build process, but in this example, we used a mocked Dockerfile, so it won’t <span class="No-Break">work as-is.</span></p>
<p>So, let’s pull the image from the official Docker images registry, at <a href="https://hub.docker.com">https://hub.docker.com</a>, or by using the <strong class="source-inline">docker.io</strong> <span class="No-Break">command-line tool:</span></p>
<pre class="console">
$ docker image pull debian:stable-slim
stable-slim: Pulling from library/debian
de661c304c1d: Pull complete
Digest: sha256:f711bda490b4e5803ee7f634483c4e6fa7dae54102654f2c231ca58eb233a2f1
Status: Downloaded newer image for debian:stable-slim
docker.io/library/debian:stable-slim</pre> <p>Here, we execute <strong class="source-inline">docker image pull debian:stable-slim</strong> to download this image from <strong class="source-inline">docker.io</strong>. All its layers will be downloaded. The Docker Hub website provides lots of useful information, such as all the tags associated with an image and the vulnerabilities detected in the <span class="No-Break">contained files.</span></p>
<p>The digest shown <a id="_idIndexMarker204"/>in<a id="_idIndexMarker205"/> the previous code snippet will identify this image uniquely. We can verify the image of our system and review its information by executing <strong class="source-inline">docker image inspect</strong>, using <a id="_idIndexMarker206"/>its <span class="No-Break"><strong class="bold">image ID</strong></span><span class="No-Break">:</span></p>
<pre class="console">
$ docker image ls --no-trunc
REPOSITORY   TAG           IMAGE ID                                                                  CREATED      SIZE
debian       stable-slim   sha256:4ea5047878b3bb91d62ac9a99cdcf9e53f4958b01000d85f541004ba587c1cb1   9 days ago   80.5MB
$ docker image inspect 4ea5047878b3bb91d62ac9a99cdcf9e53f4958b01000d85f541004ba587c1cb1 |grep -A1 -i repodigest
        "RepoDigests": [
            "debian@sha256:f711bda490b4e5803ee7f634483c4e6fa7dae54102654f2c231ca58eb233a2f1"</pre> <p>All containers’ related objects are identified by object IDs, and as such, we can use them to refer to each object. In this example, we used the image ID to inspect <span class="No-Break">the object.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">We can use <strong class="source-inline">–-digests</strong> when listing local images to retrieve all their digests – for example, with the image used in <span class="No-Break">this section:</span></p>
<p class="callout"><strong class="source-inline">$ docker image </strong><span class="No-Break"><strong class="source-inline">ls --digests</strong></span></p>
<p class="callout"><strong class="source-inline">REPOSITORY   TAG           DIGEST                   IMAGE ID       CREATED      </strong><span class="No-Break"><strong class="source-inline">SIZE</strong></span></p>
<p class="callout"><strong class="source-inline">debian       stable-slim   sha256:f711bda490b4e5803ee7f634483c4e6fa7dae54102654f2c231ca58eb233a2f1   4ea5047878b3   9 days ago   </strong><span class="No-Break"><strong class="source-inline">80.5MB</strong></span></p>
<p>It is important to<a id="_idIndexMarker207"/> note that image IDs are different <a id="_idIndexMarker208"/>from their digests. The ID represents the current compilation or identifier generated on your system, while the digest represents the compendium of all the layers and essentially identifies the image anywhere – on your laptop, on your servers, or even in the registry where it is remotely stored. The image digest is associated with the image content manifest (<a href="https://docs.docker.com/registry/spec/manifest-v2-2/">https://docs.docker.com/registry/spec/manifest-v2-2/</a>) and is used in V2 registries (the current version for most modern registry implementations). Since your local builds are not in a registry format, the digest will be displayed as <strong class="source-inline">none</strong>. Pushing your images to a V2 registry will <span class="No-Break">change this.</span></p>
<p>Let’s review this process, as well as the image IDs and digests, by looking at a quick and simple example. We will build a couple of images using the following two lines of <span class="No-Break">a Dockerfile:</span></p>
<pre class="source-code">
FROM debian:stable-slim
RUN apt-get update -qq &amp;&amp; apt-get install -qq curl</pre> <p>Using the current <strong class="source-inline">debian:stable-slim</strong> image, we will update its content and install the <span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break"> package.</span></p>
<p>We will build two images, <strong class="source-inline">one</strong> and <strong class="source-inline">two</strong>, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure 2.1 – The execution of two consecutive container image builds. No changes are expected; hence, the images are equal" height="601" src="image/B19845_02_01.jpg" width="907"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The execution of two consecutive container image builds. No changes are expected; hence, the images are equal</p>
<p>The first build <a id="_idIndexMarker209"/>process will create a layers cache, and <a id="_idIndexMarker210"/>thus, the second build will reuse them and the process will be faster as the layers are the same. No installation process will be triggered. We have used the current directory as the <strong class="bold">build context</strong>. Container<a id="_idIndexMarker211"/> runtimes such as Docker are executed in a client-server model, and as such, we talk with the Docker daemon using our Docker command line. The building process sends all files in the <em class="italic">current context</em> (<em class="italic">path</em>) to the daemon so that it can use them to create the image’s filesystem. This is critical because if we choose the wrong context, a lot of files will be sent to the daemon, and this will impact the building process. We should correctly specify which directory contains our code, and this will be used during the build process. In this context folder, we should avoid binaries, libraries, documentation, and so on. It is important to note that we can use Git repositories (in URL format) as the build context, which makes it very interesting for <span class="No-Break">CI/CD integrations.</span></p>
<p>To avoid sending irrelevant files to the daemon during the build process, we can use the <strong class="source-inline">.dockerignore</strong> file. In this file, we will add the list of files and folders that should be excluded, even if they are present in our <span class="No-Break">build context.</span></p>
<p>Let’s review the information we have from these images on our system. If we execute <strong class="source-inline">docker image ls –digest</strong>, we will obtain their image IDs and <span class="No-Break">their digests:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<img alt="Figure 2.2 – A list of the created container images, showing their completely equal IDs" height="104" src="image/B19845_02_02.jpg" width="1254"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – A list of the created container images, showing their completely equal IDs</p>
<p>The first thing we can see is that both images, <strong class="source-inline">one</strong> and <strong class="source-inline">two</strong>, have the same image ID. This is because we reused their layers. In the second build process, using the same Dockerfile, the container runtime reuses all previous equal image layers (those coming from the same execution), and the image was created very fast. They are the same image with two <span class="No-Break">different tags.</span></p>
<p>We can also<a id="_idIndexMarker212"/> see <a id="_idIndexMarker213"/>that only the base image shows its digest. As mentioned previously, it is the only one that comes from a V2 registry. If we upload one of our images to <em class="italic">Docker Hub</em> (or any other V2-compatible registry), its digest will <span class="No-Break">be created.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">To be able to upload images to Docker Hub, you need a working account. Create your account by going to <a href="https://hub.docker.com/signup">https://hub.docker.com/signup</a>. The process is pretty simple, and you will have a Docker Hub registry account within <span class="No-Break">a minute.</span></p>
<p>Let’s see how uploading the image works and how it will have its immutable and unique <span class="No-Break">reference digest.</span></p>
<p>Before initiating the process, we will just log in to Docker Hub using our account name. We will be prompted for our password, after which we should receive a <strong class="source-inline">Login </strong><span class="No-Break"><strong class="source-inline">Succeeded</strong></span><span class="No-Break"> message:</span></p>
<pre class="console">
$ docker login --username &lt;YOUR_USERNAME&gt;
Password:
Login Succeeded
Logging in with your password grants your terminal complete access to your account.
For better security, log in with a limited-privilege personal access token. Learn more at https://docs.docker.com/go/access-tokens/</pre> <p>Now that we are logged in, we need to retag our image. Image tags are the human-readable format we use to reference images. In the build processes used for this example, we used <strong class="source-inline">one</strong> and <strong class="source-inline">two</strong> as tags through the command line by writing <strong class="source-inline">docker build –t &lt;TAG&gt;</strong>. However, we <a id="_idIndexMarker214"/>saw that both were the same<a id="_idIndexMarker215"/> image; hence, we can say that tags are names for an image ID, which may cause you some confusion. <em class="italic">Can we trust image tags?</em> The short answer is, <em class="italic">no, we can’t</em>. They don’t represent a unique image state. We can have different tags for an image ID and change these images, but if you still use those tags, you will be using completely different images. In our example, anyone can change our <strong class="source-inline">debian:stable-slim</strong> image. If we rebuild some of our images, based on this tag, we will create a new image with completely different content. What if the new image contains some code exploitation because a malicious attacker included it in that base image? This should not happen in very controlled image registries such as Docker Hub, but this problem <span class="No-Break">does exist.</span></p>
<p>Let’s retag and upload our image by using <strong class="source-inline">docker tag</strong> and then <span class="No-Break"><strong class="source-inline">docker push</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<img alt="Figure 2.3 – Tagging and pushing an image to obtain its digest" height="308" src="image/B19845_02_03.jpg" width="1155"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Tagging and pushing an image to obtain its digest</p>
<p>Note that we need to push the image. Just re-tagging does not work. Now, we have a unique image, and anyone can use our tag to reference it. If we update our <strong class="source-inline">one</strong> image, by adding some new content or changing the command line to be executed, this digest will change. And <a id="_idIndexMarker216"/>even if we still use the <a id="_idIndexMarker217"/>same <strong class="source-inline">frjaraur/one</strong> tag, a new build process using our image will create <span class="No-Break">new content.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">As a developer, you should be aware of any changes introduced in the images you use as a reference for creating your images. You might be wondering which method is correct to manage these changes. The short answer would be always using image digests (following the example tags and digest, we will use <strong class="source-inline">FROM debian:stable-slim@sha256:f711bda490b4e5803ee7f634483c4e6fa7dae54102654f2c231ca58eb233a2f1</strong>). This method can be very complex, but it is the most secure. Another method would be using your own registry, isolated from the internet, where you store your images. With your own managed private registry, you may be comfortable using image tags. You will be the only one able to update your base images; hence, you manage the complete image <span class="No-Break">life cycle.</span></p>
<p>As we mentioned at the beginning of this example, we built two images using the same Dockerfile, and we realized that both images have the same image ID; hence, they are exactly the same. Let’s change this a bit and use the <strong class="source-inline">docker build –no-cache</strong> option, which avoids reusing previously <span class="No-Break">created layers:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 2.4 – Executing the image-building process without a cache" height="375" src="image/B19845_02_04.jpg" width="1139"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Executing the image-building process without a cache</p>
<p>We can see that a completely new image was built, even though we are using the same Dockerfile. This is due to the time between executions. Layers change between build executions because we made modifications at two different points in time. Of course, we can also include new changes due to package updates, but in this case, it is <span class="No-Break">even simpler.</span></p>
<p>What we can learn from this is that reusing layers helps us maintain image sizes and build times (we didn’t notice this in this example because we used a simple two-line Dockerfile, but when you are compiling or downloading a bunch of modules for your code, it can take a lot of time), but when we need to refresh the image content, disabling the cache is a must. This is very useful when we create base image files for our projects – for example, our own .NET Core and Python projects. We will use these base images, uploaded into our registry, and we will be sure of their content. When a new release arrives, we <a id="_idIndexMarker218"/>can rebuild these images and<a id="_idIndexMarker219"/> all their dependent images (our applications’ images). This process should be part of our automated <span class="No-Break">CI/CD pipelines.</span></p>
<p>Now that we understand how to build images using Dockerfiles, we will move on to a new method that can be helpful in very <span class="No-Break">specific cases.</span></p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor045"/>Creating container images interactively</h2>
<p>We haven’t <a id="_idIndexMarker220"/>mentioned it before, but it is important to comment here that the Dockerfile <strong class="source-inline">RUN</strong> lines create intermediate containers to execute the commands, written as values after the <strong class="source-inline">RUN</strong> key. Hence, the <strong class="source-inline">docker build</strong> command launches a series of chained containers that create the different layers that are finally part of an image. Before executing a new container, this process stores the modified files (container layer) in the system, using the container runtime’s <span class="No-Break"><strong class="source-inline">commit</strong></span><span class="No-Break"> feature.</span></p>
<p>These containers run one after another, using the layer created by the previous one. The interactive process we are about to describe follows this workflow in a simplified way. We will run a container, using an image as a base, and manually run and copy all the commands and content required by our application. Changes will be created on the fly, and we will commit the created container layer when we have finished. This method may be interesting when we need to install software that asks for different configurations interactively and we can’t automate <span class="No-Break">the process.</span></p>
<p>This method lacks reproducibility and shouldn’t be used if we can find a way to automate the image creation process. No one will have any clue of how you installed the content inside the image (the shell history will contain the steps if you didn’t remove it, but interactive commands will not be there). Let’s introduce a command that will help us understand how images were built – <strong class="source-inline">docker image history</strong>. This command shows all the steps taken to create an image, including the meta-information added in the process, in <a id="_idIndexMarker221"/><span class="No-Break">reverse order.</span></p>
<p>Let’s take a look at this output using one of the images from the previous section, <em class="italic">Using Dockerfiles to create </em><span class="No-Break"><em class="italic">container images</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 2.5 – Reviewing all the steps that were used to create a container image" height="259" src="image/B19845_02_05.jpg" width="831"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Reviewing all the steps that were used to create a container image</p>
<p>Image history must be read in reverse order, starting from the latest line. We will start with an <strong class="source-inline">ADD</strong> key, which represents the initial <strong class="source-inline">FROM</strong> key from our Dockerfile. This is because the <strong class="source-inline">FROM</strong> key is interpreted as copying all the base image content on top of the <span class="No-Break">base layer.</span></p>
<p>We used <strong class="source-inline">–-no-trunc</strong> to be able to read the full command line from the output. We can easily see that this image was created using the <strong class="source-inline">/bin/sh -c apt-get update –q &amp;&amp; apt-get install –qq curl</strong> command. The <strong class="source-inline">docker image history</strong> command will show us the steps that were executed to build any image created from a Dockerfile, but it won’t work for interactively <span class="No-Break">created ones.</span></p>
<p>Let’s see a <a id="_idIndexMarker222"/>simple example of installing a Postfix mail server using the <span class="No-Break"><em class="italic">Debian</em></span><span class="No-Break"> image:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 2.6 – The manual execution of a Postfix mail package" height="620" src="image/B19845_02_06.jpg" width="1119"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – The manual execution of a Postfix mail package</p>
<p>Once the installation process has finished, we will be prompted to configure various aspects of the server. This configuration is <span class="No-Break">completely interactive:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<img alt="Figure 2.7 – The Postfix installation is interactive because it asks users for specific configurations" height="606" src="image/B19845_02_07.jpg" width="1212"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – The Postfix installation is interactive because it asks users for specific configurations</p>
<p>The installation <a id="_idIndexMarker223"/>process will ask you for some configurations interactively and after that, the Postfix server will be ready to work. We can exit the container process by executing <strong class="source-inline">exit</strong>, and we will commit the container layer as a new image. We use <strong class="source-inline">docker container ls –l</strong> to only list the last container executed, and then we execute <strong class="source-inline">docker commit</strong> (or <strong class="source-inline">docker container commit</strong> – both commands will work as they both refer to containers) to save the current container layer as a <span class="No-Break">new image:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 2.8 – Committing the container layer to create an image" height="322" src="image/B19845_02_08.jpg" width="1084"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Committing the container layer to create an image</p>
<p>However, as we previously mentioned about this method, we can’t know the steps taken to create <span class="No-Break">the image.</span></p>
<p>Let’s try using the <strong class="source-inline">docker image </strong><span class="No-Break"><strong class="source-inline">history</strong></span><span class="No-Break"> command:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<img alt="Figure 2.9 – History does not show any commands when an interactive process was followed" height="283" src="image/B19845_02_09.jpg" width="1131"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – History does not show any commands when an interactive process was followed</p>
<p>All we can see in the output is that we used <strong class="source-inline">bash</strong> to do something. We will have the commands in its <strong class="source-inline">.bash_history</strong> file, but this is not how things should be done. If you must use this method in specific cases, such as when your application’s installation requires some interactive steps, remember to document all the changes you made in the file to let other developers understand <span class="No-Break">your process.</span></p>
<p>This method is not recommended because it is not reproducible, and we can’t add any meta-information to the container image. In the next section, we will describe possibly the best method<a id="_idIndexMarker224"/> to remedy this, but it requires a lot of knowledge about your application binary files, libraries, and <span class="No-Break">hidden dependencies.</span></p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor046"/>Creating images from scratch</h2>
<p>In this method, as its <a id="_idIndexMarker225"/>name already indicates, we will create an empty layer and all files will be introduced, using a packaged set of files. You may have noticed that all the image history we have seen so far involved using the <strong class="source-inline">ADD</strong> key as the first step. This is how a container runtime starts the building process – by copying the content of the <span class="No-Break">base image.</span></p>
<p>Using this method, you can ensure that only explicit required files will be included in the container image. It works very well with coding languages such as Go because you can include all their dependencies in binaries; hence, adding your compiled artifacts will probably be enough for your application to work correctly. This method also uses Dockerfile files, but in this case, we will start with a simple <strong class="source-inline">FROM scratch</strong> line. This creates an empty layer for our files. Let’s take a look at a simple <span class="No-Break">example Dockerfile:</span></p>
<pre class="source-code">
FROM scratch
ADD hello /
CMD ["/hello"]</pre> <p>This is a simple Dockerfile in which we just add files and meta-information. It will contain our binary file, on top of an empty structure, and the meta-information required to build a complete container image. As you can imagine, this method creates the most secure images because the attack surface is completely reduced to our own application. Developers can create images from scratch, packaging all the files required for their applications. This can be very tricky and lots of effort is required to include all dependencies. As mentioned earlier in this section, it works very well with applications running static binaries, which include all <span class="No-Break">their dependencies.</span></p>
<p>This method can also be used to create images based on exotic or highly customized operating systems for which we don’t have base images. In these cases, you should remove all non-required files and all references to the underlying hardware. This can be very difficult, and that’s why it is usually recommended to use official container images. We will learn a bit more about the different types of images and how to ensure their origin, immutability, and <a id="_idIndexMarker226"/>ownership in <a href="B19845_03.xhtml#_idTextAnchor082"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Shipping </em><span class="No-Break"><em class="italic">Docker Images</em></span><span class="No-Break">.</span></p>
<p>Now that we know how to make container images using different methods, we should review the most important keys we will use <span class="No-Break">in Dockerfiles.</span></p>
<h1 id="_idParaDest-44"><a id="_idTextAnchor047"/>Understanding common Dockerfile keys</h1>
<p>In this section, we<a id="_idIndexMarker227"/> will take a look at the most important keys and their best practices. For full reference, it is better to review the documentation provided by Docker <span class="No-Break">Inc. (</span><a href="https://docs.docker.com/engine/reference/builder/"><span class="No-Break">https://docs.docker.com/engine/reference/builder/</span></a><span class="No-Break">).</span></p>
<p>Container runtimes can create container images by reading a series of instructions written in a Dockerfile. Following <a id="_idIndexMarker228"/>this recipe-like file, a container runtime will assemble a <span class="No-Break">container image.</span></p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor048"/>FROM</h2>
<p>All Dockerfiles always<a id="_idIndexMarker229"/> start with a <strong class="source-inline">FROM</strong> key. This key is used to set the<a id="_idIndexMarker230"/> base image and initialize the build process. We can use any valid container image as a valid value for the <strong class="source-inline">FROM</strong> key, and a <strong class="source-inline">scratch</strong> keyword is reserved to build images based on an <span class="No-Break">empty layer.</span></p>
<p>A Dockerfile can include multiple image build processes, although usually, we will use different files for <span class="No-Break">each process.</span></p>
<p>We can refer to images using their names and tags, and we can include their digests to ensure image uniqueness. If no tag is used, <strong class="source-inline">latest</strong> will be used automatically. Try to avoid this bad practice and always use the appropriate tag, or, even better, add its digest if you use public image registries. It is also possible to define a reference for each building process using the <strong class="source-inline">AS</strong> key. This way, we can share content between container images built with a unique Dockerfile. <strong class="bold">Multi-stage building</strong> is a<a id="_idIndexMarker231"/> practice in which we copy content from an image into other<a id="_idTextAnchor049"/>s. We will explore a use case in the <em class="italic">Advanced image build processes</em> section later in <span class="No-Break">this chapter.</span></p>
<p>As mentioned previously, a Dockerfile can include multiple build definitions, and we will name them using the <strong class="source-inline">AS</strong> key, which allows us to execute only specific targets, and the <strong class="source-inline">–-</strong><span class="No-Break"><strong class="source-inline">target</strong></span><span class="No-Break"> command.</span></p>
<p>To modify the behavior of the building process, we will use the <strong class="source-inline">ARG</strong> and <strong class="source-inline">ENV</strong> keys. We can use the <strong class="source-inline">–-build-arg</strong> option to include additional arguments in the build process, and the container runtime will evaluate these values whenever the <strong class="source-inline">ARG</strong> key is found. The following line shows an example of how arguments can be passed to the <span class="No-Break"><strong class="source-inline">build</strong></span><span class="No-Break"> command:</span></p>
<pre class="console">
$ docker image build –build-arg myvariable=myvalue –tag mynewimage:mytag context-directory –file myDockerfile</pre> <p>Note here that we used a specific <strong class="source-inline">context</strong> and non-default Dockerfile by adding the <strong class="source-inline">–file</strong> argument. We also added <strong class="source-inline">myvalue</strong> to the <strong class="source-inline">myvariable</strong> variable, and we should have included <a id="_idIndexMarker232"/>the <strong class="source-inline">ARG</strong> key in the <strong class="source-inline">myDockerfile</strong> file to<a id="_idIndexMarker233"/> expand <span class="No-Break">this value.</span></p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor050"/>ARG</h2>
<p><strong class="source-inline">ARG</strong> is the only key <a id="_idIndexMarker234"/>that can be used before <strong class="source-inline">FROM</strong> to use build<a id="_idIndexMarker235"/> arguments – for example, to choose a specific base image. As a developer, you may want to have two different images for production and development, with some small changes, such as enabling debugging flags. We will use only one Dockerfile, but two build processes will be triggered, depending on the arguments passed. The following simple exampl<a id="_idTextAnchor051"/>e may help you understand <a id="_idTextAnchor052"/>this <span class="No-Break">use case:</span></p>
<pre class="source-code">
ARG CODE_VERSION=dev
FROM base:${CODE_VERSION}
CMD /code/run-app</pre> <p>We will use <strong class="source-inline">–build-arg CODE_VERSION=prod</strong> whenever we need to build a production image, using a specific base image, <strong class="source-inline">base:prod</strong>, which may contain fewer files <span class="No-Break">and binaries.</span></p>
<p>It is also usual to add the <strong class="source-inline">ENV</strong> key with <strong class="source-inline">ARG</strong>. The <strong class="source-inline">ENV</strong> key is used to add or modify environment variables for the containers that are used during the build process – for example, to add some path to <strong class="source-inline">LD_LIBRARY</strong> or change the <strong class="source-inline">PATH</strong> variable. <strong class="source-inline">ARG</strong> can then be used to modify environment variables <span class="No-Break">at runtime.</span></p>
<p>To include meta-information in our final container image, we can use the <strong class="source-inline">LABEL</strong> key. Labels will help us identify a framework that’s been used, a release version, the creator and maintainer of the content, and so on, or even a short description of <span class="No-Break">its usage.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">The OCI defines some conventional labels that may be used, and it would be interesting to use them instead of creating your own as many applications integrate this standard. You can review these labels at <a href="https://github.com/opencontainers/image-spec/blob/main/annotations.md">https://github.com/opencontainers/image-spec/blob/main/annotations.md</a>. You will find labels such as <strong class="source-inline">org.opencontainers.image.authors</strong>, <strong class="source-inline">org.opencontainers.image.vendor</strong>, and <strong class="source-inline">org.opencontainers.artifact.description</strong>, all of which are standard and integrated into<a id="_idIndexMarker236"/> many<a id="_idIndexMarker237"/> <span class="No-Break">container-related tools.</span></p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor053"/>WORKDIR</h2>
<p>All command <a id="_idIndexMarker238"/>executions defined in a Dockerfile will run<a id="_idIndexMarker239"/> relative to a working directory. We can change this by using the <strong class="source-inline">WORKDIR</strong> key. Once defined in our Dockerfile, all subsequent defined steps will use this environment – for example, to copy files inside the <span class="No-Break">image layers.</span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor054"/>COPY and ADD</h2>
<p>Adding files to <a id="_idIndexMarker240"/>image layers is always needed. We will<a id="_idIndexMarker241"/> include <a id="_idIndexMarker242"/>our code or binaries, libraries, some static files, and so on. However, we shouldn’t add certificates, tokens, passwords, and so on. In general, any content that requires some security or may change frequently must be included during runtime, and not in the <span class="No-Break">image layers.</span></p>
<p>We can use the <strong class="source-inline">COPY</strong> and <strong class="source-inline">ADD</strong> keys to add files to image layers. The <strong class="source-inline">COPY</strong> instruction copies files and directories into specified image paths. If relative paths are used for the source, files must be included in the build context directory. If relative paths are used for the destination, the <strong class="source-inline">WORKDIR</strong> key will be used as the reference path. We can also copy files from other images declared in the same Dockerfile by using <strong class="source-inline">–-from=&lt;IMAGE_TARGET_NAME&gt;</strong>. It is important to note that file ownership can be changed using the <strong class="source-inline">–-chown=&lt;USERNAME or USERID&gt;:&lt;GROUPNAME or GROUPID&gt;</strong> command; if omitted, the user from the current container execution step will <span class="No-Break">be used.</span></p>
<p><strong class="source-inline">ADD</strong> works like <strong class="source-inline">COPY</strong>, but in this case, you can use remote URLs as a source, as well as TAR and gzip packaged files. If you use a compressed and packaged file, it will be unpackaged and uncompressed automatically for you in the <span class="No-Break">specified destination.</span></p>
<p>Each file that’s passed is verified against the checksums of image files, but the modification time isn’t recorded, so you must be aware of the changes you make to your files before executing the building process. It is better to add a separate <strong class="source-inline">COPY</strong> line for those files you are often editing (for example, your application’s code), or simply disable caching if you are not sure whether your file changes were <span class="No-Break">correctly copied.</span></p>
<p>To avoid copying some files inside our project folders, we can use the <strong class="source-inline">.dockerignore</strong> file. This file<a id="_idIndexMarker243"/> contains a list of files that shouldn’t be<a id="_idIndexMarker244"/> included <a id="_idIndexMarker245"/>in the Docker build context; hence, they will not be copied into the <span class="No-Break">image layers.</span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor055"/>RUN</h2>
<p>The <strong class="source-inline">RUN</strong> key is used to<a id="_idIndexMarker246"/> execute the command line inside containers <a id="_idIndexMarker247"/>that were created during the build process. This action is fundamental to creating container images. All commands passed as a value to this key will be executed, and the resulting container layer will be committed as a new image layer; hence, all the <strong class="source-inline">RUN</strong> keys create a layer. Only the <strong class="source-inline">COPY</strong>, <strong class="source-inline">ADD</strong>, and <strong class="source-inline">RUN</strong> keys create layers; none of the other keys increase image size because they modify the resulting image behavior and add meta-information. You will probably see the <strong class="source-inline">RUN</strong> values use multiple lines, starting with <strong class="source-inline">&amp;&amp;</strong> and ending with <strong class="source-inline">\</strong>. This simple trick will avoid the creation of new layers for each command executed. This way, you can concatenate multiple executions in one line and separate them into multiple lines for easy reading. Lines will be treated as if they were just one line, and thus, only one layer will be created. You should take care here because you may lose layer reusability, and this method can also mask errors during building processes. If you are having issues with one long line that contains a lot of commands, decouple them into multiple executions to isolate the error and, once solved, concatenate the lines again to create just <span class="No-Break">one line.</span></p>
<p>A simple example would look <span class="No-Break">like this:</span></p>
<pre class="source-code">
RUN apt-get update –qq \
&amp;&amp; apt-get install --no-install-recommends --no-install-suggests –qq \
curl \
ca-certificates \
&amp;&amp; apt-get clean</pre> <p>These five lines will be interpreted like three different executions in the same container, so they will just create one layer for the <span class="No-Break">final image.</span></p>
<p>It is important to understand that the build process does not store process states. This means that if we run a process and we expect it to be running upon the next <strong class="source-inline">RUN</strong> line, it won’t because the container runtime only stores files from the container layer. This also applies to services or daemons. The build process will not work if you expect to have some processes <a id="_idIndexMarker248"/>already running and you apply some data or files to<a id="_idIndexMarker249"/> them. Each execution ends when the <strong class="source-inline">RUN</strong> line <span class="No-Break">is processed.</span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor056"/>USER</h2>
<p>By default, the<a id="_idIndexMarker250"/> container <a id="_idIndexMarker251"/>runtime will execute all commands inside containers with <strong class="source-inline">userid</strong>, which is defined in the base image, and <strong class="source-inline">root</strong> if we are creating an image from scratch. You will find that most official Docker container images will run as <strong class="source-inline">root</strong>. Docker Inc. and other vendors prepare their images to allow you to install and manage additional software and binaries. You should ensure that your images run with the principle of <em class="italic">less privilege</em>, and thus, you must declare which user will run a container’s main process. Dockerfile’s <strong class="source-inline">USER</strong> key will help us define this user and even switch them multiple times in the same Dockerfile. Switching users will ensure that each Dockerfile line runs with the appropriate user, and containers created with this image will also run with the <span class="No-Break">right user.</span></p>
<p>It is mandatory to avoid using containers with privileged users. This will essentially protect your applications and the underlying infrastructure. If you need to use <strong class="source-inline">root</strong> or any other privileged users, you should declare this situation explicitly. You can use a label, for example, to indicate that your image requires a privileged account <span class="No-Break">to run.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">If you are developing an application that requires a root user for its execution, you can use user namespace mappings. This feature lets us map a container’s root user with a normal user in our host. If you need to set up this feature, you can follow the instructions <a id="_idIndexMarker252"/>provided<a id="_idIndexMarker253"/> <span class="No-Break">at </span><a href="https://docs.docker.com/engine/security/userns-remap/"><span class="No-Break">https://docs.docker.com/engine/security/userns-remap/</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor057"/>ENTRYPOINT</h2>
<p>Now, let’s<a id="_idIndexMarker254"/> introduce how to declare which processes <a id="_idIndexMarker255"/>will run inside our container. The following keys add the meta-information required in an image to define which binary or script <span class="No-Break">will run.</span></p>
<p>We will use the <strong class="source-inline">ENTRYPOINT</strong> key to define the main process the container will run. If this key isn’t defined, the <strong class="source-inline">/bin/sh</strong> shell will be used for Linux containers and <strong class="source-inline">cmd.exe</strong> for Microsoft Windows containers. This key can come already modified in our base images, with a custom value, but we can also override it in our Dockerfile declaration to modify our <span class="No-Break">container’s behavior.</span></p>
<p>You can also use the <strong class="source-inline">CMD</strong> key, which allows you to specify which arguments should be passed to the shell, Windows command, or any other defined <strong class="source-inline">ENTRYPOINT</strong>. As such, we can think of the main process execution as the concatenation or sum of the <strong class="source-inline">ENTRYPOINT</strong> and <strong class="source-inline">CMD</strong> keys. For example, if we use the default <strong class="source-inline">/bin/sh</strong> shell’s <strong class="source-inline">ENTRYPOINT</strong>, and we define our <strong class="source-inline">CMD</strong> key as <strong class="source-inline">ping 8.8.8.8</strong>, the final command that executes inside our container will be <strong class="source-inline">/bin/sh -c ping 8.8.8.8</strong>; in other words, a shell is expanded to execute our <strong class="source-inline">ping</strong> command. We can modify any of them during container creation, but remember that the user defined with the <strong class="source-inline">USER</strong> key will be the <span class="No-Break">process’s owner.</span></p>
<p>As mentioned previously, we can change image behavior by changing these very important keys. <strong class="source-inline">ENTRYPOINT</strong> and <strong class="source-inline">CMD</strong> are managed by the container runtime as arrays, although we can define them in our Dockerfile as strings, which are also commonly used to manually execute a container. The container runtime concatenates both arrays to build the final command line. Due to this behavior, setting <strong class="source-inline">ENTRYPOINT</strong> as a string will force <strong class="source-inline">CMD</strong> to be ignored, but we can use <strong class="source-inline">CMD</strong> as a string while <strong class="source-inline">ENTRYPOINT</strong> is an array, and <strong class="source-inline">CMD</strong> will be treated as an array of <span class="No-Break">0 size.</span></p>
<p>Both values can be overridden on container execution, but usually, we will just customize the container arguments by using <strong class="source-inline">CMD</strong>; as such, this key can be used in the Dockerfile as a default value. As a developer, you should always provide as much information about your application’s behavior as possible to make it usable, and <strong class="source-inline">LABEL</strong>, <strong class="source-inline">USER</strong>, and <strong class="source-inline">CMD</strong> must be <a id="_idIndexMarker256"/>present in <a id="_idIndexMarker257"/><span class="No-Break">your Dockerfiles.</span></p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor058"/>EXPOSE</h2>
<p>We should<a id="_idIndexMarker258"/> also add the <strong class="source-inline">EXPOSE</strong> key to this list, which defines <a id="_idIndexMarker259"/>what ports will be used by your application. You can define as many ports as required using ranges and the transport protocol that will be used, be it TCP or UDP. With this information, you will ensure that anyone using your application will know which ports your processes will be <span class="No-Break">listening to.</span></p>
<p>The following scheme shows a simple Dockerfile stack in practice, including the container layer <span class="No-Break">on top:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<img alt="Figure 2.10 – The schema of container image layers created by using a Dockerfile. The container layer is on top to keep track of changes created by processes" height="647" src="image/B19845_02_10.jpg" width="1190"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – The schema of container image layers created by using a Dockerfile. The container layer is on top to keep track of changes created by processes</p>
<p>This figure represents the order obtained by using the <strong class="source-inline">docker image history</strong> command. For this example, we performed the <span class="No-Break">following steps:</span></p>
<ol>
<li>We used a simple <strong class="source-inline">alpine:3.5</strong> base image. We updated the package sources and installed <strong class="source-inline">nginx</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">.</span></li>
<li>Next, we prepared NGINX logs to stream their output to <strong class="source-inline">/dev/stdout</strong> and <strong class="source-inline">/dev/stderr</strong>. This will ensure that we can read the application logs through the container runtime because these descriptors will be used by the container’s <span class="No-Break">main process.</span></li>
<li>We copied our custom NGINX configuration file, overwriting the <span class="No-Break">default one.</span></li>
<li>We exposed port <strong class="source-inline">80</strong>, indicating that our main process will listen on <span class="No-Break">this port.</span></li>
<li>Finally, we <a id="_idIndexMarker260"/>defined the default command line. In this <a id="_idIndexMarker261"/>case, <strong class="source-inline">/bin/sh -c "nginx –g daemon off;"</strong> will be executed every time we run a container using <span class="No-Break">this image.</span></li>
</ol>
<h2 id="_idParaDest-53"><a id="_idTextAnchor059"/>HEALTHCHECK</h2>
<p>To ensure that <a id="_idIndexMarker262"/>our main process runs correctly within<a id="_idIndexMarker263"/> our container, we should add a health probe that will indicate whether this process is healthy or not. Let’s imagine we run a web server application and it gets stuck. Processes will continue running but functionality will be completely lost. To remedy this, we can use the <strong class="source-inline">HEALTHCHECK</strong> key to define a command line that will check our main application’s health. We can use a script or binary with arguments, such as <strong class="source-inline">curl</strong> for web servers, or a database client if we run a database server. What is very important for health checks is that the command exits correctly (<strong class="source-inline">exit 0</strong>) if the application is healthy. If our check process exits with any other signal, the container will die as a result of the application being set as unhealthy. The <strong class="source-inline">HEALTHCHECK</strong> key will allow us to manage how the checks must be executed, to keep the application up and running. We can modify the number of checks that will mark the main process as unhealthy and the interval for these checks. When the defined number of tries is reached with a negative response (any exit different than 0), the container runtime is informed that even if the main process seems to be running correctly, the service is not working, and the container should die. This usually means a new healthy one is created, but for this process to work, we should configure that container with the <strong class="source-inline">restart: always</strong> option. We will deep dive into<a id="_idIndexMarker264"/> container <a id="_idIndexMarker265"/>execution in <a href="B19845_03.xhtml#_idTextAnchor082"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Running </em><span class="No-Break"><em class="italic">Docker Containers</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor060"/>VOLUME</h2>
<p>To end this section, we<a id="_idIndexMarker266"/> will review the <strong class="source-inline">VOLUME</strong> key. As the <a id="_idIndexMarker267"/>container image build process is based on the execution of multiple containers and storing their layers, this key is used to avoid certain directories from a container’s life cycle. It is good practice to include this key to indicate which folders in your image you prepared for persistent storage. You can use this key after all the <strong class="source-inline">RUN</strong> keys to avoid losing an application’s folders during the <span class="No-Break">build process.</span></p>
<p>We have provided clear and simple examples of these keys to help you understand their usage at the end of this chapter, in the <span class="No-Break"><em class="italic">Labs</em></span><span class="No-Break"> section.</span></p>
<p>In the next section, we will present you with some of the most important command-line options that are commonly used to build <span class="No-Break">container images.</span></p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor061"/>The command line for creating images</h1>
<p>In this section, we will<a id="_idIndexMarker268"/> take a closer look at Docker and other tools that you will commonly use to create container images for <span class="No-Break">your projects.</span></p>
<p>We will start by reviewing the <strong class="source-inline">docker</strong> command line, which is the most popular tool for developers and users due to its simplicity and <span class="No-Break">friendly environment.</span></p>
<p>Docker uses a common schema for all its arguments and options. We will use <strong class="source-inline">docker &lt;OBJECT&gt; &lt;ACTION&gt; &lt;OPTIONS&gt;</strong>. As a Docker container runtime identifies its objects by their IDs, it is common to omit the <strong class="source-inline">&lt;OBJECT&gt;</strong> primitive, but you should make sure that you use the right object. It is improbable that you will commit an error, but it is good practice to remember to include the object as part of <span class="No-Break">the command.</span></p>
<p>Let’s start with the <a id="_idIndexMarker269"/>basics – that is, learning which command will create <span class="No-Break">an image.</span></p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor062"/>Actions for creating images</h2>
<p>We use the <strong class="source-inline">build</strong> action<a id="_idIndexMarker270"/> to create images using a Dockerfile. By default, it will search for a file in your current directory, but we can use any name and path to store our build manifests. We must always declare the build context, and usually, we will use the <strong class="source-inline">–tag</strong> option to define a name and tag for our image. Here is an example of its <span class="No-Break">common usage:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<img alt="Figure 2.11 – Executing a simple image build process" height="396" src="image/B19845_02_11.jpg" width="1314"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Executing a simple image build process</p>
<p>In this example, <strong class="source-inline">context2</strong> is the name of the folder that contains all the files that should be sent to the container runtime, some of which should be copied to the <span class="No-Break">final image.</span></p>
<p>Here are the most common options that you will probably add to <strong class="source-inline">docker </strong><span class="No-Break"><strong class="source-inline">image build</strong></span><span class="No-Break">:</span></p>
<ul>
<li><strong class="source-inline">--build-arg</strong> is the way<a id="_idIndexMarker271"/> we can provide arguments for the build process. It is commonly used with the <strong class="source-inline">ARG</strong> Dockerfile key to modify image creation – for example, we can use <strong class="source-inline">build</strong> arguments to add some <strong class="bold">c</strong><strong class="bold">ertificate </strong><strong class="bold">a</strong><strong class="bold">uthority</strong> (<strong class="bold">CA</strong>) certificates to <span class="No-Break">the commands.</span></li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">When you are behind a proxy server, it is very common to pass the well-known Linux<strong class="source-inline"> HTTPS_PROXY</strong>, <strong class="source-inline">HTTP_PROXY</strong>, and <strong class="source-inline">NO_PROXY</strong> variables as arguments <span class="No-Break">using -</span><span class="No-Break"><strong class="source-inline">–build-arg</strong></span><span class="No-Break">:</span></p>
<p class="callout"><strong class="source-inline">docker build --build-arg </strong><span class="No-Break"><strong class="source-inline">HTTP_PROXY=$http_proxy \</strong></span></p>
<p class="callout"><strong class="source-inline">--build-arg HTTPS_PROXY=$http_proxy --build-arg </strong><span class="No-Break"><strong class="source-inline">NO_PROXY="$no_proxy" \</strong></span></p>
<p class="callout"><strong class="source-inline">--build-arg http_proxy=$http_proxy --build-arg </strong><span class="No-Break"><strong class="source-inline">https_proxy=$http_proxy \</strong></span></p>
<p class="callout"><strong class="source-inline">--build-arg no_proxy="$no_proxy" -t </strong><span class="No-Break"><strong class="source-inline">myimage:tag mycontext</strong></span></p>
<ul>
<li><strong class="source-inline">--force-rm</strong> will clean<a id="_idIndexMarker272"/> all intermediate<a id="_idIndexMarker273"/> containers. By default, all containers created during the building process will remain in your host unless your process ends successfully, hence occupying disk space. It is good practice to clean intermediate containers if you know that your build will create big layers – for example, when your application is compiled in containers and many dependencies are created, after which the <span class="No-Break">process breaks.</span></li>
<li><strong class="source-inline">--label</strong> will let you add further labels to your container image. Adding all the required information, such as special library versions, the author, a short description, and anything that will let other developers understand your content, will be <span class="No-Break">greatly appreciated.</span></li>
<li><strong class="source-inline">--no-cache</strong> will let us decide whether previously created and locally stored layers will be used. Using this argument, your build process will create fresh new layers, even if they already exist in your host. Be aware that without caching, all processes will be executed and store the intermediate container data locally; hence, the build will take more time. You will gain a faster build process by reusing the layers already included in your underlying host as much as possible. This can be very important when you are compiling your applications inside your image build, where a few minor changes will restart processes completely if no caching <span class="No-Break">is used.</span></li>
<li><strong class="source-inline">--target</strong> is used to identify a build definition inside a Dockerfile. This can represent a specific compilation or a stage in a multi-stage build. We can use targets, for example, to maintain a unique Dockerfile with different build definitions, such as <strong class="source-inline">small</strong>, <strong class="source-inline">complete</strong>, and <strong class="source-inline">debug</strong>, each one requiring different steps and base images. We can trigger the build process for one specific definition to build the smallest release for a production environment. This can also be managed with arguments, with different base images chosen depending <span class="No-Break">on variables.</span></li>
<li><strong class="source-inline">--cpuquota</strong>, <strong class="source-inline">--cpu-shares</strong>, and <strong class="source-inline">--memory</strong> will help us manage the resources available <a id="_idIndexMarker274"/>per build process. This is especially interesting if you are running out of<a id="_idIndexMarker275"/> resources on your <span class="No-Break">desktop computer.</span></li>
</ul>
<p>Now that we have learned about the command line to build images, let’s look at <span class="No-Break">managing images.</span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor063"/>Managing container images</h2>
<p>Container images <a id="_idIndexMarker276"/>will reside in your host in <a id="_idIndexMarker277"/>different directories, decoupling the data files from the meta-information. The location of your files will depend on the container runtime you are using, or in the <a id="_idIndexMarker278"/>case of <strong class="bold">Podman</strong>, they will probably be in your home directory. This runtime runs in rootless mode and without any daemon, so it is ideal for user containers. Irrespective of this, you will never directly access container <span class="No-Break">image files.</span></p>
<p>One of the mos<a id="_idTextAnchor064"/>t commonly used actions within Docker (and any other container runtime client) is <strong class="source-inline">list</strong> (or <strong class="source-inline">ls</strong>), which is used to list the objects available in our host (or remote runtime). By default, images can be represented by their names (or repositories – we will learn how to store and manage images in these repositories in <a href="B19845_03.xhtml#_idTextAnchor082"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">, Shipping Docker Images</em>), IDs, tags, creation time, and size. In this context, size is the amount of space the image occupies in our host. The smaller the images, the better, and that’s why you, as a developer, should be aware of the content of your images. Include only strictly necessary files, and think about your layer strategy if you are working with projects in which you<a id="_idIndexMarker279"/> share dependencies. Use<a id="_idIndexMarker280"/> the <strong class="source-inline">.dockerignore</strong> file to avoid non-required files as this can help you save a lot <span class="No-Break">of space:</span></p>
<pre class="console">
$ docker image list
REPOSITORY     TAG           IMAGE ID       CREATED        SIZE
example1       0.0           f7bba7eac35e   22 hours ago   9.51MB
postfix        test          a8768bd1ec8f   2 days ago     169MB
four           latest        3de85feddb15   2 days ago     105MB
three          latest        55f07527310e   2 days ago     105MB
frjaraur/two   180223        8e64333d6033   2 days ago     105MB
frjaraur/one   180223        331ed31f8816   2 days ago     105MB
one            latest        331ed31f8816   2 days ago     105MB
&lt;none&gt;         &lt;none&gt;        7ed6e7202eca   About a minute ago 72.8MB
alpine         latest        b2aa39c304c2   10 days ago    7.05MB
debian         stable-slim   4ea5047878b3   12 days ago    80.5MB</pre> <p>The preceding code snippet shows that we have multiple names (repositories) with the same content; we know this because they have the same ID. Images with the same ID are equal; they just differ in their tags. Therefore, we can add more than one tag to an image. We will use <strong class="source-inline">docker tag &lt;ORIGINAL&gt; &lt;NEWTAG&gt;</strong> to tag images. This is necessary to be able to upload images to registries as they are stored in their own repositories. Tags will help you identify images in our registry, but although tags are unique in each repository, we can have a lot to refer to the same image, and you should ensure that you are using the <span class="No-Break">right image.</span></p>
<p>Developers may <a id="_idIndexMarker281"/>choose to tag their images following<a id="_idIndexMarker282"/> the application’s life cycle, and you will probably encounter many images tagged using the <strong class="source-inline">release.minor.fixes</strong> model. This is good practice, and adding some key labels to identify the author, the project, and so on will improve <span class="No-Break">your work.</span></p>
<p>You probably also noticed an image without any tag or name. This is a <em class="italic">dangling</em> container image that has been unused by others, and it is untagged because another one was created using the same repository and tag. It is not referenced by any image and now just occupies space. These dangling images should be removed, and we can use <strong class="source-inline">docker image prune</strong> to delete all <span class="No-Break">of them.</span></p>
<p>To delete individual images, we can use <strong class="source-inline">docker image rm &lt;IMAGE&gt;</strong>. It is important to understand that images cannot be removed if there are references to them in containers or other images. We can force the removal by using <strong class="source-inline">–force</strong>, but it will only work if containers are stopped (or dead). It is also worth noting that multiple image tags can be deleted by using their ID, instead of their image <span class="No-Break">repository names.</span></p>
<p>To review all the information included in the container image object, we can use <strong class="source-inline">docker image inspect &lt;IMAGE&gt;</strong>. Very useful information will be presented, including the image digest (if the image has a reference from a registry), the architecture for which the image was built, its labels, its layers, and the configuration that will be used to start the containers, such as environment variables and the main command to <span class="No-Break">be executed.</span></p>
<p>It is worth introducing some formatting and filtering options we can use with <span class="No-Break">some commands:</span></p>
<ul>
<li><strong class="source-inline">--filter</strong> will allow<a id="_idIndexMarker283"/> us to use defined labels to<a id="_idIndexMarker284"/> filter objects from a list. This will work for any list provided by the container runtime – for example, if we labeled our images with the <strong class="source-inline">environment</strong> key, we could use it to obtain only <span class="No-Break">specific images:</span><pre class="source-code">
<strong class="bold">$ docker image list --filter label=environment</strong>
<strong class="bold">REPOSITORY     TAG           IMAGE ID       CREATED        SIZE</strong>
<strong class="bold">frjaraur/two   180223        8e64333d6033   2 days ago     105MB</strong>
<strong class="bold">frjaraur/one   180223        331ed31f8816   2 days ago     105MB</strong>
<strong class="bold">$ docker image list --filter label=environment=production</strong>
<strong class="bold">REPOSITORY     TAG           IMAGE ID       CREATED        SIZE</strong>
<strong class="bold">frjaraur/one   180223        331ed31f8816   2 days ago     105MB</strong></pre></li> <li><strong class="source-inline">--format</strong> works with <em class="italic">Go templates</em> to manipulate output for listing (and logs from containers). The container runtime and clients work with <em class="italic">JSON</em> streams; hence, using these templates will help us interpret objects’ data. For example, we can use <strong class="source-inline">table</strong> to obtain a table<a id="_idTextAnchor065"/>-like output,<a id="_idTextAnchor066"/> wi<a id="_idTextAnchor067"/>th the key<a id="_idTextAnchor068"/>s we need <span class="No-Break">to review:</span><pre class="source-code">
<strong class="bold">$ docker image list \</strong>
<strong class="bold">  --format "table {{.Repository}}:{{.Tag}}\t{{.Size}}"</strong>
<strong class="bold">REPOSITORY:TAG        SIZE</strong>
<strong class="bold">example1:0.0          9.51MB</strong>
<strong class="bold">postfix:test          169MB</strong>
<strong class="bold">frjaraur/two:180223   105MB</strong>
<strong class="bold">two:latest            105MB</strong>
<strong class="bold">one:latest            105MB</strong>
<strong class="bold">frjaraur/one:180223   105MB</strong>
<strong class="bold">alpine:latest         7.05MB</strong>
<strong class="bold">debian:stable-slim    80.5MB</strong></pre><p class="list-inset">We can get all the available keys and values by using <strong class="source-inline">docker image ls --format "{{</strong><span class="No-Break"><strong class="source-inline">json .}}"</strong></span><span class="No-Break">.</span></p><p class="list-inset">To obtain <a id="_idTextAnchor069"/><a id="_idIndexMarker285"/>all the labels from a specific <a id="_idIndexMarker286"/>image, we can use <strong class="source-inline">docker image inspect &lt;IMAGE&gt; --format "{{ index .</strong><span class="No-Break"><strong class="source-inline">Config.Labels }}"</strong></span><span class="No-Break">.</span></p></li> </ul>
<p>In the next section, we will learn about the options available at the command line to share images between hosts <span class="No-Break">or users.</span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor070"/>Actions for sharing images</h2>
<p>You may be <a id="_idIndexMarker287"/>thinking, all these examples were built on a host, so we need to be able to share our images with other developers, or even move them to the servers that are prepared to manage the application’s life cycle (such as testing, staging, certification, or production). We can dump our container images and import them to new locations, but using image registries is a better option because these stores will be shared with the containers’ orchestrators, and the container runtimes will automate the pull process <span class="No-Break">for us.</span></p>
<p><strong class="bold">Container image registries</strong> are considered <a id="_idIndexMarker288"/>the best way to share images. We will use <strong class="source-inline">docker image pull</strong> and <strong class="source-inline">docker image push</strong> to pull and push images, respectively. For this to work, you’re usually required to log in to your registry. To be able to access your registry, you will require a username and a password. Docker Hub (<strong class="source-inline">docker.io</strong>) is probably the most recognized container registry. It works as a cloud service, providing an image store, scanning, and automations to build images. There are other options; all cloud providers offer registry services, and many code repositories also provide an image store (as they are considered code artifacts). We can deploy some of these solutions on-premises, but we can find also solutions such as Harbor, from VMware, which was <a id="_idIndexMarker289"/>prepared specifically for data centers. You may notice that your container runtime also stores images, and in fact, it can be considered a registry – a local registry. The <strong class="source-inline">podman</strong> command line, which supports all actions described in this chapter and can be used instead of the Docker client, will build your images as <strong class="source-inline">localhost/IMAGE_NAME:TAG</strong>, where <strong class="source-inline">IMAGE_NAME</strong> is the name of the repository. We will learn how image registries work in <a href="B19845_03.xhtml#_idTextAnchor082"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Shipping Docker Images</em>; for now, we will just review the most commonly used options to <span class="No-Break">share images.</span></p>
<p>When someone asks us for an image, we can use <strong class="source-inline">docker image save</strong> to dump a container image to a file. This will completely package all its layers and meta-information. By default, standard output will be used to stream all data, but we can use the <strong class="source-inline">–output</strong> option to specify a file. You can copy this file to another workstation or server and execute <strong class="source-inline">docker image load</strong> to import all image layers and metadata. By default, the command will use standard input, but we can add the <strong class="source-inline">–input</strong> option to specify a <span class="No-Break">file instead:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<img alt="Figure 2.12 – Saving images to files for sharing is easy" height="370" src="image/B19845_02_12.jpg" width="1351"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Saving images to files for sharing is easy</p>
<p>We can verify that the image size is retained, and if we list the files included in the package file, we will obtain the layers and <span class="No-Break">metadata files.</span></p>
<p>The Docker client can be used with <strong class="source-inline">docker image load</strong> to integrate this image into our local registry, but we can also use <strong class="source-inline">docker image import</strong> to only upload image layers. This is interesting as it can be used as the base image for builds from scratch, but be aware that without the metadata manifest JSON file, you would not be able to execute a container. You will need to add its exposed ports, user, main process, arguments, and <span class="No-Break">so on.</span></p>
<p>As you can imagine, <strong class="source-inline">docker image save</strong> and <strong class="source-inline">docker image load</strong> work in small environments, but they don’t when you need to distribute files on a dozen servers. Images are hard to sync if you don’t maintain good tag maintenance; hence, try to use representative <a id="_idIndexMarker290"/>tags and label your images to help others understand <span class="No-Break">their content.</span></p>
<p>Before reviewing some best practices and recommendations, we will learn about some topics that will help us optimize our workflow so that we can build <span class="No-Break">new images.</span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor071"/>Advanced image creation techniques</h1>
<p>In this section, we will<a id="_idIndexMarker291"/> review some options and techniques available to speed up the building process and optimize <span class="No-Break">image sizes.</span></p>
<p>In <a href="B19845_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Modern Infrastructure and Applications with Docker</em>, we learned that images are a package of layers. These layers are distributed one over another, containing all the files, and the merging of all these layers gives us a distribution of files optimized for disk space reduction, using CoW filesystems. When a file from a lower layer has to be modified, it is copied to the top layer if it doesn’t exist there yet. All unmodified files are used in read-only mode. With that said, it is easy to understand that managing the CoW process correctly will help speed up image <span class="No-Break">creation times.</span></p>
<p>Whenever we add new <strong class="source-inline">RUN</strong> commands at the end of our Dockerfile, all previous layers will be used (unless we specify <strong class="source-inline">–-no-cache</strong>); hence, the container runtime just needs to create new layers according to these new changes. However, whenever we add a new line to copy a new file in the middle of the Dockerfile, or even when a file has been modified, the layers included after this change are invalidated. This occurs with <strong class="source-inline">COPY</strong>, <strong class="source-inline">ADD</strong>, and <strong class="source-inline">RUN</strong> because these Dockerfile keys add new layers, but <strong class="source-inline">WORKDIR</strong> and <strong class="source-inline">ENV</strong> can also modify the building process behavior and, hence, the subsequent layers. Once a layer changes, the container runtime has to rebuild all downstream layers, even if we didn’t modify any line in our Dockerfile after the <span class="No-Break">aforementioned change.</span></p>
<p>Here are some<a id="_idIndexMarker292"/> recommendations that may help your <span class="No-Break">building process:</span></p>
<ul>
<li>Multi-stage builds are key to minimizing and securing container images. We will define different targets in our Dockerfile to use them as stages to compile our code and dependencies, and we will add only the required files to the final image. With this technique, we can ensure that no compilers will be included in the final image. This is a <span class="No-Break">simple example:</span><pre class="source-code">
FROM alpine:3.17.2 as git # First stage, install git on small Alpine
RUN apk add git
FROM git as fetcher # Second stage, fetching repository
WORKDIR /src
RUN git clone https://gitlab.com/myrepo/mycode.git .
FROM nginx: 1.22.1-alpine as webserver
COPY --from=fetcher /src/html/ /usr/share/nginx/html</pre><p class="list-inset">This is a very simple Dockerfile; the final image contains only the docs directory, retrieved from our Git code repository. We will see a better example in this chapter’s <span class="No-Break"><em class="italic">Labs</em></span><span class="No-Break"> section.</span></p></li> <li>Ordering layers is key to speeding up and maintaining application changes. Try to find the best logical order to declare your Dockerfile’s recipe. If we have some time-intensive tasks, such as installing a lot of software packages, it is preferable to make these changes at the beginning of the build process. Conversely, the files that we change more often, probably our application’s code, should be close to the end of <span class="No-Break">the Dockerfile.</span></li>
<li>This also works with the <strong class="source-inline">COPY</strong> key; if your application has a lot of dependencies, copying all your code and requirements at once can be problematic. It is better to split your<a id="_idIndexMarker293"/> files into different <strong class="source-inline">COPY</strong> sentences and copy your module requirements declaration files, then update these dependencies, and after that, copy the code for building. This ensures that all our code changes will not cause the dependencies to be downloaded again in the <span class="No-Break">container-building process.</span></li>
<li>We have to remind you again that you should only keep the necessary files inside container images. Avoid any unnecessary files. This will increase the building time and the final image size, and sometimes, it may be relevant to decide where to store them. Also, using <strong class="source-inline">.dockerignore</strong> will help you avoid sending unnecessary files to the container <a id="_idTextAnchor072"/>runtime, even if they will not be kept in the final image. Avoid copying full directories using <strong class="source-inline">COPY . /src</strong> if you are unsure of the content, any previous artifact builds, whether you are going to re-build them during image creation, or the logs, <span class="No-Break">for example.</span></li>
<li>Avoid non-required dependencies when you install packages. Depending on your base operating system distribution, you will have different arguments or options to only install specific packages, avoiding, for example, the recommended, but not required, associated packages. You will probably need to update the packages list before installing; do this once at the beginning if you don’t add or modify any package repository. It is also recommended to clean a package cache when you are not going to install any other package. We can use <strong class="source-inline">RUN</strong> <strong class="source-inline">--mount type=cache,target=DIRECTORY_PATH &lt;INSTALL_EXPRESSION&gt;</strong> to install packages. This option will keep the content of the defined directory between different build processes, which will speed up installing <span class="No-Break">new software.</span></li>
<li>Sensitive information shouldn’t be included inside container images. It is possible to include some files with passwords, certificates, tokens, and so on in your Dockerfile <a id="_idIndexMarker294"/>using the <strong class="source-inline">COPY</strong> or <strong class="source-inline">ADD</strong> keys, or even as arguments for your <strong class="source-inline">docker build</strong> command, and remove them before finishing. Although these don’t look like bad solutions at first, they are not good enough because unconsciously, you can leave sensible data behind. A multi-stage build can help us if secrets are used to download binaries or libraries, and we can easily copy them to a final stage without adding any sensible data to its layers. However, there is a better solution – using <strong class="source-inline">buildx</strong>. This Docker tool includes the option to mount secrets only during specific <strong class="source-inline">RUN</strong> steps, without storing them in any layer, as if they were <a id="_idTextAnchor073"/>a file from a volume. Here is a simple example of <span class="No-Break">its usage:</span><pre class="source-code">
FROM python: 3.9.16-alpine3.17
COPY mycript.sh .
RUN --mount=type=secret,id=mysecret ./myscript.sh</pre><p class="list-inset">To pass a value to the <strong class="source-inline">mysecret</strong> key, we can use an environment variable – for example, we can execute the build process with the following <span class="No-Break">command line:</span></p><pre class="source-code"><strong class="bold">$ SECRETVALUE="mysecretpass" docker image buildx build --secret id= SECRETVALUE &lt;CONTEXT&gt;</strong></pre></li> </ul>
<p class="callout-heading">Important note</p>
<p class="callout"><strong class="source-inline">buildx</strong> even allows us to mount files with data, such as user credentials, tokens, certificates, and so on, for use as secrets inside containers running within the build process, by using <strong class="source-inline">docker image buildx build –secret id=mysecret,src=&lt;FULLPATH_TO_SECRETFILE&gt;</strong>. By default, these files will be included inside containers in <strong class="source-inline">/run/secrets/&lt;SECRETID&gt;</strong>, but we can add <strong class="source-inline">target</strong> to the Dockerfile’s <strong class="source-inline">mount</strong> definition with a full path to the destination file we want <span class="No-Break">to create.</span></p>
<ul>
<li>It is good practice to keep layers as small as possible. We will try to use <strong class="source-inline">RUN</strong>, <strong class="source-inline">COPY</strong>, and <strong class="source-inline">ADD</strong>, executing as many changes as possible, although this may impact layer <a id="_idIndexMarker295"/>reusability. We will combine multiple <strong class="source-inline">RUN</strong> executions into one line. Fewer Dockerfile lines mean smaller caching, which is good, but you can’t reuse layers too often for new images. Any small variation between your Dockerfiles will invalidate caching from one Dockerfile <span class="No-Break">to another.</span></li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">We can use the heredocs format to combine multiple lines. This improves Dockerfile readability. For example, we can write <span class="No-Break">the following:</span></p>
<p class="callout"><span class="No-Break"><strong class="source-inline">RUN &lt;&lt;EOF</strong></span></p>
<p class="callout"><span class="No-Break"><strong class="source-inline">set -e</strong></span></p>
<p class="callout"><strong class="source-inline">apt-get </strong><span class="No-Break"><strong class="source-inline">update -qq</strong></span></p>
<p class="callout"><strong class="source-inline">apt-get install </strong><span class="No-Break"><strong class="source-inline">mypackage1 mypackage2</strong></span></p>
<p class="callout"><span class="No-Break"><strong class="source-inline">EOF</strong></span></p>
<ul>
<li>Docker client installation also provides the unique features of <strong class="source-inline">buildx</strong> to help us reduce building times and size. We can configure garbage collections to remove unused layers, based on time, and enable remote caching locations. This feature improves CI/CD pipelines that use distributed caches for projects that must compile a lot of dependencies or low-level languages, such as <em class="italic">C</em> <span class="No-Break">or </span><span class="No-Break"><em class="italic">Rust</em></span><span class="No-Break">.</span></li>
<li>Multiple-processor architectures, such as <strong class="source-inline">riscv64</strong> or <strong class="source-inline">arm64</strong>, can be built by using <strong class="source-inline">docker buildx build –platform</strong>, with one unique Dockerfile. In the past, we usually had different Dockerfiles, one for each architecture. Machines to use these different processors were also required, and the building process was <a id="_idIndexMarker296"/>executed on each one. This new feature allows you to prepare images for different platforms on your laptop with Docker Desktop. We will prepare a container image for <strong class="source-inline">arm64</strong> in this chapter’s <span class="No-Break"><em class="italic">Labs</em></span><span class="No-Break"> section.</span></li>
<li>We can considerably reduce the final image size by using <strong class="source-inline">–squash</strong> when the image contains many layers. Squashing container images is an experimental feature that’s available in the Docker container runtime. This means that we need to <a id="_idIndexMarker297"/>enable <strong class="bold">experimental features</strong> in our Docker Engine’s <strong class="source-inline">docker.json</strong> file, and once configured, we will be able to use the <strong class="source-inline">docker image build –squash</strong> command. Reducing the number of layers to one will reduce its size, but you will lose the advantage of sharing layers. It’s important to mention here that you shouldn’t expect miracles. Squashing images depends on the number of layers used; hence, the final size may be pretty much the same as when fewer layers <span class="No-Break">are used.</span></li>
</ul>
<p>Before starting <a id="_idIndexMarker298"/>with the labs, we will review the content learned in this chapter by providing an overview of the best practices to build your <span class="No-Break">container images.</span></p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor074"/>Best practices for container image creation</h1>
<p>In this section, we<a id="_idIndexMarker299"/> are going to recommend a list of the best practices you can follow to create your applications, thus improving your applications’ security, reusability, and <span class="No-Break">building processes:</span></p>
<ul>
<li>Only include the files that are strictly necessary for your application. Don’t install packages, binaries, libraries, and any file your application doesn’t need, and keep image content as small as possible, exposing a minimal <span class="No-Break">attack surface.</span></li>
<li>Use the <strong class="source-inline">.dockerignore</strong> file to avoid passing unnecessary files from your build context to <span class="No-Break">container runtimes.</span></li>
<li>Prepare debugging versions of your images, including some binaries or tools that may help you resolve an issue, but never use these images <span class="No-Break">in production.</span></li>
<li>Prepare the logic of your Dockerfiles to accommodate your changes; hence, include your code close to the end of the file, and think about how many modules or dependencies may need to be changed to execute the updates in the <span class="No-Break">proper section.</span></li>
<li>Use layer caching whenever it is possible to speed up the build process and remember that using many layers will allow reusability but affect performance when files need <span class="No-Break">runtime changes.</span></li>
<li>Never use <strong class="source-inline">root</strong> in your applications unless it is strictly required. If you do, you should understand its risks and manage them. You can use the <strong class="source-inline">USER</strong> key multiple times to change the execution user during builds, but always finish your Dockerfile with a <span class="No-Break">non-root user.</span></li>
<li>Never include sensitive information, such as certificates, passwords, and tokens, in your final container images. This information should be provided at runtime. Use Docker’s <strong class="source-inline">buildx</strong> to include secrets only during the <span class="No-Break">build process.</span></li>
<li>Declare all your application requirements, such as your process user, the exposed ports, and the command line to be executed, in your Dockerfile. This will help other developers use <span class="No-Break">your applications.</span></li>
<li>Use labels to add information about your application’s life cycle, maintainer, special libraries that are required, and so on. This information will be great for other developers to help them understand how they can integrate their code into your images or evolve <span class="No-Break">your Dockerfiles.</span></li>
<li>Image size<a id="_idIndexMarker300"/> matters, especially if you are running your containerized applications in a distributed environment. Container runtimes must download images if a container must be created on a host. Depending on the number of changes you make to your images, this can be a challenge, and resilience in the face of application issues may be affected if your platform defines an <em class="italic">always-pull</em> policy. We have covered some techniques to reduce image size; use them, but remember that a layer’s reusability may <span class="No-Break">be affected.</span></li>
</ul>
<p>With this list, you can prepare your own container image creation workflow. Some of this advice can be tricky and requires some practice, but I can assure you that it is worth it, and you will deliver quality images for <span class="No-Break">your applications.</span></p>
<p>Now that we <a id="_idIndexMarker301"/>have seen the different methods to build images, the command line we will commonly use, and some advanced techniques and advice to create good and secure images, it’s time to put all this into practice with some labs in the <span class="No-Break">next section.</span></p>
<h1 id="_idParaDest-61"><a id="_idTextAnchor075"/>Labs</h1>
<p>The following labs will provide examples to help you put the concepts and procedures you’ve learned in this chapter into practice. We will use Docker Desktop or any other container runtime. We will use different tools such <a id="_idIndexMarker302"/>as <strong class="bold">Podman</strong> and <strong class="bold">nerdctl</strong> to show<a id="_idIndexMarker303"/> you some of the possibilities you have at hand, although some of the features that are required for specific labs may be only available with a specific tool (or one tool has a more friendly interface). In these cases, we will ask you to use a specific <span class="No-Break">command-line interface.</span></p>
<p>The first step for all labs would be to download the most updated version of this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Docker-for-Developers-Handbook.git">https://github.com/PacktPublishing/Docker-for-Developers-Handbook.git</a>. To do this, simply execute <strong class="source-inline">git clone https://github.com/PacktPublishing/Docker-for-Developers-Handbook.git</strong> to download all its content. If you have already downloaded it before, ensure you have the newest version by executing <strong class="source-inline">git pull</strong> inside <span class="No-Break">its directory.</span></p>
<p>We will start this section with a simple lab about using caching to speed up the building process. All commands presented in these labs will be executed inside the <span class="No-Break"><strong class="source-inline">Docker-for-Developers-Handbook/Chapter2</strong></span><span class="No-Break"> directory.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">To show you the different tools to work with containers, we will use <strong class="source-inline">nerdctl</strong> in these labs, but you can use <strong class="source-inline">podman</strong> or <strong class="source-inline">docker</strong> (standalone or within Docker Desktop). Each tool has features and particularities, but most of the work within containers will execute similarly. We will explicitly notify you if some command shown requires a specific tool. Follow the specific instructions in this book’s GitHub code repository to install each tool. We will use <strong class="bold">Rancher Desktop</strong>, which<a id="_idIndexMarker304"/> runs <strong class="source-inline">containerd</strong> as the container runtime and integrates the <strong class="source-inline">nerdctl</strong> command line inside WSL 2, but all labs can be executed with the Docker command line as well, with <strong class="source-inline">docker</strong> <span class="No-Break">replacing </span><span class="No-Break"><strong class="source-inline">nerdctl</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor076"/>Caching layers</h2>
<p>In this first lab, we<a id="_idIndexMarker305"/> will review the importance of caching to speed <a id="_idIndexMarker306"/>up the building process. We are going to use <strong class="source-inline">nerdctl</strong>, but <strong class="source-inline">docker</strong> or <strong class="source-inline">podman</strong> will work, as well as <strong class="source-inline">buildah</strong> (<a href="https://buildah.io">https://buildah.io</a>), which is another open source tool prepared specifically to enhance the <span class="No-Break">build process.</span></p>
<p>We will build a simple <em class="italic">Node.js</em> application that I prepared for quick demos a few years ago. Its only purpose is to show some information regarding the container in which it runs, the request headers, and its version. It will be interesting to better understand the load balancing processes within container orchestrators later on in this book, but we will focus on the build process <span class="No-Break">f<a id="_idTextAnchor077"/>or now:</span></p>
<ol>
<li>First, we will move inside the <strong class="source-inline">Chapter2/colors/nodejs</strong> folder and execute a simple build, using <strong class="source-inline">ch2lab1:first</strong> as the image name and tag. We will use the following Dockerfile in <span class="No-Break">this process:</span><pre class="source-code">
FROM docker.io/node:18.14.2-alpine3.16
ENV APPDIR /APP
WORKDIR ${APPDIR}
COPY package.json package.json
RUN apk add --no-cache --update curl \
&amp;&amp; rm -rf /var/cache/apk \
&amp;&amp; npm install
COPY app.js app.js
COPY index.xhtml index.xhtml
CMD ["node","app.js","3000"]
EXPOSE 3000</pre><p class="list-inset">Note that here, we have separated the content copy into three lines, although we could have used just one with all the content – for example, by using <strong class="source-inline">COPY . .</strong>.</p></li> </ol>
<p class="callout-heading">Important note</p>
<p class="callout">As you may have noticed, this Dockerfile does not include any <strong class="source-inline">USER</strong> directive, but its application runs without any privileges because it is very simple and doesn’t use any Linux capability or privileged port. Anyway, it is good practice to include the <strong class="source-inline">USER</strong> directive, and you can add it to your local repository. Everything described in the following steps <span class="No-Break">will work.</span></p>
<ol>
<li value="2">We will add <strong class="source-inline">time</strong> to <a id="_idIndexMarker307"/>the <strong class="source-inline">build</strong> command to measure <a id="_idIndexMarker308"/>the time the build <span class="No-Break">process takes:</span><pre class="source-code">
 <strong class="bold">$ time nerdctl build -t ch2lab1:one \</strong>
<strong class="bold">  --label nodejs=18.14.2 \</strong>
<strong class="bold">  --label=base=alpine3.16 \</strong>
<strong class="bold">  nodejs  --progress plain</strong>
<strong class="bold">#1 [internal] load .dockerignore</strong>
<strong class="bold">#1 transferring context: 2B done</strong>
<strong class="bold">#1 DONE 0.0s</strong>
<strong class="bold">#2 [internal] load build definition from Dockerfile</strong>
<strong class="bold">#2 transferring dockerfile: 311B done</strong>
<strong class="bold">#2 DONE 0.0s</strong>
<strong class="bold">#3 [internal] load metadata for docker.io/library/node:18.14.2-alpine3.16</strong>
<strong class="bold">#3 DONE 1.1s</strong>
<strong class="bold">#4 [internal] load build context</strong>
<strong class="bold">#4 transferring context: 90B done</strong>
<strong class="bold">#4 DONE 0.0s</strong></pre><p class="list-inset">After these lines, our <a id="_idIndexMarker309"/>Dockerfile starts to be processed<a id="_idIndexMarker310"/> by the <span class="No-Break">container runtime:</span></p><pre class="source-code"><strong class="bold">#5 [1/6] FROM docker.io/library/node:18.14.2-alpine3.16@sha256:84b677af19caffafe781722d4bf42142ad765ac4233960e18bc526ce036306fe</strong>
<strong class="bold">#5 resolve docker.io/library/node:18.14.2-alpine3.16@sha256:84b677af19caffafe781722d4bf42142ad765ac4233960e18bc526ce036306fe 0.0s done</strong>
<strong class="bold">#5 DONE 0.1s</strong>
<strong class="bold">#5 [1/6] FROM docker.<a id="_idTextAnchor078"/>io/library/node:18.14.2-alpine3.16@sha256:84b677af19caffafe781722d4bf42142ad765ac4233960e18bc526ce036306fe</strong>
<strong class="bold">#5 sha256:aef46d6998490e32dcd27364100923d0c33b16165d2ee39c307b6d5b74e7a184 0B / 2.35MB 0.2s</strong></pre><p class="list-inset">Once the required layers have been loaded, our tasks to execute commands start. In our example, many packages must <span class="No-Break">be installed:</span></p><pre class="source-code"><strong class="bold">#8 [4/6] RUN apk add --no-cache --update curl &amp;&amp; rm -rf /var/cache/apk &amp;&amp; npm install</strong>
<strong class="bold">#0 0.115 fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/x86_64/APKINDEX.tar.gz</strong>
<strong class="bold">#8 0.273 fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/x86_64/APKINDEX.tar.gz</strong>
<strong class="bold">#8 0.503 (1/5) Installing ca-certificates (20220614-r0)</strong>
<strong class="bold">...</strong>
<strong class="bold">#8 0.601 (5/5) Installing curl (7.83.1-r6)</strong>
<strong class="bold">#8 0.618 Executing busybox-1.35.0-r17.trigger</strong>
<strong class="bold">#8 0.620 Executing ca-certificates-20220614-r0.trigger</strong>
<strong class="bold">#8 0.637 OK: 10 MiB in 21 packages</strong>
<strong class="bold">#8 3.247</strong>
<strong class="bold">#8 3.247 added 3 packages, and audited 4 packages in 2s</strong>
<strong class="bold">#8 3.247</strong>
<strong class="bold">#8 3.247 found 0 vulnerabilities</strong>
<strong class="bold">#8 3.248 npm notice</strong>
<strong class="bold">#8 3.248 npm notice New patch version of npm available! 9.5.0 -&gt; 9.5.1</strong>
<strong class="bold">#8 3.248 npm notice Changelog: &lt;https://github.com/npm/cli/releases/tag/v9.5.1&gt;</strong>
<strong class="bold">#8 3.248 npm notice Run `npm install -g npm@9.5.1` to update!</strong>
<strong class="bold">#8 3.248 npm notice</strong>
<strong class="bold">#8 DONE 3.3s</strong>
<strong class="bold">Once all execution lines are concluded, a tar file is created with the layer where changes were made:</strong>
<strong class="bold">#11 sending tarball</strong>
<strong class="bold">#11 sending tarball 0.6s done</strong>
<strong class="bold">#11 DONE 0.8s</strong>
<strong class="bold">unpacking docker.io/library/ch2lab1:one (sha256:7f63598f21445e5c6a051c9eca9c89367152dd59a4f1af366dc3291ae3e01930)…</strong></pre><p class="list-inset">Finally, our image is created, as we can observe in the <span class="No-Break">latest line:</span></p><pre class="source-code"><strong class="bold">Loaded image: docker.io/library/ch2lab1:one</strong></pre><p class="list-inset">Note that we also obtained the time spent for the process because we included the <strong class="source-inline">time</strong> command before <span class="No-Break"><strong class="source-inline">nerdctl build</strong></span><span class="No-Break">:</span></p><pre class="source-code"><strong class="bold">real    0m12.588s</strong>
<strong class="bold">user    0m0.009s</strong>
<strong class="bold">sys     0m0.000s</strong></pre><p class="list-inset">You can <a id="_idIndexMarker311"/>review the full output in the GitHub code<a id="_idIndexMarker312"/> repository for this book, but what’s important here is the time it took to build this image: 12.588 seconds. This isn’t bad, but as I recall, this project has few dependencies and Node.js is a <em class="italic">just-in-time</em> code language. Imagine this process if we needed to download code dependencies and compile them to obtain some binaries. It could take minutes or <span class="No-Break">even longer.</span></p></li> <li>Let’s execute a new build after making some small changes to our code. We will just modify the version variable, which is <em class="italic">line 30</em> in the <strong class="source-inline">nodejs/app.js</strong> file. Change <strong class="source-inline">var APP_VERSION="1.0";</strong> to any other value, such as <span class="No-Break">the following:</span><pre class="source-code">
var APP_VERSION="1.1";.</pre><p class="list-inset">Execute the first step again with a new tag, and note the <strong class="source-inline">CACHED</strong> lines in <span class="No-Break">the output:</span></p><pre class="source-code"><strong class="bold">$ time nerdctl build -t ch2lab1:two \</strong>
<strong class="bold">--label nodejs=18.14.2 \</strong>
<strong class="bold">--label=base=alpine3.16  nodejs  \</strong>
<strong class="bold">--progress plain</strong>
<strong class="bold">#1 [internal] load .dockerignore</strong>
<strong class="bold">#1 transferring context: 2B done</strong>
<strong class="bold">#1 DONE 0.0s</strong></pre><p class="list-inset">Lines containing <strong class="source-inline">CACHED</strong> indicate that the layers were already created; we use these instead of executing the actual line to create <span class="No-Break">a layer:</span></p><pre class="source-code"><strong class="bold">#7 [3/6] COPY package.json package.json</strong>
<strong class="bold">#7 CACHED</strong></pre><p class="list-inset">We changed the <a id="_idIndexMarker313"/>content of the <strong class="source-inline">app.js</strong> file; hence, a <a id="_idIndexMarker314"/>new layer must <span class="No-Break">be created:</span></p><pre class="source-code"><strong class="bold">#9 [5/6] COPY app.js app.js</strong>
<strong class="bold">#9 DONE 0.0s</strong>
<strong class="bold">#10 [6/6] COPY index.xhtml index.xhtml</strong>
<strong class="bold">#10 DONE 0.0s</strong></pre><p class="list-inset">All successive lines will also create new layers because we <em class="italic">broke the cache</em>. A new line of changes <span class="No-Break">was created:</span></p><pre class="source-code"><strong class="bold">#11 sending tarball</strong>
<strong class="bold">#11 sending tarball 0.6s done</strong>
<strong class="bold">#11 DONE 0.7s</strong>
<strong class="bold">unpacking docker.io/library/ch2lab1:two (sha256:bfffba0cd2d7cc82f686195b0b996731d0d5a49e4f689a3d39c7b0e6c57dcf0e)…</strong></pre><p class="list-inset">Finally, we obtained our <span class="No-Break">new image:</span></p><pre class="source-code"><strong class="bold">Loaded image: docker.io/library/ch2lab1:two</strong>
<strong class="bold">real    0m1.272s</strong>
<strong class="bold">user    0m0.007s</strong>
<strong class="bold">sys     0m0.000s</strong></pre><p class="list-inset">All steps before copying our <strong class="source-inline">app.js</strong> file (<em class="italic">Step 9</em> in the preceding snippet) used the cached layers. Starting from <em class="italic">Step 9</em>, everything has to be recreated. However, because we used the appropriate logic in our Dockerfile, everything worked as expected. If you copy all the content of your code folder at once, any change will trigger a new layer; hence, if we change the content of <strong class="source-inline">index.xhtml</strong> or our<a id="_idIndexMarker315"/> simple code in <strong class="source-inline">app.js</strong>, all the <a id="_idIndexMarker316"/>packages will be <span class="No-Break">downloaded again.</span></p></li> <li>Let’s repeat this process by changing the copy process in <span class="No-Break">our Dockerfile:</span><pre class="source-code">
FROM docker.io/node:18.14.2-alpine3.16
ENV APPDIR /APP
WORKDIR ${APPDIR}
COPY . .
RUN apk add --no-cache --update curl \
&amp;&amp; rm -rf /var/cache/apk \
&amp;&amp; npm install
CMD ["node","app.js","3000"]
EXPOSE 3000</pre><p class="list-inset">We execute the build process again. We expect it to last less than 12 seconds because the base image is already in <span class="No-Break">our host:</span></p><pre class="source-code"><strong class="bold">$ time nerdctl build -t ch2lab1:three \</strong>
<strong class="bold">--label nodejs=18.14.2 \</strong>
<strong class="bold">--label=base=alpine3.16  nodejs  \</strong>
<strong class="bold">--progress plain</strong>
<strong class="bold">…</strong>
<strong class="bold">…</strong>
<strong class="bold">#6 [2/4] WORKDIR /APP</strong>
<strong class="bold">#6 CACHED</strong>
<strong class="bold">…</strong>
<strong class="bold">…</strong></pre><p class="list-inset">The same thing happens here. Changes were made in the Dockerfile; hence, a new build process <a id="_idIndexMarker317"/>changed the layers from this <strong class="source-inline">COPY</strong> step, and<a id="_idIndexMarker318"/> no cache can <span class="No-Break">be used:</span></p><pre class="source-code"><strong class="bold">#7 [3/4] COPY . .</strong>
<strong class="bold">#7 DONE 0.0s</strong>
<strong class="bold">#8 [4/4] RUN apk add --no-cache --update curl &amp;&amp; rm -rf /var/cache/apk &amp;&amp; npm install</strong>
<strong class="bold">...</strong>
<strong class="bold">...</strong>
<strong class="bold">#8 DONE 2.8s</strong>
<strong class="bold">...</strong>
<strong class="bold">...</strong>
<strong class="bold">#9 sending tarball 0.6s done</strong>
<strong class="bold">#9 DONE 0.8s</strong>
<strong class="bold">unpacking docker.io/library/ch2lab1:three (sha256:b38074f0ee5a9e6c4ee7f68e90d8a25575dc7df9560b0b66906b29f3feb8741c)...</strong>
<strong class="bold">Loaded image: docker.io/library/ch2lab1:three</strong>
<strong class="bold">real    0m4.634s</strong>
<strong class="bold">user    0m0.004s</strong>
<strong class="bold">sys     0m0.003s</strong></pre><p class="list-inset">It took 4.634 seconds, which is not bad, but remember that this is <span class="No-Break">an example.</span></p></li> <li>Once again, change <strong class="source-inline">APP_VERSION</strong> to a new value variable to see what happens if we <a id="_idIndexMarker319"/>build<a id="_idIndexMarker320"/> again. Change it from <strong class="source-inline">var APP_VERSION="1.1";</strong> to <strong class="source-inline">var APP_VERSION="1.2";</strong>, and execute <span class="No-Break">it again:</span><pre class="source-code">
<strong class="bold">$ time nerdctl build -t ch2lab1:four \</strong>
<strong class="bold">--label nodejs=18.14.2 \</strong>
<strong class="bold">--label=base=alpine3.16  nodejs  \</strong>
<strong class="bold">--progress plain</strong>
<strong class="bold">#1 [internal] load build definition from Dockerfile</strong>
<strong class="bold">…</strong>
<strong class="bold">…</strong>
<strong class="bold">#6 [2/4] WORKDIR /APP</strong>
<strong class="bold">#6 CACHED</strong></pre><p class="list-inset">The previous layers were cached, but a minimal change broke all the processes, and the layers must <span class="No-Break">be recreated:</span></p><pre class="source-code"><strong class="bold">#7 [3/4] COPY . .</strong>
<strong class="bold">#7 DONE 0.0s</strong>
<strong class="bold">#8 [4/4] RUN apk add --no-cache --update curl &amp;&amp; rm -rf /var/cache/apk &amp;&amp; npm install</strong>
<strong class="bold">#0 0.084 fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/x86_64/APKINDEX.tar.gz</strong>
<strong class="bold">#8 0.172 fetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/x86_64/APKINDEX.tar.gz</strong>
<strong class="bold">#8 0.307 (1/5) Installing ca-certificates (20220614-r0)</strong>
<strong class="bold">…</strong>
<strong class="bold">#8 0.376 OK: 10 MiB in 21 packages</strong>
<strong class="bold">…</strong>
<strong class="bold">#8 3.433 added 3 packages, and audited 4 packages in 3s</strong>
<strong class="bold">…</strong>
<strong class="bold">#8 DONE 3.5s</strong>
<strong class="bold">…</strong>
<strong class="bold">…</strong>
<strong class="bold">#9 DONE 0.8s</strong>
<strong class="bold">unpacking docker.io/library/ch2lab1:four (sha256:75ba902c55459593f792c816b8da55a673ffce3633f1504800c90ec9fd214d26)...</strong>
<strong class="bold">Loaded image: docker.io/library/ch2lab1:four</strong>
<strong class="bold">real    0m5.210s</strong>
<strong class="bold">user    0m0.007s</strong>
<strong class="bold">sys     0m0.000s</strong></pre><p class="list-inset">As you can see, it takes the same time as the previous execution because the container runtime can’t identify and isolate the small changes and reuse the layers that were <span class="No-Break">created previously.</span></p></li> </ol>
<p>In this lab, we reviewed how caching layers works and how to avoid build problems by choosing the <a id="_idIndexMarker321"/>right<a id="_idIndexMarker322"/> logic for our <span class="No-Break">application’s Dockerfile.</span></p>
<p>In the next lab, we will execute a multi-stage build process using an empty layer for the <span class="No-Break">final image.</span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor079"/>Executing a multi-stage build process</h2>
<p>This is a very<a id="_idIndexMarker323"/> interesting use case since our <a id="_idIndexMarker324"/>code is in the Go language and we will be including <span class="No-Break">static dependencies:</span></p>
<ol>
<li>Move to the <strong class="source-inline">Chapter2/colors</strong> folder and use the <strong class="source-inline">go</strong> sub-folder this time. The multi-stage Dockerfile looks <span class="No-Break">like this:</span><pre class="source-code">
FROM golang:1.20-alpine3.17 AS builder
WORKDIR /src
COPY ./src/* .
RUN mkdir bin &amp;&amp; go build -o bin/webserver /src/webserver.go
FROM scratch
WORKDIR /app
COPY --from=builder /src/bin/webserver .
CMD ["/app/webserver"]
USER 1000
EXPOSE 3000</pre></li> <li>We will use a <strong class="source-inline">golang:1.20-alpine3.17</strong> image to compile our code. The compiled<a id="_idIndexMarker325"/> binary is copied from the <em class="italic">builder</em> image<a id="_idIndexMarker326"/> to our <span class="No-Break">final image:</span><pre class="source-code">
<strong class="bold">$ nerdctl build -t ch2lab1:go.1 \</strong>
<strong class="bold">--label golang=1.20 --label=base=alpine3.17  go  \</strong>
<strong class="bold">--progress plain</strong>
<strong class="bold">#1 [internal] load .dockerignore</strong>
<strong class="bold">#1 transferring context: 2B done</strong>
<strong class="bold">...</strong>
<strong class="bold">…</strong></pre></li> <li>The first <strong class="source-inline">FROM</strong> key is reached and the image build <span class="No-Break">process starts:</span><pre class="source-code">
<strong class="bold">#6 [builder 1/4] FROM docker.io/library/golang:1.20-alpine3.17@sha256:48f336ef8366b9d6246293e3047259d0f614ee167db1869bdbc343d6e09aed8a</strong>
<strong class="bold">…</strong>
<strong class="bold">…</strong>
<strong class="bold">#6 DONE 3.2s</strong>
<strong class="bold">#6 [builder 1/4] FROM docker.io/library/golang:1.20-alpine3.17@sha256:48f336ef8366b9d6246293e3047259d0f614ee167db1869bdbc343d6e09aed8a</strong>
<strong class="bold">#6 extracting sha256:752c438cb1864d6b2151010a811031b48f0c3511c7aa49f540322590991c949d</strong>
<strong class="bold">...</strong>
<strong class="bold">…</strong>
<strong class="bold">#6 DONE 4.8s</strong>
<strong class="bold">#7 [builder 2/4] WORKDIR /src</strong>
<strong class="bold">#7 DONE 0.2s</strong>
<strong class="bold">#8 [builder 3/4] COPY ./src/* .</strong>
<strong class="bold">#8 DONE 0.0s</strong>
<strong class="bold">#9 [builder 4/4] RUN mkdir bin &amp;&amp; go build -o bin/webserver /src/webserver.go</strong>
<strong class="bold">#9 DONE 3.3s</strong></pre></li> <li>The second <strong class="source-inline">FROM</strong> key is reached and a new image build process starts – in this case, just<a id="_idIndexMarker327"/> copying the content<a id="_idIndexMarker328"/> from the <span class="No-Break">previous one:</span><pre class="source-code">
<strong class="bold">#10 [stage-1 2/2] COPY --from=builder /src/bin/webserver .</strong>
<strong class="bold">#10 DONE 0.0s</strong>
<strong class="bold">#11 exporting to oci image format</strong>
<strong class="bold">...</strong>
<strong class="bold">...</strong>
<strong class="bold">#11 sending tarball 0.1s done</strong>
<strong class="bold">#11 DONE 0.3s</strong>
<strong class="bold">unpacking docker.io/library/ch2lab1:go.1 (sha256:527a2d2f49c7ea0083f0ddba1560e0fc725eb26ade22c3990bb05260f1558b0b)...</strong>
<strong class="bold">Loaded image: docker.io/library/ch2lab1:go.1</strong></pre></li> <li>The final image is really small because it only contains our <span class="No-Break">application code:</span><pre class="source-code">
<strong class="bold">$ nerdctl image ls</strong>
<strong class="bold">REPOSITORY    TAG       IMAGE ID        CREATED              PLATFORM       SIZE         BLOB SIZE</strong>
<strong class="bold">ch2lab1       one     7f63598f2144    2 hours ago          linux/amd64    186.6 MiB    51.7 MiB</strong>
<strong class="bold">ch2lab1       go.1      527a2d2f49c7    4 minutes ago        linux/amd64    6.3 MiB      3.6 MiB</strong></pre><p class="list-inset">In this output, you can compare the different sizes we obtained (sizes may change because some updates may be expected in the code in this book’s <span class="No-Break">GitHub repository).</span></p></li> </ol>
<p>Creating images from scratch using binaries can be very tricky, but they are the best way of delivering <span class="No-Break">our applications.</span></p>
<p>This lab showed you how you can create a container image from scratch by using static build binaries, which<a id="_idIndexMarker329"/> are the best application <a id="_idIndexMarker330"/>images you <span class="No-Break">can create.</span></p>
<p>For the next lab, we will use Docker’s <strong class="source-inline">buildx</strong> features, and therefore, we will use the <strong class="source-inline">docker</strong> <span class="No-Break">command line.</span></p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor080"/>Building images for different architectures</h2>
<p>If you followed <a id="_idIndexMarker331"/>the lab with <strong class="bold">Rancher Desktop</strong>, <strong class="bold">WSL</strong>, and the <strong class="source-inline">nerdctl</strong> command line, please exit <strong class="bold">Rancher Desktop</strong> and launch <strong class="bold">Docker Desktop</strong> (or your own Docker <span class="No-Break">engine implementation).</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Podman and nerdctl also provide multiplatform support on new releases, and a multi-architecture build is commonly available; hence, any of these tools will be right for <span class="No-Break">this lab.</span></p>
<p>Note that when you change from one container runtime to another, the list of images is completely different. Each container runtime manages its own environment <span class="No-Break">as expected.</span></p>
<p>We will continue this lab inside the <strong class="source-inline">Chapter2/colors</strong> folder. We are going to build the image for multiple architectures – that is, <strong class="source-inline">amd64</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">arm64</strong></span><span class="No-Break">:</span></p>
<ol>
<li>We will use <strong class="source-inline">buildx</strong> with the <strong class="source-inline">–-platform</strong> argument and <strong class="source-inline">arm64</strong>. But first, we will ensure that we can build images for other architectures by executing the <strong class="source-inline">docker buildx </strong><span class="No-Break"><strong class="source-inline">ls</strong></span><span class="No-Break"> command:</span><pre class="source-code">
<strong class="bold">$ docker buildx ls</strong>
<strong class="bold">NAME/NODE     DRIVER/ENDPOINT STATUS  BUILDKIT PLATFORMS</strong>
<strong class="bold">default *     docker</strong>
<strong class="bold">  default     default         running 20.10.22 linux/amd64, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6</strong></pre></li> <li>Now, we are<a id="_idIndexMarker332"/> ready to execute the <strong class="source-inline">arm64</strong> <span class="No-Break">architecture build:</span><pre class="source-code">
<strong class="bold">$ docker buildx build -t ch2lab1:six \</strong>
<strong class="bold">  --label nodejs=18.14.2 \</strong>
<strong class="bold">  --label=base=alpine3.16 \</strong>
<strong class="bold">  nodejs --progress plain \</strong>
<strong class="bold">  --platform arm64 \</strong>
<strong class="bold">  --load –no-cache</strong>
<strong class="bold">#1 [internal] load build definition from Dockerfile</strong>
<strong class="bold">#1 transferring dockerfile: 32B done</strong>
<strong class="bold">#1 DONE 0.0s</strong>
<strong class="bold">...</strong>
<strong class="bold">…</strong></pre><p class="list-inset">Note that the <strong class="source-inline">aarch64</strong> architecture image is downloaded during <span class="No-Break">the process:</span></p><pre class="source-code"><strong class="bold">#8 0.348 fetch https://dl-cdn.alpinelinux.org/alpine/v3.17/main/aarch64/APKINDEX.tar.gz</strong>
<strong class="bold">#8 0.753 fetch https://dl-cdn.alpinelinux.org/alpine/v3.17/community/aarch64/APKINDEX.tar.gz</strong>
<strong class="bold">#8 1.204 (1/5) Installing ca-certificates (20220614-r4)</strong>
<strong class="bold">…</strong>
<strong class="bold">…</strong>
<strong class="bold">#8 1.341 Executing busybox-1.35.0-r29.trigger</strong>
<strong class="bold">#8 1.366 Executing ca-certificates-20220614-r4.trigger</strong>
<strong class="bold">...</strong>
<strong class="bold">…</strong>
<strong class="bold">#11 writing image sha256:2588e9451f156ca179694c5c5623bf1c80b9a36455e5f162dae6b111d8ee00fd done</strong>
<strong class="bold">#11 naming to docker.io/library/ch2lab1:six done</strong>
<strong class="bold">#11 DONE 0.1s</strong></pre><p class="list-inset">As you can<a id="_idIndexMarker333"/> see, the <strong class="source-inline">arm64</strong> Alpine image was used, even though we used the same Dockerfile from <span class="No-Break">previous labs.</span></p></li> <li>We can verify this image architecture by using <span class="No-Break"><strong class="source-inline">docker inspect</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ docker image inspect ch2lab1:six \</strong>
<strong class="bold">--format='{{.Architecture}}'</strong>
<strong class="bold">arm64</strong></pre><p class="list-inset">The final image is prepared for the <strong class="source-inline">arm64</strong> architectures and can be used in some QNAP <span class="No-Break">NAS platforms.</span></p></li> </ol>
<p>In this build process, we also used <strong class="source-inline">--load</strong> and <strong class="source-inline">–-no-cache</strong>. The first argument is used to load the image that was built into our container runtime. If we don’t use this with Docker’s <strong class="source-inline">buildx</strong>, the image is used as a cache for new builds only by default. To avoid any cached layer within this build process, we used <strong class="source-inline">–-no-cache</strong>, and this ensures the complete execution of each step defined in <span class="No-Break">the Dockerfile.</span></p>
<p>This lab showed <a id="_idIndexMarker334"/>you that you can prepare your images for any available architecture by using a unified Dockerfile and executing the build process with the <strong class="source-inline">–-</strong><span class="No-Break"><strong class="source-inline">platform</strong></span><span class="No-Break"> argument.</span></p>
<h1 id="_idParaDest-65"><a id="_idTextAnchor081"/>Summary</h1>
<p>In this chapter, we learned how to create container images for applications. We started with an overview of CoW filesystems, which are the base for creating container images using layers. We looked at different methods to build images, along with their pros, cons, and examples. Using Dockerfiles is the best method because it provides a reproducible way of creating images by using different steps, written in order in these files. We provided a quick overview of the most important directives we can use in Dockerfiles and the command line and the arguments for using them. As the container-image-building process can be tricky, we presented some advanced features and practices we can use to improve our workflow in terms of speed, reusability, <span class="No-Break">and quality.</span></p>
<p>In the next chapter, we will provide a quick overview of image registries, learn how to store and tag our images in them, and learn how to improve integrity and security by signing and scanning <span class="No-Break">container images.</span></p>
</div>
</div></body></html>