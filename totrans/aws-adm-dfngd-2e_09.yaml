- en: Powering Analytics Using Amazon EMR and Amazon Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned about two really useful services that developers
    can leverage to build highly scalable and decoupled applications in the cloud:
    Amazon SNS and Amazon SQS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be turning things up a notch and exploring two amazingly
    powerful AWS services that are ideal for processing and running large-scale analytics
    and data warehousing in the cloud: Amazon EMR and Amazon Redshift.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keeping this in mind, let''s have a quick look at the various topics that we
    will be covering in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the AWS analytics suite of services with an in-depth look at Amazon
    EMR, along with its use cases and benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing a few key EMR concepts and terminologies, along with a quick getting
    started tour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a sample workload on EMR, using steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Amazon Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with an Amazon Redshift cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Redshift databases and tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading data from Amazon EMR into Amazon Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So without any further ado, let's get started right away!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the AWS analytics suite of services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the growth of big data and its adoption across organizations on the rise,
    many cloud providers today provide a plethora of services that are specifically
    designed to run massive computations and analytics on large volumes of data. AWS
    is one such cloud provider that also has invested a lot into the big data and
    analytics paradigm with a host of services offering ready-to-use frameworks, business
    insights and data warehousing solutions, as well. Here is a brief explanation
    of the AWS analytics suite of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon EMR**: Amazon **Elastic MapReduce** or **EMR** is a quick and easy
    to use service that provides users with a scalable, managed Hadoop ecosystem and
    framework. You can leverage EMR to process vast amounts of data without having
    to worry about configuring the underlying Hadoop platform. We will be learning
    and exploring more on EMR in the subsequent sections of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Athena**: Amazon Athena takes big data processing up a notch by providing
    a standard SQL interface for querying data that is stored directly on Amazon S3\.
    With Athena, you do not have any underlying hardware to manage or maintain; it
    is all managed by AWS itself. This *serverless*Â approach makes Athena ideal for
    processing data that does not require any complex ETL processing. All you need
    to do is create a schema, point Athena to your data on Amazon S3, and start querying
    it using simple SQL syntax.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Elasticsearch Service**: Amazon Elasticsearch Service provides a managed
    deployment of the popular open source search and analytics engine: Elasticsearch.
    This service comes in really handy when you wish to process streams of data originating
    from various sources such as logs generated from instances, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Kinesis**: Unlike the other services discussed so far, Amazon Kinesis
    is more of a streaming service provided by AWS. You can use Amazon Kinesis to
    push vast amounts of data originating from multiple sources, into one or more
    streams that can be consumed by other AWS services for performing analytics and
    other data processing processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon QuickSight**: Amazon QuickSight is an extremely cost-effective business
    insights solution that can be used to perform fast ad hoc analysis on data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Redshift**: Amazon Redshift is a petabyte-scale data warehousing solution
    provided by AWS that you can leverage for analyzing your data, using an existing
    set of tools. We will be learning more about Redshift a bit later during this
    chapter. The services are depicted here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/18d0ba95-6fe1-473d-84e2-f3aab87c99c3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**AWS Data Pipeline:** Moving large amounts of data between AWS services can
    be difficult to perform, especially when the data sources vary. AWS Data Pipeline
    makes it easier to transfer data between different AWS storage and compute services,
    as well as helping in the initial transformation and processing of data. You can
    even use Data Pipeline to transfer data reliably from an on-premise location into
    AWS storage services, as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Glue**: AWS Glue is a managed **ETL** (**Extract**, **Transform** and
    **Load**) service recently launched by AWS. Using AWS Glue greatly simplifies
    the process of preparing, extracting, and loading data from large datasets into
    an AWS storage service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this brief overview of the AWS analytics suite of services, let's now move
    forward and get started with understanding a bit more about Amazon EMR!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Amazon EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, Amazon EMR is a managed service that provides big data
    analytics frameworks, such as Apache Hadoop and Apache Spark straight out of the
    box and ready for use. Using Amazon EMR, you can easily perform a variety of use
    cases such as batch processing, big data analytics, low-latency querying, data
    streaming, or even use EMR as a large datastore itself!
  prefs: []
  type: TYPE_NORMAL
- en: 'With Amazon EMR, there is very little underlying infrastructure to manage on
    your part. You simply have to decide the number of instances you initially want
    to run your EMR cluster on and start consuming the framework for analytics and
    processing. Amazon EMR provides you with features that enable you to scale your
    infrastructure based on your requirements, without affecting the existing setups.
    Here is a brief look at some of the benefits that you can obtain by leveraging
    Amazon EMR for your own workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pricing**: Amazon EMR relies on EC2 instances to spin up your Apache Hadoop
    or Apache Spark clusters. Although you can vary costs by selecting the instance
    types for your cluster from large to extra large and so on, the best part of EMR
    is that you can also opt between using a combination of on-demand EC2 instances,
    reserved and spot instances based on your setup, thus providing you with flexibility
    at significantly lower costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Amazon EMR provides you with a simple way of scaling running
    workloads, depending on their processing requirements. You can resize your cluster
    or its individual components as you see fit and additionally, configure one or
    more instance groups for a guaranteed instance availability and processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability**: Although you, as an end user, have to specify the initial
    instances and their sizes, AWS ultimately ensures the reliability of the cluster
    by swapping out instances that either have failed or are going to in the due course
    of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration**: Amazon EMR integrates with the likes of other AWS services
    to provide your cluster with additional storage, network, and security requirements.
    You can use services such as Amazon S3 to store both the input as well as the
    output data, AWS CloudTrail for auditing the requests made to EMR, VPC to ensure
    the security of your launched EMR instances and much more!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these details in mind, let's move an inch closer to launching our very
    own EMR cluster by first visiting some of its key concepts and terminologies.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts and terminologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get started with Amazon EMR, it is important to understand some of
    its key concepts and terminologies, starting out with clusters and nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clusters**: Clusters are the core functioning component in Amazon EMR. A
    cluster is a group of EC2 instances that together can be used to process your
    workloads. Each instance within a cluster is termed as a node and each node has
    a different role to perform within the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes**: Amazon EMR distinguishes between clusters instances by providing
    them with one of these three roles:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Master node**: An instance that is responsible for the overall manageability,
    working and monitoring of your cluster. The *master node* takes care of all the
    data and task distributions that occur within the cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Core node**: The core nodes are very similar to the master node; however,
    they are primarily used to run tasks and store data on your **Hadoop Distributed
    File System** (**HDFS**). The core node can also contain some additional software
    components of Hadoop applications within itself.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task node**: Task nodes are only designed to run tasks. They do not contain
    any additional software components of Hadoop applications within themselves and
    are optional when it comes to the cluster''s deployment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Steps**: Steps are simple tasks or jobs that are submitted to a cluster for
    processing. Each step contains some instructions on how the particular job is
    to be performed. Steps can be ordered such that a particular step can be used
    to fetch the input data from Amazon S3, while a second step can be used to run
    a Pig or Hive query against it, and finally a third step to store output data
    to say Amazon DynamoDB. If one step fails, the subsequent steps are automatically
    cancelled from execution, however, you can choose to overwrite this behavior by
    selecting your steps to ignore failures and process further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apart from these concepts, you will additionally be required to brush up on
    your Apache Hadoop framework and terminologies, as well. Here''s a quick look
    at some of the Apache frameworks and applications that you will come across while
    working with Amazon EMR:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Storage**: A big part of EMR is how the data is actually stored and retrieved.
    The following are some of the storage options that are provided to you while using
    Amazon EMR:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Distributed File System** (**HDFS**): As the name suggests, HDFS is
    a distributed and scalable filesystem that allows data to be stored across the
    underlying node instances. By default, the data is duplicated and stored across
    the instances present in the cluster. This provides high availability and data
    resiliency in case of an instance failure. You can read more about HDFS at:Â [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EMR File System** (**EMRFS**): EMRFS is an extension of the HDFS filesystem,
    using which you can access and store data directly on Amazon S3, just as a normal
    filesystem.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local filesystem**: Apart from HDFS, each instance within the cluster is
    also provided with a small block of pre-attached ephemeral disks which is also
    called the local filesystem. You can use this local filesystem to store additional
    software or applications required by your Hadoop frameworks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frameworks**: As mentioned before, Amazon EMR provides two data processing
    frameworks that you can leverage based on your processing needs: Apache Hadoop
    MapReduce and Apache Spark:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Hadoop MapReduce**: MapReduce is by far the most commonly used and
    widely known programming model when it comes to building distributed applications.
    The open source model relies on a `Mapper`Â function that maps the data to sets
    of key-value pairs and a `Reducer`Â function that combines these key-value pairs,
    applies some additional processing, and finally generates the desired output.
    To know more about MapReduce and how you can leverage it, check out this URL:Â [https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Spark**: Apache Spark is a fast, in-memory data processing model using
    which a developer can process streaming, machine learning or SQL workloads that
    require fast iterative access to datasets. It is a cluster framework similar to
    Apache Hadoop; however, Spark leverages graphs and in-memory databases for accessing
    your data. You can read more about Spark at [https://spark.apache.org/](https://spark.apache.org/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applications and programs**: With the standard data processing framework,
    Amazon EMR also provides you with additional applications and programs that you
    can leverage to build native distributed applications. Here''s a quick look into
    a couple of them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YARN**: **Yet Another Resource Negotiator**, is a part of the Hadoop framework
    and provides management for your cluster''s data resources'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hive**: Hive is a distributed data warehousing application that leverages
    standard SQL to query extremely large datasets stored on the HDFS filesystem.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There are yet many other applications and programs made available for use by
    Amazon EMR, such as Apache Pig, Apache HBase, Apache Zookeeper, and so on. In
    the next section, we will be looking at how to leverage these concepts and terminologies
    to create our very own Amazon EMR Cluster, so let's get busy!
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Amazon EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the basics covered, in this section we will be working with the Amazon
    EMR dashboard to create our very first cluster. However, before we get going,
    here's a small list of prerequisite steps that we need to complete first.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we will need to create an Amazon S3 bucket that will be used
    to store the output, logs generated by EMR, as well as some additional script
    and software files:'
  prefs: []
  type: TYPE_NORMAL
- en: From the AWS Management Console, filter and select theÂ Amazon S3Â service by
    using the FilterÂ option. Alternatively, launch the Amazon S3 dashboard by navigating
    to this URL:Â [https://s3.console.aws.amazon.com/s3/](https://s3.console.aws.amazon.com/s3/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, select the Create bucketÂ option. In the Create bucketÂ wizard, provide
    a suitable Bucket nameÂ followed by the selection of an appropriate RegionÂ to create
    the bucket in. For this use case, the EMR cluster, as well as the S3 buckets,
    are created in the **US East (Ohio)**Â region, however you can select an alternative
    based on your requirements. Click on NextÂ to continue with the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the Set propertiesÂ page, you can optionally choose to provide some *tags*Â for
    your bucket for cost allocations and tracking purposes. Click NextÂ to continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Set permissions page, ensure that the no public read access is granted
    to the bucket. Click on NextÂ to review the settings and finally, select Create
    bucketÂ to complete the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the bucket is created, use the Create folderÂ option to create dedicated
    folders for storing the logs, output, as well as some additional scripts that
    we might use in the near future. Here is a representational screenshot of the
    bucket after you have completed all of the previous steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e2af9cde-bd0b-4a0b-9f3d-bdd10f47210c.png)'
  prefs: []
  type: TYPE_IMG
- en: With the bucket created and ready for use, the next prerequisite item left to
    create is a key pair using which you can SSH into your EC2 instances. Ensure that
    the key pair is created in the same region (**US East (Ohio)** in this case) as
    your EMR cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the prerequisites are out of the way, we can finally get started with
    our EMR cluster setup!
  prefs: []
  type: TYPE_NORMAL
- en: From the AWS Management Console, filter and select theÂ Amazon EMRÂ service by
    using the FilterÂ option. Alternatively, launch the Amazon EMR dashboard by selecting
    this URL:Â [https://us-east-2.console.aws.amazon.com/elasticmapreduce/home](https://us-east-2.console.aws.amazon.com/elasticmapreduce/home).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since this is the first time we've created an EMR cluster, select the Create
    clusterÂ option to get started.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can configure your EMR cluster using two ways: a fast and easy Quick OptionsÂ which
    is shown to you by default, and an Advanced optionsÂ page where you can select
    and configure the individual items for your cluster. In this case, we will go
    ahead and select Go to advanced options.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Advanced options pageÂ provides us with a four-step wizard that essentially
    guides us to configuring a fully functional EMR cluster. To begin with, the first
    step is where you can select and customize the *software* that you wish to install
    on your EMR cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the ReleaseÂ drop-down list, select the appropriate EMR release that you
    would like to work with. The latest version released as of writing this book is
    `emr-5.11.1`. Each release contains several distributed applications available
    for installation on your cluster. For example, selecting emr-5.11.1 which is a
    2018 release, contains Hadoop v2.7.3, Flink v1.3.2, Ganglia v3.7.2, HBase v1.3.1,
    and many other such applications and software.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a complete list of available EMR releases and their associated software
    versions, go toÂ [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, I have gone ahead and selected the basic applications that we
    will be requiring for this scenario, including Hadoop, Hive and Hue. Feel free
    to select other applications as per your requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next couple of sections are optional, however, it is important to know
    their purpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AWS Glue Data Catalog settings**: With EMR version 5.8.0 and above, you optionally
    have the choice to configure Spark SQL to use the AWS Glue Data Catalog (an external
    Hive table) as its metastore.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edit software settings**: You can use this option to override the default
    configuration settings for certain applications. This is achieved by providing
    a configuration object in the form of a JSON file. You can either Enter configurationÂ or
    Load JSON from S3Â as well:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/bc0d8804-5937-4c76-8614-3a2621dea03f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Add steps**: The final optional parameter left on the Software ConfigurationÂ page
    is the *add steps*. As discussed briefly earlier in this chapter, steps are essentially
    a unit of work that we submit to the cluster. This can be something as trivial
    as loading input data from S3, or processing and running a MapReduce job on the
    data. We will be exploring steps a little more in detail a bit later in this chapter,
    so leave this field to its default value and select NextÂ to continue with the
    process.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: The second step in the Advanced optionsÂ wizard is configuring the cluster's
    hardware, or the instance configurations, as well as the cluster's networking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'EMR provides two options: instance fleets and instance groups; both explained
    briefly here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instance fleets**: Instance fleets allows you to specify a target capacity
    for the instances present in a cluster. With this option, you get the widest variety
    of instance provisioning options where you can leverage mixed instance types for
    your nodes, and even go for different purchasing options for the same. With each
    instance fleet that created, you get to establish a target capacity for on-demand,
    as well as for spot instances.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: You can have only one instance fleet per node type (master, core, task).
  prefs: []
  type: TYPE_NORMAL
- en: '**Instance groups**: Instance groups on the other hand do not offer many custom
    configurable options per node type. In instance groups, each node consists of
    the same instance type and the same purchasing option, as well. Once these settings
    are configured during the cluster''s creation, they cannot be altered; however,
    you can always add more instances as you see fit.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this particular use case, we are going to go ahead and select Uniform instance
    groups,Â as depicted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6a18bcf6-788e-4f31-84a4-b95cef2065d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, from the NetworkÂ drop-down list, select the appropriate *VPC*Â in which
    you wish to launch your EMR cluster. You can alternatively choose to create a
    new VPC specifically for EMR, using the adjoining Create a VPCÂ option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, select the appropriate subnet from the EC2 SubnetÂ drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, assign a value for the Root device EBS volume sizeÂ that will be provisioned
    for each instance in the cluster. You can provide values between 10 GB and 100
    GB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the edit options provided, you can additionally configure the Instance
    type, the Instance countÂ as well as the Purchasing optionÂ for each node type,
    as depicted in the following screenshot. Note that these options are provided
    because we selected instance groups as our preferred mode of instance configurations.
    The options will vary if the Instance FleetÂ option is selected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ee8245bb-2d24-4907-be69-600417084e67.png)'
  prefs: []
  type: TYPE_IMG
- en: You can additionally choose to enable autoscaling for the Core and Task nodes
    by selecting the Not enabledÂ option under the Auto scalingÂ column. Subsequently,
    you can add additional task instance groups by selecting the Add task instance
    groupÂ option, as well. Once done, select the NextÂ option to proceed with the set
    up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The third step in the Advanced optionsÂ provides general configurations that
    you can set, based on your requirements. To start off, provide a suitable Cluster
    nameÂ followed by selecting the LoggingÂ option for your EMR cluster. Use the folder
    option to browse to our newly created S3 bucket, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aa154fd4-d9ef-41bd-9df1-130767cb5bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: You can additionally enable the Termination protectionÂ option to prevent against
    accidental deletions of your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving on, the final configuration item left on the cluster's General OptionsÂ page
    is the Bootstrap Actions. Bootstrap actions as the name implies are certain scripts
    or code that you wish to execute on your cluster's instances at the time of booting
    up. This feature thus comes in very handy when you have to add new instances to
    an existing running cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bootstrap actions are executed using the Hadoop user by default. You can switch
    to root privileges by using the `sudo`Â command.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of Bootstrap actions that you can execute on your instances:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run if: The Run ifÂ action executes an action when an *instance-specific*Â value
    is found in either the `instance.json`Â or the `job-flow.json`Â file. This is a
    predefined bootstrap action and comes in very handy when you only want to execute
    the action on a particular type of instance, for example, execute the bootstrap
    action only if the instance type is `master`.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Custom action: Custom actions leverage your own scripts to perform a customized
    bootstrap action.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To create a bootstrap action, select the Configure and addÂ option from the Add
    Bootstrap Action. Make sure the Run ifÂ action is selected before proceeding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will bring up the Add Bootstrap ActionÂ dialog as depicted in the following
    screenshot. Type in a suitable NameÂ for your Run ifÂ action. Since the Run ifÂ action
    is a predefined bootstrap action, the script''s location is not an editable field.
    You can, however, add Optional arguments for the script, as shown here. In this
    case, the Run ifÂ action will only echo the message if the instance is a **master**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9d1acc81-06a7-49d4-acbc-861e2219d5cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on AddÂ once done. Similarly, you can add your custom bootstrap actions
    as well, by placing the executable scripts in the Amazon S3 bucket that we created
    during the prerequisite phase of this chapter and providing that path here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving on to the final step in this cluster creation process, on the Security
    OptionsÂ page, you can review the various permissions, roles, authentication, and
    encryption settings that the cluster will use once it's deployed. Start off by
    selecting the EC2 key pairÂ that we created at the start of this chapter. You can
    additionally opt to change the PermissionsÂ or use the default ones provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once done, click on Create clusterÂ to complete the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster's creation takes a couple of minutes, depending on the number of
    instances selected for the cluster, as well as the software identified to be installed.
    Once done, you can use the EMR dashboard to view the cluster's health status and
    other vital information.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to your EMR cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have provisioned the EMR cluster, you should see its state change from
    StartingÂ to BootstrappingÂ to finally into a RunningÂ state. If you do not have
    any jobs currently executing, then your cluster may go into a WaitingÂ state as
    well. Here, you can now start using the EMR cluster for running your various jobs
    and analysis. But before that, here's a quick introduction of a few ways in which
    you can connect to your running EMR cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'First up, connecting to the master node using a simple SSH. Connecting to the
    master node via SSH can be used for monitoring the cluster, viewing Hadoop''s
    log flies or for even running an interactive shell for Hive or Pig programming:'
  prefs: []
  type: TYPE_NORMAL
- en: To do so, log in to your Amazon EMR dashboard and select your newly created
    cluster's name from the Cluster listÂ page. This will display the clusters DetailsÂ page
    where you can manage, as well as monitor your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, copy the Master public DNSÂ address. Once copied, open up a PuTTY Terminal
    and paste the copied public DNS in the Host Name (or IP Address) field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the key pair that you associated with this EMR cluster into a private
    key and attach that private key in PuTTY by selecting the AuthÂ option present
    under the SSHÂ section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once done, click on OpenÂ to establish the connection. At the certificate dialog,
    accept the certificate and type in `Hadoop`Â as the username when prompted. You
    should get SSH access into your cluster's master node now!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The same task can be performed using the AWS CLI as well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Terminal, first type in the following command to retrieve the running
    cluster''s ID. The cluster''s ID will be in this format `j-XXXXXXXX`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To list the instances running in your cluster, use the cluster ID obtained
    from the previous command''s output in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Copy the `PublicDnsName`Â value from the output of this command. You can then
    use the following set of commands to get access to your master node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that the cluster''s private key has the necessary permissions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once done, SSH to the master node using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can additionally connect to the various application web interfaces, such
    as *Hue*Â or the *Hadoop HDFS NameNode,*Â using a few simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To get started, you will once again require the public DNS name of your master
    node. You can obtain that from the EMR dashboard or by using the CLI steps we
    just walked through.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, using PuTTY , paste the public DNS name in the Host Name (or IP Address)Â field
    as done earlier. Browse and load the private key using the AuthÂ option as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the SSHÂ option from PuTTY's navigation pane, select Tunnels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fill in the required details as mentioned in the following list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set source port field to `8157`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable the DynamicÂ and AutoÂ options
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once completed, select AddÂ and finally OpenÂ the connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This form of tunnelling or port forwarding is essential as the web interfaces
    can only be viewed from the master node''s local web server. Once completed, launch
    your favorite browser and view the respective web interfaces, as given here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For accessing Hue, type in the following in your web browser:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For accessing the Hadoop HDFS NameNode, type in the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cad43e66-7ae7-415f-a84f-50ece801869f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can even use the CLI to create a tunnel. To do so, substitute the public
    DNS name and the private key values in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `-D`Â flag indicates that the port forwarding is dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: Running a job on the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the connectivity established, you can now execute jobs as one or more
    steps on your cluster. In this section, we will be demonstrating the working of
    a step using a simple example which involves the processing of a few Amazon CloudFront
    logs. The details of the sample data and script can be found at:Â [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-prepare-data-and-script.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-prepare-data-and-script.html).
    You can use similar techniques and bases to create and execute your own jobs as
    well:'
  prefs: []
  type: TYPE_NORMAL
- en: To get started with a job, from the EMR dashboard select your cluster's name
    from the Cluster listÂ page. This will bring up the newly created clusters details
    page. Here, select the StepsÂ tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since this is going to be our first step, go ahead and click on theÂ Add stepÂ option.
    This brings up the Add stepÂ dialog as shown in the following screenshot. Fill
    in the required information as described and, once all the fields are filled in,
    click on AddÂ to complete the step''s creation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/389a4fb3-1fe5-4c5b-a0ab-0d1baaaa8466.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step type: You can choose between various options such as Streaming programÂ which
    essentially will prompt you to provideÂ `Mapper` and `Reducer` function details,
    or alternatively, you can also select Hive program, Pig program, Spark programÂ or
    a Custom application. In this case, we select the Hive programÂ option.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Name: A suitable name for your step.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Script S3 location: Provide the Hive script''s location here. Since we are
    using a predefined script, simply replace the `<REGION>` field with your EMR''s
    operating region: `s3://<REGION>.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input S3 location: Provide the input data file''s location here. Replace the
    `<REGION>` placeholder with your EMR''s operating region as done before: `s3://<REGION>.elasticmapreduce.samples`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output S3 location: Specify where the processed output files have to be stored.
    In this case, I''m using the custom S3 bucket that we created as a prerequisite
    step during the EMR cluster creation. You can provide any other alternative bucket
    as well.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arguments: You can use this field to provide any optional arguments required
    by the script to run. In this case, copy, and paste the following `-hiveconf hive.support.sql11.reserved.keywords=false`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action on failure: You can optionally choose what EMR should do in case the
    step''s execution undergoes a failure. In this case, we have selected the default
    ContinueÂ value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the required fields are filled out, click on AddÂ to complete the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The step now starts executing the supplied script on the EMR cluster. You can
    view the progress by viewing the changes in the step''s status from PendingÂ to
    RunningÂ to Completed,Â as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff0fe102-f8c0-4afe-b976-dbbe1629343a.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the job completes its execution, head back to your Amazon S3's output bucket
    and view the output of the processing. In this case, the output contains the number
    of access requests made to CloudFront, sorted by the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring EMR clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EMR dashboard provides a rich feature set using which you can manage and
    monitor your EMR clusters all from one place. You can additionally view logs and
    leverage Amazon CloudWatch as well to track the performance of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will be looking at a few simple ways using which you can
    monitor your EMR clusters. To start off, let''s look at how to monitor the status
    of your cluster using the EMR dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the EMR dashboard, select your cluster name from the cluster listÂ page.
    This will bring up the newly created cluster''s details page. Here, select the
    EventsÂ tab, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1dc7acf7-44de-4553-ab9f-c7a2326fff89.png)'
  prefs: []
  type: TYPE_IMG
- en: The Events tab allows you to view the event logged by your cluster. You can
    use this to view events generated by the cluster, by running applications, by
    step execution and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard also provides an in-depth look into the performance of the cluster
    over a period. To view the performance indicators, select the MonitoringÂ tab from
    the cluster's DetailsÂ page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, you can view essential details and status about your cluster, the running
    nodes, as well as the underlying I/O and data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can also use Amazon CloudWatch to view and monitor the cluster's
    various metrics. To do so, launch the Amazon CloudWatch dashboard by selecting
    this URL:Â [https://console.aws.amazon.com/cloudwatch/home](https://console.aws.amazon.com/cloudwatch/home).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, from the navigation pane, select the MetricsÂ option to view all the metrics
    associated with EMR. Use the `JobFlowID`Â dimension to filter the EMR cluster in
    case you have multiple clusters running in the same environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a list of some important EMR metrics worth monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric name** | **Metric description** |'
  prefs: []
  type: TYPE_TB
- en: '| `AppsFailed` | The number of applications submitted to the EMR cluster that
    have failed to complete. This application status is monitored internally and reported
    by YARN. |'
  prefs: []
  type: TYPE_TB
- en: '| `MRUnhealthyNodes` | The number of nodes available to MapReduce jobs marked
    in an `UNHEALTHY`Â state. |'
  prefs: []
  type: TYPE_TB
- en: '| `MRLostNodes` | The number of nodes allocated to MapReduce that have been
    marked in a `LOST`Â state. |'
  prefs: []
  type: TYPE_TB
- en: '| `CorruptBlocks` | The number of blocks that HDFS reports as corrupted. |'
  prefs: []
  type: TYPE_TB
- en: You can view the complete list of monitored metrics at:Â [https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html).
  prefs: []
  type: TYPE_NORMAL
- en: Once a Metric is identified, select the Metric and click on the Graphed metricsÂ tab.
    Here, select the Create alarmÂ option provided under the ActionsÂ column to create
    and set an alarm threshold, as well as its corresponding action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this way, you can also leverage Amazon CloudWatch events to periodically
    monitor the events generated by the cluster. Remember, however, that EMR tracks
    and records events only for a period of seven days. With this, we come to the
    end of this particular section and EMR, as well. In the next section, we will
    be learning and exploring a bit about yet another awesome analytics service called
    Amazon Redshift!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Amazon Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon Redshift is one of theÂ **database as a service** (**DBaaS**) offerings
    from AWS that provides a massively scalable data warehouse as a managed service,
    at significantly lower costs. The data warehouse is based on the open source PostgreSQL
    database technology however; not all features offered in PostgreSQL are present
    in Amazon Redshift. Here''s a look at some of the essential concepts and terminologies
    that you ought to keep in mind when working with Amazon Redshift:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clusters**: Just like Amazon EMR, Amazon Redshift too relies on the concept
    of clusters. Clusters here are logical containers containing one or more instances
    or compute nodes, and one leader node that is responsible for the cluster''s overall
    management. Here''s a brief look at what each node provides:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leader node**: The leader node is a single node present in a cluster that
    is responsible for orchestrating and executing various database operations, as
    well as facilitating communication between the database and associate client programs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute node**: Compute nodes are responsible for executing the code provided
    by the leader node. Once executed, the compute nodes share the results back to
    the leader node for aggregation. Amazon Redshift supports two types of compute
    nodes: dense storage nodes and dense compute nodes. The dense storage nodes provide
    standard hard disk drives for creating large data warehouses; whereas, the dense
    compute nodes provide higher performance SSDs. You can start off by using a single
    node that provides 160 GB of storage and scale up to petabytes by leveraging one
    or more 16 TB capacity instances as well.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node slices**: Each compute node is partitioned into one or more smaller
    chunks or slices by the leader node, based on the cluster''s initial size. Each
    slice contains a portion of the compute nodes memory, CPU and disk resource, and
    uses these resources to process certain workloads that are assigned to it. The
    assignment of workloads is again performed by the leader node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Databases**: As mentioned earlier, Amazon Redshift provides a scalable database
    that you can leverage for a data warehouse, as well as analytical purposes. With
    each cluster that you spin in Redshift, you can create one or more associated
    databases with it. The database is based on the open source relational database
    PostgreSQL (v8.0.2) and thus, can be used in conjunction with other RDBMS tools
    and functionalities. Applications and clients can communicate with the database
    using standard PostgreSQL JDBC and ODBC drivers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a representational image of a working data warehouse cluster powered
    by Amazon Redshift:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c270279-61dc-49d6-8ebd-1027dd8c20fa.png)'
  prefs: []
  type: TYPE_IMG
- en: With this basic information in mind, let's look at some simple and easy to follow
    steps using which you can set up and get started with your Amazon Redshift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Amazon Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be looking at a few simple steps which you can take
    to have a fully functioning Amazon Redshift cluster up and running in a matter
    of minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: First up, we have a few prerequisite steps that need to be completed before
    we begin with the actual set up of the Redshift cluster. From the AWS Management
    Console, use the FilterÂ option to filter out IAM. Alternatively, you can also
    launch the IAM dashboard by selecting this URL:Â [https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once logged in, we need to create and assign a role that will grant our Redshift
    cluster read-only access to Amazon S3 buckets. This role will come in handy later
    on in this chapter when we load some sample data on an Amazon S3 bucket and use
    Amazon Redshift's `COPY` command to copy the data locally into the Redshift cluster
    for processing. To create the custom role, select the RoleÂ option from the IAM
    dashboards' navigation pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the RolesÂ page, select the Create roleÂ option. This will bring up a simple
    wizard using which we will create and associate the required permissions to our
    role.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the RedshiftÂ option from under the AWS ServiceÂ group section and opt
    for the Redshift - CustomizableÂ option provided under the Select your use caseÂ field.
    Click NextÂ to proceed with the set up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the Attach permissions policiesÂ page, filter and select the AmazonS3ReadOnlyAccessÂ permission.
    Once done, select Next: Review.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the final ReviewÂ page, type in a suitable name for the role and select the
    Create RoleÂ option to complete the process. Make a note of the role''s ARN as
    we will be requiring this in the later steps. Here is snippet of the role policy
    for your reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With the role created, we can now move on to creating the Redshift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, log in to the AWS Management Console and use the FilterÂ option to
    filter out Amazon Redshift. Alternatively, you can also launch the Redshift dashboard
    by selecting this URL:Â [https://console.aws.amazon.com/redshift/](https://console.aws.amazon.com/redshift/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Launch ClusterÂ to get started with the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, on the CLUSTER DETAILSÂ page, fill in the required information pertaining
    to your cluster as mentioned in the following list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cluster identifier: A suitable name for your new Redshift cluster. Note that
    this name only supports *lowercase* strings.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Database name: A suitable name for your Redshift database. You can always create
    more databases within a single Redshift cluster at a later stage. By default,
    a database named `dev`Â is created if no value is provided:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/65d00270-7593-4bd4-93a9-0cf6670605a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Database port: The port number on which the database will accept connections.
    By default, the value is set to `5439,`Â however you can change this value based
    on your security requirements.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Master user name: Provide a suitable username for accessing the database.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Master user password: Type in a strong password with at least one uppercase
    character, one lowercase character and one numeric value. Confirm the password
    by retyping it in the Confirm passwordÂ field.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once completed, hit ContinueÂ to move on to the next step of the wizard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the NODE CONFIGURATIONÂ page, select the appropriate Node typeÂ for your cluster,
    as well as the Cluster typeÂ based on your functional requirements. Since this
    particular cluster setup is for demonstration purposes, I've opted to select the
    dc2.largeÂ as the Node typeÂ and a Single NodeÂ deployment with *1*Â compute node.
    Click ContinueÂ to move on the next page once done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is important to note here that the cluster that you are about to launch will
    be live and not running in a sandbox-like environment. As a result, you will incur
    the standard Amazon Redshift usage fees for the cluster until you delete it. You
    can read more about Redshift's pricing at:Â [https://aws.amazon.com/redshift/pricing/](https://aws.amazon.com/redshift/pricing/).
  prefs: []
  type: TYPE_NORMAL
- en: In the ADDITIONAL CONFIGURATION page, you can configure add-on settings, such
    as encryption enablement, selecting the default VPC for your cluster, whether
    or not the cluster should have direct internet access, as well as any preferences
    for a particularÂ Availability Zone out of which the cluster should operate. Most
    of these settings do not require any changes at the moment and can be left to
    their default values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The only changes required on this page is associating the previously created
    IAM role with the cluster. To do so, from the Available RolesÂ drop-down list,
    select the custom Redshift role that we created in our prerequisite section. Once
    completed, click on Continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the settings and changes on the ReviewÂ page and select the Launch ClusterÂ option
    when completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cluster takes a few minutes to spin up depending on whether or not you
    have opted for a single instance deployment or multiple instances. Once completed,
    you should see your cluster listed on the ClustersÂ page, as shown in the following
    screenshot. Ensure that the status of your cluster is shown as healthyÂ under the
    DB HealthÂ column. You can additionally make a note of the cluster''s endpoint
    as well, for accessing it programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb5c62d5-ac0c-4a6f-a546-ce35680aca42.png)'
  prefs: []
  type: TYPE_IMG
- en: With the cluster all set up, the next thing to do is connect to the same. In
    the next section, we will be looking at a few simple steps you can take to connect
    to your newly deployed Redshift cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to your Redshift cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use a number of tools to connect to your Redshift cluster once its up
    and running. Most of these tools are PostgreSQL compliant and easily available
    off the shelf. In this case, we are going to install and use an open source SQL
    client tool called **SQL Workbench/J**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, you will need to have Java runtime installed on your local workstation.
    The Java runtime version will have to *match* the requirements of SQL Workbench/J,
    otherwise it simply won''t work. You can check the version of the installed Java
    runtime on your local desktop by either locating the Java configuration on the
    Control Panel or by typing in the following command in a Terminal if you are working
    with a Linux distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we are using a simple Windows desktop for installing SQL Workbench/J.
    Download the correct version of the software from here: [http://www.sql-workbench.net/downloads.html](http://www.sql-workbench.net/downloads.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the software downloaded, the installation is pretty straightforward. Accept
    the end user license agreement, select a path for the software''s installation
    and that''s it! You should have the SQL Workbench/J up and running now:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To connect SQL Workbench/J with your Redshift cluster, you will need your newly
    created database''s JDBC URL. You can copy it by selecting the Connect clientÂ option
    from Redshift''s navigation pane and selecting your newly deployed cluster from
    the Get cluster connection URLÂ section, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e45e8ab8-5527-4152-9b9b-54b9f1f3ee12.png)'
  prefs: []
  type: TYPE_IMG
- en: You will also need to download the correct version of the associated Amazon
    Redshift JDBC DriverÂ JAR using the same page as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once completed, from the SQL Workbench/J client, select File, followed by the
    Connect windowÂ option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, click on Create a new connection profileÂ to get started. This will pop
    up a New profileÂ box where you will need to enter a name for this new profile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the profile is created; select the Manage driversÂ option. This will display
    the Manage driversÂ dialog box, as shown in the following screenshot. Select theÂ Amazon
    RedshiftÂ option and provide a suitable NameÂ for your connection driver, as well.
    Click on the browse icon and select the downloaded Amazon Redshift driver JAR
    that we downloaded from Redshift a while back. Click on OKÂ to complete the driver
    settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/73ecfa93-949c-4f41-9f72-33b834760c25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the driver in place, the final thing left to do is connect to the database
    and test it. For that, select the newly created Connection profileÂ from SQL Workbench/J
    and paste the copied database JDBC URLÂ in the URLÂ field as shown. Provide the
    database''s UsernameÂ and PasswordÂ as configured during the cluster''s setup. Additionally,
    ensure that the AutocommitÂ option is checked as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c3dba302-05a6-457f-bb9a-5465f415c94b.png)'
  prefs: []
  type: TYPE_IMG
- en: You can also test the connection by selecting the TestÂ option on the SQL Workbench/J
    screen. Once completed, click OKÂ to establish and open the SQL prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this step completed, you should have a running Redshift cluster connected
    to the SQL Workbench/J client as well. The next and final step left for us is
    to run a few sample queries and test the cluster's functionality, so let's get
    started with that right away!
  prefs: []
  type: TYPE_NORMAL
- en: Working with Redshift databases and tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start querying the Redshift database, we will first need to upload
    some same data to it. For this particular scenario, we are going to use a small
    subset of HTTP request logs that originated from a web server at the NASA Kennedy
    Space Center in Florida. This data is available for public use and can be downloaded
    from here: [http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The log file essentially contains the following set of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Host: The host that is making the web request to the web server. This field
    contains fully qualified hostnames or IP addresses as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timestamp: The timestamp of the particular web request. The format is `DAY
    MON DD HH:MM:SS YYYY`. This timestamp uses a 24-hour clock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Request: The method used to request the server (`GET`/`HEAD`/`POST`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: The URL of the resource that was requested by the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Response: This contains the HTTP response code (`200`, `302`, `304`, and `404`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bytes: The size of the reply in bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a snippet of the data for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can download the sample CSV file (2.14 MB containing 30,970 entries) used
    for this scenario using the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/yoyoclouds/Administering-AWS-Volume2.](https://github.com/yoyoclouds/Administering-AWS-Volume2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the file downloaded, all you need to do is upload it to one of your Amazon
    S3 buckets. Remember, that this bucket should be accessible by Amazon Redshift
    otherwise you may get a `S3ServiceException: Access Denied`Â exception during execution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, from the SQL Workbench/J client, type in the following code to create
    a new table within our Redshift database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can find the complete copy of the previous code at:Â [https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the Execute Query button. You should receive an output stating that
    the table is created, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/264efe17-0c6c-4b8d-bb87-952232c6d57a.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, use the `COPY`Â command to load the contents of the data file stored in
    Amazon S3 into the newly created Redshift table. The `COPY`Â command is a very
    versatile command and can be used to load data residing in Amazon S3, Amazon EMR,
    or even from an Amazon DynamoDB table into Amazon Redshift. To know more about
    the `COPY`Â command, navigate to this URL:Â [https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Substitute the values of `<REDSHIFT_TABLE_NAME>` with the name of the newly
    created table, the `<BUCKET_NAME>` with the name of the S3 bucket that contains
    the data file, and `<REDSHIFT_IAM_ROLE_ARN>` with the ARN of the IAM read-only
    access role that we created as a part of Amazon Redshift''s prerequisite process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the code is pasted into the SQL Workbench/J, click on the Execute Query
    button. Here is a snapshot of the command execution from SQL Workbench/J:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a1d3eec-116d-4eef-9599-e47775ab0769.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the data loaded, you can now use simple queries to query the dataset,
    as described in this section.Â The following command will list all 30,970 records
    from the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command will list only those records whose response value was
    `404`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command will list all the hosts that have requested for the particular
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the Redshift dashboard to view the performance and runtime
    of each individual query by first selecting your Redshift cluster nameÂ from the
    ClusterÂ page. Next, select the QueriesÂ tab to bring up the list of the most recently
    executed queries, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d7a32a0-9ba1-440d-87f7-650ef68454d6.png)'
  prefs: []
  type: TYPE_IMG
- en: You can drill down into each query by further selecting the *query identification
    number*Â as well.
  prefs: []
  type: TYPE_NORMAL
- en: Planning your next steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we conclude by summarizing the chapter, there are a few things I highly
    recommend that you try out with Amazon EMR, as well as with Amazon Redshift. First
    up, EMRFS.
  prefs: []
  type: TYPE_NORMAL
- en: We briefly touched upon the topic of EMRFS while deciding which filesystem to
    opt for when it comes to deploying the EMR Cluster. **EMR File System** (**EMRFS**)
    is an implementation of the traditional HDFS that allows for reading and writing
    files from Amazon EMR directly to Amazon S3\. This essentially allows you to leverage
    the consistency provided by S3, as well as some of its other feature sets, such
    as data encryption. To read more about EMRFS and how you can use it for your EMR
    clusters, visit:Â [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html).
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, Amazon EMR also provides an enterprise-grade Hadoop distribution in
    the form of MapR. The MapR distribution of Hadoop provides you with a plethora
    of features that enhances your overall experience when it comes to building distributed
    applications, as well as managing the overall Hadoop cluster. For example, selecting
    MapR as the Hadoop distribution provides support for industry-standard interfaces,
    such as NFS and ODBC, using which you can connect your EMR cluster with any major
    BI tool, including Tableau and Toad. MapR internally also provides built-in high
    availability, data protection, higher performances, and a whole list of additional
    features. You can read more about the MapR distribution for Hadoop at EMR at:Â [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-mapr.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-mapr.html).
  prefs: []
  type: TYPE_NORMAL
- en: Last but not the least, I would also recommend that you try out some of Amazon
    Redshift's advanced features in the form of reserved nodes and parameter groups.
    Parameter groups are essentially a group of parameters that are applied to the
    database when it is created. You can find the parameter group for your existing
    database by selecting the Parameter GroupÂ option from the Redshift's navigation
    pane. You can use and tweak these parameter groups based on your requirements
    to fine tune and customize the database. To know how to leverage parameter groups
    for your database tuning, visit:Â [https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-parameter-groups.html](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-parameter-groups.html).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well that brings us to the end of yet another amazing chapter. Let's quickly
    summarize what we have learnt so far!
  prefs: []
  type: TYPE_NORMAL
- en: To start off, we began by learning a bit about the various services offered
    by AWS for big data analytics followed by a quick getting started with Amazon
    EMR guide. We learnt about a few of Amazon EMR's concepts as well as launched
    our very first EMR cluster, as well. We also ran our first simple job on the EMR
    cluster and learnt how to monitor its performance using the likes of Amazon CloudWatch.
  prefs: []
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we got to know Amazon Redshift along with its
    core concepts and workings. We also created our first Redshift cluster, connected
    to it using an open source client and ran a couple of SQL queries, as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be learning and exploring yet another AWS service
    designed for data orchestration so stick around, we still have much to learn!
  prefs: []
  type: TYPE_NORMAL
