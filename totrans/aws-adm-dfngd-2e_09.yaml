- en: Powering Analytics Using Amazon EMR and Amazon Redshift
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon EMR 和 Amazon Redshift 强化分析功能
- en: 'In the previous chapter, we learned about two really useful services that developers
    can leverage to build highly scalable and decoupled applications in the cloud:
    Amazon SNS and Amazon SQS.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了两个非常有用的服务，开发人员可以利用这些服务在云中构建高度可扩展且解耦的应用程序：Amazon SNS 和 Amazon SQS。
- en: 'In this chapter, we will be turning things up a notch and exploring two amazingly
    powerful AWS services that are ideal for processing and running large-scale analytics
    and data warehousing in the cloud: Amazon EMR and Amazon Redshift.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提高一个层次，探索两个非常强大的 AWS 服务，这些服务非常适合在云中处理和运行大规模分析和数据仓储：Amazon EMR 和 Amazon
    Redshift。
- en: 'Keeping this in mind, let''s have a quick look at the various topics that we
    will be covering in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，让我们快速浏览一下本章将要涵盖的各个主题：
- en: Understanding the AWS analytics suite of services with an in-depth look at Amazon
    EMR, along with its use cases and benefits
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过深入了解 Amazon EMR、其用例和好处来理解 AWS 分析服务套件
- en: Introducing a few key EMR concepts and terminologies, along with a quick getting
    started tour
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍一些关键的 EMR 概念和术语，并进行快速入门指导
- en: Running a sample workload on EMR, using steps
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 EMR 上运行样本工作负载，使用步骤
- en: Introducing Amazon Redshift
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Amazon Redshift
- en: Getting started with an Amazon Redshift cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 Amazon Redshift 集群
- en: Working with Redshift databases and tables
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Redshift 数据库和表
- en: Loading data from Amazon EMR into Amazon Redshift
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Amazon EMR 加载数据到 Amazon Redshift
- en: So without any further ado, let's get started right away!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，事不宜迟，让我们立即开始吧！
- en: Understanding the AWS analytics suite of services
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 AWS 分析服务套件
- en: 'With the growth of big data and its adoption across organizations on the rise,
    many cloud providers today provide a plethora of services that are specifically
    designed to run massive computations and analytics on large volumes of data. AWS
    is one such cloud provider that also has invested a lot into the big data and
    analytics paradigm with a host of services offering ready-to-use frameworks, business
    insights and data warehousing solutions, as well. Here is a brief explanation
    of the AWS analytics suite of services:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据的增长以及各大组织对其的广泛应用，许多云服务提供商现在提供了大量专门设计用于在海量数据上进行大规模计算和分析的服务。AWS 就是其中之一，它也在大数据和分析范式上投入了大量资源，提供了一系列现成的框架、商业洞察和数据仓储解决方案。以下是
    AWS 分析服务套件的简要说明：
- en: '**Amazon EMR**: Amazon **Elastic MapReduce** or **EMR** is a quick and easy
    to use service that provides users with a scalable, managed Hadoop ecosystem and
    framework. You can leverage EMR to process vast amounts of data without having
    to worry about configuring the underlying Hadoop platform. We will be learning
    and exploring more on EMR in the subsequent sections of this chapter.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon EMR**：Amazon **Elastic MapReduce** 或 **EMR** 是一项快速且易于使用的服务，提供可扩展的托管
    Hadoop 生态系统和框架。你可以利用 EMR 处理大量数据，而无需担心配置底层的 Hadoop 平台。我们将在本章的后续部分中深入学习和探索 EMR。'
- en: '**Amazon Athena**: Amazon Athena takes big data processing up a notch by providing
    a standard SQL interface for querying data that is stored directly on Amazon S3\.
    With Athena, you do not have any underlying hardware to manage or maintain; it
    is all managed by AWS itself. This *serverless* approach makes Athena ideal for
    processing data that does not require any complex ETL processing. All you need
    to do is create a schema, point Athena to your data on Amazon S3, and start querying
    it using simple SQL syntax.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Athena**：Amazon Athena 通过提供标准 SQL 接口来查询直接存储在 Amazon S3 上的数据，提升了大数据处理能力。使用
    Athena 时，你不需要管理或维护任何底层硬件；这一切都由 AWS 管理。这种 *无服务器* 的方式使得 Athena 非常适合处理不需要复杂 ETL 处理的数据。你需要做的只是创建一个模式，将
    Athena 指向存储在 Amazon S3 上的数据，并使用简单的 SQL 语法开始查询。'
- en: '**Amazon Elasticsearch Service**: Amazon Elasticsearch Service provides a managed
    deployment of the popular open source search and analytics engine: Elasticsearch.
    This service comes in really handy when you wish to process streams of data originating
    from various sources such as logs generated from instances, and so on.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elasticsearch Service**：Amazon Elasticsearch Service 提供了流行的开源搜索和分析引擎
    Elasticsearch 的托管部署。当你希望处理来自各种来源（如实例生成的日志等）的数据流时，这项服务非常有用。'
- en: '**Amazon Kinesis**: Unlike the other services discussed so far, Amazon Kinesis
    is more of a streaming service provided by AWS. You can use Amazon Kinesis to
    push vast amounts of data originating from multiple sources, into one or more
    streams that can be consumed by other AWS services for performing analytics and
    other data processing processes.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Kinesis**：与之前讨论的其他服务不同，Amazon Kinesis 更像是一个由 AWS 提供的流处理服务。您可以使用 Amazon
    Kinesis 将来自多个源的大量数据推送到一个或多个数据流中，这些流可以被其他 AWS 服务消费，用于执行分析和其他数据处理操作。'
- en: '**Amazon QuickSight**: Amazon QuickSight is an extremely cost-effective business
    insights solution that can be used to perform fast ad hoc analysis on data.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon QuickSight**：Amazon QuickSight 是一个极具成本效益的商业洞察解决方案，可用于对数据进行快速的临时分析。'
- en: '**Amazon Redshift**: Amazon Redshift is a petabyte-scale data warehousing solution
    provided by AWS that you can leverage for analyzing your data, using an existing
    set of tools. We will be learning more about Redshift a bit later during this
    chapter. The services are depicted here:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Redshift**：Amazon Redshift 是 AWS 提供的 PB 级数据仓库解决方案，您可以利用它使用现有的工具集来分析数据。在本章稍后，我们将更详细地了解
    Redshift。以下是这些服务的示意图：'
- en: '![](img/18d0ba95-6fe1-473d-84e2-f3aab87c99c3.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18d0ba95-6fe1-473d-84e2-f3aab87c99c3.png)'
- en: '**AWS Data Pipeline:** Moving large amounts of data between AWS services can
    be difficult to perform, especially when the data sources vary. AWS Data Pipeline
    makes it easier to transfer data between different AWS storage and compute services,
    as well as helping in the initial transformation and processing of data. You can
    even use Data Pipeline to transfer data reliably from an on-premise location into
    AWS storage services, as well.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Data Pipeline**：在 AWS 服务之间迁移大量数据可能非常困难，尤其是在数据源各异的情况下。AWS Data Pipeline
    使得在不同的 AWS 存储和计算服务之间传输数据变得更加容易，同时也帮助进行数据的初步转换和处理。您甚至可以使用 Data Pipeline 将数据可靠地从本地环境传输到
    AWS 存储服务。'
- en: '**AWS Glue**: AWS Glue is a managed **ETL** (**Extract**, **Transform** and
    **Load**) service recently launched by AWS. Using AWS Glue greatly simplifies
    the process of preparing, extracting, and loading data from large datasets into
    an AWS storage service.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Glue**：AWS Glue 是 AWS 最近推出的托管**ETL**（**提取**、**转换**和**加载**）服务。使用 AWS Glue
    可以大大简化从大型数据集准备、提取和加载数据到 AWS 存储服务的过程。'
- en: With this brief overview of the AWS analytics suite of services, let's now move
    forward and get started with understanding a bit more about Amazon EMR!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对 AWS 分析服务套件的简要概述，我们现在可以继续并开始进一步了解 Amazon EMR！
- en: Introducing Amazon EMR
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Amazon EMR
- en: As mentioned earlier, Amazon EMR is a managed service that provides big data
    analytics frameworks, such as Apache Hadoop and Apache Spark straight out of the
    box and ready for use. Using Amazon EMR, you can easily perform a variety of use
    cases such as batch processing, big data analytics, low-latency querying, data
    streaming, or even use EMR as a large datastore itself!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Amazon EMR 是一种托管服务，提供开箱即用的大数据分析框架，如 Apache Hadoop 和 Apache Spark，并准备好供使用。通过使用
    Amazon EMR，您可以轻松执行各种用例，如批处理、大数据分析、低延迟查询、数据流处理，甚至将 EMR 作为一个大型数据存储来使用！
- en: 'With Amazon EMR, there is very little underlying infrastructure to manage on
    your part. You simply have to decide the number of instances you initially want
    to run your EMR cluster on and start consuming the framework for analytics and
    processing. Amazon EMR provides you with features that enable you to scale your
    infrastructure based on your requirements, without affecting the existing setups.
    Here is a brief look at some of the benefits that you can obtain by leveraging
    Amazon EMR for your own workloads:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Amazon EMR，您无需管理太多底层基础设施。您只需决定最初希望在哪些实例上运行您的 EMR 集群，并开始使用该框架进行分析和处理。Amazon
    EMR 为您提供了能够根据需求扩展基础设施的功能，而不影响现有设置。以下是您可以通过利用 Amazon EMR 处理您自己工作负载所获得的一些好处：
- en: '**Pricing**: Amazon EMR relies on EC2 instances to spin up your Apache Hadoop
    or Apache Spark clusters. Although you can vary costs by selecting the instance
    types for your cluster from large to extra large and so on, the best part of EMR
    is that you can also opt between using a combination of on-demand EC2 instances,
    reserved and spot instances based on your setup, thus providing you with flexibility
    at significantly lower costs.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定价**：Amazon EMR依赖于EC2实例来启动您的Apache Hadoop或Apache Spark集群。尽管您可以通过选择集群的实例类型（从大到超大等）来调整成本，但EMR的最大优势是，您还可以根据您的设置选择使用按需EC2实例、预留实例和竞价实例的组合，从而为您提供灵活性，并显著降低成本。'
- en: '**Scalability**: Amazon EMR provides you with a simple way of scaling running
    workloads, depending on their processing requirements. You can resize your cluster
    or its individual components as you see fit and additionally, configure one or
    more instance groups for a guaranteed instance availability and processing.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：Amazon EMR为您提供了一种简单的方式来根据工作负载的处理需求扩展运行中的工作负载。您可以根据需要调整集群或其各个组件的大小，并且可以配置一个或多个实例组，以保证实例的可用性和处理能力。'
- en: '**Reliability**: Although you, as an end user, have to specify the initial
    instances and their sizes, AWS ultimately ensures the reliability of the cluster
    by swapping out instances that either have failed or are going to in the due course
    of time.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：尽管作为最终用户，您需要指定初始实例及其大小，但AWS最终通过更换那些已失败或即将失败的实例来确保集群的可靠性。'
- en: '**Integration**: Amazon EMR integrates with the likes of other AWS services
    to provide your cluster with additional storage, network, and security requirements.
    You can use services such as Amazon S3 to store both the input as well as the
    output data, AWS CloudTrail for auditing the requests made to EMR, VPC to ensure
    the security of your launched EMR instances and much more!'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：Amazon EMR与其他AWS服务集成，为您的集群提供额外的存储、网络和安全要求。您可以使用诸如Amazon S3来存储输入和输出数据，AWS
    CloudTrail用于审计对EMR的请求，VPC确保您启动的EMR实例的安全性，等等！'
- en: With these details in mind, let's move an inch closer to launching our very
    own EMR cluster by first visiting some of its key concepts and terminologies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这些细节后，让我们通过先了解一些关键概念和术语，逐步迈向启动我们自己的EMR集群。
- en: Concepts and terminologies
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念和术语
- en: 'Before we get started with Amazon EMR, it is important to understand some of
    its key concepts and terminologies, starting out with clusters and nodes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用Amazon EMR之前，了解其一些关键概念和术语是很重要的，首先从集群和节点开始：
- en: '**Clusters**: Clusters are the core functioning component in Amazon EMR. A
    cluster is a group of EC2 instances that together can be used to process your
    workloads. Each instance within a cluster is termed as a node and each node has
    a different role to perform within the cluster.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群**：集群是Amazon EMR的核心功能组件。集群是由一组EC2实例组成的，这些实例可以一起用来处理您的工作负载。集群中的每个实例称为一个节点，每个节点在集群内有不同的角色。'
- en: '**Nodes**: Amazon EMR distinguishes between clusters instances by providing
    them with one of these three roles:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：Amazon EMR通过为集群实例提供以下三种角色之一来区分集群实例：'
- en: '**Master node**: An instance that is responsible for the overall manageability,
    working and monitoring of your cluster. The *master node* takes care of all the
    data and task distributions that occur within the cluster.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主节点**：负责集群的整体可管理性、工作和监控的实例。*主节点*负责处理集群内发生的所有数据和任务分配。'
- en: '**Core node**: The core nodes are very similar to the master node; however,
    they are primarily used to run tasks and store data on your **Hadoop Distributed
    File System** (**HDFS**). The core node can also contain some additional software
    components of Hadoop applications within itself.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心节点**：核心节点与主节点非常相似；然而，它们主要用于运行任务并在您的**Hadoop分布式文件系统**（**HDFS**）上存储数据。核心节点还可以包含一些Hadoop应用程序的额外软件组件。'
- en: '**Task node**: Task nodes are only designed to run tasks. They do not contain
    any additional software components of Hadoop applications within themselves and
    are optional when it comes to the cluster''s deployment.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务节点**：任务节点仅设计用于运行任务。它们不包含任何Hadoop应用程序的额外软件组件，在集群部署时是可选的。'
- en: '**Steps**: Steps are simple tasks or jobs that are submitted to a cluster for
    processing. Each step contains some instructions on how the particular job is
    to be performed. Steps can be ordered such that a particular step can be used
    to fetch the input data from Amazon S3, while a second step can be used to run
    a Pig or Hive query against it, and finally a third step to store output data
    to say Amazon DynamoDB. If one step fails, the subsequent steps are automatically
    cancelled from execution, however, you can choose to overwrite this behavior by
    selecting your steps to ignore failures and process further.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤**：步骤是提交到集群进行处理的简单任务或作业。每个步骤包含一些指示，说明如何执行特定的作业。步骤可以按顺序安排，例如，某个步骤可以用于从 Amazon
    S3 获取输入数据，而第二个步骤可以用于运行 Pig 或 Hive 查询，最后第三个步骤用于将输出数据存储到 Amazon DynamoDB。如果某个步骤失败，后续的步骤会自动取消执行，然而，您可以选择通过设置步骤忽略失败并继续处理来覆盖这一行为。'
- en: 'Apart from these concepts, you will additionally be required to brush up on
    your Apache Hadoop framework and terminologies, as well. Here''s a quick look
    at some of the Apache frameworks and applications that you will come across while
    working with Amazon EMR:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些概念，您还需要复习 Apache Hadoop 框架和术语。以下是一些在使用 Amazon EMR 时可能会遇到的 Apache 框架和应用程序的快速介绍：
- en: '**Storage**: A big part of EMR is how the data is actually stored and retrieved.
    The following are some of the storage options that are provided to you while using
    Amazon EMR:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**：EMR 的一个重要部分是数据如何实际存储和检索。以下是使用 Amazon EMR 时提供的一些存储选项：'
- en: '**Hadoop Distributed File System** (**HDFS**): As the name suggests, HDFS is
    a distributed and scalable filesystem that allows data to be stored across the
    underlying node instances. By default, the data is duplicated and stored across
    the instances present in the cluster. This provides high availability and data
    resiliency in case of an instance failure. You can read more about HDFS at: [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html).'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 分布式文件系统**（**HDFS**）：顾名思义，HDFS 是一个分布式和可扩展的文件系统，允许数据跨底层节点实例进行存储。默认情况下，数据会在集群中的实例间进行复制和存储。这提供了高可用性和数据弹性，以防实例发生故障。您可以在此处了解更多关于
    HDFS 的信息：[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)。'
- en: '**EMR File System** (**EMRFS**): EMRFS is an extension of the HDFS filesystem,
    using which you can access and store data directly on Amazon S3, just as a normal
    filesystem.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EMR 文件系统**（**EMRFS**）：EMRFS 是 HDFS 文件系统的扩展，您可以使用它直接在 Amazon S3 上访问和存储数据，就像使用普通文件系统一样。'
- en: '**Local filesystem**: Apart from HDFS, each instance within the cluster is
    also provided with a small block of pre-attached ephemeral disks which is also
    called the local filesystem. You can use this local filesystem to store additional
    software or applications required by your Hadoop frameworks.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地文件系统**：除了 HDFS，每个集群中的实例还会提供一个小块的预附加临时磁盘，这也被称为本地文件系统。您可以使用这个本地文件系统来存储 Hadoop
    框架所需的额外软件或应用程序。'
- en: '**Frameworks**: As mentioned before, Amazon EMR provides two data processing
    frameworks that you can leverage based on your processing needs: Apache Hadoop
    MapReduce and Apache Spark:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架**：如前所述，Amazon EMR 提供了两种数据处理框架，您可以根据处理需求选择使用：Apache Hadoop MapReduce 和
    Apache Spark：'
- en: '**Apache Hadoop MapReduce**: MapReduce is by far the most commonly used and
    widely known programming model when it comes to building distributed applications.
    The open source model relies on a `Mapper` function that maps the data to sets
    of key-value pairs and a `Reducer` function that combines these key-value pairs,
    applies some additional processing, and finally generates the desired output.
    To know more about MapReduce and how you can leverage it, check out this URL: [https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html).'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Hadoop MapReduce**：MapReduce 是迄今为止构建分布式应用程序时最常用和最广为人知的编程模型。开源模型依赖于一个
    `Mapper` 函数，该函数将数据映射为一组键值对，并且还有一个 `Reducer` 函数，它将这些键值对组合，应用一些额外的处理，最后生成所需的输出。要了解更多关于
    MapReduce 以及如何利用它的信息，请访问此 URL：[https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html)。'
- en: '**Apache Spark**: Apache Spark is a fast, in-memory data processing model using
    which a developer can process streaming, machine learning or SQL workloads that
    require fast iterative access to datasets. It is a cluster framework similar to
    Apache Hadoop; however, Spark leverages graphs and in-memory databases for accessing
    your data. You can read more about Spark at [https://spark.apache.org/](https://spark.apache.org/).'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark**：Apache Spark 是一种快速的内存数据处理模型，开发者可以使用它来处理需要快速迭代访问数据集的流处理、机器学习或
    SQL 工作负载。它是一个类似于 Apache Hadoop 的集群框架；然而，Spark 利用图形和内存数据库来访问数据。你可以在 [https://spark.apache.org/](https://spark.apache.org/)
    阅读更多关于 Spark 的信息。'
- en: '**Applications and programs**: With the standard data processing framework,
    Amazon EMR also provides you with additional applications and programs that you
    can leverage to build native distributed applications. Here''s a quick look into
    a couple of them:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序和程序**：借助标准数据处理框架，Amazon EMR 还提供了其他应用程序和程序，供你用于构建原生分布式应用程序。下面是其中几个的简要介绍：'
- en: '**YARN**: **Yet Another Resource Negotiator**, is a part of the Hadoop framework
    and provides management for your cluster''s data resources'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YARN**：**Yet Another Resource Negotiator**（另一个资源调度器），是 Hadoop 框架的一部分，提供集群数据资源的管理功能。'
- en: '**Hive**: Hive is a distributed data warehousing application that leverages
    standard SQL to query extremely large datasets stored on the HDFS filesystem.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hive**：Hive 是一个分布式数据仓库应用程序，利用标准 SQL 查询存储在 HDFS 文件系统中的超大数据集。'
- en: There are yet many other applications and programs made available for use by
    Amazon EMR, such as Apache Pig, Apache HBase, Apache Zookeeper, and so on. In
    the next section, we will be looking at how to leverage these concepts and terminologies
    to create our very own Amazon EMR Cluster, so let's get busy!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Amazon EMR 提供了许多其他应用程序和程序可供使用，例如 Apache Pig、Apache HBase、Apache Zookeeper
    等等。在接下来的部分，我们将探讨如何利用这些概念和术语来创建我们自己的 Amazon EMR 集群，快来开始吧！
- en: Getting started with Amazon EMR
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 Amazon EMR
- en: With the basics covered, in this section we will be working with the Amazon
    EMR dashboard to create our very first cluster. However, before we get going,
    here's a small list of prerequisite steps that we need to complete first.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础内容介绍完之后，在本节中，我们将使用 Amazon EMR 仪表板创建我们的第一个集群。然而，在开始之前，以下是我们需要首先完成的小列表前提步骤。
- en: 'To begin with, we will need to create an Amazon S3 bucket that will be used
    to store the output, logs generated by EMR, as well as some additional script
    and software files:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个 Amazon S3 存储桶，用于存储输出、EMR 生成的日志以及一些附加脚本和软件文件：
- en: From the AWS Management Console, filter and select the Amazon S3 service by
    using the Filter option. Alternatively, launch the Amazon S3 dashboard by navigating
    to this URL: [https://s3.console.aws.amazon.com/s3/](https://s3.console.aws.amazon.com/s3/).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 管理控制台中，使用“筛选”选项筛选并选择 Amazon S3 服务。或者，通过访问以下网址启动 Amazon S3 仪表板：[https://s3.console.aws.amazon.com/s3/](https://s3.console.aws.amazon.com/s3/)。
- en: Next, select the Create bucket option. In the Create bucket wizard, provide
    a suitable Bucket name followed by the selection of an appropriate Region to create
    the bucket in. For this use case, the EMR cluster, as well as the S3 buckets,
    are created in the **US East (Ohio)** region, however you can select an alternative
    based on your requirements. Click on Next to continue with the process.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择“创建存储桶”选项。在创建存储桶向导中，提供一个合适的存储桶名称，并选择一个适当的区域来创建存储桶。对于本案例，EMR 集群以及 S3 存储桶都创建在
    **美国东部（俄亥俄州）** 区域，不过你可以根据需求选择其他区域。点击“下一步”继续操作。
- en: On the Set properties page, you can optionally choose to provide some *tags* for
    your bucket for cost allocations and tracking purposes. Click Next to continue.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置属性页面，你可以选择为存储桶提供一些 *标签*，用于成本分配和跟踪目的。点击“下一步”继续。
- en: In the Set permissions page, ensure that the no public read access is granted
    to the bucket. Click on Next to review the settings and finally, select Create
    bucket to complete the process.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置权限页面，确保不授予存储桶公共读取权限。点击“下一步”以查看设置，最后选择“创建存储桶”以完成操作。
- en: 'Once the bucket is created, use the Create folder option to create dedicated
    folders for storing the logs, output, as well as some additional scripts that
    we might use in the near future. Here is a representational screenshot of the
    bucket after you have completed all of the previous steps:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建存储桶后，使用“创建文件夹”选项来为存储日志、输出以及未来可能使用的其他脚本创建专用文件夹。以下是完成所有前面步骤后存储桶的代表性截图：
- en: '![](img/e2af9cde-bd0b-4a0b-9f3d-bdd10f47210c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2af9cde-bd0b-4a0b-9f3d-bdd10f47210c.png)'
- en: With the bucket created and ready for use, the next prerequisite item left to
    create is a key pair using which you can SSH into your EC2 instances. Ensure that
    the key pair is created in the same region (**US East (Ohio)** in this case) as
    your EMR cluster.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在存储桶创建并准备好使用后，剩下的一个先决条件是创建一个密钥对，您可以使用它通过 SSH 访问 EC2 实例。确保密钥对是在与 EMR 集群相同的区域（此处为
    **美国东部（俄亥俄州）**）中创建的。
- en: Now that the prerequisites are out of the way, we can finally get started with
    our EMR cluster setup!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 既然先决条件已经处理完毕，我们终于可以开始设置 EMR 集群了！
- en: From the AWS Management Console, filter and select the Amazon EMR service by
    using the Filter option. Alternatively, launch the Amazon EMR dashboard by selecting
    this URL: [https://us-east-2.console.aws.amazon.com/elasticmapreduce/home](https://us-east-2.console.aws.amazon.com/elasticmapreduce/home).
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 AWS 管理控制台中，使用筛选选项筛选并选择 Amazon EMR 服务。或者，可以通过选择此 URL 启动 Amazon EMR 仪表盘：[https://us-east-2.console.aws.amazon.com/elasticmapreduce/home](https://us-east-2.console.aws.amazon.com/elasticmapreduce/home)。
- en: Since this is the first time we've created an EMR cluster, select the Create
    cluster option to get started.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是我们第一次创建 EMR 集群，请选择“创建集群”选项开始。
- en: 'You can configure your EMR cluster using two ways: a fast and easy Quick Options which
    is shown to you by default, and an Advanced options page where you can select
    and configure the individual items for your cluster. In this case, we will go
    ahead and select Go to advanced options.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过两种方式配置 EMR 集群：快速简便的快速选项（默认显示给您），以及高级选项页面，在该页面上您可以选择并配置集群的各个项。在这种情况下，我们将选择进入高级选项。
- en: The Advanced options page provides us with a four-step wizard that essentially
    guides us to configuring a fully functional EMR cluster. To begin with, the first
    step is where you can select and customize the *software* that you wish to install
    on your EMR cluster.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高级选项页面为我们提供了一个四步向导，基本上引导我们配置一个完全功能的 EMR 集群。首先，第一步是您可以选择并自定义要在 EMR 集群上安装的*软件*。
- en: From the Release drop-down list, select the appropriate EMR release that you
    would like to work with. The latest version released as of writing this book is
    `emr-5.11.1`. Each release contains several distributed applications available
    for installation on your cluster. For example, selecting emr-5.11.1 which is a
    2018 release, contains Hadoop v2.7.3, Flink v1.3.2, Ganglia v3.7.2, HBase v1.3.1,
    and many other such applications and software.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从版本下拉列表中，选择您希望使用的 EMR 版本。本书写作时发布的最新版本是 `emr-5.11.1`。每个版本包含几个可供安装在集群上的分布式应用程序。例如，选择
    emr-5.11.1（2018 年发布）包含 Hadoop v2.7.3、Flink v1.3.2、Ganglia v3.7.2、HBase v1.3.1
    以及其他许多应用程序和软件。
- en: For a complete list of available EMR releases and their associated software
    versions, go to [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可用的 EMR 版本及其相关软件版本的完整列表，请访问：[https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html)。
- en: In this case, I have gone ahead and selected the basic applications that we
    will be requiring for this scenario, including Hadoop, Hive and Hue. Feel free
    to select other applications as per your requirements.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，我已经选择了我们在此场景中所需的基本应用程序，包括 Hadoop、Hive 和 Hue。根据您的需求，可以选择其他应用程序。
- en: 'The next couple of sections are optional, however, it is important to know
    their purpose:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的几个部分是可选的，但了解它们的目的非常重要：
- en: '**AWS Glue Data Catalog settings**: With EMR version 5.8.0 and above, you optionally
    have the choice to configure Spark SQL to use the AWS Glue Data Catalog (an external
    Hive table) as its metastore.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Glue 数据目录设置**：在 EMR 版本 5.8.0 及以上版本中，您可以选择配置 Spark SQL 使用 AWS Glue 数据目录（外部
    Hive 表）作为其元数据存储。'
- en: '**Edit software settings**: You can use this option to override the default
    configuration settings for certain applications. This is achieved by providing
    a configuration object in the form of a JSON file. You can either Enter configuration or
    Load JSON from S3 as well:'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编辑软件设置**：您可以使用此选项覆盖某些应用程序的默认配置设置。通过提供一个 JSON 文件形式的配置对象来实现这一点。您可以选择输入配置或从
    S3 加载 JSON 文件：'
- en: '![](img/bc0d8804-5937-4c76-8614-3a2621dea03f.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc0d8804-5937-4c76-8614-3a2621dea03f.png)'
- en: '**Add steps**: The final optional parameter left on the Software Configuration page
    is the *add steps*. As discussed briefly earlier in this chapter, steps are essentially
    a unit of work that we submit to the cluster. This can be something as trivial
    as loading input data from S3, or processing and running a MapReduce job on the
    data. We will be exploring steps a little more in detail a bit later in this chapter,
    so leave this field to its default value and select Next to continue with the
    process.'
  id: totrans-74
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加步骤**：软件配置页面上的最后一个可选参数是*添加步骤*。如本章前面简要讨论的那样，步骤本质上是我们提交给集群的工作单元。它可以是从 S3 加载输入数据，或者处理并运行
    MapReduce 作业在数据上。我们将在本章稍后更详细地探索步骤，因此请将此字段保持默认值，并选择“下一步”继续流程。'
- en: The second step in the Advanced options wizard is configuring the cluster's
    hardware, or the instance configurations, as well as the cluster's networking.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高级选项向导的第二步是配置集群的硬件，或实例配置，以及集群的网络设置。
- en: 'EMR provides two options: instance fleets and instance groups; both explained
    briefly here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: EMR 提供两种选项：实例舰队和实例组；下面简要解释：
- en: '**Instance fleets**: Instance fleets allows you to specify a target capacity
    for the instances present in a cluster. With this option, you get the widest variety
    of instance provisioning options where you can leverage mixed instance types for
    your nodes, and even go for different purchasing options for the same. With each
    instance fleet that created, you get to establish a target capacity for on-demand,
    as well as for spot instances.'
  id: totrans-77
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例舰队**：实例舰队允许您为集群中存在的实例指定目标容量。通过此选项，您可以获得最多样化的实例配置选项，可以为节点利用混合实例类型，甚至为同一节点选择不同的购买选项。每创建一个实例舰队，您可以为按需实例和竞价实例设定目标容量。'
- en: You can have only one instance fleet per node type (master, core, task).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点类型（主节点、核心节点、任务节点）只能有一个实例舰队。
- en: '**Instance groups**: Instance groups on the other hand do not offer many custom
    configurable options per node type. In instance groups, each node consists of
    the same instance type and the same purchasing option, as well. Once these settings
    are configured during the cluster''s creation, they cannot be altered; however,
    you can always add more instances as you see fit.'
  id: totrans-79
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例组**：另一方面，实例组每个节点类型的可自定义配置选项较少。在实例组中，每个节点由相同的实例类型和相同的购买选项组成。集群创建过程中配置了这些设置后，无法更改；不过，您可以随时根据需要添加更多实例。'
- en: 'For this particular use case, we are going to go ahead and select Uniform instance
    groups, as depicted in the following screenshot:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个特定的使用案例，我们将选择统一实例组，如下图所示：
- en: '![](img/6a18bcf6-788e-4f31-84a4-b95cef2065d8.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a18bcf6-788e-4f31-84a4-b95cef2065d8.png)'
- en: Next, from the Network drop-down list, select the appropriate *VPC* in which
    you wish to launch your EMR cluster. You can alternatively choose to create a
    new VPC specifically for EMR, using the adjoining Create a VPC option.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从网络下拉列表中选择您希望启动 EMR 集群的适当*VPC*。您也可以选择使用旁边的“创建 VPC”选项为 EMR 创建一个新的 VPC。
- en: Similarly, select the appropriate subnet from the EC2 Subnet drop-down list.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，从 EC2 子网下拉列表中选择适当的子网。
- en: Finally, assign a value for the Root device EBS volume size that will be provisioned
    for each instance in the cluster. You can provide values between 10 GB and 100
    GB.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为集群中每个实例分配将要配置的根设备 EBS 卷的大小。您可以提供介于 10 GB 和 100 GB 之间的值。
- en: 'Using the edit options provided, you can additionally configure the Instance
    type, the Instance count as well as the Purchasing option for each node type,
    as depicted in the following screenshot. Note that these options are provided
    because we selected instance groups as our preferred mode of instance configurations.
    The options will vary if the Instance Fleet option is selected:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提供的编辑选项，您还可以为每个节点类型配置实例类型、实例数量以及购买选项，如下图所示。请注意，这些选项是因为我们选择了实例组作为首选的实例配置模式。如果选择了实例舰队选项，选项将会有所不同：
- en: '![](img/ee8245bb-2d24-4907-be69-600417084e67.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee8245bb-2d24-4907-be69-600417084e67.png)'
- en: You can additionally choose to enable autoscaling for the Core and Task nodes
    by selecting the Not enabled option under the Auto scaling column. Subsequently,
    you can add additional task instance groups by selecting the Add task instance
    group option, as well. Once done, select the Next option to proceed with the set
    up.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以选择启用核心和任务节点的自动扩展，通过在自动扩展列下选择“未启用”选项。随后，您也可以选择“添加任务实例组”选项来添加额外的任务实例组。完成后，选择“下一步”选项继续设置。
- en: 'The third step in the Advanced options provides general configurations that
    you can set, based on your requirements. To start off, provide a suitable Cluster
    name followed by selecting the Logging option for your EMR cluster. Use the folder
    option to browse to our newly created S3 bucket, as shown in the following screenshot:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高级选项中的第三步提供了您可以根据需求设置的一般配置。首先，提供一个合适的集群名称，然后选择“日志记录”选项来为您的 EMR 集群设置日志记录。使用文件夹选项浏览到我们新创建的
    S3 存储桶，如下图所示：
- en: '![](img/aa154fd4-d9ef-41bd-9df1-130767cb5bb3.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa154fd4-d9ef-41bd-9df1-130767cb5bb3.png)'
- en: You can additionally enable the Termination protection option to prevent against
    accidental deletions of your cluster.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以启用“终止保护”选项，以防止集群被意外删除。
- en: Moving on, the final configuration item left on the cluster's General Options page
    is the Bootstrap Actions. Bootstrap actions as the name implies are certain scripts
    or code that you wish to execute on your cluster's instances at the time of booting
    up. This feature thus comes in very handy when you have to add new instances to
    an existing running cluster.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，集群“常规选项”页面上的最后一个配置项是引导操作。顾名思义，引导操作是您希望在集群实例启动时执行的脚本或代码。因此，当您需要向现有运行中的集群添加新实例时，这个功能非常有用。
- en: Bootstrap actions are executed using the Hadoop user by default. You can switch
    to root privileges by using the `sudo` command.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，引导操作是使用 Hadoop 用户执行的。您可以通过使用`sudo`命令切换到 root 权限。
- en: 'There are two types of Bootstrap actions that you can execute on your instances:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在实例上执行两种类型的引导操作：
- en: 'Run if: The Run if action executes an action when an *instance-specific* value
    is found in either the `instance.json` or the `job-flow.json` file. This is a
    predefined bootstrap action and comes in very handy when you only want to execute
    the action on a particular type of instance, for example, execute the bootstrap
    action only if the instance type is `master`.'
  id: totrans-94
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时：运行时操作会在`instance.json`或`job-flow.json`文件中找到与*实例特定*相关的值时执行操作。这是一个预定义的引导操作，当您只希望在特定类型的实例上执行该操作时非常有用，例如，仅当实例类型为`master`时才执行引导操作。
- en: 'Custom action: Custom actions leverage your own scripts to perform a customized
    bootstrap action.'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义操作：自定义操作利用您自己的脚本来执行定制的引导操作。
- en: To create a bootstrap action, select the Configure and add option from the Add
    Bootstrap Action. Make sure the Run if action is selected before proceeding.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建引导操作，从“添加引导操作”中选择“配置并添加”选项。确保在继续之前选择了“运行时”操作。
- en: 'This will bring up the Add Bootstrap Action dialog as depicted in the following
    screenshot. Type in a suitable Name for your Run if action. Since the Run if action
    is a predefined bootstrap action, the script''s location is not an editable field.
    You can, however, add Optional arguments for the script, as shown here. In this
    case, the Run if action will only echo the message if the instance is a **master**:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将弹出“添加引导操作”对话框，如下图所示。为您的运行时操作输入一个合适的名称。由于运行时操作是预定义的引导操作，因此脚本的位置不是可编辑字段。然而，您可以为脚本添加可选参数，如下所示。在此案例中，只有当实例是**主节点**时，运行时操作才会回显消息：
- en: '![](img/9d1acc81-06a7-49d4-acbc-861e2219d5cd.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d1acc81-06a7-49d4-acbc-861e2219d5cd.png)'
- en: Click on Add once done. Similarly, you can add your custom bootstrap actions
    as well, by placing the executable scripts in the Amazon S3 bucket that we created
    during the prerequisite phase of this chapter and providing that path here.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后点击“添加”。类似地，您也可以通过将可执行脚本放置在我们在本章前置阶段创建的 Amazon S3 存储桶中，并提供该路径来添加自定义引导操作。
- en: Moving on to the final step in this cluster creation process, on the Security
    Options page, you can review the various permissions, roles, authentication, and
    encryption settings that the cluster will use once it's deployed. Start off by
    selecting the EC2 key pair that we created at the start of this chapter. You can
    additionally opt to change the Permissions or use the default ones provided.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once done, click on Create cluster to complete the process.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster's creation takes a couple of minutes, depending on the number of
    instances selected for the cluster, as well as the software identified to be installed.
    Once done, you can use the EMR dashboard to view the cluster's health status and
    other vital information.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to your EMR cluster
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have provisioned the EMR cluster, you should see its state change from
    Starting to Bootstrapping to finally into a Running state. If you do not have
    any jobs currently executing, then your cluster may go into a Waiting state as
    well. Here, you can now start using the EMR cluster for running your various jobs
    and analysis. But before that, here's a quick introduction of a few ways in which
    you can connect to your running EMR cluster.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'First up, connecting to the master node using a simple SSH. Connecting to the
    master node via SSH can be used for monitoring the cluster, viewing Hadoop''s
    log flies or for even running an interactive shell for Hive or Pig programming:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: To do so, log in to your Amazon EMR dashboard and select your newly created
    cluster's name from the Cluster list page. This will display the clusters Details page
    where you can manage, as well as monitor your cluster.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, copy the Master public DNS address. Once copied, open up a PuTTY Terminal
    and paste the copied public DNS in the Host Name (or IP Address) field.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the key pair that you associated with this EMR cluster into a private
    key and attach that private key in PuTTY by selecting the Auth option present
    under the SSH section.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once done, click on Open to establish the connection. At the certificate dialog,
    accept the certificate and type in `Hadoop` as the username when prompted. You
    should get SSH access into your cluster's master node now!
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The same task can be performed using the AWS CLI as well:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Terminal, first type in the following command to retrieve the running
    cluster''s ID. The cluster''s ID will be in this format `j-XXXXXXXX`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To list the instances running in your cluster, use the cluster ID obtained
    from the previous command''s output in the following command:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Copy the `PublicDnsName` value from the output of this command. You can then
    use the following set of commands to get access to your master node.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that the cluster''s private key has the necessary permissions:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once done, SSH to the master node using the following command:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can additionally connect to the various application web interfaces, such
    as *Hue* or the *Hadoop HDFS NameNode,* using a few simple steps:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: To get started, you will once again require the public DNS name of your master
    node. You can obtain that from the EMR dashboard or by using the CLI steps we
    just walked through.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, using PuTTY , paste the public DNS name in the Host Name (or IP Address) field
    as done earlier. Browse and load the private key using the Auth option as well.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the SSH option from PuTTY's navigation pane, select Tunnels.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fill in the required details as mentioned in the following list:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set source port field to `8157`
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable the Dynamic and Auto options
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once completed, select Add and finally Open the connection.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This form of tunnelling or port forwarding is essential as the web interfaces
    can only be viewed from the master node''s local web server. Once completed, launch
    your favorite browser and view the respective web interfaces, as given here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'For accessing Hue, type in the following in your web browser:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For accessing the Hadoop HDFS NameNode, type in the following:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/cad43e66-7ae7-415f-a84f-50ece801869f.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'You can even use the CLI to create a tunnel. To do so, substitute the public
    DNS name and the private key values in the following command:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `-D` flag indicates that the port forwarding is dynamic.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Running a job on the cluster
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the connectivity established, you can now execute jobs as one or more
    steps on your cluster. In this section, we will be demonstrating the working of
    a step using a simple example which involves the processing of a few Amazon CloudFront
    logs. The details of the sample data and script can be found at: [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-prepare-data-and-script.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs-prepare-data-and-script.html).
    You can use similar techniques and bases to create and execute your own jobs as
    well:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: To get started with a job, from the EMR dashboard select your cluster's name
    from the Cluster list page. This will bring up the newly created clusters details
    page. Here, select the Steps tab.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since this is going to be our first step, go ahead and click on the Add step option.
    This brings up the Add step dialog as shown in the following screenshot. Fill
    in the required information as described and, once all the fields are filled in,
    click on Add to complete the step''s creation:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/389a4fb3-1fe5-4c5b-a0ab-0d1baaaa8466.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Step type: You can choose between various options such as Streaming program which
    essentially will prompt you to provide `Mapper` and `Reducer` function details,
    or alternatively, you can also select Hive program, Pig program, Spark program or
    a Custom application. In this case, we select the Hive program option.'
  id: totrans-142
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Name: A suitable name for your step.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Script S3 location: Provide the Hive script''s location here. Since we are
    using a predefined script, simply replace the `<REGION>` field with your EMR''s
    operating region: `s3://<REGION>.elasticmapreduce.samples/cloudfront/code/Hive_CloudFront.q`.'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input S3 location: Provide the input data file''s location here. Replace the
    `<REGION>` placeholder with your EMR''s operating region as done before: `s3://<REGION>.elasticmapreduce.samples`.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output S3 location: Specify where the processed output files have to be stored.
    In this case, I''m using the custom S3 bucket that we created as a prerequisite
    step during the EMR cluster creation. You can provide any other alternative bucket
    as well.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arguments: You can use this field to provide any optional arguments required
    by the script to run. In this case, copy, and paste the following `-hiveconf hive.support.sql11.reserved.keywords=false`.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action on failure: You can optionally choose what EMR should do in case the
    step''s execution undergoes a failure. In this case, we have selected the default
    Continue value.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the required fields are filled out, click on Add to complete the process.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The step now starts executing the supplied script on the EMR cluster. You can
    view the progress by viewing the changes in the step''s status from Pending to
    Running to Completed, as shown in the following screenshot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff0fe102-f8c0-4afe-b976-dbbe1629343a.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Once the job completes its execution, head back to your Amazon S3's output bucket
    and view the output of the processing. In this case, the output contains the number
    of access requests made to CloudFront, sorted by the operating system.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring EMR clusters
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EMR dashboard provides a rich feature set using which you can manage and
    monitor your EMR clusters all from one place. You can additionally view logs and
    leverage Amazon CloudWatch as well to track the performance of your cluster.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will be looking at a few simple ways using which you can
    monitor your EMR clusters. To start off, let''s look at how to monitor the status
    of your cluster using the EMR dashboard:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'From the EMR dashboard, select your cluster name from the cluster list page.
    This will bring up the newly created cluster''s details page. Here, select the
    Events tab, as shown in the following screenshot:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1dc7acf7-44de-4553-ab9f-c7a2326fff89.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: The Events tab allows you to view the event logged by your cluster. You can
    use this to view events generated by the cluster, by running applications, by
    step execution and much more.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard also provides an in-depth look into the performance of the cluster
    over a period. To view the performance indicators, select the Monitoring tab from
    the cluster's Details page.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, you can view essential details and status about your cluster, the running
    nodes, as well as the underlying I/O and data storage.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can also use Amazon CloudWatch to view and monitor the cluster's
    various metrics. To do so, launch the Amazon CloudWatch dashboard by selecting
    this URL: [https://console.aws.amazon.com/cloudwatch/home](https://console.aws.amazon.com/cloudwatch/home).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, from the navigation pane, select the Metrics option to view all the metrics
    associated with EMR. Use the `JobFlowID` dimension to filter the EMR cluster in
    case you have multiple clusters running in the same environment.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a list of some important EMR metrics worth monitoring:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric name** | **Metric description** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| `AppsFailed` | The number of applications submitted to the EMR cluster that
    have failed to complete. This application status is monitored internally and reported
    by YARN. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| `MRUnhealthyNodes` | The number of nodes available to MapReduce jobs marked
    in an `UNHEALTHY` state. |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| `MRLostNodes` | The number of nodes allocated to MapReduce that have been
    marked in a `LOST` state. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| `CorruptBlocks` | The number of blocks that HDFS reports as corrupted. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: You can view the complete list of monitored metrics at: [https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Once a Metric is identified, select the Metric and click on the Graphed metrics tab.
    Here, select the Create alarm option provided under the Actions column to create
    and set an alarm threshold, as well as its corresponding action.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this way, you can also leverage Amazon CloudWatch events to periodically
    monitor the events generated by the cluster. Remember, however, that EMR tracks
    and records events only for a period of seven days. With this, we come to the
    end of this particular section and EMR, as well. In the next section, we will
    be learning and exploring a bit about yet another awesome analytics service called
    Amazon Redshift!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Amazon Redshift
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon Redshift is one of the **database as a service** (**DBaaS**) offerings
    from AWS that provides a massively scalable data warehouse as a managed service,
    at significantly lower costs. The data warehouse is based on the open source PostgreSQL
    database technology however; not all features offered in PostgreSQL are present
    in Amazon Redshift. Here''s a look at some of the essential concepts and terminologies
    that you ought to keep in mind when working with Amazon Redshift:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '**Clusters**: Just like Amazon EMR, Amazon Redshift too relies on the concept
    of clusters. Clusters here are logical containers containing one or more instances
    or compute nodes, and one leader node that is responsible for the cluster''s overall
    management. Here''s a brief look at what each node provides:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leader node**: The leader node is a single node present in a cluster that
    is responsible for orchestrating and executing various database operations, as
    well as facilitating communication between the database and associate client programs.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute node**: Compute nodes are responsible for executing the code provided
    by the leader node. Once executed, the compute nodes share the results back to
    the leader node for aggregation. Amazon Redshift supports two types of compute
    nodes: dense storage nodes and dense compute nodes. The dense storage nodes provide
    standard hard disk drives for creating large data warehouses; whereas, the dense
    compute nodes provide higher performance SSDs. You can start off by using a single
    node that provides 160 GB of storage and scale up to petabytes by leveraging one
    or more 16 TB capacity instances as well.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node slices**: Each compute node is partitioned into one or more smaller
    chunks or slices by the leader node, based on the cluster''s initial size. Each
    slice contains a portion of the compute nodes memory, CPU and disk resource, and
    uses these resources to process certain workloads that are assigned to it. The
    assignment of workloads is again performed by the leader node.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Databases**: As mentioned earlier, Amazon Redshift provides a scalable database
    that you can leverage for a data warehouse, as well as analytical purposes. With
    each cluster that you spin in Redshift, you can create one or more associated
    databases with it. The database is based on the open source relational database
    PostgreSQL (v8.0.2) and thus, can be used in conjunction with other RDBMS tools
    and functionalities. Applications and clients can communicate with the database
    using standard PostgreSQL JDBC and ODBC drivers.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a representational image of a working data warehouse cluster powered
    by Amazon Redshift:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c270279-61dc-49d6-8ebd-1027dd8c20fa.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: With this basic information in mind, let's look at some simple and easy to follow
    steps using which you can set up and get started with your Amazon Redshift cluster.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Amazon Redshift
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be looking at a few simple steps which you can take
    to have a fully functioning Amazon Redshift cluster up and running in a matter
    of minutes:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: First up, we have a few prerequisite steps that need to be completed before
    we begin with the actual set up of the Redshift cluster. From the AWS Management
    Console, use the Filter option to filter out IAM. Alternatively, you can also
    launch the IAM dashboard by selecting this URL: [https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/).
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once logged in, we need to create and assign a role that will grant our Redshift
    cluster read-only access to Amazon S3 buckets. This role will come in handy later
    on in this chapter when we load some sample data on an Amazon S3 bucket and use
    Amazon Redshift's `COPY` command to copy the data locally into the Redshift cluster
    for processing. To create the custom role, select the Role option from the IAM
    dashboards' navigation pane.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the Roles page, select the Create role option. This will bring up a simple
    wizard using which we will create and associate the required permissions to our
    role.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Redshift option from under the AWS Service group section and opt
    for the Redshift - Customizable option provided under the Select your use case field.
    Click Next to proceed with the set up.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the Attach permissions policies page, filter and select the AmazonS3ReadOnlyAccess permission.
    Once done, select Next: Review.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the final Review page, type in a suitable name for the role and select the
    Create Role option to complete the process. Make a note of the role''s ARN as
    we will be requiring this in the later steps. Here is snippet of the role policy
    for your reference:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the role created, we can now move on to creating the Redshift cluster.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: To do so, log in to the AWS Management Console and use the Filter option to
    filter out Amazon Redshift. Alternatively, you can also launch the Redshift dashboard
    by selecting this URL: [https://console.aws.amazon.com/redshift/](https://console.aws.amazon.com/redshift/).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Launch Cluster to get started with the process.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, on the CLUSTER DETAILS page, fill in the required information pertaining
    to your cluster as mentioned in the following list:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cluster identifier: A suitable name for your new Redshift cluster. Note that
    this name only supports *lowercase* strings.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Database name: A suitable name for your Redshift database. You can always create
    more databases within a single Redshift cluster at a later stage. By default,
    a database named `dev` is created if no value is provided:'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/65d00270-7593-4bd4-93a9-0cf6670605a0.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: 'Database port: The port number on which the database will accept connections.
    By default, the value is set to `5439,` however you can change this value based
    on your security requirements.'
  id: totrans-198
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Master user name: Provide a suitable username for accessing the database.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Master user password: Type in a strong password with at least one uppercase
    character, one lowercase character and one numeric value. Confirm the password
    by retyping it in the Confirm password field.'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once completed, hit Continue to move on to the next step of the wizard.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the NODE CONFIGURATION page, select the appropriate Node type for your cluster,
    as well as the Cluster type based on your functional requirements. Since this
    particular cluster setup is for demonstration purposes, I've opted to select the
    dc2.large as the Node type and a Single Node deployment with *1* compute node.
    Click Continue to move on the next page once done.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is important to note here that the cluster that you are about to launch will
    be live and not running in a sandbox-like environment. As a result, you will incur
    the standard Amazon Redshift usage fees for the cluster until you delete it. You
    can read more about Redshift's pricing at: [https://aws.amazon.com/redshift/pricing/](https://aws.amazon.com/redshift/pricing/).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In the ADDITIONAL CONFIGURATION page, you can configure add-on settings, such
    as encryption enablement, selecting the default VPC for your cluster, whether
    or not the cluster should have direct internet access, as well as any preferences
    for a particular Availability Zone out of which the cluster should operate. Most
    of these settings do not require any changes at the moment and can be left to
    their default values.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The only changes required on this page is associating the previously created
    IAM role with the cluster. To do so, from the Available Roles drop-down list,
    select the custom Redshift role that we created in our prerequisite section. Once
    completed, click on Continue.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the settings and changes on the Review page and select the Launch Cluster option
    when completed.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cluster takes a few minutes to spin up depending on whether or not you
    have opted for a single instance deployment or multiple instances. Once completed,
    you should see your cluster listed on the Clusters page, as shown in the following
    screenshot. Ensure that the status of your cluster is shown as healthy under the
    DB Health column. You can additionally make a note of the cluster''s endpoint
    as well, for accessing it programmatically:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb5c62d5-ac0c-4a6f-a546-ce35680aca42.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: With the cluster all set up, the next thing to do is connect to the same. In
    the next section, we will be looking at a few simple steps you can take to connect
    to your newly deployed Redshift cluster.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to your Redshift cluster
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use a number of tools to connect to your Redshift cluster once its up
    and running. Most of these tools are PostgreSQL compliant and easily available
    off the shelf. In this case, we are going to install and use an open source SQL
    client tool called **SQL Workbench/J**.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, you will need to have Java runtime installed on your local workstation.
    The Java runtime version will have to *match* the requirements of SQL Workbench/J,
    otherwise it simply won''t work. You can check the version of the installed Java
    runtime on your local desktop by either locating the Java configuration on the
    Control Panel or by typing in the following command in a Terminal if you are working
    with a Linux distribution:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this case, we are using a simple Windows desktop for installing SQL Workbench/J.
    Download the correct version of the software from here: [http://www.sql-workbench.net/downloads.html](http://www.sql-workbench.net/downloads.html).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'With the software downloaded, the installation is pretty straightforward. Accept
    the end user license agreement, select a path for the software''s installation
    and that''s it! You should have the SQL Workbench/J up and running now:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'To connect SQL Workbench/J with your Redshift cluster, you will need your newly
    created database''s JDBC URL. You can copy it by selecting the Connect client option
    from Redshift''s navigation pane and selecting your newly deployed cluster from
    the Get cluster connection URL section, as shown in the following screenshot:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e45e8ab8-5527-4152-9b9b-54b9f1f3ee12.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: You will also need to download the correct version of the associated Amazon
    Redshift JDBC Driver JAR using the same page as well.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once completed, from the SQL Workbench/J client, select File, followed by the
    Connect window option.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, click on Create a new connection profile to get started. This will pop
    up a New profile box where you will need to enter a name for this new profile.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the profile is created; select the Manage drivers option. This will display
    the Manage drivers dialog box, as shown in the following screenshot. Select the Amazon
    Redshift option and provide a suitable Name for your connection driver, as well.
    Click on the browse icon and select the downloaded Amazon Redshift driver JAR
    that we downloaded from Redshift a while back. Click on OK to complete the driver
    settings:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/73ecfa93-949c-4f41-9f72-33b834760c25.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'With the driver in place, the final thing left to do is connect to the database
    and test it. For that, select the newly created Connection profile from SQL Workbench/J
    and paste the copied database JDBC URL in the URL field as shown. Provide the
    database''s Username and Password as configured during the cluster''s setup. Additionally,
    ensure that the Autocommit option is checked as shown here:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c3dba302-05a6-457f-bb9a-5465f415c94b.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: You can also test the connection by selecting the Test option on the SQL Workbench/J
    screen. Once completed, click OK to establish and open the SQL prompt.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this step completed, you should have a running Redshift cluster connected
    to the SQL Workbench/J client as well. The next and final step left for us is
    to run a few sample queries and test the cluster's functionality, so let's get
    started with that right away!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Working with Redshift databases and tables
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start querying the Redshift database, we will first need to upload
    some same data to it. For this particular scenario, we are going to use a small
    subset of HTTP request logs that originated from a web server at the NASA Kennedy
    Space Center in Florida. This data is available for public use and can be downloaded
    from here: [http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'The log file essentially contains the following set of columns:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Host: The host that is making the web request to the web server. This field
    contains fully qualified hostnames or IP addresses as well.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timestamp: The timestamp of the particular web request. The format is `DAY
    MON DD HH:MM:SS YYYY`. This timestamp uses a 24-hour clock.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Request: The method used to request the server (`GET`/`HEAD`/`POST`).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: The URL of the resource that was requested by the client.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Response: This contains the HTTP response code (`200`, `302`, `304`, and `404`).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bytes: The size of the reply in bytes.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a snippet of the data for your reference:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can download the sample CSV file (2.14 MB containing 30,970 entries) used
    for this scenario using the following link:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/yoyoclouds/Administering-AWS-Volume2.](https://github.com/yoyoclouds/Administering-AWS-Volume2)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'With the file downloaded, all you need to do is upload it to one of your Amazon
    S3 buckets. Remember, that this bucket should be accessible by Amazon Redshift
    otherwise you may get a `S3ServiceException: Access Denied` exception during execution.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, from the SQL Workbench/J client, type in the following code to create
    a new table within our Redshift database:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can find the complete copy of the previous code at: [https://github.com/yoyoclouds/Administering-AWS-Volume2](https://github.com/yoyoclouds/Administering-AWS-Volume2).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the Execute Query button. You should receive an output stating that
    the table is created, as shown in the following screenshot:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/264efe17-0c6c-4b8d-bb87-952232c6d57a.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: Next, use the `COPY` command to load the contents of the data file stored in
    Amazon S3 into the newly created Redshift table. The `COPY` command is a very
    versatile command and can be used to load data residing in Amazon S3, Amazon EMR,
    or even from an Amazon DynamoDB table into Amazon Redshift. To know more about
    the `COPY` command, navigate to this URL: [https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Substitute the values of `<REDSHIFT_TABLE_NAME>` with the name of the newly
    created table, the `<BUCKET_NAME>` with the name of the S3 bucket that contains
    the data file, and `<REDSHIFT_IAM_ROLE_ARN>` with the ARN of the IAM read-only
    access role that we created as a part of Amazon Redshift''s prerequisite process:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once the code is pasted into the SQL Workbench/J, click on the Execute Query
    button. Here is a snapshot of the command execution from SQL Workbench/J:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a1d3eec-116d-4eef-9599-e47775ab0769.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: 'With the data loaded, you can now use simple queries to query the dataset,
    as described in this section. The following command will list all 30,970 records
    from the table:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following command will list only those records whose response value was
    `404`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following command will list all the hosts that have requested for the particular
    resource:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can also use the Redshift dashboard to view the performance and runtime
    of each individual query by first selecting your Redshift cluster name from the
    Cluster page. Next, select the Queries tab to bring up the list of the most recently
    executed queries, as shown in the following screenshot:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d7a32a0-9ba1-440d-87f7-650ef68454d6.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: You can drill down into each query by further selecting the *query identification
    number* as well.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Planning your next steps
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we conclude by summarizing the chapter, there are a few things I highly
    recommend that you try out with Amazon EMR, as well as with Amazon Redshift. First
    up, EMRFS.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: We briefly touched upon the topic of EMRFS while deciding which filesystem to
    opt for when it comes to deploying the EMR Cluster. **EMR File System** (**EMRFS**)
    is an implementation of the traditional HDFS that allows for reading and writing
    files from Amazon EMR directly to Amazon S3\. This essentially allows you to leverage
    the consistency provided by S3, as well as some of its other feature sets, such
    as data encryption. To read more about EMRFS and how you can use it for your EMR
    clusters, visit: [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, Amazon EMR also provides an enterprise-grade Hadoop distribution in
    the form of MapR. The MapR distribution of Hadoop provides you with a plethora
    of features that enhances your overall experience when it comes to building distributed
    applications, as well as managing the overall Hadoop cluster. For example, selecting
    MapR as the Hadoop distribution provides support for industry-standard interfaces,
    such as NFS and ODBC, using which you can connect your EMR cluster with any major
    BI tool, including Tableau and Toad. MapR internally also provides built-in high
    availability, data protection, higher performances, and a whole list of additional
    features. You can read more about the MapR distribution for Hadoop at EMR at: [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-mapr.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-mapr.html).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Last but not the least, I would also recommend that you try out some of Amazon
    Redshift's advanced features in the form of reserved nodes and parameter groups.
    Parameter groups are essentially a group of parameters that are applied to the
    database when it is created. You can find the parameter group for your existing
    database by selecting the Parameter Group option from the Redshift's navigation
    pane. You can use and tweak these parameter groups based on your requirements
    to fine tune and customize the database. To know how to leverage parameter groups
    for your database tuning, visit: [https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-parameter-groups.html](https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-parameter-groups.html).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well that brings us to the end of yet another amazing chapter. Let's quickly
    summarize what we have learnt so far!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: To start off, we began by learning a bit about the various services offered
    by AWS for big data analytics followed by a quick getting started with Amazon
    EMR guide. We learnt about a few of Amazon EMR's concepts as well as launched
    our very first EMR cluster, as well. We also ran our first simple job on the EMR
    cluster and learnt how to monitor its performance using the likes of Amazon CloudWatch.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we got to know Amazon Redshift along with its
    core concepts and workings. We also created our first Redshift cluster, connected
    to it using an open source client and ran a couple of SQL queries, as well.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be learning and exploring yet another AWS service
    designed for data orchestration so stick around, we still have much to learn!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
