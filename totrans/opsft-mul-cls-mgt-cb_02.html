<html><head></head><body>
		<div id="_idContainer065">
			<h1 id="_idParaDest-29" class="chapter-number"><a id="_idTextAnchor028"/>2</h1>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Architecture Overview and Definitions</h1>
			<p>Kubernetes is an amazing technology; however, as we saw in the last chapter, it is not a simple technology. I consider Kubernetes not only as container orchestration, but besides that, it is also a platform with standard interfaces to integrate containers with the broader infrastructure, including storage, networks, and hypervisors. That said, you must consider all the prerequisites and aspects involved in an OpenShift <span class="No-Break">self-managed cluster.</span></p>
			<p>In this chapter, we will walk through the main concepts related to the Kubernetes and OpenShift architecture. The main purpose here is you <em class="italic">think before doing</em> and make important decisions, to avoid <span class="No-Break">rework later.</span></p>
			<p>The following main topics will be covered in <span class="No-Break">the chapter:</span></p>
			<ul>
				<li>Understanding the <span class="No-Break">foundational concepts</span></li>
				<li>OpenShift architectural concepts and <span class="No-Break">best practices</span></li>
				<li><span class="No-Break">Infrastructure/cloud provider</span></li>
				<li><span class="No-Break">Network considerations</span></li>
				<li><span class="No-Break">Other considerations</span></li>
				<li>OpenShift <span class="No-Break">architectural checklists</span></li>
			</ul>
			<p>Let's <span class="No-Break">get started!</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Technical requirements</h1>
			<p>As we are covering the architecture side of OpenShift in this chapter, you still don't need access to any specific hardware or software to follow this chapter, but this will be expected some chapters ahead. However, it is important you have some pre-existing knowledge of OpenShift and Kubernetes for you to achieve the best possible result from <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Prerequisites</h2>
			<p>This chapter is intended to be for <strong class="bold">Information Technology</strong> (<strong class="bold">IT</strong>) architects that already have some basic <a id="_idIndexMarker038"/>knowledge of Kubernetes or OpenShift use. That said, we are not covering in this chapter basic concepts such as what a Pod, Service, or Persistent Volume is. But if you don't know these basic concepts yet, don't freak out! We have prepared a list of recommended training and references for you in the last chapter of this book. We suggest you to take the Kubernetes Basics and Kube by Example before moving forward with <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Understanding the foundational concepts</h1>
			<p>Let's start by understanding the main concepts related to Kubernetes and OpenShift components and servers. First, any OpenShift cluster is composed of two types of servers: <strong class="bold">master</strong> and <span class="No-Break"><strong class="bold">worker</strong></span><span class="No-Break"> nodes.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Master nodes</h2>
			<p>This server <a id="_idIndexMarker039"/>contains the <strong class="bold">control plane</strong> of a Kubernetes cluster. Master servers on <a id="_idIndexMarker040"/>OpenShift run over the <strong class="bold">Red Hat Enterprise Linux CoreOS</strong> (<strong class="bold">RHCOS</strong>) operating system and are composed <a id="_idIndexMarker041"/>of several main components, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Application programming interface (API) server</strong> (<strong class="source-inline">kube-apiserver</strong>): Responsible for <a id="_idIndexMarker042"/>exposing all Kubernetes <a id="_idIndexMarker043"/>APIs. All actions performed on a Kubernetes cluster are done through an API call—whenever you use the <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) or a <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>), an API call will always <a id="_idIndexMarker044"/><span class="No-Break">be used.</span></li>
				<li><strong class="bold">Database</strong> (<strong class="source-inline">etcd</strong>): The database stores all cluster data. <strong class="source-inline">etcd</strong> is a highly available distributed <a id="_idIndexMarker045"/>key-value database. For in-depth information <a id="_idIndexMarker046"/>about <strong class="source-inline">etcd</strong>, refer to its documentation <span class="No-Break">here: </span><a href="https://etcd.io/docs/latest/"><span class="No-Break">https://etcd.io/docs/latest/</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Scheduler</strong> (<strong class="source-inline">kube-scheduler</strong>): It is the responsibility of <strong class="source-inline">kube-scheduler</strong> to assign a <a id="_idIndexMarker047"/>node for a Pod to run over. It uses complex algorithms that consider a large set of aspects to decide which is the best node to host the Pod, such as computing resource available versus required node selectors, affinity and anti-affinity rules, <span class="No-Break">and others.</span></li>
				<li><strong class="bold">Controller manager</strong> (<strong class="source-inline">kube-controller-manager</strong>): Controllers are an endless loop that works to ensure that an object is always in the desired state. As an illustration, think about <a id="_idIndexMarker048"/>smart home automation equipment: a controller is responsible for orchestrating the equipment to make sure the environment will always be in the desired programmed state—for example, by turning the air conditioning on and off from time to time to keep the temperature as close as possible to the desired state. Kubernetes controllers have the same function— they are responsible for monitoring objects and responding accordingly to keep them at the desired states. There are a bunch of controllers that are used in a Kubernetes cluster, such as replication controller, endpoints controller, namespace controller, and serviceaccounts controller. For more information <a id="_idIndexMarker049"/>about controllers, check out this <span class="No-Break">page: </span><a href="https://kubernetes.io/docs/concepts/architecture/controller/"><span class="No-Break">https://kubernetes.io/docs/concepts/architecture/controller/</span></a></li>
			</ul>
			<p>These are the components <a id="_idIndexMarker050"/>of the Kubernetes control plane that runs on the master nodes; however, OpenShift has some additional services to extend Kubernetes functionality, as <span class="No-Break">outlined here:</span></p>
			<ul>
				<li><strong class="bold">OpenShift API Server</strong> (<strong class="source-inline">openshift-apiserver</strong>): This validates and configures data <a id="_idIndexMarker051"/>for OpenShift exclusive resources, such as routes, templates, <span class="No-Break">and projects.</span></li>
				<li><strong class="bold">OpenShift controller manager</strong> (<strong class="source-inline">openshift-controler-manager</strong>): This works to <a id="_idIndexMarker052"/>ensure that OpenShift exclusive <a id="_idIndexMarker053"/>resources reach the <span class="No-Break">desired state.</span></li>
				<li><strong class="bold">OpenShift Open Authorization (OAuth) server and API</strong> (<strong class="source-inline">openshift-oauth-apiserver</strong>): Responsible for validating and configuring data to authenticate <a id="_idIndexMarker054"/>a user, group, and token <span class="No-Break">with OpenShift.</span></li>
			</ul>
			<p>The following <a id="_idIndexMarker055"/>figure shows the main control plane components of Kubernetes <span class="No-Break">and OpenShift:</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B18015_02_01.jpg" alt="Figure 2.1 – OpenShift control plane components"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – OpenShift control plane components </p>
			<p>These components can <a id="_idIndexMarker056"/>be found in multiple namespaces, as you can see in the <span class="No-Break">following table:</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B18015_02_Table_01.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">What Are Operators?</p>
			<p class="callout">If you've never heard about Operators, you may be thinking: <em class="italic">What are Operators, after all?</em> Operators are nothing more than a standard method to package, deploy, and maintain Kubernetes applications and objects. They use <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>) to extend the Kubernetes API functionality and also some standards for the <a id="_idIndexMarker057"/>application's life cycle: deploy, patch, keep the desired state, and even auto-pilot it (autoscaling, tuning, failure detections, and so on). Check <a id="_idIndexMarker058"/>out this link for more <span class="No-Break">information: </span><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"><span class="No-Break">https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Bootstrap node</h2>
			<p>The bootstrap node is a temporary server—used only during cluster deployment—that is responsible for injecting <a id="_idIndexMarker059"/>the cluster's components into the control plane. It is removed by the installation program when the bootstrap is finished successfully. As it is a temporary server that lives only during the deployment, it is usually not considered in the <span class="No-Break">OpenShift architecture.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Workers</h2>
			<p>Workers are the servers where <a id="_idIndexMarker060"/>the workloads themselves run. On OpenShift, workers can run over RHCOS or <strong class="bold">Red Hat Enterprise Linux</strong> (<strong class="bold">RHEL</strong>). Although RHEL is also supported <a id="_idIndexMarker061"/>for OpenShift workers, RHCOS, in general, is preferred for the <span class="No-Break">following reasons:</span></p>
			<ul>
				<li><strong class="bold">Immutable</strong>: RHCOS is a tight operating system designed to be managed remotely by OpenShift Container Platform itself. This enables consistency and makes upgrades a much easier and safer procedure, as OpenShift will always know and manage the actual and desired states of <span class="No-Break">the servers.</span></li>
				<li><strong class="source-inline">rpm-ostree</strong>: RHCOS uses the <strong class="source-inline">rpm-ostree</strong> system, which enables transactional upgrades and adds consistency to the infrastructure. Check out this link for more <span class="No-Break">information: </span><a href="https://coreos.github.io/rpm-ostree/"><span class="No-Break">https://coreos.github.io/rpm-ostree/</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">CRI-O container runtime and container tools</strong>: RHCOS's default container runtime <a id="_idIndexMarker062"/>is <strong class="bold">CRI-O</strong>, which is optimized for Kubernetes (see <a href="https://cri-o.io/">https://cri-o.io/</a>). It also <a id="_idIndexMarker063"/>comes with a set of tools to work with containers, such as <strong class="source-inline">podman</strong> and <strong class="source-inline">skopeo</strong>. During normal functioning, you are not encouraged to access and run commands on workers directly (as they are managed by the OpenShift platform itself); however, those tools are helpful for troubleshooting purposes—as we will see in detail in <a href="B18015_06.xhtml#_idTextAnchor113"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> of this book, <em class="italic">OpenShift Troubleshooting, Performance, and </em><span class="No-Break"><em class="italic">Best Practices</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Based on RHEL</strong>: RHCOS is based on RHEL—it uses the same well-known and safe RHEL kernel <a id="_idIndexMarker064"/>with some services managed by <strong class="source-inline">systemd</strong>, which ensures the same level of <a id="_idIndexMarker065"/>security and quality you would have by using the standard RHEL <span class="No-Break">operating system.</span></li>
				<li><strong class="bold">Managed by Machine Config Operator (MCO)</strong>: To allow a high level of automation <a id="_idIndexMarker066"/>and also keep secure upgrades, OpenShift uses the MCO to manage the configurations of the operating system. It uses the <strong class="source-inline">rpm-ostree</strong> system to make atomic upgrades, which allows safer and easier upgrade and rollback (<span class="No-Break">if needed).</span></li>
			</ul>
			<p>In the following figure, you can view how these objects and concepts are used in an OpenShift <span class="No-Break">worker node:</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B18015_02_02.jpg" alt="Figure 2.2 – RHCOS worker node "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – RHCOS worker node</p>
			<h3>Types of workers</h3>
			<p>There are some common types of workers, the most usual <span class="No-Break">being these:</span></p>
			<ul>
				<li><strong class="bold">Application workers</strong>: Responsible for <a id="_idIndexMarker067"/>hosting the workloads—this is where the <a id="_idIndexMarker068"/>application <span class="No-Break">containers run.</span></li>
				<li><strong class="bold">Infrastructure workers</strong>: This type of server is usually used to host the platform <a id="_idIndexMarker069"/>infrastructure tools, such as the ingress (routers), internal <a id="_idIndexMarker070"/>registry, the monitoring stack (Prometheus and Grafana), and also the logging tool (Elasticsearch <span class="No-Break">and Kibana).</span></li>
				<li><strong class="bold">Storage workers</strong>: Container <a id="_idIndexMarker071"/>storage <a id="_idIndexMarker072"/>solutions, such as <strong class="bold">Red Hat OpenShift Data Foundation</strong>, usually <a id="_idIndexMarker073"/>require some dedicated worker nodes to host their Pods. In such cases, a best practice is to use a dedicated node group <span class="No-Break">for them.</span></li>
			</ul>
			<p>In the next section, we will see how to use different types of workers to design a highly available and resilient <span class="No-Break">OpenShift cluster.</span></p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Highly available cluster</h2>
			<p>It is not uncommon for OpenShift clusters to become critical for the enterprise—sometimes, they start <a id="_idIndexMarker074"/>small but become large really quickly. Due to that, you should consider in <a id="_idIndexMarker075"/>your OpenShift cluster architecture <strong class="bold">non-functional requirements</strong> (<strong class="bold">NFRs</strong>) such as <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>) from day one. A highly available cluster is <a id="_idIndexMarker076"/>comprised of the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Master nodes</strong>: <strong class="source-inline">etcd</strong> uses a <a id="_idIndexMarker077"/>distributed consensus algorithm named <strong class="bold">Raft protocol</strong>, which requires at least <em class="italic">three nodes to be highly available</em>. It is not the <a id="_idIndexMarker078"/>focus of this book to explain the Raft protocol, but if you want to understand it better, refer to these links: <ul><li>Raft <a id="_idIndexMarker079"/><span class="No-Break">description: </span><a href="https://raft.github.io/"><span class="No-Break">https://raft.github.io/</span></a></li>
<li>Illustrated <span class="No-Break">example: </span><a href="http://thesecretlivesofdata.com/raft/"><span class="No-Break">http://thesecretlivesofdata.com/raft/</span></a></li>
</ul></li>
				<li><strong class="bold">Infrastructure worker nodes</strong>: At least two nodes are required to have ingress highly <a id="_idIndexMarker080"/>available. We will discuss later in this chapter what you should consider for other infrastructure components such as monitoring <span class="No-Break">and logging.</span></li>
				<li><strong class="bold">Application worker nodes</strong>: At least two nodes are also required to be considered highly available; however, you may have as many nodes as required to provide enough <a id="_idIndexMarker081"/>capacity for expected workloads. In this chapter, we will walk through some sizing guidance to determine the number of workers required for a workload, if you have an estimated <span class="No-Break">required capacity.</span></li>
			</ul>
			<p>The following figure shows what a highly available cluster architecture <span class="No-Break">looks like:</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B18015_02_03.jpg" alt="Figure 2.3 – OpenShift highly available cluster  "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – OpenShift highly available cluster </p>
			<p>Now that we are on board with the foundational concepts of Kubernetes and OpenShift, let's dive further and look at OpenShift's architecture, along with some <span class="No-Break">best practices.</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>OpenShift architectural concepts and 
best practices</h1>
			<p>In this section, we will discuss the main concepts related to the OpenShift architecture design and <a id="_idIndexMarker082"/>some best practices you should consider. In general, when we are designing an architecture for an OpenShift cluster, the aspects in the following table need to be defined accordingly: </p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18015_02_Table_02.jpg" alt=""/>
				</div>
			</div>
			<p>Details of how to address these aspects (deployment, configurations, and so on) will be covered from <a href="B18015_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">,</em> <em class="italic">OpenShift Deployment,</em> onward. Another key point to note is that we are still <a id="_idIndexMarker083"/>focusing on one single OpenShift cluster only—the main objective here is to help you to define a standard cluster architecture that best fits your case. In the next chapter, we will explore aspects you need to consider <a id="_idIndexMarker084"/>when working with multiple environments, clusters, <span class="No-Break">and providers.</span></p>
			<p>In the following sections, we are going to walk through each of these points, highlighting the most important items you need <span class="No-Break">to cover.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Installation mode</h2>
			<p>You already know <a id="_idIndexMarker085"/>from the previous chapter that we have three installation modes with OpenShift, as follows: </p>
			<ul>
				<li><strong class="bold">Installer-provisioned </strong><span class="No-Break"><strong class="bold">infrastructure</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">IPI</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">User-provisioned </strong><span class="No-Break"><strong class="bold">infrastructure</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">UPI</strong></span><span class="No-Break">)</span></li>
				<li>Provider-agnostic (if you haven't seen it, review the <em class="italic">OpenShift installation modes</em> section from the last chapter) </li>
			</ul>
			<p>Here, we will briefly discuss important aspects you need to consider from each option to drive the best decision for <span class="No-Break">your case.</span></p>
			<h3>IPI</h3>
			<p>This mode is a simplified opinionated method for cluster provisioning and is also a fully automated <a id="_idIndexMarker086"/>method for <a id="_idIndexMarker087"/>installation and upgrades. With this model, you can make the operational overhead lower; however, it is less flexible <span class="No-Break">than UPI.</span></p>
			<p>You can see an example of <span class="No-Break">IPI here:</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B18015_02_04.jpg" alt="Figure 2.4 – IPI  "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – IPI </p>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.4</em> shows <a id="_idIndexMarker088"/>all layers that <a id="_idIndexMarker089"/>are automated by the installer during the <span class="No-Break">cluster deployment.</span></p>
			<h3>UPI</h3>
			<p>In this mode, you provision the servers manually—you are also responsible for managing them. As <a id="_idIndexMarker090"/>such, you have more flexibility within <a id="_idIndexMarker091"/>the infrastructure layer. In this mode, OpenShift still has some level of integration with the infrastructure or cloud provider to provide storage services for <span class="No-Break">the platform.</span></p>
			<h3>Agnostic installer</h3>
			<p>This mode is similar to UPI; however, there is no integration between OpenShift and the infrastructure <a id="_idIndexMarker092"/>or cloud provider. Therefore, in this <a id="_idIndexMarker093"/>mode, you will not have any storage plugins installed with the <a id="_idIndexMarker094"/>platform—you will need to deploy an in-tree or <strong class="bold">Container Storage Interface</strong> (<strong class="bold">CSI</strong>) plugin on day two to provide persistent volumes to your workloads (we are going to cover storage-related aspects later in <span class="No-Break">this chapter).</span></p>
			<p>You can see an example of UPI/an agnostic <span class="No-Break">installer here:</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B18015_02_05.jpg" alt="Figure 2.5 – UPI/Agnostic installer  "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – UPI/Agnostic installer </p>
			<p>As you can see in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.5</em>, with UPI or an agnostic installer, there are some layers you are responsible for providing, as prerequisites, to deploy a cluster (and also maintain it on day two), as opposed to IPI, from <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.4</em>, which is <span class="No-Break">fully automated.</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Computing</h2>
			<p>From a computing perspective, the following are important attributes that must be considered during <a id="_idIndexMarker095"/>the <span class="No-Break">architecture design:</span></p>
			<ul>
				<li><strong class="bold">Nodes and cluster sizing</strong>: Define the number and size of worker nodes to host workloads expected for the platform. Some important factors need to be considered here to have a resilient cluster—this topic will be covered later in <span class="No-Break">this chapter.</span></li>
				<li><strong class="bold">Environment segmentation</strong>: It is possible to have one cluster only that provides a segregated group of nodes for specific reasons. Sometimes, it makes sense to have a dedicated group of nodes to provide services for specific environments in one single cluster—it is possible to have one single cluster with nodes dedicated for a development environment, another group for staging, and another one for production, for instance. That said, this is a crucial decision that needs to be made—going for one cluster for each environment or having one single cluster that serves multiple environments. We are going to explore this point in the next chapter and see what the pros and cons of each <span class="No-Break">case are.</span></li>
			</ul>
			<h3>Master nodes' sizing</h3>
			<p>To define the <a id="_idIndexMarker096"/>master nodes' size, we recommend <a id="_idIndexMarker097"/>you follow Red Hat's benchmark, based on expected cluster load and number of nodes, <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B18015_02_Table_03.jpg" alt=""/>
				</div>
			</div>
			<h3>Infrastructure node sizing</h3>
			<p>Similarly, infrastructure nodes' size also has <a id="_idIndexMarker098"/>a benchmark, based <a id="_idIndexMarker099"/>on expected cluster size, <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B18015_02_Table_04.jpg" alt=""/>
				</div>
			</div>
			<p>However, the preceding table does not consider OpenShift logging. Therefore, if you are planning to use it, add <a id="_idIndexMarker100"/>at least four more <strong class="bold">virtual CPUs</strong> (<strong class="bold">vCPUs</strong>) and 16 GB to the nodes on which Elasticsearch instances will be hosted. </p>
			<h3>Worker nodes' sizing</h3>
			<p>There isn't just one algorithm to estimate the size of an OpenShift cluster. The sizing algorithm we <a id="_idIndexMarker101"/>listed here is based on our personal experience along <a id="_idIndexMarker102"/>the years working with it, and also great articles and resources we have studied so far—some good references on this topic are available at the end of this chapter in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
			<h4>Allocatable resources</h4>
			<p>The sizing estimation <a id="_idIndexMarker103"/>rationale for computing resources needs to consider the nodes' allocatable resources. The allocatable resource is the real amount that can be used for workloads in a node, considering the number of resources that are reserved for the operating system and <strong class="source-inline">kubelet</strong>. The calculation of allocatable resources is given by the following formula: </p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B18015_02_001.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">OpenShift Default Values</p>
			<p class="callout">The default values for OpenShift workers are as follows (at the time of this writing): </p>
			<p class="callout"><span class="No-Break"><strong class="bold">CPU</strong></span><span class="No-Break">:</span></p>
			<p class="callout">- <strong class="source-inline">system-reserved = 500m (*)</strong> </p>
			<p class="callout">- <strong class="source-inline">kube-reserved = </strong><span class="No-Break"><strong class="source-inline">0m (*)</strong></span></p>
			<p class="callout">- <strong class="source-inline">hard-eviction = </strong><span class="No-Break"><strong class="source-inline">0m (*)</strong></span></p>
			<p class="callout"><span class="No-Break"><strong class="bold">Memory</strong></span><span class="No-Break">:</span></p>
			<p class="callout">- <strong class="source-inline">system-reserved = 1Gi</strong> </p>
			<p class="callout">- <strong class="source-inline">kube-reserved = 0Gi</strong> </p>
			<p class="callout">- <strong class="source-inline">hard-eviction = </strong><span class="No-Break"><strong class="source-inline">100Mi</strong></span></p>
			<p class="callout">(*) "<strong class="source-inline">m</strong>" stands for <em class="italic">millicore</em>, a standard Kubernetes unit that represents one vCPU divided into <span class="No-Break">1,000 parts.</span></p>
			<h4>Recommended allocatable resources</h4>
			<p>Besides the standard aforementioned allocatable resources, it should also be considered as a best practice to <a id="_idIndexMarker104"/>keep at least 25% of resources available in a node, for resilience purposes. I'll explain: when one node goes down, the native Kubernetes resilience mechanism, after some time, will move the Pods to other nodes with available resources—that means if you don't plan to have extra capacity on the nodes, this resilience mechanism is at risk. You should also consider extra capacity for autoscaling at peak times and future growth. Therefore, it is recommended you consider this extra capacity in the calculation of workers' computing sizing, as follows: </p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B18015_02_002.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Usually, some level of CPU overcommitment is—somewhat—handled well by the operating system. That said, the extra capacity mentioned previously doesn't always apply to the CPU. However, this is a workload-dependent characteristic: most container applications are more memory- than CPU-bound, meaning that CPU overcommitment will not have a great impact on overall application performance, while the same does not happen with memory—but again, check your application's requirement to <span class="No-Break">understand that.</span></p>
			<p>Let's use an <a id="_idIndexMarker105"/>example to make this sizing <span class="No-Break">logic clear.</span></p>
			<h4>Example</h4>
			<p>Imagine that you use servers with 8 vCPUs and 32 GB random-access memory (RAM) as the <a id="_idIndexMarker106"/>default size. A worker of this size will have, in the end, the following recommended <span class="No-Break">allocatable resources:</span></p>
			<ul>
				<li><span class="No-Break">CPU</span></li>
			</ul>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B18015_02_Table_05.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><span class="No-Break">Memory:</span><div id="_idContainer030" class="IMG---Figure"><img src="image/B18015_02_Table_06.jpg" alt=""/></div></li>
			</ul>
			<p><span class="No-Break"><strong class="bold">Legend</strong></span><span class="No-Break">:</span></p>
			<p><em class="italic">ar = </em><span class="No-Break"><em class="italic">allocatable resources</em></span></p>
			<p><em class="italic">nc = </em><span class="No-Break"><em class="italic">node capacity</em></span></p>
			<p><em class="italic">kr = </em><span class="No-Break"><em class="italic">kube-reserved</em></span></p>
			<p><em class="italic">sr = </em><span class="No-Break"><em class="italic">system-reserved</em></span></p>
			<p><em class="italic">he = </em><span class="No-Break"><em class="italic">hard-eviction threshold</em></span></p>
			<p>Therefore, a worker with 8 vCPUs <a id="_idIndexMarker107"/>and 32 GB RAM will have approximately <strong class="bold">5 vCPUs and 23 GB RAM</strong> considered as the usable capacity for applications. Considering an example in which an application Pod requires on average 200 millicores and 1 GB RAM, a worker of this size would be able to host approximately 23 Pods (limited by memory). </p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Aggregated logging</h2>
			<p>You can optionally <a id="_idIndexMarker108"/>deploy the <strong class="bold">OpenShift Logging</strong> tool that is <a id="_idIndexMarker109"/>based on <strong class="bold">Elasticsearch</strong>, <strong class="bold">Kibana</strong>, and <strong class="bold">Fluentd</strong>. The following diagram <a id="_idIndexMarker110"/>explains how <a id="_idIndexMarker111"/>this <span class="No-Break">tool works:</span></p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B18015_02_06.jpg" alt="Figure 2.6 – OpenShift Logging components  "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – OpenShift Logging components </p>
			<p>You are not required <a id="_idIndexMarker112"/>to use OpenShift Logging, though, if you <a id="_idIndexMarker113"/>have your logging solution and want to keep it. You only need to configure the <strong class="source-inline">ClusterLogForwarder</strong>, as you are going to see in later chapters of this book (from <a href="B18015_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">OpenShift </em><span class="No-Break"><em class="italic">Deployment,</em></span><span class="No-Break"> onward).</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Monitoring</h2>
			<p>Another important tool that any container orchestration platform needs to have is a monitoring tool that <a id="_idIndexMarker114"/>can monitor your infrastructure <a id="_idIndexMarker115"/>and applications. OpenShift comes natively with a monitoring <a id="_idIndexMarker116"/>solution based on <strong class="bold">Prometheus</strong>, <strong class="bold">AlertManager</strong>, and <strong class="bold">Grafana</strong>. The following diagram <a id="_idIndexMarker117"/>explains the <span class="No-Break">monitoring components:</span></p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B18015_02_07.jpg" alt="Figure 2.7 – OpenShift monitoring components  "/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – OpenShift monitoring components </p>
			<p>OpenShift monitoring is not optional; it is used by many internal platform components. However, if you do not <a id="_idIndexMarker118"/>intend to use it in favor of another monitoring tool, you may keep it using ephemeral storage. On the other hand, if you are planning to use it, we recommend you provide persistent storage to save the <span class="No-Break">monitoring metrics.</span></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Storage</h2>
			<p>Containers are stateless by <a id="_idIndexMarker119"/>nature, but this does not mean that it is not possible to have stateful containers on OpenShift. There are multiple ways to mount storage volumes inside containers and enable stateful workloads. In the following sections, we will <a id="_idIndexMarker120"/>walk through the common storage requirements of an OpenShift cluster that you should consider in your architectural design. </p>
			<h3>Storage backends</h3>
			<p>There are two types of storage implementations: in-tree and <span class="No-Break">CSI plugins.</span></p>
			<h4>In-tree volume plugins</h4>
			<p>In-tree plugins are implementations that allow a Kubernetes platform to access and use external storage <a id="_idIndexMarker121"/>backends. The name <em class="italic">in-tree</em> comes from the fact <a id="_idIndexMarker122"/>that these implementations are developed and released in the main Kubernetes repositories, as <em class="italic">in-tree</em> modules. There are several types of supported in-tree plugins with OpenShift, as <span class="No-Break">follows (*):</span></p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B18015_02_Table_07.jpg" alt=""/>
				</div>
			</div>
			<p>(*) At the time this <a id="_idIndexMarker123"/>book was written. Check the currently <a id="_idIndexMarker124"/>supported options <span class="No-Break">at </span><a href="https://access.redhat.com/articles/4128421"><span class="No-Break">https://access.redhat.com/articles/4128421</span></a><span class="No-Break">.</span></p>
			<h4>CSI drivers</h4>
			<p>With more and more storage providers supporting Kubernetes, the development and maintenance of in-tree plugins became difficult and was no longer the most efficient model. The CSI has been created in this <a id="_idIndexMarker125"/>context: to provide a standard way to <a id="_idIndexMarker126"/>extend Kubernetes storage capabilities using API interfaces—as such, you can easily add new CSI plugins for different storage providers and use them with OpenShift. With CSI, it is possible to also have interesting features such as <strong class="bold">snapshots, resizing, and volume cloning</strong>; however, it is up to the storage provider to implement these features or not, so check with them if they have a CSI driver implementation available and which operations are implemented <span class="No-Break">and supported.</span></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Red Hat supports the CSI APIs and implementation from the OpenShift side; however, support of the storage side is a storage vendor's responsibility. Check with your storage vendor if there is a supported CSI option <span class="No-Break">for OpenShift.</span></p>
			<h3>Storage requirements</h3>
			<p>Now that you have learned <a id="_idIndexMarker127"/>about the types of storage plugins available for OpenShift, let's review the storage requirements you usually have with an <span class="No-Break">OpenShift cluster.</span></p>
			<h4>Server disks</h4>
			<p>OpenShift servers <a id="_idIndexMarker128"/>use one disk with 120 GB by default. Large clusters <a id="_idIndexMarker129"/>require master nodes with low latency and high <a id="_idIndexMarker130"/>throughput disks, which can provide at <a id="_idIndexMarker131"/>least 500 sequential <strong class="bold">input/output operations per second</strong> (<strong class="bold">IOPS</strong>) (usually <strong class="bold">solid-state drive</strong> (<strong class="bold">SSD</strong>) or <strong class="bold">Non-Volatile Memory Express</strong> (<strong class="bold">NVMe</strong>) disks). We <a id="_idIndexMarker132"/>are also going to see in-depth details about this in <a href="B18015_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">OpenShift Deployment</em></span><span class="No-Break">.</span></p>
			<h4>OpenShift internal registry</h4>
			<p>This depends on the number and size of application images to be stored on it. If you do not have an estimated <a id="_idIndexMarker133"/>value for the images, an initial size of 200 GB is <a id="_idIndexMarker134"/>usually enough for the first few weeks. As a best practice, consider setting image pruner policies to automatically delete images that are no longer used—we are going to cover these best practices with examples in <a href="B18015_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">OpenShift Deployment</em></span><span class="No-Break">.</span></p>
			<p>Volume type used by OpenShift internal registry: <strong class="bold">RWX </strong></p>
			<h4>OpenShift Logging</h4>
			<p>This depends on the number of logs generated by the applications. Here is an example of the volume size <a id="_idIndexMarker135"/>required for an application that generates 10 lines of logs<a id="_idIndexMarker136"/> per second (lines-per-second); the lines have 256 bytes (bytes-per-line) on average, considering a retention period of 7 days for <span class="No-Break">the logs:</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B18015_02_011.jpg" alt=""/>
				</div>
			</div>
			<p>This means that one single Pod of that application will consume nearly 1.5 GB over 7 days (the period for which a log will be stored on Elasticsearch). Another important thing to consider is Elasticsearch's replication factor, which will require more storage depending on the replication factor selected. There following replication factors <span class="No-Break">are available:</span></p>
			<ul>
				<li><strong class="source-inline">FullRedundancy</strong>: Replicates the primary shards for each index to every <span class="No-Break">Elasticsearch node</span></li>
				<li><strong class="source-inline">MultipleRedundancy</strong>: Replicates <a id="_idIndexMarker137"/>the primary shards for <a id="_idIndexMarker138"/>each index to 50% of the <span class="No-Break">Elasticsearch nodes</span></li>
				<li><strong class="source-inline">SingleRedundancy</strong>: Makes one copy of the primary shards for <span class="No-Break">each index</span></li>
				<li><strong class="source-inline">ZeroRedundancy</strong>: Does not make a copy of the <span class="No-Break">primary shards</span></li>
			</ul>
			<p>Volume type used by OpenShift <span class="No-Break">Logging: </span><span class="No-Break"><strong class="bold">RWO</strong></span></p>
			<h4>OpenShift monitoring</h4>
			<p>OpenShift monitoring is installed <a id="_idIndexMarker139"/>by default with the platform using ephemeral storage (also known as <strong class="bold">emptyDir</strong>), meaning <a id="_idIndexMarker140"/>that, for some reason, when the <a id="_idIndexMarker141"/>Prometheus pod <a id="_idIndexMarker142"/>gets restarted, all metrics data will be lost. To avoid losing metrics <a id="_idIndexMarker143"/>data, consider a persistent volume for <strong class="bold">Prometheus</strong> and <strong class="bold">AlertManager</strong> Pods. </p>
			<p>Red Hat has a benchmark based on various tests performed, as represented here. This empirical data is good guidance to estimate the volume required <span class="No-Break">for Prometheus:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B18015_02_Table_08.jpg" alt=""/>
				</div>
			</div>
			<p>(*) 15 days is the default <span class="No-Break">retention period.</span></p>
			<p>You also need to <a id="_idIndexMarker144"/>consider volumes for <strong class="bold">AlertManager</strong>: typically, a volume size of <strong class="bold">20 GB</strong> is enough for <span class="No-Break">most cases.</span></p>
			<p>By default, an HA configuration is composed of <strong class="bold">two Prometheus replicas and three </strong><span class="No-Break"><strong class="bold">AlertManager replicas</strong></span><span class="No-Break">.</span></p>
			<p>Using the preceding <a id="_idIndexMarker145"/>reference, we can estimate the volumes <a id="_idIndexMarker146"/>required for OpenShift monitoring. For example, let's say that we are planning a cluster that will have no more than 50 nodes and 1,800 Pods. In that case, we'd need to use the <span class="No-Break">following formula<a id="_idTextAnchor043"/>:</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B18015_02_012.jpg" alt=""/>
				</div>
			</div>
			<p><a id="_idTextAnchor044"/>

</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B18015_02_013.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B18015_02_014.jpg" alt=""/>
				</div>
			</div>
			<p>Volume type used by OpenShift <span class="No-Break">monitoring: </span><span class="No-Break"><strong class="bold">RWO</strong></span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding requirements are based on empirical data. The real consumption observed can be higher depending on the workloads and resource usage. For more information, refer to the official <span class="No-Break">documentation: </span><a href="https://docs.openshift.com/container-platform/latest/scalability_and_performance/scaling-cluster-monitoring-operator.html"><span class="No-Break">https://docs.openshift.com/container-platform/latest/scalability_and_performance/scaling-cluster-monitoring-operator.html</span></a><span class="No-Break">.</span></p>
			<p class="callout">At this time, you don't need to know in-depth details about the OpenShift components such as logging or monitoring, as we are only covering the amount of storage required (or estimated) for them. These tools will be covered in detail later in <span class="No-Break">this book.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor045"/>Example</h2>
			<p>As we have already <a id="_idIndexMarker147"/>addressed sizing guidelines for an OpenShift cluster, let's use an example to make it clearer. Imagine that we are designing an OpenShift cluster architecture that is planned to host a three-tier Node.js application with the <span class="No-Break">following capacity:</span></p>
			<ul>
				<li>Up to 20 Pods on the frontend consume 300 millicores and 1 GB RAM each at peak load. Each pod generates 30 lines of logs per second (256 bytes per line). <span class="No-Break">Stateless Pods.</span></li>
				<li>Up to 4 Pods on the backend need 500 millicores and 1 GB RAM each at peak load. Each pod generates 10 lines of logs per second (256 bytes per line). <span class="No-Break">Stateless Pods.</span></li>
				<li>1 MongoDB database <a id="_idIndexMarker148"/>instance with 8 GB RAM and 2 vCPUs. It generates 1 line of logs per second (256 bytes per line). An <strong class="bold">RWO</strong> volume is required of <span class="No-Break">500 GB.</span></li>
			</ul>
			<p>Our logging stack is configured with <strong class="source-inline">ZeroRedundancy</strong> (there is no <span class="No-Break">data replication).</span></p>
			<h3>Compute sizing</h3>
			<p>First, let's see the <a id="_idIndexMarker149"/>total amount of CPU and memory required (for workloads only), <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">CP<a id="_idTextAnchor046"/>U</span></li>
				<li><img src="image/B18015_02_015.png" alt=""/><a id="_idTextAnchor047"/>
<img src="image/B18015_02_016.png" alt=""/>
<img src="image/B18015_02_017.png" alt=""/></li>
				<li><span class="No-Break">Memory</span></li>
				<li><img src="image/B18015_02_018.png" alt=""/><a id="_idTextAnchor048"/>
<img src="image/B18015_02_019.png" alt=""/>
<a id="_idTextAnchor049"/><img src="image/B18015_02_020.png" alt=""/>
<img src="image/B18015_02_021.png" alt=""/><a id="_idTextAnchor050"/></li>
				<li><span class="No-Break">Volume</span></li>
				<li><img src="image/B18015_02_022.png" alt=""/></li>
			</ul>
			<p>We will assume nodes with 4 vCPUs and 16 GB RAM by default. As we saw in this chapter, we need to <a id="_idIndexMarker150"/>apply the following formula to define the recommended <span class="No-Break">allocatable resources:</span></p>
			<ul>
				<li><span class="No-Break">CPU</span></li>
				<li><div id="_idContainer047" class="IMG---Figure"><img src="image/B18015_02_023.jpg" alt=""/></div></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">We are considering, in this case, that some level of CPU overcommit is acceptable, and due to that, we are not considering the 25% of extra capacity here (recommended <span class="No-Break">allocatable resources).</span></p>
			<ul>
				<li><span class="No-Break">Memory</span><div id="_idContainer048" class="IMG---Figure"><img src="image/B18015_02_024.jpg" alt=""/></div></li>
			</ul>
			<p>
</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B18015_02_025.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li>Th<a id="_idTextAnchor051"/>erefore, three nodes are required to host <span class="No-Break">this workload:</span></li>
			</ul>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18015_02_026.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B18015_02_027.jpg" alt=""/>
				</div>
			</div>
			<p>Tha<a id="_idTextAnchor052"/>t means we will need <strong class="bold">3 nodes with 4 vCPU and 16 GB RAM</strong> to provide the capacity <a id="_idIndexMarker151"/>required for <span class="No-Break">this application.</span></p>
			<h3>Storage sizing</h3>
			<p>Now, let's calculate <a id="_idIndexMarker152"/>the number of volumes required, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Virtual machines</strong> (<strong class="bold">VMs</strong>): 3 (nodes) * 120 <a id="_idIndexMarker153"/>GB (recommended per server) = <strong class="bold">360 </strong><span class="No-Break"><strong class="bold">GB disk</strong></span></li>
				<li>Workload: <strong class="bold">500 </strong><span class="No-Break"><strong class="bold">GB RWO</strong></span></li>
				<li>Internal registry: <strong class="bold">200 </strong><span class="No-Break"><strong class="bold">GB RWX</strong></span></li>
				<li>Logging: <strong class="bold">106 GB RWO (</strong><span class="No-Break"><strong class="bold">see next)</strong></span></li>
			</ul>
			<p><span class="No-Break"><strong class="bold">Frontend</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B18015_02_028.jpg" alt=""/>
				</div>
			</div>
			<p><span class="No-Break"><strong class="bold">Backend</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B18015_02_029.jpg" alt=""/>
				</div>
			</div>
			<p><span class="No-Break"><strong class="bold">MongoDB</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B18015_02_030.jpg" alt=""/>
				</div>
			</div>
			<p><span class="No-Break"><strong class="bold">Total</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B18015_02_031.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Monitoring</strong>: 248 GB RWO (as we saw in the previous section about the sizing for monitoring in a <a id="_idIndexMarker154"/>cluster up to 50 nodes and <span class="No-Break">1,800 Pods)</span></li>
			</ul>
			<h3>Summary</h3>
			<p>The following table summarizes the servers required for this cluster, considering three additional servers <a id="_idIndexMarker155"/>dedicated to hosting the OpenShift infrastructure components (<strong class="bold">Logging, Monitoring, Registry, </strong><span class="No-Break"><strong class="bold">and Ingress)</strong></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B18015_02_Table_09.jpg" alt=""/>
				</div>
			</div>
			<p>In the previous table, the bootstrap node is not being considered as it is a temporary node that is removed after <span class="No-Break">cluster installation.</span></p>
			<p>And finally, the requirements for Persistent Volumes are summarized in the <span class="No-Break">following table:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18015_02_Table_10.jpg" alt=""/>
				</div>
			</div>
			<p>Now that we already know <a id="_idIndexMarker156"/>some best practices to observe in an OpenShift cluster, let's discuss in the next section some surrounding aspects you should also consider when designing an <span class="No-Break">OpenShift architecture.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor053"/>Infrastructure/cloud provider</h1>
			<p>As the OpenShift platform is integrated with the infrastructure or cloud provider, some prerequisites are also <a id="_idIndexMarker157"/>required, but for now, during the architecture design phase, you basically need to define which provider you will go for and be aware that they have specific prerequisites. We are not covering these pre requisites in this chapter, as this is going to be explained in depth in <a href="B18015_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">OpenShift Deployment</em></span><span class="No-Break">.</span></p>
			<p>In that chapter, we will practice the deployment process itself, starting by preparing the infrastructure or cloud prerequisites, setting up installer parameters, storage, network, the virtualization/cloud layer, and so on. However, during the architecture design phase, in general, you don't need to go deeper into these details yet, but just choose which provider to go for <a id="_idIndexMarker158"/>and keep in mind some specifications you will have to fulfill for the provider you <span class="No-Break">have chosen.</span></p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor054"/>Network considerations</h1>
			<p>An OpenShift cluster uses an SDN layer to allow communication between workloads and cluster objects. The default plugin <a id="_idIndexMarker159"/>used with OpenShift at the time <a id="_idIndexMarker160"/>this book was written is <strong class="bold">OpenvSwitch</strong> (<strong class="bold">OvS</strong>), but OpenShift is <a id="_idIndexMarker161"/>also compatible (and supported) with the <strong class="bold">OVN-Kubernetes</strong> plugin. Check this link to better understand the differences between the <span class="No-Break">plugins: </span><a href="https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/about-openshift-sdn.html#nw-ovn-kubernetes-matrix_about-openshift-sdn"><span class="No-Break">https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/about-openshift-sdn.html#nw-ovn-kubernetes-matrix_about-openshift-sdn</span></a><span class="No-Break">.</span></p>
			<p>Within the SDN, there are two virtual subnets—the first one has the Internet Protocol (IP) addresses that a Pod <a id="_idIndexMarker162"/>inside the cluster uses, while the second is always used when you create a service object. The default values for these subnets are listed in the <span class="No-Break">following table:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B18015_02_Table_11.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The preceding ranges are customizable during the platform installation process only! You cannot modify these <span class="No-Break">after installation.</span></p>
			<p class="callout">Make sure these two ranges don't conflict with the existing one in your physical infrastructure. If you have conflicts, you may experience routing problems between Pods on OpenShift and external services that have a real IP within these ranges. The reason is simple: OpenShift SDN will always think that anything with an IP within the Pods' range is a <a id="_idIndexMarker163"/>pod inside the cluster—and in this case, the SDN will never deliver this package to the external network (network address translation, or NAT). Therefore, a pod on OpenShift will never be able to communicate with a real service out of the cluster that has an IP within the Pods' or services' range. So, be careful to define these two ranges with ones that will <em class="italic">never</em> be used in <span class="No-Break">your infrastructure.</span></p>
			<p>Let's move on to some <a id="_idIndexMarker164"/>other important aspects you need to consider from the network <span class="No-Break">perspective, then!</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor055"/>VPC/VNet</h2>
			<p>If you are deploying OpenShift on <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), Azure, or <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), you <a id="_idIndexMarker165"/>may choose to install an OpenShift cluster <a id="_idIndexMarker166"/>in a new or existing VPC/<strong class="bold">virtual network</strong> (<strong class="bold">VNet</strong>). If you go for existing VPC/VNet components such as subnets, NAT, internet <a id="_idIndexMarker167"/>gateways, route tables, and others, these will no <a id="_idIndexMarker168"/>longer be created automatically by the installer—you will need to configure <span class="No-Break">them manually.</span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor056"/>DNS</h2>
			<p>Depending on the installation method and the provider, different DNS requirements are needed. Again, we are <a id="_idIndexMarker169"/>going to cover this point in detail later in this book, but <a id="_idIndexMarker170"/>keep in mind that a set of DNS requirements depends on the provider and installation method <span class="No-Break">you choose.</span></p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor057"/>Load balancers</h2>
			<p>The <em class="italic">IPI</em> in on-premises environments already comes with an embedded highly available load balancer included. In cloud <a id="_idIndexMarker171"/>environments, OpenShift uses load balancers provided by the cloud <a id="_idIndexMarker172"/>provider (for example, AWS Elastic Load Balancing (ELB), Azure's Network <a id="_idIndexMarker173"/>LB, GCP's Cloud Load Balancing). With <em class="italic">UPI</em>, you <a id="_idIndexMarker174"/>need to provide an external load balancer and set it up before <span class="No-Break">cluster deployment.</span></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor058"/>DHCP/IPMI/PXE</h2>
			<p>If you go for OpenShift on <a id="_idIndexMarker175"/>bare metal, observe other requirements specified <a id="_idIndexMarker176"/>for this type <a id="_idIndexMarker177"/>of environment. DHCP, IPMI, and PXE are <a id="_idIndexMarker178"/>optional; however, they are <a id="_idIndexMarker179"/>recommended <a id="_idIndexMarker180"/>to have a higher level of automation. Therefore, consider that in your cluster <span class="No-Break">architectural design.</span></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor059"/>Internet access</h2>
			<p>The OpenShift platform needs download access from a list of websites—the Red Hat public registries to <a id="_idIndexMarker181"/>download the images used with it, either using a <a id="_idIndexMarker182"/>proxy or direct access. However, it is possible to install it on restricted networks as well. Additional work is required, though: you need to establish an internal registry first and mirror all required images from Red Hat's registries to there. If you use a proxy, also check the proxy's performance to avoid timeout errors during the image pulling process <span class="No-Break">with OpenShift.</span></p>
			<p>Well, we've covered great content so far, from foundation concepts to best practices you need to observe related to the installation mode, computing, network, and storage. We are almost done with the most important aspects of an OpenShift cluster architecture, but we can't miss some considerations related to authentication and security. See in the following section some final considerations we brought to this chapter to help you with your cluster's <span class="No-Break">architecture design.</span></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor060"/>Other considerations</h1>
			<p>Finally, there are a few more things that you should also consider during the design phase of your <span class="No-Break">OpenShift cluster.</span></p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor061"/>SSL certificates</h2>
			<p>OpenShift uses SSL for all cluster communication. During the platform installation, self-signed certificates are <a id="_idIndexMarker183"/>generated; however, it is possible to replace the API and ingress certificates. At this point, you only need to know that this is possible; later in this book, you will see how to <span class="No-Break">do it.</span></p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor062"/>IdPs</h2>
			<p>OpenShift is deployed using a <a id="_idIndexMarker184"/>temporary <strong class="source-inline">kubeadmin</strong> user. It is highly recommended you configure new IdPs to allow users to log in to the platform using a convenient and safe authentication method. There are several supported IdPs with OpenShift; here is a current list of supported options (at the time of writing <span class="No-Break">this book):</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B18015_02_Table_12.jpg" alt=""/>
				</div>
			</div>
			<p>To wrap up this chapter <a id="_idIndexMarker185"/>and give you a quick reference guide, look at the OpenShift architectural checklists we <span class="No-Break">provide next.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor063"/>OpenShift architectural checklists</h1>
			<p>These checklists will help you define the main decisions you may need to take during the OpenShift architecture <a id="_idIndexMarker186"/>design and can also be used as a summary of the concepts covered in <span class="No-Break">this chapter.</span></p>
			<p>Here's a checklist for installation mode <span class="No-Break">and computing:</span></p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B18015_02_Table_13.jpg" alt=""/>
				</div>
			</div>
			<p>Here's a checklist of <a id="_idIndexMarker187"/><span class="No-Break">additional tools:</span></p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B18015_02_Table_14.jpg" alt=""/>
				</div>
			</div>
			<p>Here's a <a id="_idIndexMarker188"/>checklist <span class="No-Break">for storage:</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18015_02_Table_15.jpg" alt=""/>
				</div>
			</div>
			<p>Here's a checklist <a id="_idIndexMarker189"/>for <span class="No-Break">the network:</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B18015_02_Table_16.jpg" alt=""/>
				</div>
			</div>
			<p>Here's a checklist <a id="_idIndexMarker190"/>for other <span class="No-Break">general considerations:</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B18015_02_Table_17.jpg" alt=""/>
				</div>
			</div>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor064"/>Summary</h1>
			<p>In this chapter, we went through some of the most important aspects you need to consider and define before starting a cluster deployment, at the architectural design phase. You now understand the different choices you have with the platform and how to estimate the number and size of your nodes and storage. </p>
			<p>Check the next chapter—<a href="B18015_03.xhtml#_idTextAnchor066"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Multi-Tenant Considerations</em>—to acquire more knowledge about the multi-tenant aspects of the <span class="No-Break">OpenShift architecture.</span></p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor065"/>Further reading</h1>
			<p>If you want to go deeper into the topics we covered in this chapter, look at the <span class="No-Break">following references:</span></p>
			<ul>
				<li><em class="italic">etcd </em><span class="No-Break"><em class="italic">documentation:</em></span><span class="No-Break"> </span><a href="https://etcd.io/docs/latest/"><span class="No-Break">https://etcd.io/docs/latest/</span></a></li>
				<li><em class="italic">Kubernetes official </em><span class="No-Break"><em class="italic">documentation: </em></span><a href="https://kubernetes.io/docs/home/"><span class="No-Break">https://kubernetes.io/docs/home/</span></a></li>
				<li><em class="italic">About Kubernetes </em><span class="No-Break"><em class="italic">Operators:</em></span><span class="No-Break"> </span><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"><span class="No-Break">https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</span></a></li>
				<li><em class="italic">Documentation about</em> <span class="No-Break"><strong class="source-inline">rpm-ostree</strong></span><span class="No-Break">: </span><a href="https://coreos.github.io/rpm-ostree/"><span class="No-Break">https://coreos.github.io/rpm-ostree/</span></a></li>
				<li><em class="italic">CSI drivers supported by Open Container Platform (OCP)</em>: <a href="https://docs.openshift.com/container-platform/4.8/storage/container_storage_interface/persistent-storage-csi.html#csi-drivers-supported_persistent-storage-csi">https://docs.openshift.com/container-platform/4.8/storage/container_storage_interface/persistent-storage-csi.html#csi-drivers-supported_persistent-storage-csi</a> </li>
				<li><em class="italic">Graphical explanation about allocatable </em><span class="No-Break"><em class="italic">resources:</em></span><span class="No-Break"> </span><a href="https://learnk8s.io/allocatable-resources"><span class="No-Break">https://learnk8s.io/allocatable-resources</span></a></li>
				<li><em class="italic">How to plan your environment according to application </em><span class="No-Break"><em class="italic">requirements:</em></span><span class="No-Break"> </span><a href="https://docs.openshift.com/container-platform/latest/scalability_and_performance/planning-your-environment-according-to-object-maximums.html#how-to-plan-according-to-application-requirements_object-limits"><span class="No-Break">https://docs.openshift.com/container-platform/latest/scalability_and_performance/planning-your-environment-according-to-object-maximums.html#how-to-plan-according-to-application-requirements_object-limits</span></a></li>
				<li><em class="italic">Recommended host practices, sizing, and </em><span class="No-Break"><em class="italic">others:</em></span><span class="No-Break"> </span><a href="https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-host-practices.html"><span class="No-Break">https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-host-practices.html</span></a></li>
			</ul>
		</div>
</body></html>