["```\n# Multi-Stage pipeline\ntrigger:\n- main\npool:\n  vmImage: ubuntu-latest\nstages:\n- stage: build\n  displayName: Build\n  jobs:\n  - template: build-apps.yml\n  - template: build-iac.yml\n- stage: deployTest\n  displayName: Deploy Test\n  dependsOn: build\n  jobs:\n  - template: deploy.yml\n    parameters:\n      envName: test\n- stage: deployProduction\n  displayName: Deploy Production\n  dependsOn: deployTest\n  jobs:\n  - template: deploy.yml\n    parameters:\n      envName: production\n```", "```\naz acr create -n packtadocicd -g packt –sku Standard -l eastus –admin-enabled\n```", "```\n$id=az ad sp list –display-name azure-pipelines –query \"[].id\" -o tsv\naz role assignment create –assignee-object-id $id –scope /subscriptions/build-apps.yml file, which will be used to build the application containers and push them to the Azure Container Registry.\nTo build and push the containers, you can use the Docker Compose task. However, this must be done as a two-step process; you must build the images first and then push them. To make the task easier to read, let’s first look at the build portion in the following YAML code:\n\n```", "```\n\n Let’s break this code down:\n\n*   The `azureSubscription` parameter is a reference to the Azure Resource Manager Service connection created in the previous chapter.\n*   The `azureContainerRegistry` parameter is a little tricky; it is a JSON document that includes the `loginServer` and `id` properties related to the Container Registry resource in Azure.\n*   The Docker Compose task with the `'Build Containers'` `displayName` uses the `docker-compose.yml` file to build container images locally in the agent, as indicated by the `action` property. Note the use of `additionalImageTags`, where you provide a predefined variable, `$(Build.BuildNumber)`, and set to `true` the `includeLatestTag` property; we will elaborate on this in the *Understanding container image* *tags* section.\n\nWith the images built, the next step is to push them to the registry, which is done with the same task, with just a change to the `action` property, as shown in the following code snippet:\n\n```", "```\n\n The Docker Compose task with `displayName` `'Push Containers'` uses the `docker-compose.yml` file to push the previously built container images to the Container Registry in Azure as indicated by the `action` property.\nRemember that the two portions of YAML presented here are part of the `build-apps.yaml` file.\nNow that we have covered how to build and push container images, let’s take a break to discuss how container image tags work and why they are important.\nUnderstanding container image tags\nBuilding a container is like compiling an application and packaging all its files into a ZIP archive that you can then use for deployment, along with all the OS dependencies needed for the application to run.\nHowever, the result is called a container image and it is typically a complex artifact made up of multiple layers stored in a registry and is not manageable via the filesystem. For this reason, just like you would name a ZIP file based on a versioning convention to track when the artifact was generated, when working with containers, it is important to tag them.\nThe `latest` tag mentioned previously is a convention in the container world that allows you to retrieve the newest image available without specifying a specific tag. This is very helpful during development cycles for experimentation purposes.\nImportant note\nAlways tag your containers with a specific version number and deploy that version number across all your environments for proper traceability. The `latest` tag is only a convenience to easily pull the newest version of a container image and it should never be used for environment deployments, because it can be a reference to different builds depending on the date and time that you pull it.\nNow that you understand the importance of container image tags, let’s see what you can use within Azure Pipelines to name them.\nUnderstanding your pipeline build number\nThe `$(Build.BuildNumber)` predefined variable is a convenient way to get a unique label in every pipeline run to version your artifacts, and its default value is a timestamp and revision number with the format `YYYYMMDD.R`, where `YYYY` is the current year, `MM` is the month, `DD` is the day, and `R` is a sequential automatically incremented number.\nIf you don’t set your build name explicitly, it will use the following default format for your YAML pipelines:\n\n```", "```\n\n This special notation will use the current date in the given format, automatically increase the number generated by the `$(Rev:r)` token, and reset it to `1` if the portion of text before it is changed.\nMost organizations prefer to use semantic versioning for artifacts or APIs, which follows a `MAJOR.MINOR.PATCH` format, like `1.0.1`:\n\n*   `MAJOR` changes indicate incompatible API changes\n*   `MINOR` changes indicate that functionality is added with backward compatibility\n*   `PATCH` changes indicate a bug fix without impact on functionality\n\nIf you need to use semantic versioning in your pipelines, an easy way to implement this is by adding `name` at the very top of your YAML file, as follows:\n\n```", "```\n\n Notice that in this case, you are responsible for increasing the `MAJOR` and `MINOR` portions of the name based on your code changes.\nImportant note\nAlways consider the implications of your pipeline build number, its format, and where you will be using it. This value can have adverse effects depending on where you use it.\nNow that you have the container images available, let’s talk a bit about Helm, a tool you will be using to deploy applications in Kubernetes environments.\nUnderstanding Helm\n**Helm** is a package manager for Kubernetes. Typically, you take advantage of it to deploy third-party or open source applications into your Kubernetes clusters. The packages created with Helm are referred to as **Helm charts**.\nHelm is also extremely useful for packaging your own applications, since they will likely have more than one manifest needed to configure all required components in Kubernetes, and Helm provides facilities to override parameters with ease.\nFor example, a simple Helm chart will contain the following files:\n![Figure 9.6 – Basic Helm chart contents](img/B18875_09_6.jpg)\n\nFigure 9.6 – Basic Helm chart contents\nIf you want to learn more about Helm, go to [https://helm.sh/](https://helm.sh/).\nValidating Helm charts is not a trivial task; several options are available for this. Helm provides a basic `lint` command to accomplish this task, but it covers only basic format issues. In this book, you are using an open source tool called **kube-linter**, available as a Docker container. This will validate the YAML syntax of the Kubernetes manifests used to deploy the application and perform a series of best-practice checks. If you want to learn more about this tool, go to [https://docs.kubelinter.io/](https://docs.kubelinter.io/).\nNow that you have learned about Helm, let’s work on the IaC.\nVerifying and packaging IaC\nYou learned how to work with Azure Resource Manager templates in the previous chapter, so you need to validate the templates and publish them as artifacts to the pipeline.\nTo do this, you will create a `build-iac.yml` file in the repository and add the following seven segments to it (they have been separated in this section only to make it easier to read):\n\n*   `azure-pipeline.yaml` file:\n\n    ```", "```\n\n     *   **The jobs segment**: This segment groups all the subsequent segments that include only tasks:\n\n    ```", "```\n\n     *   **IaC catalog tasks segment**: This can be written as follows:\n\n    ```", "```\n\n    Let’s break it down:\n\n    *   The `AzureResourceManagerTemplateDeployment@3` task is used to validate the ARM templates for the catalog application\n    *   The `PublishPipelineArtifact@1` task is then used to publish the artifacts to be used for deployment *   **Catalog helm chart segment**: You can write this block as follows:\n\n    ```", "```\n\n    Let’s break it down:\n\n    *   The script task with `displayName` `'Lint Catalog Helm Chart'` performs a validation of the Helm chart\n    *   The `HelmInstaller@1` task installs the Helm tool\n    *   The `HelmDeploy@0` task is used to package the Helm chart *   **IaC cart tasks segment**: An example of this is as follows:\n\n    ```", "```\n\n    Let’s break it down:\n\n    *   The `AzureResourceManagerTemplateDeployment@3` task is used to validate the ARM templates for the cart application\n    *   The `PublishPipelineArtifact@1` task is then used to publish the artifacts to be used for deployment *   **IaC checkout tasks segment**: This section looks like the following:\n\n    ```", "```\n\n    Let’s break it down:\n\n    *   The `AzureResourceManagerTemplateDeployment@3` task is used to validate the ARM templates for the checkout application\n    *   The `PublishPipelineArtifact@1` task is then used to publish the artifacts to be used for deployment *   **IaC frontend tasks segment**: Here is a sample of this part of the code:\n\n    ```", "```\n\n    For simplicity, let’s break down what is happening:\n\n    *   The `AzureResourceManagerTemplateDeployment@3` task is used to validate the ARM templates for the frontend application\n    *   The `PublishPipelineArtifact@1` task is then used to publish the artifacts to be used for deployment \nThat brings us to the end of the `build-iac.yaml` file; make sure you keep all the segments together in the same file.\nNow that you have all the artifacts ready, let’s move on to create our environments.\nManaging environments\nIn this section, you will learn about how to create environments and deploy to them.\nConfiguring environments\nIn this section, you will define the environments in Azure Pipelines, which will be logical representations of the deployment targets. This will allow us to add approval and checks to control how the pipeline advances from one stage to the next:\n\n1.  You start by clicking on the **Environments** option under **Pipelines** in the main menu on the left, as follows:\n\n![Figure 9.7 – Accessing the Environments option in the menu](img/B18875_09_7.jpg)\n\nFigure 9.7 – Accessing the Environments option in the menu\n\n1.  If you have no environments, you will see a screen like the following; click on **Create environment**:\n\n![Figure 9.8 – Creating your first environment](img/B18875_09_8.jpg)\n\nFigure 9.8 – Creating your first environment\nOtherwise, you will see a **New environment** option in the top-right part of the screen above your existing environments.\n\n1.  Once the pop-up screen shows up to create the new environment, enter `test` for `Test Environment` for **Description**, leave the **Resource** option as **None**, and click on the **Create** button, as shown here:\n\n![Figure 9.9 – Creating the test environment](img/B18875_09_9.jpg)\n\nFigure 9.9 – Creating the test environment\n\n1.  Repeat *steps 2 and 3* to create another environment using `production` for `Production Environment` for **Description**.\n2.  With the two environments created, click on the **production** one in the list, as shown here:\n\n![Figure 9.10 – Environments](img/B18875_09_10.jpg)\n\nFigure 9.10 – Environments\n\n1.  In the next screen, you are going to add an approval check, to ensure you can only deploy to production once a human indicates it is possible.\n\n    For this, start by clicking on the ellipsis button in the top-right part of the screen and then on the **Approvals and checks** item, as shown here:\n\n![Figure 9.11 – Adding an environment approval gate](img/B18875_09_11.jpg)\n\nFigure 9.11 – Adding an environment approval gate\n\n1.  Since no checks have been added yet, you should see a screen like the following; selecting **Approvals** will get us to the next step to complete the check configuration:\n\n![Figure 9.12 – Adding an approval check](img/B18875_09_12.jpg)\n\nFigure 9.12 – Adding an approval check\nIn this form, provide the required approvers (it could be yourself initially). You can optionally provide instructions, such as manual steps to verify by the approver, and change **Timeout**. If you are the approver, you must make sure the **Allow approvers to approve their own runs** option is checked under **Advanced**. Finally, hit the **Create** button and you will be ready to move on to the next steps:\n![Figure 9.13 – Creating an approval check](img/B18875_09_13.jpg)\n\nFigure 9.13 – Creating an approval check\n\n1.  Lastly, you must also add permissions for each environment to allow them to be used in the **E2E-Azure** pipeline. Like before, click on the ellipsis option in the top-right part of the screen and click the **Security** option from the menu.\n\n![Figure 9.14 – Environment security settings](img/B18875_09_14.jpg)\n\nFigure 9.14 – Environment security settings\n\n1.  Then click on the **+** button and search for the **E2E-Azure** pipeline to add the permissions; just click on the name to add it.\n\n![Figure 9.15 – Adding pipeline permissions to environment](img/B18875_09_15.jpg)\n\nFigure 9.15 – Adding pipeline permissions to environment\nA properly configured environment will look like the following:\n![Figure 9.16 – Environment with pipeline permissions](img/B18875_09_16.jpg)\n\nFigure 9.16 – Environment with pipeline permissions\nImportant\nIt is a good practice to create environments for all deployment stages; this allows us to be modular and templatize deployment steps, giving us the opportunity to add approvals or gates later if need be.\nNow that you have our environments configured, let’s move on to the deployment steps.\nDeploying to environments\nYou will deploy the environment by creating a `deploy.yml` file and start by adding the steps needed for AKS deployment and the Python catalog service.\nThe `deploy.yml` file will start with the following content; you will be adding to it in every section hereafter:\n\n```", "```\n\n So far, you don’t have much in this file, but let’s break it down:\n\n*   The `parameters` section defines all the values available for reuse within the pipeline definition. The only one being used from the main pipeline is the `envName` one, but this gives you the flexibility to change them when needed.\n*   The `jobs` collection includes a new job type you haven’t used before called `deployment`, which allows us to implement different rollout strategies. For simplicity, here you will be using the `runOnce` strategy, but you can also use `canary` and `rolling` where appropriate. To learn more about these, go to [https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/jobs-deployment-strategy](https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/jobs-deployment-strategy).\n\nNow, you can proceed with the first service deployment steps.\nDeploying the Python catalog service to AKS\nThe deployment of the Python catalog service to Azure Kubernetes Service is performed as a two-phase process:\n\n1.  Deploy the ARM template to create and configure the AKS cluster using IaC. Refer to [*Chapter 8*](B18875_08.xhtml#_idTextAnchor103) for more information on how to do this.\n2.  Deploy the application using the Helm chart provided in the repository.\n\nIn our `deploy.yml` file, you will add the following six steps:\n\n1.  The `download` task `Download catalog iac` retrieves the pipeline artifact:\n\n    ```", "```\n\n     2.  The `AzureResourceGroupDeployment@2` task performs the AKS deployment:\n\n    ```", "```\n\n    Notice that you are using the `deploymentOutputs` property to set the name of a variable that will contain the outputs generated in the ARM template; this will be needed later for the frontend deployment.\n\n     3.  The `PowerShell@2` task parses out the cluster name from the output parameters of the ARM template deployment and makes it available as another variable for the duration of the job:\n\n    ```", "```\n\n     4.  The `download` task `Download catalog helm chart` retrieves the artifact:\n\n    ```", "```\n\n     5.  The `HelmInstaller@1` task installs Helm in the agent:\n\n    ```", "```\n\n     6.  The `HelmDeploy@0` task performs an `upgrade` command with the `install` option set to `true`; this will guarantee that, if it’s not found, it will be created:\n\n    ```", "```\n\n    This is called a `name` and `value`.\n\n    If the variable does not exist before running the command, then it will be created and made available at runtime.\n\nTo learn more about how to work with variables in Azure Pipelines, go to [https://learn.microsoft.com/en-us/azure/devops/pipelines/process/set-variables-scripts](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/set-variables-scripts).\nNow let’s move on to the Node.js cart service.\nDeploying a Node.js cart service to ACA\nThe deployment of the cart service to ACA is a bit simpler; it just requires the use of the `AzureResourceGroupDeployment@2` task after the artifact is downloaded:\n\n```", "```\n\n Notice, in this case, the use of `overrideParameters` to pass in the value of the `containerTag` parameter using `BuildNumber`.\nNext, you will add the deployment of the checkout service.\nDeploying a .NET checkout service to ACI\nThe deployment of the checkout service is very similar to the ACA deployment; the only difference is the use of the ACI service instead. See the following steps:\n\n```", "```\n\n Just like you did in the Node.js deployment to ACA, in this section, you used `overrideParameters` to pass in the value of the `containerTag` parameter using `BuildNumber`.\nNow let’s move on to the last application, the frontend.\nDeploying an Angular frontend app to AAS\nFor the frontend application, there are a few more steps necessary because of the need to gather information before being able to use the ARM template.\nYou will be adding the following steps to `deploy.yaml`:\n\n1.  The `download` task retrieves the frontend pipeline artifact:\n\n    ```", "```\n\n     2.  The `AzureCLI@2` task `Get Catalog App IP from AKS` is a script needed to retrieve the IP assigned to the exposed entry point of the catalog application. It uses the `az` CLI, the `kubectl` CLI, and the `jq` tool in Linux to parse out the information from Kubernetes. This is very specific to how this application was deployed. This script might not be reusable, but it is meant to show the flexibility of the tools if needed:\n\n    ```", "```\n\n     3.  The `PowerShell@2` task is used to parse out the fully qualified domain name contained in the output variables generated by the previous steps. It is used to deploy the catalog, cart, and checkout services:\n\n    ```", "```\n\n     4.  The `AzureResourceGroupDeployment@2` task deploys the Azure App Service instance and provides all the information necessary for the service to pull in the image, including the given `BuildNumber` as the tag. There are also the application URLs necessary to be stored in the service for the application to work:\n\n    ```", "```\n\n     5.  The `PowerShell@2` task `Get Frontend URL` then uses another script to parse the output of the ARM template deployment to provide it both in the logs and as a variable that could ultimately be used in additional steps, such as a web request to smoke test the endpoint. Alternatively, it could be used in automated test execution:\n\n    ```", "```\n\nWow, that was a lot of deployments, but you are not done! Once the test environment is complete, you get a chance to approve the continuation of deployment to production in the next section.\nApproving environment deployments\nWith the deployment to the test environment complete, you should be able to see the pipeline in the **Waiting** state, as follows:\n![Figure 9.17 – Stage awaiting for checks](img/B18875_09_17.jpg)\n\nFigure 9.17 – Stage awaiting for checks\nThis will only look like this if you are the reviewer configured for the manual approval check. Click on the **Review** button and a new screen will pop up with **Reject** and **Approve** options and the ability to provide a comment, as shown here:\n![Figure 9.18 – Approving an environment check](img/B18875_09_18.jpg)\n\nFigure 9.18 – Approving an environment check\nIf click the **Approve** button, the deployment will proceed. If you click the **Reject** button, the deployment will be canceled; also, if you don’t do anything and the timeout runs out, the pipeline will be canceled.\nNow that you have completed all the deployments, it is worth pointing out what to do if you run into issues with deployments; let’s talk now about some of the typical ones.\nTroubleshooting deployment issues\nCreating a stable and reliable CI/CD pipeline takes time, especially when performing deployments to a cloud platform such as Azure. Let’s walk through some of the typical issues you can run into.\nIssues deploying IaC\nYou used ARM templates to deploy the infrastructure to host the services that run the applications in Azure in this chapter, which means you are relying on that infrastructure to succeed before you can deploy the applications.\nThere are several situations in which the `AzureResourceGroupDeploment` task can fail:\n\n*   **Internal server errors**: These can occur if the Azure region you are trying to deploy is suddenly going through capacity issues or undergoing maintenance, or even if the Azure Pipelines service is going through issues itself.\n\n    *How to fix it*: Usually there is no recovery from this except for attempting to run the failed pipeline again. If the region in Azure becomes unavailable, you will have to wait until it becomes available again or target a different region for deployment as part of your disaster recovery strategy.\n\n*   **Timeout**: The deployment took too long, which could have been caused by the pipeline agent or the Azure deployment. The default timeout for Microsoft-hosted agents is 60 minutes in the free tier and 360 minutes when paying for parallel jobs.\n\n    *How to fix it*: You have a way to increase the timeout at the job level if required, but most likely there are other reasons why your deployment is failing. You will have to analyze the errors in the pipeline to find the root cause.\n\nFor other tips regarding deployments using ARM templates, head to [https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/best-practices](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/best-practices).\nIssues with scripts\nScripts used in your pipelines need to be written in an *idempotent* manner, meaning that for every command or task to execute properly, the script must verify whether the operation is required and whether the result code is what is expected. This approach ensures that a script only performs the operations required to reach the desired state and, in doing so, checks every step of the way whether the operation is indeed required. Not following this approach leads to brittle scripts (scripts that are easily broken), especially when interacting with Azure resources where the current state might not match the desired state.\nTo address this issue, always write your scripts in an idempotent way. Follow the `if-not-then` pattern for every operation.\nIssues with Helm\nHere are some issues commonly observed when working with Helm:\n\n*   Helm is a very convenient tool, but you are still responsible for the proper formatting of each of the manifests and making sure that they conform with and are valid for the Kubernetes API of your cluster. In this chapter, you learned how to use **kube-linter** to validate your Helm charts, but this is only a tool, and as such it can fail to detect issues. This tool only validates against the latest stable Kubernetes API, and if your cluster is not running this version, the validation will not catch issues that will arise when performing a deployment.\n\n    *How to fix it*: There are other open source tools that can validate against specific Kubernetes versions and perform different checks against your Kubernetes objects. A couple of examples are **kube-score** ([https://kube-score.com/](https://kube-score.com/)) and **Kubeconform** ([https://github.com/yannh/kubeconform](https://github.com/yannh/kubeconform)); put each of them to the test and evaluate which works better for your applications.\n\n*   Another issue to expect with Helm is the deployment of Kubernetes objects and the underlying consequences of this in Azure, such as deploying additional services in the case of ingress controllers. This scenario entails the creation of other Azure resources that in turn can sometimes fail.\n\n    *How to fix it*: If an operation failed due to a platform timeout or retriable error, there is nothing else to do but deploy again.\n\nWith this, we’ve finished the chapter. Let’s wrap it up.\nWinding up\nIf you completed all the steps, you will have deployed test and production environments, so it is time to clean up! This is important because you have deployed many resources into Azure. Make sure to delete them if you do not want to keep paying for them. You can do this via the Azure portal or the following Azure CLI command:\n\n```", "```\n\n If you missed anything or got stuck and are having trouble putting the entire solution together, the complete pipeline definitions can be found in the GitHub repository mentioned in the *Technical requirements* section; look in the `ch09/azure` directory, specifically the **complete** branch.\nNow, let’s recap what we have learned in this chapter.\nSummary\nIn this chapter, we took a complex solution and learned how to create CI/CD pipelines in a modular way, taking advantage of stages, environments, and templates. We also learned about adding checks throughout the stages of a pipeline. In this case, we added manual approval, but we saw that there are other controls that can be put in place to implement more complex scenarios. We learned briefly about containers and how building container-based applications with Docker Compose is easy and facilitates working with different programming languages at the same time in your pipelines; it also reduces the complexities of compiling and packaging them. We learned about semantic versioning and its applicability while learning about how build numbers can be used to tag or name artifacts from your pipelines, along with the importance of tracking artifacts. Lastly, we walked through the deployment of different services in Azure using ARM templates, learning about some of the intricacies of tying them together and the flexibility of pipelines to coordinate templates, regardless of the number of services to be deployed.\nNow that we have learned how to build and deploy this complex solution to different Azure services, the next chapter will be about doing this using **Amazon Web** **Services** instead.\n\n```"]