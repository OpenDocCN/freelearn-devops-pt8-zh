- en: Scaling Puppet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展Puppet
- en: Puppet is built to centrally manage all servers in an organization. In some
    organizations, the total node count may be in the hundreds. Other organizations
    have thousands or even tens of thousands of servers. For a smaller set of servers,
    we can configure a single monolithic Puppet Master (Puppetserver, PuppetDB or
    PE Console) on one server. Once we reach a certain size, we can export the components
    of Puppet Enterprise into separate servers. With even larger server sizes, we
    can begin to scale each component individually. This chapter will cover models
    of installing Puppet Enterprise, scaling to three servers, and finally load balancing
    multiple puppet components to support very large installations of Puppet.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet用于集中管理组织中的所有服务器。在一些组织中，节点总数可能达到数百个。其他组织则拥有成千上万的服务器。对于较少的服务器，我们可以在一台服务器上配置单一的整体Puppet
    Master（Puppetserver、PuppetDB或PE Console）。一旦达到一定规模，我们可以将Puppet Enterprise的组件导出到独立的服务器上。随着服务器规模的增大，我们可以开始逐个扩展每个组件。本章将介绍安装Puppet
    Enterprise的模型、扩展到三台服务器，并最终通过负载均衡多个Puppet组件来支持非常大的Puppet安装。
- en: When supporting a smaller subset of servers, the first stage is to optimize
    our settings on a monolithic master.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持较小子集的服务器时，第一阶段是优化我们在单一主节点上的设置。
- en: This chapter will primarily cover scaling Puppet Enterprise. Open source techniques
    will also be discussed in the context of this scaling, but full implementation
    methods will be left up to individual users of Puppet open source.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将主要讲解如何扩展Puppet Enterprise。开源技术也将在扩展的背景下讨论，但完整的实施方法将留给Puppet开源的用户自行完成。
- en: Inspection
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查
- en: Before we begin scaling our services, lets understand how to collect and understand
    metrics on those systems. A dashboard is included for both PuppetDB and the Puppet
    Enterprise console. We can use these dashboards to inspect the metrics of our
    system and identify problems along the way. As an environment grows, we want to
    ensure we have enough system resources available to Puppet to ensure that catalogs
    can be compiled and served to agents. A separate dashboard is provided for both
    PuppetDB and Puppetserver.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始扩展服务之前，首先了解如何收集和理解这些系统的指标。PuppetDB和Puppet Enterprise控制台都提供了仪表板。我们可以使用这些仪表板检查系统的指标并识别问题。随着环境的增长，我们希望确保Puppet有足够的系统资源，以确保能够编译并提供目录给代理。在PuppetDB和Puppetserver上分别提供了独立的仪表板。
- en: Puppetserver
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Puppetserver
- en: Puppetserver is the primary driver behind Puppet and is the only required component
    in open source Puppet. The Puppetserver developer dashboard is used to track the
    Puppet Master's ability to serve catalogs to agents. The primary area of tracking
    on this dashboard focuses on Puppetserver's JRubies. JRubies on the Puppetserver
    are simply small ruby instances contained in the **Java Virtual Machine** (**JVM**),
    dedicated to compiling catalogs for agents.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Puppetserver是Puppet的主要驱动程序，是开源Puppet中唯一必需的组件。Puppetserver开发者仪表板用于跟踪Puppet Master为代理提供目录的能力。该仪表板的主要关注点是Puppetserver上的JRuby。Puppetserver中的JRuby只是包含在**Java虚拟机**（**JVM**）中的小型ruby实例，专门用于为代理编译目录。
- en: You can reach the Puppetserver developer dashboard at `https://<puppetserver>:8140/puppet/experimental/dashboard.html`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下链接访问Puppetserver开发者仪表板：`https://<puppetserver>:8140/puppet/experimental/dashboard.html`。
- en: 'The dashboard contains a few live metrics about the Puppetserver, broken down
    into current metrics and average metrics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板包含一些关于Puppetserver的实时指标，分为当前指标和平均指标：
- en: '**Free JRubies**: The number of available JRuby instances to serve Puppet catalogs'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空闲JRuby**：可用于提供Puppet目录的可用JRuby实例数量。'
- en: '**Requested JRubies**: How many JRubies have been requested by agents'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**请求的JRuby**：代理请求的JRuby数量。'
- en: '**JRuby Borrow Time**: The amount of time in milliseconds the Puppetserver
    holds for a single request from an agent'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JRuby借用时间**：Puppetserver为单个请求从代理方保留的时间（毫秒）。'
- en: '**JRuby Wait Time**: How long an agent has to wait on average for a JRuby'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JRuby等待时间**：代理平均需要等待JRuby的时间。'
- en: '**JVM Heap Memory Used**: The amount of system memory the JVM containing the
    JRubies is consuming'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JVM堆内存使用量**：JVM中包含JRuby的系统内存使用量。'
- en: '**CPU Usage**: The CPU used by the Puppetserver'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU使用率**：Puppetserver所使用的CPU。'
- en: '**GC CPU Usage**: The amount of CPU used by **Garbage Collection** (**GC**)
    on the Puppetserver'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GC CPU使用率**：Puppetserver上**垃圾回收**（**GC**）所使用的CPU量。'
- en: '![](img/b8f9dd2d-37bd-4878-bcd3-b6201ba169ff.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8f9dd2d-37bd-4878-bcd3-b6201ba169ff.png)'
- en: We can inspect this data to get quite a bit of information about the primary
    job of the Puppetserver, which is to compile and serve catalogs. One of the first
    key components to look at is **JRuby Wait Time**. Are our nodes often waiting
    in line to receive catalogs? If we find the wait time increasing, we'll need more
    total JRubies available to serve the agents. This can also be indicated by a low
    average free JRubies count, or a high current requested JRubies status. We can
    also inspect the **JRuby Borrow Time** to get an idea of how big our catalogs
    are and how much time each node expects to be able to talk to the Puppetserver.
    Finally, we have some metrics to let us know if we've allocated enough memory
    and CPU to the Puppetserver.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查这些数据，获取关于 Puppetserver 主要任务的许多信息，这些任务包括编译和提供目录。首先要查看的关键组件之一是**JRuby 等待时间**。我们的节点是否经常等待接收目录？如果我们发现等待时间增加，可能需要更多的
    JRuby 来处理代理请求。这也可能表现为低平均空闲 JRuby 数量，或当前请求的 JRuby 状态较高。我们还可以检查**JRuby 借用时间**，以了解我们的目录有多大，以及每个节点需要多少时间与
    Puppetserver 进行通信。最后，我们还可以查看一些指标，以了解是否已为 Puppetserver 分配足够的内存和 CPU。
- en: We can also get some useful data about our API usage on **Top 10 Requests**,
    letting us know which APIs are being used most heavily in our infrastructure.
    **Top 10 Functions** help to identify which Puppet functions are being used most
    heavily on the master, and our **Top 10 Resources** can help us understand our
    most used code in an environment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以获取一些关于 API 使用的有用数据，例如**前 10 个请求**，让我们了解在我们的基础设施中最常用的 API。**前 10 个功能**有助于识别在主服务器上最常使用的
    Puppet 功能，而我们的**前 10 个资源**则可以帮助我们了解在环境中最常用的代码。
- en: PuppetDB dashboard
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PuppetDB 仪表盘
- en: 'PuppetDB has it''s own dashboard, designed to show what''s going on in the
    server. It is primarily aimed at making sense of the data that PuppetDB stores.
    It covers some performance metrics, like the JVM Heap, and also a quick active
    and inactive node count. The following information is available on PuppetDB:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PuppetDB 拥有自己的仪表盘，旨在展示服务器的运行情况。它主要用于帮助理解 PuppetDB 存储的数据。它涵盖了一些性能指标，如 JVM 堆，还能快速展示活动和非活动节点的数量。以下是
    PuppetDB 提供的信息：
- en: '**JVM Heap**: Total memory heap size of database'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JVM 堆**：数据库的总内存堆大小'
- en: '**Active and inactive nodes**: Nodes with information inside of PuppetDB'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动和非活动节点**：PuppetDB 中包含信息的节点'
- en: '**Resources**: Total resources seen in PuppetDB'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源**：PuppetDB 中看到的总资源数'
- en: '**Resource Duplication**: Total resources that are a duplicate that PuppetDB
    can serve (higher is better)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源重复**：PuppetDB 可以提供的重复资源总数（数量越高越好）'
- en: '**Catalog Duplication**: Total catalogs that are a duplicate that PuppetDB
    can serve (higher is better)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目录重复**：PuppetDB 可以提供的重复目录总数（数量越高越好）'
- en: '**Command Queue**: Number of commands waiting to be run'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命令队列**：等待执行的命令数量'
- en: '**Command Processing**: How long commands take to execute against the database'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命令处理**：执行命令时与数据库交互的耗时'
- en: '**Processed**: Number of queries processed since startup'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已处理**：自启动以来已处理的查询数量'
- en: '**Retried**: Number of queries that had to be run more than once'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重试**：需要多次执行的查询数量'
- en: '**Discard**: Number of queries that did not return a value'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丢弃**：没有返回值的查询数量'
- en: '**Rejected**: Number of queries that were rejected'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拒绝**：被拒绝的查询数量'
- en: '**Enqueuing**: Average amount of time spent waiting to write to the database'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排队**：等待写入数据库的平均时间'
- en: '**Command Persistence**: The time it takes to move data from memory to disk'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命令持久化**：将数据从内存移动到磁盘所需的时间'
- en: '**Collection Queries**: Collection query service time in seconds'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合查询**：集合查询服务时间（秒）'
- en: '**DB Compaction**: Round trip time for database compaction'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库压缩**：数据库压缩的往返时间'
- en: '**DLO Size on Disk**: Dynamic large object size on disk'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**磁盘上的 DLO 大小**：磁盘上动态大对象的大小'
- en: '**Discarded Messages**: Messages that did not enter PuppetDB'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丢弃的消息**：未进入 PuppetDB 的消息'
- en: '**Sync Duration**: Amount of time it takes to sync data between databases'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步时长**：同步数据到数据库之间所需的时间'
- en: '**Last Synced**: How many seconds since the last database sync'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最后同步**：自上次数据库同步以来的秒数'
- en: By default, PuppetDB runs the PuppetDB Dashboard on port `8080`, but restricts
    this to localhost. We can reach this locally on our machine by forwarding the
    web port onto our workstation. The command `ssh -L 8080:localhost:8080 <user>@<puppetdb-server>`
    will allow you reach the PuppetDB dashboard at `http://localhost:8080` on the
    same workstation the command was run on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PuppetDB 在端口 `8080` 上运行 PuppetDB 仪表盘，但将此限制为本地主机。我们可以通过将 Web 端口转发到工作站，在本地访问它。命令
    `ssh -L 8080:localhost:8080 <user>@<puppetdb-server>` 将允许你在运行该命令的同一工作站上访问 PuppetDB
    仪表盘，地址为 `http://localhost:8080`。
- en: '![](img/7e0e763e-7c7b-498b-931c-c0fe3d3b6583.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e0e763e-7c7b-498b-931c-c0fe3d3b6583.png)'
- en: We can use this information to check the status of our PuppetDB server. We want
    to see a high resource duplication and catalog duplication, which speeds up our
    overall runs of Puppet using PuppetDB. Our JVM heap can let us know how we're
    doing on memory usage. Active and inactive nodes help us understand what's being
    stored in PuppetDB, and what is on it's way out. Most other data is metrics surrounding
    the database itself, letting us know the health of the PostgreSQL server. Once
    we understand some simple live metrics, we can start looking at tuning our environment.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些信息检查 PuppetDB 服务器的状态。我们希望看到高资源重复和目录重复，这能加速使用 PuppetDB 运行 Puppet 的整体速度。我们的
    JVM 堆内存可以告诉我们内存使用情况。活动和非活动节点帮助我们了解存储在 PuppetDB 中的内容，以及即将被淘汰的内容。其他大多数数据是围绕数据库本身的指标，帮助我们了解
    PostgreSQL 服务器的健康状况。一旦我们了解一些简单的实时指标，就可以开始着手调整环境。
- en: Tuning
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调优
- en: Before moving into horizontal scaling of services, we should optimize the workload
    we have. The best horizontal scaling is scaling you don't need to do. Don't build
    more puppet component nodes until you can't support your workload with a single
    large monolithic instance. Adding more resources to Puppet allows it to serve
    more agents. There is no hard and fast rule on how many agents can be served by
    a monolithic Puppet Master, even with additional compile masters. The size of
    Puppet catalogs differs for every organization and is the primary unknown variable
    for most organizations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行服务的横向扩展之前，我们应该先优化现有的工作负载。最好的横向扩展是你根本不需要做的扩展。不要在你还可以用单个大型单体实例支持工作负载时就构建更多的
    Puppet 组件节点。为 Puppet 添加更多资源可以让它服务更多的代理。对于一个单体 Puppet 主节点能够服务多少代理没有硬性规定，即使有额外的编译主节点也是如此。每个组织的
    Puppet 目录大小都不同，这也是大多数组织的主要不确定变量。
- en: 'If you just need some simple settings to get started, Puppet keeps a list of
    standard recommended settings for small monolithic masters and monolithic masters
    with additional compile masters at: [https://puppet.com/docs/pe/latest/tuning_monolithic.html](https://puppet.com/docs/pe/latest/tuning_monolithic.html).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只需要一些简单的设置来入门，Puppet 为小型单体主节点以及带有附加编译主节点的单体主节点提供了标准的推荐设置，详情请见：[https://puppet.com/docs/pe/latest/tuning_monolithic.html](https://puppet.com/docs/pe/latest/tuning_monolithic.html)。
- en: Puppetserver tuning
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Puppetserver 调优
- en: The Puppetserver generates catalogs for each of our agents, using the code placed
    in our environments and served via JRubies. We'll be configuring the JVM and implementing
    our changes in Puppet in both an Enterprise and open source installation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Puppetserver 为每个代理生成目录，使用存放在我们环境中的代码并通过 JRuby 提供服务。我们将配置 JVM 并在企业版和开源版本的 Puppet
    中实施我们的更改。
- en: Puppetserver's primary job in our infrastructure is handling agent requests
    and returning a catalog. In older versions of Puppet, the RubyGem Passenger was
    commonly used to concurrently serve requests to multiple agents. Today Puppet
    runs multiple JRuby instances on the Puppetserver to handle concurrent requests.
    While Ruby itself runs with the operating system's native compiler, JRuby runs
    Ruby in an isolated JVM instance. These JRubies allow for better scaling with
    Puppet, providing multiple concurrent and thread-safe runs of Puppet. Each JRuby
    can serve one agent at a time, and Puppet will queue agents until a JRuby is available.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Puppetserver 在我们基础设施中的主要工作是处理代理请求并返回目录。在 Puppet 的旧版本中，常用 RubyGem Passenger 来并发处理多个代理的请求。今天，Puppet
    在 Puppetserver 上运行多个 JRuby 实例来处理并发请求。虽然 Ruby 本身是使用操作系统的本地编译器运行的，但 JRuby 在一个独立的
    JVM 实例中运行 Ruby。这些 JRuby 实例可以提供更好的扩展性，支持 Puppet 的多个并发和线程安全的运行。每个 JRuby 每次只能服务一个代理，Puppet
    会将代理排队，直到有 JRuby 可用。
- en: Every JVM (containing JRuby instances) has a minimum and maximum heap size.
    The maximum heap size determines how much memory a JVM can consume before garbage
    collection begins. Garbage collection is simply the process of clearing data from
    memory, starting from the oldest data to the newest. The minimum heap size ensures
    that new JVMs are started with enough memory allocated to run the application.
    If the JRuby can not allocate enough memory to the Puppet instance, it will trigger
    an `OutOfMemory` error and shut down the Puppetserver. We generally set our Java
    maximum heap size (sometimes referred to as -Xmx) and our minimum heap size (-Xms)
    to the same value, so that new JRubies start with the memory they need. We can
    also set the maximum number of JRuby instances using the `max-active-instances`.
    Puppet generally recommends this number be close to the number of CPUs available
    to the Puppetserver.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 JVM（包含 JRuby 实例）都有最小和最大堆大小。最大堆大小确定 JVM 在垃圾收集开始前可以消耗多少内存。垃圾收集只是从最旧的数据到最新的数据开始清除内存的过程。最小堆大小确保新的
    JVM 分配足够的内存来运行应用程序。如果 JRuby 无法为 Puppet 实例分配足够的内存，将触发 `OutOfMemory` 错误并关闭 Puppetserver。通常我们将我们的
    Java 最大堆大小（有时称为 -Xmx）和最小堆大小（-Xms）设置为相同的值，以便新的 JRuby 以所需的内存启动。我们还可以使用 `max-active-instances`
    设置最大 JRuby 实例数。Puppet 通常建议此数字接近 Puppetserver 可用的 CPU 数量。
- en: Puppet Enterprise implementation
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Puppet Enterprise 实现
- en: 'In Puppet Enterprise, we can configure our Java settings in Puppet with the
    following settings in Hiera:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Puppet Enterprise 中，我们可以通过 Hiera 中的以下设置配置我们的 Java 设置：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Open source implementation
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源实现
- en: 'In open source, we need to manage our settings with our own module. Luckily,
    `camptocamp/puppetserver` provides exactly what we need! We can use this module
    to create a profile that applies to our Puppetservers:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源中，我们需要使用自己的模块来管理我们的设置。幸运的是，`camptocamp/puppetserver` 提供了我们所需的一切！我们可以使用此模块创建适用于我们
    Puppetserver 的配置文件：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In an open source installation, the ulimits required for each component in larger
    installations may not be present. You can follow the instructions at [https://puppet.com/docs/pe/latest/config_ulimit.html](https://puppet.com/docs/pe/latest/config_ulimit.html)
    if your master is serving an immense number of nodes and is unable to open more
    files on the Linux operating system.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源安装中，较大安装中每个组件所需的 ulimits 可能不存在。如果您的主节点正在为大量节点提供服务且无法在 Linux 操作系统上打开更多文件，则可以按照
    [https://puppet.com/docs/pe/latest/config_ulimit.html](https://puppet.com/docs/pe/latest/config_ulimit.html)
    上的说明操作。
- en: PuppetDB tuning
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整 PuppetDB
- en: 'PuppetDB is installed on a PostgreSQL instance, and can generally be managed
    the same as any PostgreSQL server. We do have a few configuration options that
    can help tune your PostgreSQL PuppetDB instance to your environment:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: PuppetDB 安装在一个 PostgreSQL 实例上，通常可以像管理任何 PostgreSQL 服务器一样进行管理。我们有一些配置选项可以帮助调整您的
    PostgreSQL PuppetDB 实例以适应您的环境：
- en: Deactivate and purge nodes
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用和清除节点
- en: Tune max heap size
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整最大堆大小
- en: Tune threads
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整线程数
- en: Deactivating and purging nodes
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用和清除节点
- en: PuppetDB keeps records on every node that checks into your Puppet Enterprise
    installation. In an environment where nodes often come and go, such as an immutable
    infrastructure, lots of data can pile up about nodes that impact the performance
    of the database and infrastructure. By default, Puppet will expire nodes that
    have not checked in for seven days, and will cease exporting objects from the
    catalog. This setting can be managed with the `node-ttl` setting underneath the
    `[database]` section of `puppet.conf`. An additional setting, `node-purge-ttl`,
    lets the database know when to drop records for a node. By default, 14 days is
    the purge time for Puppet Enterprise. We can also perform these tasks manually
    with `puppet node deactivate` and `puppet node purge`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PuppetDB 保存每个节点在您的 Puppet Enterprise 安装中检查的记录。在节点经常出现和消失的环境中，例如不可变基础设施中，可能会堆积大量有关影响数据库和基础设施性能的节点数据。默认情况下，Puppet
    将过期未在七天内检查的节点，并将停止从目录导出对象。此设置可以在 `puppet.conf` 的 `[database]` 部分下管理 `node-ttl`
    设置。另一个设置 `node-purge-ttl` 让数据库知道何时删除节点的记录。默认情况下，Puppet Enterprise 的清除时间为 14 天。我们也可以使用
    `puppet node deactivate` 和 `puppet node purge` 手动执行这些任务。
- en: 'We can manage the default settings using `puppetlabs/inifile` as shown below:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `puppetlabs/inifile` 如下所示来管理默认设置：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Managing the heap size
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理堆大小
- en: The maximum heap size of our PuppetDB will depend on the total number of nodes
    checking into the system, the frequency of the Puppet runs, and the amount of
    resources managed by Puppet. The easiest way to determine heap size needs is to
    estimate or use defaults, and monitor the performance dashboard.  If your database
    triggers an `OutOfMemory` exception, just provide a larger memory allocation and
    restart the service. If the JVM heap metric often gets close to maximum, you'll
    need to increase the max heap size using Java args, managed by the PostgreSQL init
    script. PuppetDB will begin handling requests from the same point in the queue
    as when the service died. In an open source installation, this file will be named
    `puppetdb`, and will be named `pe-puppetdb` in a Puppet Enterprise installation.
    On an Enterprise Linux distribution (such as Red Hat), these files will be located
    in `/etc/sysconfig`. Debian based systems such as Ubuntu will place this file
    in `/etc/default`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 PuppetDB 的最大堆大小将取决于节点总数，Puppet 运行的频率以及 Puppet 管理的资源量。确定堆大小需求的最简单方法是估计或使用默认值，并监视性能仪表板。如果您的数据库触发
    `OutOfMemory` 异常，只需提供更大的内存分配并重新启动服务。如果 JVM 堆指标经常接近最大值，则需要使用 Java args 增加最大堆大小，由
    PostgreSQL 的 init 脚本管理。PuppetDB 将从服务死亡时的队列相同点开始处理请求。在开源安装中，此文件将被命名为 `puppetdb`，在
    Puppet Enterprise 安装中将被命名为 `pe-puppetdb`。在 Enterprise Linux 发行版（如 Red Hat）中，这些文件将位于
    `/etc/sysconfig`。像 Ubuntu 这样的基于 Debian 的系统将把此文件放在 `/etc/default` 中。
- en: 'In a Puppet Enterprise installation, we can set our heap size using the following
    Hiera values:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Puppet Enterprise 安装中，我们可以使用以下 Hiera 值设置我们的堆大小：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In an open source installation, preferably using `puppet/puppetdb` from the
    forge, we can simply set the Java args via the `puppetdb` class:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源安装中，最好使用来自 Forge 的 `puppet/puppetdb`，我们可以简单地通过 `puppetdb` 类设置 Java args：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tuning CPU threads
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整 CPU 线程
- en: Tuning CPU threads for PuppetDB is not always a simple case of *add more and it
    will perform better*. CPUs on the PuppetDB are in use for the PostgreSQL instance,
    the **Message Queue** (**MQ**) and web server provided by PuppetDB. If your server
    does have CPUs to spare, consider adding more CPU threads to process more messages
    at a time. If increasing the number of CPUs to PuppetDB is actually decreasing
    throughput, instead make sure more CPU resources are available for the MQ and
    web server. The setting for CPU threads is also found in `puppet.conf`, under
    the `[command-processing]` section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对 PuppetDB 进行 CPU 线程调整并不总是一个简单的情况，*添加更多并且它将表现更好*。PuppetDB 上的 CPU 正在使用 PostgreSQL
    实例、**消息队列**（**MQ**）和 PuppetDB 提供的 Web 服务器。如果您的服务器确实有可用的 CPU 资源，请考虑添加更多 CPU 线程以一次处理更多消息。如果增加
    CPU 数量到 PuppetDB 实际上降低了吞吐量，那么确保更多的 CPU 资源可供 MQ 和 Web 服务器使用。CPU 线程的设置也可以在 `puppet.conf`
    的 `[command-processing]` 部分找到。
- en: 'On a Puppet Enterprise installation, we''ll find this setting managed by Hiera:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Puppet Enterprise 安装中，此设置将由 Hiera 管理：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In an open source installation, we will again use `puppetlabs/puppetdb` to
    manage this setting:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源安装中，我们将再次使用 `puppetlabs/puppetdb` 来管理此设置：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Automatically determining settings
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动确定设置
- en: Now that we've seen some of the settings, we can look at some tools to help
    us deliver a decent baseline using our hardware. To begin with, we'll be looking
    at automatically tuning our full Puppet Enterprise installation and using PGTune
    to tune our PuppetDB instance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过一些设置，我们可以看一些工具，帮助我们利用我们的硬件提供一个体面的基线。首先，我们将自动调整我们的完整 Puppet Enterprise
    安装，并使用 PGTune 调整我们的 PuppetDB 实例。
- en: Puppet Enterprise
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Puppet Enterprise
- en: 'Before we inspect and tune our system, we will find a set of recommended settings
    based on the hardware available. Thomas Kishel at Puppet has designed a Puppet
    Face that queries PuppetDB for Puppet Enterprise Infrastructure. This command
    inspects available resources on the system and provides a sane default for the
    following Puppet Enterprise installations:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查和调整系统之前，我们将找到一组基于可用硬件的推荐设置。Puppet 的 Thomas Kishel 设计了一个 Puppet Face，用于查询
    PuppetDB 以获取 Puppet Enterprise 基础设施。此命令检查系统上可用的资源，并为以下 Puppet Enterprise 安装提供合理的默认设置：
- en: Monolithic infrastructure
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单体基础架构
- en: Monolithic with compile masters
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有编译主服务器的单体结构
- en: Monolithic with external PostgreSQL
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有外部 PostgreSQL 的单体结构
- en: Monolithic with compile masters with external PostgreSQL
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有外部 PostgreSQL 的单体结构
- en: Monolithic with HA
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有高可用性的单体结构
- en: Monolithic with compile masters with HA
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有编译主服务器和高可用性的单体结构
- en: Split infrastructure
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割基础架构
- en: Split with compile masters
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有编译主服务器的分割结构
- en: Split with external PostgreSQL
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有外部 PostgreSQL 的分割结构
- en: Split with compile masters with external PostgreSQL
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部PostgreSQL拆分编译主服务器
- en: 'To get started with `tkishel/pe_tune`, we''ll want to clone the Git repository
    onto our Puppet Enterprise on our primary master, and make the `tune.rb` script
    executable:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用`tkishel/pe_tune`，我们需要将Git仓库克隆到主服务器上的Puppet Enterprise中，并使`tune.rb`脚本可执行：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When we have the binary cloned and executable, we''ll want to run `tune.rb`
    to get information back about our system and return sane Puppet Enterprise settings
    in Hiera:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们克隆并使二进制文件可执行后，我们将运行`tune.rb`来获取有关系统的信息，并在Hiera中返回合理的Puppet Enterprise设置：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can then place these values in Hiera anywhere that the Puppet Enterprise
    installation would be able to pick them up. I recommend `common.yaml`, unless
    you have a Hiera layer specifically set aside for Puppet settings.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将这些值放入Hiera中的任何地方，Puppet Enterprise安装可以从那里获取。我建议使用`common.yaml`，除非你有专门为Puppet设置而保留的Hiera层。
- en: This script will fail to run by default on infrastructure hosts with less than
    4 CPUs or 8 GB of RAM. You can run the command with the `--force` flag to get
    results, even on nodes that are smaller than the recommended 4 CPUs and 8GB of
    memory.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，该脚本将在少于4个CPU或8GB内存的基础设施主机上无法运行。即使在小于推荐的4个CPU和8GB内存的节点上，你也可以使用`--force`标志来运行命令，获取结果。
- en: PuppetDB – PostgreSQL with PGTune
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PuppetDB – 使用PGTune的PostgreSQL
- en: When in doubt about how to tune a PostgreSQL server, try PGTune. This project
    will read your current `postgresql.conf` and output a new one with tuning settings
    designed for the machine it's running on. As an important side note, this will
    not take into account the necessary memory for the message queue or the web server,
    so leaving a small amount of extra resources by slightly tuning down these settings
    can help with performance.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当不确定如何调整PostgreSQL服务器时，尝试使用PGTune。该项目会读取当前的`postgresql.conf`并输出一个新的配置文件，包含为当前机器设计的调优设置。值得注意的是，这不会考虑消息队列或Web服务器所需的内存，因此通过稍微调低这些设置来留下少量额外资源，可以帮助提升性能。
- en: Please note that PGTune assumes the only purpose of the node it is running on
    is to serve a Postgres server. These settings will be difficult to use on a single
    monolithic master, and `tkishel/pe_tune` will be a much more useful tool for configuring
    these servers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PGTune假设它运行的节点唯一的目的是提供Postgres服务器。这些设置在单一的整体主服务器上使用会比较困难，而`tkishel/pe_tune`会是配置这些服务器时更有用的工具。
- en: 'We''ll want to begin by cloning and entering the current PGTune project:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从克隆并进入当前的PGTune项目开始：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we run PGTune against our Puppet Enterprise `postgresql.conf`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将对Puppet Enterprise的`postgresql.conf`运行PGTune：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'These settings come back in a form for manually managing a `postgresql.conf`.
    Let''s translate these values into Puppet Enterprise Hiera settings that can be
    placed in `common.yaml` to drive our PuppetDB:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置以手动管理`postgresql.conf`的形式返回。我们将这些值转换为Puppet Enterprise Hiera设置，并可以放入`common.yaml`中来驱动我们的PuppetDB：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When using open source, we''ll instead want to lean on the `puppetlabs/postgresql`
    module that is a dependency of `puppetlabs/puppetdb`. Each value we want to set
    is an individual resource, and can be represented in Hiera at the PuppetDB level.
    I would not recommend putting these particular settings in `common.yaml` if you
    have other PostgreSQL servers in your environment:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源时，我们将依赖于`puppetlabs/postgresql`模块，它是`puppetlabs/puppetdb`的一个依赖。我们想要设置的每个值都是一个独立的资源，可以在PuppetDB级别的Hiera中表示。如果你在环境中有其他PostgreSQL服务器，建议不要将这些特定设置放在`common.yaml`中：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Understanding these key concepts allows us to configure our individual nodes
    to maximize performance. For many users, this will be enough to run Puppet in
    their environment. For more extreme cases, we can turn to horizontal scaling,
    allowing more copies of our Puppetservers and PuppetDBs to support more agents.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些关键概念可以让我们配置各个节点以最大化性能。对于许多用户来说，这已经足够在他们的环境中运行Puppet。如果是更为极端的情况，我们可以转向水平扩展，允许更多的Puppetserver和PuppetDB副本支持更多的代理。
- en: Horizontal scaling
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平扩展
- en: 'When a single monolithic master can no longer serve our environment, we split
    our master into distinct components: console, Puppetserver and PuppetDB.  This
    allows us to serve more clients with a smaller footprint. In an ever growing environment,
    even this setup may not be able to cover your needs for all agents.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当单一的整体主服务器无法再满足我们的环境需求时，我们将主服务器拆分为独立的组件：控制台、Puppetserver和PuppetDB。这使我们能够以更小的足迹服务更多的客户端。在不断增长的环境中，即便是这种设置，也可能无法满足所有代理的需求。
- en: In this section, we'll be discussing the scaling of Puppetserver, PuppetDB and
    our certificate authority to serve more agents. With concepts of vertical tuning
    and horizontal scaling, we can serve a very large installation of nodes, up to
    the tens of thousands of individual servers on a single setup.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何扩展 Puppetserver、PuppetDB 和证书授权中心，以便为更多的代理提供服务。通过垂直调优和水平扩展的概念，我们可以为大量节点提供服务，最多可在单一设置中支持数万个单独的服务器。
- en: Puppetserver
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Puppetserver
- en: Generally, the first component that is required to scale in any Puppet setup
    is the Puppetserver. The Puppetserver does the bulk of the work in Puppet, compiling
    catalogs to agents. In this section, we're going to explore some of the theory
    behind how many agents a Puppetserver can support, how to create new Puppetservers,
    and some load balancing strategies around your Puppet Masters. We'll be viewing
    this from the lens of open source and Enterprise.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在任何 Puppet 设置中，首个需要扩展的组件是 Puppetserver。Puppetserver 承担了 Puppet 中的大部分工作，负责将目录编译传输到代理。在本节中，我们将探讨一些关于
    Puppetserver 能支持多少个代理的理论，如何创建新的 Puppetservers，以及围绕 Puppet 主节点的一些负载均衡策略。我们将从开源和企业版的角度来进行分析。
- en: Estimating the number of agents a Puppetserver supports
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估算一个 Puppetserver 能支持的代理数量
- en: Puppet has a mathematics equation for estimating how many nodes a Puppetserver
    can support. This equation is an estimate and should not replace actual benchmarks,
    as things such as catalog compile size often shift over time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet 有一个数学公式用于估算一个 Puppetserver 能够支持多少个节点。这个公式只是一个估算值，不能替代实际的基准测试，因为如目录编译大小等因素通常会随着时间的推移发生变化。
- en: 'The estimation of Puppetservers is represented as *j = ns/mr*. In this equation,
    we see the following values:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Puppetserver 的估算公式表示为 *j = ns/mr*。在这个公式中，我们看到以下的值：
- en: '*j*: JRuby instances per master'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*j*：每个主节点的 JRuby 实例数量'
- en: '*m*: Number of compile masters (Puppetservers)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*：编译主节点数量（Puppetservers）'
- en: '*n*: Number of nodes served by the master'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n*：主节点服务的节点数量'
- en: '*s*: Catalog compile size in seconds'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s*：目录编译大小（秒）'
- en: '*r*: Run interval in seconds'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r*：运行间隔（秒）'
- en: 'Using this equation, let''s post a simple metric to work with: how many nodes
    can a single Puppetserver with one JRuby instance serve, with an average catalog
    compile time of 10 seconds and a default run interval of 30 minutes? Our equation
    looks like this: *1 = n10 / 1*1800*. We can simplify this to *1 = n10 / 1800*.
    We can multiple both sides of our equation to get *1800 = n10*. Simplifying by
    dividing both sides by 10 gives us *n = 180*.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个公式，我们可以设定一个简单的指标来进行计算：一台拥有一个 JRuby 实例的 Puppetserver，平均目录编译时间为 10 秒，默认运行间隔为
    30 分钟，最多能为多少个节点提供服务？我们的公式如下：*1 = n10 / 1*1800*。我们可以简化为 *1 = n10 / 1800*。将公式两边相乘得到
    *1800 = n10*。通过将两边同时除以 10，我们得到 *n = 180*。
- en: 'A single master, with one JRuby instance, with a run interval of 30 minutes
    and catalog compile time of 10 seconds can serve 180 agents. If we want to serve
    more agents, we have the following options:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一台主节点，拥有一个 JRuby 实例，运行间隔为 30 分钟，目录编译时间为 10 秒，可以为 180 个代理提供服务。如果我们想要支持更多代理，可以选择以下方式：
- en: Increase the number of JRuby instances per master
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加每个主节点的 JRuby 实例数量
- en: Increase the number of compile masters
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加编译主节点数量
- en: Decrease run interval
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少运行间隔
- en: Decrease catalog compilation times with more efficient code
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更高效的代码减少目录编译时间
- en: Just increasing this tiny server to a server with 8 CPUs, and setting the `jruby_max_active_instances`
    setting to 8 would allow us to serve 1,440 agents on this server. Adding two more
    compile masters with the same number of CPUs would get us to 4,320 agents to serve.
    We can continually add more Puppetservers to this until we have the ability to
    serve all the nodes in our infrastructure.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 只需将这个小型服务器升级为具有 8 个 CPU 的服务器，并将 `jruby_max_active_instances` 设置为 8，就可以在此服务器上支持
    1,440 个代理。再增加两个具有相同数量 CPU 的编译主节点后，可以支持 4,320 个代理。我们可以不断增加更多的 Puppetservers，直到能够为我们基础设施中的所有节点提供服务。
- en: Adding new compile masters
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加新的编译主节点
- en: 'In a Puppet Enterprise installation, bringing on new compile masters is very
    easy. Simply add a new node to the PE Master **Classification** group underneath
    the PE Infrastructure:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Puppet Enterprise 安装中，增加新的编译主节点非常容易。只需将新节点添加到 PE Master **分类**组下的 PE 基础设施中：
- en: '![](img/21a8fd6c-27e0-4e14-8453-82118a0b64cf.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21a8fd6c-27e0-4e14-8453-82118a0b64cf.png)'
- en: These nodes will receive the same configuration as the Primary Master, including
    code manager configuration and necessary connections to PuppetDB. There are no
    hidden tricks to managing additional compile masters in Puppet Enterprise. Classify
    and add them to a load balancer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些节点将接收与主节点相同的配置，包括代码管理器配置和与PuppetDB的必要连接。管理Puppet Enterprise中额外编译主节点没有任何隐藏的技巧。只需将它们分类并添加到负载均衡器中。
- en: 'In open source, we need to ensure each Puppet Master is configured to use PuppetDB.
    Luckily, `puppetlabs/puppetdb` provides that connection for us:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源版本中，我们需要确保每个Puppet Master都配置为使用PuppetDB。幸运的是，`puppetlabs/puppetdb`为我们提供了这个连接：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We''ll still need to make sure this open source installation has the ability
    to retrieve code. r10k does not federate across servers, unlike Code Manager,
    so you''ll need to determine a strategy for deploying code out to these masters.
    One easy method of managing this is included in the `puppet/r10k module`! Not
    only can the `puppet/r10k` module configure r10k in the same way across each Puppetserver,
    but a new Puppet task is available for deploying code in that module. This can
    be run from the command line, or preferably from a CI/CD server on commit:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要确保这个开源安装能够检索代码。与Code Manager不同，r10k不能跨服务器进行联邦管理，因此你需要确定将代码部署到这些主节点的策略。管理此问题的一种简便方法包含在`puppet/r10k模块`中！`puppet/r10k`模块不仅可以在每个Puppetserver上以相同方式配置r10k，还可以在该模块中使用新的Puppet任务来部署代码。可以从命令行运行此任务，或者最好是在CI/CD服务器上进行提交时运行：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Load balancing
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: When we have multiple Puppetservers, it's important we decide how agents determine
    which server to connect to. We'll be inspecting a simple strategy of placing Puppetservers
    closest to the nodes they serve, as well as load balancing strategies that cover
    larger infrastructure needs. These two methods can be combined if there is a security
    requirement for isolated masters and a technical requirement for more catalog
    compilation from additional Puppetservers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有多个Puppetserver时，重要的是要决定代理如何确定连接到哪个服务器。我们将检查一种简单的策略，即将Puppetserver放置在最接近它们所服务节点的位置，以及适用于更大基础设施需求的负载均衡策略。如果存在隔离主节点的安全需求和更多Puppetserver进行目录编译的技术需求，这两种方法可以结合使用。
- en: Simple setup – direct connection
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单设置 – 直接连接
- en: 'One of the simplest setups many organizations use is to isolate data centers
    and provide a Puppetserver for each data center. Some organizations have data
    centers across the world, whether in the cloud in regions, or on site in various
    locations. Providing a compile master to these individual data centers is a fairly
    simple task and only requires a few things:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织使用的最简单的设置之一是隔离数据中心并为每个数据中心提供一个Puppetserver。一些组织在全球各地都有数据中心，无论是在云中不同的区域，还是在不同地点的本地数据中心。为这些单独的数据中心提供编译主节点是一个相对简单的任务，只需要做几件事：
- en: The agent is aware of compile master FQDN and has network connectivity to it
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理知道编译主节点的完全限定域名（FQDN）并具有与其的网络连接。
- en: Compile master has connectivity back to the primary master, sometimes called
    **Master of Masters**
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译主节点与主节点有连接，主节点有时被称为**主节点的主节点**。
- en: In this setup, during provisioning an agent would reach out to the local compile
    master for it's agent installation. On a Puppet Enterprise installation, the agent
    can simply run `curl -k https://<compile_master>:8140/packages/current/install.bash` command
    during provisioning, and it will retrieve an agent thanks to the `pe_repo` classification
    found in the PE Master node group. This agent will not need network connectivity
    to PuppetDB, the Primary Master, or the PE console, as information will be handled
    by the compile master in the middle.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，在配置过程中，代理会联系本地的编译主节点进行代理安装。在Puppet Enterprise安装中，代理只需在配置过程中运行`curl -k
    https://<compile_master>:8140/packages/current/install.bash`命令，它将通过在PE Master节点组中找到的`pe_repo`分类来获取代理。该代理不需要与PuppetDB、主节点或PE控制台的网络连接，因为信息将由中间的编译主节点处理。
- en: 'The following infographic from Puppet shows the necessary firewall connections
    required for each component in a large environment installation of Puppet Enterprise:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自Puppet的图示，展示了在Puppet Enterprise的大型环境安装中，每个组件所需的防火墙连接：
- en: '![](img/4073ac03-1ee4-4b75-bdf9-9b9f97394d6a.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4073ac03-1ee4-4b75-bdf9-9b9f97394d6a.png)'
- en: These same ports remain true in an open source installation, although the node
    classifier API endpoint will not be available from the Puppet console.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在开源安装中，这些端口依然适用，尽管从Puppet控制台无法访问节点分类器API端点。
- en: If a single data center grows so large that it needs multiple compile masters,
    or we want to centralize our compile masters for every data center, we'll instead
    need to focus on load balancing. Everything in this section still applies in a
    load balanced cluster, but there are a few new pieces to work with behind a load
    balancer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单个数据中心发展到需要多个编译主机，或者我们希望为每个数据中心集中管理编译主机，那么我们就需要关注负载均衡。在负载均衡的集群中，本节的所有内容仍然适用，但在负载均衡器后面有一些新要处理的内容。
- en: Load balancing
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'In very large environments, we may worry about having enough resources to serve
    all of our agents. We start building more compile masters and our agents need
    to connect to them. There are only a few key additional concerns when placing
    our compile masters behind a load balancer: certificate management and load balancing
    strategy.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常大的环境中，我们可能会担心是否有足够的资源为所有代理提供服务。我们开始构建更多的编译主机，代理需要连接到它们。当将编译主机放置在负载均衡器后时，只有几个关键的附加问题需要关注：证书管理和负载均衡策略。
- en: 'Puppet builds trusted SSL connections between agents and masters at compile
    time using self-signed certificates. The FQDN of both the master and the agent
    are recorded in their respective certs by default. During each connection, the
    agent inspects the certificate to ensure that the requested domain name is in
    the certificate. If our agent uses DNS or a VIP from load balancing to connect
    to a master at `puppet.example.com`, and the certificate does not contain that
    name explicitly, the agent will reject the connection. We want to identify a common
    name for our pool of compile masters (often just a shortname, such as `puppet`),
    and embed that into the certificate. We can include multiple DNS alt names in
    the `puppet.conf` in the main section on each compile master:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Puppet在编译时使用自签名证书在代理和主机之间建立受信任的SSL连接。默认情况下，主机和代理的FQDN会被记录在各自的证书中。在每次连接时，代理会检查证书以确保请求的域名包含在证书中。如果我们的代理通过DNS或负载均衡的VIP连接到`puppet.example.com`，但证书中没有明确包含该名称，代理将拒绝连接。我们需要为我们的编译主机池确定一个通用名称（通常只是一个短名称，例如`puppet`），并将其嵌入到证书中。我们可以在每个编译主机的`puppet.conf`文件的主配置部分中包含多个DNS备用名称：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When we connect to the Puppet Master for the first time, these `dns_alt_names`
    will be embedded into our certificate. For Enterprise users, this certificate
    will not show up in the Puppet Enterprise console, so that no one can accidentally
    approve DNS alt names from the GUI. You'll need to log in to the Puppet Master
    and run `puppet cert sign <name> --allow-dns-alt-names` to sign the certificate,
    and accept it with alternate names. If you have already built this compile master
    and need to regenerate the certificates, you can run `puppet cert clean <name>`
    on the Master of Masters, and remove the SSL directory with `sudo rm -r $(puppet
    master --configprint ssldir)` on the compile master prior to running the agent
    again.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们第一次连接到Puppet Master时，这些`dns_alt_names`将被嵌入到我们的证书中。对于企业用户，这些证书不会出现在Puppet
    Enterprise控制台中，以防止有人意外地通过GUI批准DNS备用名称。您需要登录到Puppet Master并运行`puppet cert sign
    <name> --allow-dns-alt-names`来签署证书，并接受带有备用名称的证书。如果您已经构建了这个编译主机并需要重新生成证书，您可以在Master
    of Masters上运行`puppet cert clean <name>`，并在重新运行代理之前，在编译主机上使用`sudo rm -r $(puppet
    master --configprint ssldir)`删除SSL目录。
- en: It is generally considered safe to remove the SSL directory on any agent, including
    compile masters. Running this on the Master of Masters, which acts as the centralized
    Certificate Authority, on the other hand, will cause all SSL connections and all
    Puppet runs to stop in the environment. If you do this, you'll need to rebuild
    your certificate authority on the Master of Masters. Directions can be found at: [https://docs.puppet.com/puppet/4.4/ssl_regenerate_certificates.html](https://docs.puppet.com/puppet/4.4/ssl_regenerate_certificates.html).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为，在任何代理上，包括编译主机，删除SSL目录是安全的。然而，在作为集中式证书授权机构的主机上执行此操作，将会导致所有SSL连接和所有Puppet运行在环境中停止。如果这样做，您将需要在主机上重建证书授权。相关的操作说明可以参考：[https://docs.puppet.com/puppet/4.4/ssl_regenerate_certificates.html](https://docs.puppet.com/puppet/4.4/ssl_regenerate_certificates.html)。
- en: 'Your agents should now be referring to all compile masters by their common
    DNS alt name. You''ll need to decide a load balancing strategy: using DNS round
    robin, DNS SRV records, or a dedicated load balancer. Major DNS providers provide
    a mechanism for DNS round robin and SRV records, and you should consult their
    documentation. We''ll walk through a sample of setting up HAProxy as a software
    load balancer for our compile masters, as if they were all in a single pool. We''ll
    be using `puppetlabs/haproxy` and the usage sample on the forge to build a HAProxy
    instance for multiple compile masters. We could use our exported resources sample
    from [Chapter 9](036a4b96-b91a-4c72-83dc-e5505efc26cd.xhtml), *Exported Resources*,
    but we''ll use a simple example as we don''t often add Puppet Masters to our load
    balancer:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的代理应该通过它们的公共 DNS 替代名称引用所有编译主机。你需要决定负载均衡策略：使用 DNS 轮询、DNS SRV 记录，或使用专用的负载均衡器。主要的
    DNS 提供商都提供了 DNS 轮询和 SRV 记录机制，你应该参考他们的文档。我们将通过一个示例，设置 HAProxy 作为我们的编译主机的软件负载均衡器，假设它们都在一个单独的池中。我们将使用`puppetlabs/haproxy`及其使用示例来为多个编译主机构建一个
    HAProxy 实例。我们也可以使用从[第 9 章](036a4b96-b91a-4c72-83dc-e5505efc26cd.xhtml)中导出的资源示例，*导出的资源*，但是由于我们通常不会将
    Puppet 主机添加到负载均衡器中，所以我们将使用一个简单的示例。
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using this configuration, our HAProxy will be able to serve requests to all
    agents requesting a connection to a compile master.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，我们的 HAProxy 将能够为所有请求连接到编译主机的代理提供服务。
- en: Certificate authority
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 证书颁发机构
- en: In a Puppet Enterprise installation, the certificate authority portion of compile
    masters is fairly easy to solve. Puppet Enterprise uses separate node groups for
    a CA and compile master. By adding additional compile masters to the PE Master
    classification group, each master is configured to use the centralized certificate
    authority on the Master of Masters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Puppet Enterprise 安装中，编译主机的证书颁发机构部分相对容易解决。Puppet Enterprise 为 CA 和编译主机使用不同的节点组。通过将额外的编译主机添加到
    PE 主分类组中，每个主机都配置为使用 Master of Masters 上的集中式证书颁发机构。
- en: In Puppet open source, we'll need to disable the certificate authority on each
    of our compile masters using Trapperkeeper. You can simply open `/etc/puppetlabs/puppetserver/services.d/ca.cfg`
    and comment out the line `puppetlabs.services.ca.certificate-authority-service/certificate-authority-service`
    and uncomment `#puppetlabs.services.ca.certificate-authority-disabled-service/certificate-authority-disabled-service`.
    Finally, you'll need each agent in your infrastructure (including the compile
    masters) to add the `ca_server` setting into the `[main]` section of the `puppet.conf`,
    pointing at the Master of Masters. Note that this requires network connectivity
    over the CA port to the Master of Masters, which by default is `8140`, but can
    be toggled with the `ca_port` setting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Puppet 开源版本中，我们需要在每个编译主机上禁用证书颁发机构（CA），可以通过 Trapperkeeper 来完成。你只需打开`/etc/puppetlabs/puppetserver/services.d/ca.cfg`文件，注释掉`puppetlabs.services.ca.certificate-authority-service/certificate-authority-service`这一行，并取消注释`#puppetlabs.services.ca.certificate-authority-disabled-service/certificate-authority-disabled-service`。最后，你需要在基础设施中的每个代理（包括编译主机）上，在`puppet.conf`的`[main]`部分中添加`ca_server`设置，指向
    Master of Masters。请注意，这要求通过 CA 端口与 Master of Masters 建立网络连接，默认端口是`8140`，但可以通过`ca_port`设置进行调整。
- en: The final goal of this setup is that each compile master has a DNS alt name,
    and every agent is connecting to the master via that DNS alt name, while using
    the Master of Master as the certificate authority for all nodes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置的最终目标是每个编译主机都有一个 DNS 替代名称，并且每个代理都通过该 DNS 替代名称连接到主机，同时使用 Master of Masters
    作为所有节点的证书颁发机构。
- en: PuppetDB
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PuppetDB
- en: 'Scaling PuppetDB is generally scaling PostgreSQL. A single PuppetDB can cover
    a large number of nodes and compile masters, but should you need to scale PuppetDB,
    consult PostgreSQL documentation and organizational database guidance. Known methodologies
    of scaling PostgreSQL that work with Puppet include:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 PuppetDB 通常是扩展 PostgreSQL。一个单独的 PuppetDB 可以覆盖大量的节点和编译主机，但如果需要扩展 PuppetDB，请参考
    PostgreSQL 文档和组织数据库指导。已知的扩展 PostgreSQL 方法，包括与 Puppet 配合使用的有：
- en: High availability setups
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性设置
- en: Load balancing
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Database replication
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库复制
- en: Database clustering
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库集群
- en: Connection pooling
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接池
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we talked about scaling Puppet. We started by learning how
    to monitor the components inside Puppet and how to tune individual Puppet components.
    We then discussed horizontal scaling, adding more compile masters to serve more
    agents. We discussed how to load balance our Puppetservers behind a HAProxy and
    discussed that PuppetDB can be scaled like any PostgreSQL database.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何扩展 Puppet。我们首先学习了如何监控 Puppet 内部组件，并如何调整单个 Puppet 组件的性能。接着，我们讨论了横向扩展，增加更多的编译主机以服务更多的代理。我们还讨论了如何通过
    HAProxy 对 Puppet 服务器进行负载均衡，并提到 PuppetDB 可以像任何 PostgreSQL 数据库一样进行扩展。
- en: In our next chapter, we'll look at troubleshooting Puppet Enterprise. Learning
    to read and understand the errors you may see in Puppet will teach you to be a
    better practitioner, and allow you to really understand the Puppet system.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨 Puppet Enterprise 的故障排除。学习如何阅读和理解 Puppet 中可能出现的错误，将帮助你成为更好的实践者，并让你真正理解
    Puppet 系统。
