- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Revisiting OpenStack – Design Considerations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重访 OpenStack – 设计考虑因素
- en: “I have found you have got to look back at the old things and see them in a
    new light.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “我发现你必须回顾过去的事物，并从新的角度看待它们。”
- en: '- John Coltrane'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '- 约翰·科尔特兰'
- en: Since the last edition of the *Mastering OpenStack* series, the OpenStack community
    has kept the momentum by growing and innovating around its ecosystem. At the time
    of writing this edition, the OpenStack design has been going through different
    cycles of improvements by including new projects and providing seamless integration
    with existing systems to respond to organization demands and custom features.
    Since the first *Austin* release back in 2010, the innovation kept opening new
    opportunities for companies and quickly adopting a stable private cloud setup
    to stay ahead in the market. The challenges of handling a scalable infrastructure
    have been felt by big and medium players who joined the OpenStack evolvement.
    Today, as the first page of this edition is being written, 14 years and 28 releases
    have already passed through different OpenStack releases, from *Austin* back in
    the day to the latest *Dalmatian* release. That is a full circle back to the beginning
    of the alphabet. A lot of experiences and insights have been revealed that have
    boosted the OpenStack community and made it the world’s leading open source cloud
    platform.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自从*精通 OpenStack*系列的上一个版本发布以来，OpenStack 社区一直保持着势头，围绕其生态系统不断增长和创新。在撰写本版本时，OpenStack
    设计经历了不同周期的改进，通过引入新项目并与现有系统提供无缝集成，以应对组织需求和定制功能。自2010年首次发布的*Austin*版本以来，创新不断为公司开辟了新的机会，并迅速采用稳定的私有云设置，以在市场中保持领先地位。大公司和中型企业在加入
    OpenStack 的演变过程中，感受到了处理可扩展基础设施的挑战。如今，在本版的第一页正在书写之际，14年和28个版本已经穿越了不同的 OpenStack
    发布，从最初的*Austin*版本到最新的*Dalmatian*版本。这是一个回到字母表开头的完整循环。许多经验和见解已经揭示出来，推动了 OpenStack
    社区的发展，使其成为全球领先的开源云平台。
- en: Numbers show how successfully OpenStack has been adopted by mid to large enterprises.
    As per the annual *OpenStack User Survey* results in the OpenStack blog in November
    2022, it was declared that more than 40 million cores are running in production
    across 300 deployments empowered by OpenStack.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据显示 OpenStack 已经成功地被中型和大型企业广泛采用。根据2022年11月在 OpenStack 博客上发布的年度*OpenStack 用户调查*结果，报告称，超过4000万个核心正在生产环境中运行，支持
    OpenStack 的300个部署。
- en: The OpenInfra community has been committing enormously to hardening the most
    stable versions of OpenStack software. Starting from the *Antelope* release that
    was launched in March 2023 and, more recently, the *Dalmatian* release in October
    2024, organizations have quickly upgraded, and the good news is a more stable
    and reliable version than ever. The secret sauce of this great achievement is
    that the community has switched gears on stabilizing basic OpenStack services,
    including compute, storage, and network services, increasing the cadence of release
    testing of each service extensively and thus leaving neither possible gaps nor
    potential vulnerabilities that could raise bugs in deployments in production.
    On the other side of the story, by going through the map of each OpenStack release,
    you might notice some extension projects appearing and disappearing. Community
    contributors did the right thing, discounting non-stable features that would hinder
    the maturity of the OpenStack private setup. Today, the release is exposing basic
    services in addition to more projects and features in a stable upstream. The other
    bright side of the story is that the OpenStack ecosystem has always been one step
    ahead to adopt new trends of market technologies that include containerization,
    serverless, big data analysis, and so on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: OpenInfra 社区在加强 OpenStack 软件最稳定版本方面做出了巨大贡献。从2023年3月发布的*Antelope*版本开始，到最近的2024年10月发布的*Dalmatian*版本，组织们迅速完成了升级，值得庆幸的是，这是一个比以往任何时候都更稳定可靠的版本。这一巨大成就的秘诀在于，社区已经改变了稳定
    OpenStack 基础服务（包括计算、存储和网络服务）的工作方式，极大地提高了每个服务发布测试的频率，从而避免了可能的漏洞和潜在的安全问题，这些问题可能会在生产环境部署时引发故障。从故事的另一面来看，通过分析每个
    OpenStack 版本的图谱，你可能会注意到一些扩展项目的出现和消失。社区贡献者做得对，摒弃了那些不稳定的特性，以免影响 OpenStack 私有部署的成熟度。今天，发布的版本暴露了基本服务，并在稳定的上游增加了更多的项目和功能。这个故事的另一个亮点是，OpenStack
    生态系统始终领先一步，及时采用市场技术的新趋势，包括容器化、无服务器架构、大数据分析等。
- en: Today, either starting a new OpenStack adventure or turning a new page for a
    new update would bring great cost savings, flexibility in IT handling compared
    to old-school virtualization alternatives, and an efficient solution to hyperscale
    with minimum concerns.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，无论是开始新的 OpenStack 探险，还是翻开新的一页来进行更新，都能带来巨大的成本节约，相比传统的虚拟化替代方案，提供更高的 IT 灵活性，并在最小的担忧下实现高效的超大规模解决方案。
- en: OpenStack’s state-of-the-art documentation and resources can be found more abundantly
    than ever on the internet. There are plenty of different options to design and
    deploy a complete OpenStack environment. On the other hand, the paradox of choices
    between different ways of designing and running a full ecosystem can be challenging
    even for experienced administrators, architects, and developers. That could be
    phrased differently as *too much can be too little* ! The adoption of OpenStack
    can be a struggle and has always been a challenge, from design phases to deployments
    and operational days. As stated previously, you can think of many reasons for
    this, but mainly, being such a versatile software can make it overwhelming!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 最先进的文档和资源现在比以往任何时候都更容易在互联网上找到。设计和部署一个完整的 OpenStack 环境有许多不同的选择。另一方面，关于不同设计和运行完整生态系统的选择之间的矛盾，即便对于有经验的管理员、架构师和开发者来说，也是一个挑战。这可以用*选择太多反而太少*来形容！OpenStack
    的采用一直是一个挑战，从设计阶段到部署和运维阶段。正如前面所说，你可以想到很多原因，但最主要的原因是，这款软件功能如此多样，容易让人感到不知所措！
- en: Throughout this book, we will go through the updated pieces of the OpenStack
    ecosystem and divide and conquer together with a step-by-step approach to design,
    deploy, and operate a scalable OpenStack environment that would respond to your
    needs and requirements.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将通过逐步的方式，带你了解 OpenStack 生态系统的更新部分，并通过分步实施的方式，设计、部署和运营一个可扩展的 OpenStack 环境，满足你的需求和要求。
- en: To address the challenges of the OpenStack ecosystem complexity and allow newcomers
    to enjoy this cloud journey, we will define a state vision for a private cloud
    adoption strategy throughout the first chapter. Without a well-crafted basic knowledge
    of this ecosystem, it would be more difficult to make later decisions on empowering
    running clusters in production when production days start knocking on the door.
    A very attractive citation from Robert Waterman, an expert on business management
    practices, is *“A strategy is necessary because the future* *is unpredictable.”*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 OpenStack 生态系统复杂性带来的挑战，并让新手能够享受这一云计算旅程，我们将在第一章中定义一个私有云采用策略的愿景。如果没有一个精心设计的基础知识框架，将很难在生产阶段开始时做出有关如何支持生产集群的决策。正如商业管理实践专家罗伯特·沃特曼所说，*“战略是必要的，因为未来*
    *是不可预测的。”*
- en: When it comes to a complex system such as the OpenStack ecosystem, setting up
    the right resources is required as we cannot predict with 100% accuracy. OpenStack
    is designed with much more flexibility and a loosely coupled architecture. This
    way, it is up to us to use those key elements to make capacity decisions considering
    short- and long-term goals for growth and availability and respond to new workloads
    based on existing resources that we aim to apply in the near and long term.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像 OpenStack 生态系统这样的复杂系统，设置正确的资源是必要的，因为我们无法做到 100% 准确预测。OpenStack 设计时具有更大的灵活性和松耦合架构。这样，关键在于我们如何利用这些元素做出容量决策，考虑短期和长期的增长及可用性目标，并根据我们在短期和长期内打算应用的现有资源来响应新的工作负载。
- en: 'Our OpenStack journey will continue in this edition by covering the following
    topics in this chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 OpenStack 之旅将在本版本中继续，涵盖以下主题：
- en: Revisiting and highlighting the latest updates of the OpenStack core ecosystem
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾并突出展示 OpenStack 核心生态系统的最新更新
- en: Demystifying the logical architecture based on the latest releases introduced
    from the *Antelope* release
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解密基于最新发布版本的逻辑架构，特别是从 *Antelope* 发布版开始引入的内容
- en: Drafting a first physical design to ensure a seamless deployment in later stages
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起草初步的物理设计，以确保后续阶段的无缝部署
- en: Preparing for a large-scale OpenStack environment through capacity planning
    as part of our cloud strategy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过容量规划为大规模 OpenStack 环境做准备，作为我们云策略的一部分
- en: OpenStack – a plethora of innovations
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack – 无数创新
- en: 'It might be confusing to understand the state of the art of innovation when
    it comes to OpenStack software offerings. The rise of this open source project
    has been unique since its birth 14 years ago. Through its different releases,
    the OpenStack community has kept developing what a private cloud solution can
    offer based on ongoing technology trends. When the first releases of Kubernetes
    (container orchestration technology) were made public, subsequent OpenStack releases
    quickly dedicated a cycle to include services around the OpenStack ecosystem that
    would facilitate container management out of the box. One of the OpenStack success
    stories is undoubtedly its vision of staying one step ahead. There is a clear
    reason why OpenStack remains the fourth largest open source community worldwide:
    trust in its main core services. Since the first days of OpenStack, mid and large
    enterprises have invested and keep contributing to its software to boost its major
    core services. As we will see later, compute, network, identity, image, and storage
    services are counted as the most basic and core components of the OpenStack ecosystem.
    Within each new release, more enhancements and extensive development have been
    done to bring a more alive version of each of them. That brings more contributors
    to see services more alive, taking more workloads, addressing more demand, and
    unblocking enterprises to take advantage of a more agile and flexible infrastructure.
    More big players have joined the cloud era from the early days, such as IBM, Red
    Hat, HP, Rackspace, eBay, and more, to name but a few. Each of them associates
    different goals and projects from different expertise, such as bringing new features,
    plugins, bug fixing, and so on. Some of those contributors have built their own
    dedicated private cloud platform based on OpenStack and keep contributing to the
    open source world, making it a win-win deal.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到 OpenStack 软件产品时，理解创新的最新状态可能会让人感到困惑。自 14 年前诞生以来，这个开源项目的崛起具有独特性。通过不同的版本，OpenStack
    社区一直在根据持续的技术趋势不断开发私有云解决方案可以提供的功能。当 Kubernetes（容器编排技术）首次公开发布时，随后的 OpenStack 版本迅速
    dedicating 了一个周期，加入了 OpenStack 生态系统中的服务，以便开箱即用地支持容器管理。OpenStack 成功的故事之一无疑是它始终保持领先一步的愿景。OpenStack
    仍然是全球第四大开源社区的原因非常明确：对其主要核心服务的信任。自 OpenStack 诞生之日起，中大型企业便不断投资并持续贡献代码，以增强其核心服务。正如我们稍后会看到的，计算、网络、身份、镜像和存储服务被视为
    OpenStack 生态系统中最基础和核心的组件。在每一个新版本中，都会对这些服务进行更多的增强和广泛的开发，使它们更加生动。这样做吸引了更多的贡献者，使得这些服务更加活跃，能够承载更多的工作负载，满足更多的需求，并且帮助企业解锁更具敏捷性和灵活性的基础设施。早期，更多的大型企业如
    IBM、Red Hat、HP、Rackspace、eBay 等加入了云计算时代，他们的目标和项目各不相同，包括引入新功能、插件、修复 Bug 等等。一些贡献者基于
    OpenStack 构建了自己的专用私有云平台，并继续为开源世界贡献代码，形成了双赢局面。
- en: Another key aspect of OpenStack novelty is the speed of extending its capabilities
    from a wider range of services similar to public major cloud providers such as
    **Amazon Web Services** ( **AWS** ), **Microsoft** **Azure** , and **Google Cloud
    Platform** ( **GCP** ). Starting from traditional **Infrastructure as a Service**
    ( **IaaS** ), the OpenStack ecosystem has tailored the next levels by bringing
    managed services and offering **Platform as a Service** ( **PaaS** ) as well as
    **Software as a Service** ( **SaaS** ) models. By skimming the surface of the
    speed of OpenStack growth, we might find out that the core services have reached
    a level of maturity that could unlock gazillions of doors of innovative features
    and solutions. On top of those services, more PaaS environments can be offered,
    such as databases, big data, container services, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 新颖性的另一个关键方面是其快速扩展能力，涵盖了与公共大型云服务提供商（如 **Amazon Web Services** (**AWS**)、**Microsoft**
    **Azure**、**Google Cloud Platform** (**GCP**)）类似的更广泛的服务。OpenStack 生态系统从传统的 **基础设施即服务**
    (**IaaS**) 开始，逐步推出了托管服务，并提供 **平台即服务** (**PaaS**) 以及 **软件即服务** (**SaaS**) 模型。通过快速浏览
    OpenStack 的增长速度，我们可能会发现其核心服务已经达到了成熟的水平，可以解锁大量创新功能和解决方案的门扉。在这些服务的基础上，可以提供更多的 PaaS
    环境，如数据库、大数据、容器服务等。
- en: The other side of the success of the OpenStack growth is the nature of the software
    itself. Written mostly in Python, since the first days of its development, the
    OpenStack software has been designed to offer a rich **application programming
    interface** ( **API** ). This was a game changer by enabling automation in almost
    everything and everywhere. Hence, each contributor can use the domain of expertise
    and seamlessly integrate new features into the software ecosystem, thanks to the
    nature of API design.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack增长成功的另一面是软件本身的特性。自开发初期以来，OpenStack软件大部分是用Python编写的，旨在提供丰富的**应用程序编程接口**（**API**）。这是一项改变游戏规则的创新，几乎在任何地方都能够实现自动化。因此，每个贡献者可以利用自己的专业领域，利用API设计的特性，顺利地将新功能集成到软件生态系统中。
- en: OpenStack’s APIs have been of considerable significance in adopting a hybrid
    cloud approach. As we will see in [*Chapter 11*](B21716_11.xhtml#_idTextAnchor230)
    , *A Hybrid Cloud Hyperscale Use Case – Scaling a Kubernetes Workload* , OpenStack
    offers the **Elastic Compute Cloud** ( **EC2** ) API, a compatible API to interact
    with the AWS world.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack的API在采用混合云方法中具有重要意义。正如我们在[*第11章*](B21716_11.xhtml#_idTextAnchor230)《混合云超大规模用例——扩展Kubernetes工作负载》中所看到的，OpenStack提供了**弹性计算云**（**EC2**）API，这是与AWS世界兼容的API。
- en: The innovation and continuous growth of the OpenStack ecosystem are without
    a doubt a success story thanks to the community’s consistency and, most importantly,
    the development of OpenStack naturally with the modular design. Nowadays, it is
    possible to build an OpenStack private cloud for one or several purposes that
    can serve web applications for public hosting workloads, managed databases, big
    data, high-performance computing (HPC), or even several optimized containerized
    environments for rapid application development. Most importantly, getting acquainted
    with core and latest service updates is the must-have key to unlocking the cloud
    journey.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack生态系统的创新和持续增长无疑是一个成功的故事，这得益于社区的一致性，最重要的是，OpenStack凭借模块化设计自然发展。如今，构建一个OpenStack私有云已经可以用于一个或多个目的，它可以为公共托管工作负载、托管数据库、大数据、高性能计算（HPC），甚至多个优化的容器化环境提供Web应用程序，满足快速应用开发的需求。最重要的是，了解核心和最新服务的更新是开启云计算之旅的必备钥匙。
- en: Building blocks Of OpenStack – the control plane
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack的构建模块——控制平面
- en: 'The OpenStack project has been launched to solve the IaaS paradigm. By adopting
    the pay-as-you-use model, underlying resources, including compute, networking,
    and storage, are exposed as pools and reserved on demand securely per the user’s
    request. Throughout the development of the OpenStack ecosystem (considered in
    other literature as the *cloud operating system* ), more open source projects
    have joined the emerging cloud software life cycle to extend its capabilities.
    As mentioned previously, the secret sauce of such project development comes from
    the natural API design, which consistently facilitates communication between services.
    The versatile number of services could sidetrack newcomers in understanding where
    and how fundamental parts of the ecosystem work to enable a well-designed architecture
    for custom needs and requirements. The following sections will iterate through
    what are considered the *core services* of OpenStack, to begin with. It is essential
    to get acquainted with each of these services as each OpenStack release, coming
    with new projects or services, relies on them. Being strategic in mastering core
    components and their capabilities will allow easier deployment of your future
    private cloud setup and, hence, extend its capabilities with less wasted time
    and effort. The following table shows the core services that will be tackled in
    the following sections:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack项目的启动是为了解决IaaS范式问题。通过采用按需付费模式，底层资源，包括计算、网络和存储，作为池子被按需安全地暴露并根据用户需求进行预留。在OpenStack生态系统的发展过程中（在其他文献中被称为*云操作系统*），更多的开源项目加入了新兴的云软件生命周期，扩展了其功能。正如前面所提到的，这种项目开发的秘诀在于自然的API设计，它始终促进了服务之间的通信。大量的服务可能会让新手困惑，不知道生态系统中的基本部分如何工作，从而启用为定制需求和要求设计的良好架构。接下来的章节将逐步介绍OpenStack的*核心服务*，作为开始。了解这些服务中的每一个至关重要，因为每个OpenStack版本都会引入新的项目或服务，并依赖于它们。战略性地掌握核心组件及其功能，将使您更容易部署未来的私有云设置，从而以更少的时间和精力扩展其能力。下表展示了接下来章节将要讨论的核心服务：
- en: '| **Service** | **Code name** | **Description** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **服务** | **代号** | **描述** |'
- en: '| --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Compute | Nova | Manages the **virtual machine** ( **VM** ) life cycle |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 计算 | Nova | 管理 **虚拟机**（**VM**）生命周期 |'
- en: '| Network | Neutron | Manages network environment per project |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | Neutron | 管理每个项目的网络环境 |'
- en: '| Identity | Keystone | Provides authentication and authorization information
    service |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 身份 | Keystone | 提供身份验证与授权信息服务 |'
- en: '| Block storage | Cinder | Manages the VM disks and snapshots life cycle |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 块存储 | Cinder | 管理 VM 磁盘和快照生命周期 |'
- en: '| Object Storage | Swift | Accessible REST API storage for object data types
    such as images and media files |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 对象存储 | Swift | 可访问的 REST API 存储，用于存储图像、媒体文件等对象数据类型 |'
- en: '| Image | Glance | Manages the VM image life cycle |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 镜像 | Glance | 管理 VM 镜像生命周期 |'
- en: '| Dashboard | Horizon | OpenStack frontend web interface |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 仪表盘 | Horizon | OpenStack 前端 Web 界面 |'
- en: '| File sharing | Manila | Manages projects across shared filesystems |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 文件共享 | Manila | 管理跨共享文件系统的项目 |'
- en: '| Scheduling | Placement | Helps to track provider’s resource inventories and
    usage |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 调度 | 放置 | 帮助跟踪提供商的资源库存和使用情况 |'
- en: '| Telemetry | Ceilometer | Provides data collection service for resource tracking
    and billing |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 遥测 | Ceilometer | 提供资源跟踪和计费的数据收集服务 |'
- en: '| Alarming | Aodh | Triggers actions and alarms based on collected metrics
    and configured rules |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 报警 | Aodh | 基于收集的指标和配置的规则触发动作和报警 |'
- en: Table 1.1 – OpenStack core services
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1.1 – OpenStack 核心服务
- en: Important note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is still possible to deploy an OpenStack environment without object storage
    and a dashboard. Some distributions come by default with both services enabled.
    For example, spinning up a VM does not necessarily require object storage. Additionally,
    the OpenStack **command-line interface** ( **CLI** ) supports all operations for
    each installation that can be directed without the OpenStack dashboard.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然可以在没有对象存储和仪表盘的情况下部署 OpenStack 环境。某些发行版默认启用了这两项服务。例如，启动一个虚拟机不一定需要对象存储。此外，OpenStack
    **命令行界面**（**CLI**）支持每个安装的所有操作，并且可以无需 OpenStack 仪表盘进行指令。
- en: Keystone – the authentication and authorization service
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keystone – 身份验证与授权服务
- en: The keywords *authentication* and *authorization* are the main functions of
    this identity service. **Keystone** is part of the control plane of the OpenStack
    ecosystem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 关键字 *身份验证* 和 *授权* 是此身份服务的主要功能。**Keystone** 是 OpenStack 生态系统控制平面的一部分。
- en: Security has been brought over from the earliest releases, and it became a necessity
    to dive further into the **AAA** (short for **Authentication, Authorization, Accounting**
    ) service in OpenStack. Each request made to contact an OpenStack service must
    be validated by Keystone (via the identity API). The API response will return
    an authentication token to be used against the requested service ( API call).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性从最早的版本开始被引入，并且深入研究 OpenStack 中的 **AAA**（**身份验证、授权、计费**）服务成为了必需。每一个联系 OpenStack
    服务的请求都必须由 Keystone 验证（通过身份 API）。API 响应将返回一个身份验证令牌，用于访问请求的服务（API 调用）。
- en: At this stage, the Keystone workflow might seem simpler than we thought. On
    the other hand, when we scale hundreds of hosts to fulfill thousands of requests
    in a short time, we should consider how to make sure that Keystone, as a critical
    service, is fully operational. That will be discussed in later chapters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，Keystone 工作流可能看起来比我们想象的简单。另一方面，当我们扩展数百个主机以在短时间内处理成千上万的请求时，我们应考虑如何确保 Keystone
    作为关键服务能够完全正常运行。这将在后面的章节中讨论。
- en: What has been brought from old OpenStack releases to the latest stable ones
    within the identity service is mainly the version of the API. Formerly, when installing
    the identity service, it was possible to keep running with version 2; starting
    from the *Grizzly* release, version 3 has been launched and can be installed.
    Today, the identity API version 2 has been deprecated in favor of newer releases.
    We will consider version 3 and above (at the writing of this book, version 3.14
    is the latest within the *Ussuri* OpenStack release).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从旧版 OpenStack 版本到最新稳定版，在身份服务中的主要变化是 API 的版本。以前，在安装身份服务时，可以继续使用版本 2；从 *Grizzly*
    版本开始，推出了版本 3，并且可以安装。今天，身份 API 版本 2 已被弃用，取而代之的是更新的版本。我们将考虑版本 3 及以上（截至本书撰写时，版本 3.14
    是 *Ussuri* OpenStack 版本中的最新版本）。
- en: But what can an identity API offer? As mentioned previously, the design of all
    communications between OpenStack services is made via APIs (later, we will skim
    the surface for interaction even with non-OpenStack services via APIs). Consider
    that a failed simple authentication request might reflect a service failure from
    the end user’s perspective. A good practice is to make sure, on the first iteration,
    that there are no issues with Keystone requests when troubleshooting a failing
    service request. Although the previous editions of this book did not dive deep
    into the Keystone workflow, we will revisit it in more detail in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane –* *Shared Servi ces* .
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 那么身份 API 能提供什么呢？如前所述，所有 OpenStack 服务之间的通信设计都是通过 API 实现的（稍后，我们将简单介绍如何通过 API 与非
    OpenStack 服务进行交互）。考虑到一个失败的简单身份验证请求可能会从最终用户的角度反映出服务故障。一个好的做法是，在首次排查故障时，确保 Keystone
    请求没有问题，特别是在服务请求失败时。尽管本书的前几个版本没有深入探讨 Keystone 工作流，但我们将在 [*第3章*](B21716_03.xhtml#_idTextAnchor108)
    中以更详细的方式重新审视它，*OpenStack 控制平面 - * *共享服务*。
- en: Nova – the compute service
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nova – 计算服务
- en: '*Austin* , the first OpenStack release, brought the first version of the **Nova**
    project to spin up an instance and started dealing with compute management through
    a rich API. Unlike the aforementioned identity service, Nova has evolved through
    different releases to accommodate more amazing functions but at the cost of its
    complexity compared to other core services. The Nova service design has not changed
    a lot from the *Grizzly* release. On the other hand, it is still required to reiterate
    through the different Nova pieces that form the compute service engine in the
    OpenStack ecosystem.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*Austin*，OpenStack 的第一个版本，推出了 **Nova** 项目的第一个版本，用于启动实例，并开始通过丰富的 API 处理计算管理。与前述的身份服务不同，Nova
    在不同版本中不断发展，增加了更多惊人的功能，但相较于其他核心服务，它的复杂性也有所提高。Nova 服务的设计自 *Grizzly* 版本以来变化不大。另一方面，仍然需要反复讨论构成
    OpenStack 生态系统中计算服务引擎的不同 Nova 组件。'
- en: Important note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There are plenty of capabilities brought by Nova in the latest OpenStack releases,
    which will be detailed in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125) , *OpenStack
    Compute – Compute Capacity* *and Flavors* .
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新的 OpenStack 版本中，Nova 带来了许多新功能，详细内容请参见 [*第4章*](B21716_04.xhtml#_idTextAnchor125)，*OpenStack
    计算 - 计算能力* *与规格*。
- en: The **nova-api** service interacts with user API calls that manage compute instances.
    It communicates with other components of the compute service over a message bus.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-api** 服务与用户 API 调用进行交互，管理计算实例。它通过消息总线与计算服务的其他组件进行通信。'
- en: The **nova-scheduler** service listens to the new instance request on the message
    bus. The job of this service is to select the best compute node for the new instance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-scheduler** 服务监听来自消息总线的新实例请求。该服务的任务是为新实例选择最佳计算节点。'
- en: The **nova-compute** service is the process responsible for starting and terminating
    VMs. This service runs on the compute nodes and listens for new requests over
    the message bus.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-compute** 服务是负责启动和终止虚拟机的进程。该服务运行在计算节点上，并通过消息总线监听新的请求。'
- en: The **nova-conductor** service handles database access calls from the compute
    nodes to limit the risk of database access by an attacker via a compromis ed host.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**nova-conductor** 服务处理计算节点的数据库访问请求，以限制攻击者通过被妥协的主机对数据库的访问风险。'
- en: Placement – the scheduling service
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Placement – 调度服务
- en: Whenever it comes to a new system improvement or logical addition, the OpenStack
    community will create, move, or place different services or components to make
    that happen. One of the major updates of the OpenStack ecosystem since the *Newton*
    release is the introduction of the **Placement** API service. Before the latter
    release, users struggled to identify the counter on different sets of resource
    providers, such as computation, networking, and storage allocated pools. That
    is where it is useful to dedicate a separate service, hence an API well connected
    with other services (such as Nova), to track those resource providers and keep
    an eye on their usage. The Placement service acts mainly as a resource inventory.
    Not only that, but the most pertinent addition has also made the process of filtering
    (via **nova-scheduler** ) more fine-grained. In some other glossaries, you might
    find the word *prefiltering* as a reference for the new Placement service. This
    step is the preparation of available compute nodes by starting a filtering process
    before dealing with the scheduler based on some configurable specs and traits.
    The Placement service will be demonstrated in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity* *and Flavors* .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每当涉及到新的系统改进或逻辑添加时，OpenStack 社区会创建、移动或部署不同的服务或组件来实现这一目标。自 *Newton* 版本以来，OpenStack
    生态系统的一个重要更新是引入了 **Placement** API 服务。在此版本之前，用户在识别不同资源提供者（如计算、网络和存储分配池）上的计数器时遇到困难。正是在这里，专门为此创建一个独立的服务，进而通过与其他服务（如
    Nova）的良好连接的 API 来跟踪这些资源提供者并监控其使用情况，就显得尤为重要。Placement 服务主要作为资源清单进行工作。不仅如此，最重要的改进还使得过滤过程（通过
    **nova-scheduler** ）更加精细。在一些其他术语中，你可能会看到 *预过滤* 这个词作为对新 Placement 服务的引用。此步骤是在处理调度器之前，根据一些可配置的规格和特性，启动过滤过程以准备可用的计算节点。Placement
    服务将在 [*第 4 章*](B21716_04.xhtml#_idTextAnchor125) ，*OpenStack 计算 – 计算容量* *与规格*
    中演示。
- en: Glance – the imaging service
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Glance – 镜像服务
- en: To launch an instance in OpenStack via the Nova service, an instance image will
    be required to successfully accomplish VM provisioning. **Glance** is one of the
    core services that deals not only with instance images (as a template source image)
    but also the snapshots that can be created from an instance. When it comes to
    the question of where Glance will store images and snapshots, the answer can be
    found in its latest supported driver configurations. As with many other OpenStack
    services, a number of storage options can be used to back the storage of images
    by Glance. Again, if we think about the OpenStack infrastructure extension in
    the short and long term, we must think about which backend storage Glance should
    use. The OpenStack community has developed the most commonly used storage backend
    either within proper OpenStack-supported storage services such as *Swift Object
    Storage* and *Cinder Block Storage* or a third-party extension such as *Ceph*
    storage based on **RADOS Block Device** ( **RBD** ), VMware storage, and even
    AWS **Simple Storage Service** ( **S3** ) storage.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过 Nova 服务在 OpenStack 中启动一个实例，成功完成虚拟机配置需要一个实例镜像。**Glance** 是一个核心服务，它不仅处理实例镜像（作为模板源镜像），还处理可以从实例创建的快照。当涉及到
    Glance 存储镜像和快照的位置问题时，答案可以在其最新支持的驱动配置中找到。与许多其他 OpenStack 服务一样，可以使用多种存储选项来支持 Glance
    的镜像存储。再次提醒，如果我们从短期和长期来看待 OpenStack 基础设施扩展，我们必须考虑 Glance 应该使用哪种后端存储。OpenStack 社区已经开发了最常用的存储后端，无论是在适当的
    OpenStack 支持的存储服务中，如 *Swift 对象存储* 和 *Cinder 块存储*，还是基于 **RADOS 块设备**（**RBD**）的第三方扩展，如
    *Ceph* 存储、VMware 存储，甚至是 AWS **简单存储服务**（**S3**）存储。
- en: This variety of backend storage options might increase the paradox of architectural
    choices, but it raises the question of which needs we would address from business
    and capacity perspectives. As an example, Glance configuration enables multiple
    storage backends in the same configuration layout that can be customized to instruct
    the imaging service to use an existing block storage pool to attach images directly
    and reduce the time of waiting to download from scratch. A massive Ceph infrastructure
    with dozens of object storage nodes can be leveraged if they are operational by
    dedicating a Ceph pool for production images, making use of existing resources
    granted by the stable driver integration between Glance and the Ceph backend.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样化的存储后端选项可能增加架构选择的矛盾，但它提出了一个问题：我们从业务和容量角度应该解决哪些需求。例如，Glance 配置允许在相同的配置布局中启用多个存储后端，可以定制指示映像服务使用现有的块存储池来直接附加图像，减少从头开始下载的等待时间。如果
    Ceph 基础设施庞大，并且拥有数十个对象存储节点，可以通过将一个 Ceph 存储池专用于生产图像，利用 Glance 与 Ceph 后端之间稳定的驱动程序集成所提供的现有资源来提高其使用效率。
- en: Important note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is important to understand your requirements before selecting any type of
    backend, as not all backends will support the same features. For example, within
    the *Antelope* release, the OpenStack Glance team included a new feature in the
    block storage backend – *Cinder* – by allowing the extension on the running attached
    volumes. The Ceph storage extension will be discussed in more detail in [*Chapter
    5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage – Block, Object, and*
    *File Shares* .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择任何类型的后端之前，理解您的需求非常重要，因为并非所有后端都支持相同的功能。例如，在 *Antelope* 版本中，OpenStack Glance
    团队在块存储后端 *Cinder* 中加入了一个新特性——允许扩展运行中的附加卷。Ceph 存储扩展将在 [*第 5 章*](B21716_05.xhtml#_idTextAnchor146)
    中详细讨论， *OpenStack 存储 – 块存储、对象存储和* *文件共享*。
- en: Conversely, more interesting use cases would leverage Swift as a storage backend
    for Glance snapshots and templates. That can be considered a safe internal backup
    scenario. Going further, increasing your **fault tolerance** ( **FT** ) domain
    can cross public cloud services such as the AWS S3 backend.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，更多有趣的使用案例将利用 Swift 作为 Glance 快照和模板的存储后端。这可以被认为是一种安全的内部备份场景。进一步说，增加您的 **容错能力**（**FT**）领域可以跨越公共云服务，比如
    AWS S3 后端。
- en: 'One of the most impressive progressions in the Glance service is the wider
    list of supported images. The *Antelope* release came with at least 11 supported
    formats, including RAW, QCOW2, VDI, VHD, ISO, OVA, PLOOP, and Docker, and great
    additional formats dealing with Amazon public cloud compatibility are AKI, AMI,
    and ARI:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Glance 服务中最令人印象深刻的进展之一是支持的图像格式列表的扩展。*Antelope* 版本支持至少 11 种格式，包括 RAW、QCOW2、VDI、VHD、ISO、OVA、PLOOP
    和 Docker，另外还增加了处理亚马逊公共云兼容性的格式，分别是 AKI、AMI 和 ARI：
- en: '![Figure 1.1 – Glance supported backends](img/B21716_01_01.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – Glance 支持的后端](img/B21716_01_01.jpg)'
- en: Figure 1.1 – Glance supported backends
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – Glance 支持的后端
- en: Swift has been one of the most preferred options to store Glance images in large
    deployments, which will be highlighted in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Swift 一直是大型部署中存储 Glance 图像的最优选项之一，下一节将重点介绍这一点。
- en: Swift – the object storage service
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swift – 对象存储服务
- en: Together with Nova, **Swift** came with the first-ever OpenStack release, *Austin*
    , back in 2010. Object storage has added great value to enterprises that resolve
    several storage challenges, unlike the traditional persistent storage design.
    Swift itself has been a revolutionary service at a time when cloud technologies
    just started to warm up. As per the nature of the object storage model, objects
    are stored in a flat hierarchy. Most of the main usage of Swift in a given OpenStack
    deployment is to perform archiving and backups. Many services around the OpenStack
    ecosystem that deal with storage are compatible with the Swift storage backend
    either for direct storage, backup purposes, or even both.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Nova 一起，**Swift** 在 2010 年推出了首个 OpenStack 版本 *Austin*。对象存储为解决多个存储挑战的企业提供了巨大价值，区别于传统的持久化存储设计。Swift
    本身是一个革命性的服务，尤其在云技术刚刚起步的时期。根据对象存储模型的特点，对象以平面层级结构存储。在特定的 OpenStack 部署中，Swift 的主要用途是进行归档和备份。许多与
    OpenStack 生态系统中存储相关的服务，都与 Swift 存储后端兼容，无论是直接存储、备份目的，还是两者兼有。
- en: 'As we will unleash a few of the latest additions on the Swift service in [*Chapter
    5*](B21716_05.xhtml#_idTextAnchor146) , *OpenStack Storage – Block, Object, and
    File Shares* , a brief revisit of object storage common specs are listed as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在[*第5章*](B21716_05.xhtml#_idTextAnchor146)《*OpenStack存储——块、对象和文件共享*》中展示的那样，以下是对象存储常见规格的简要回顾：
- en: Developed with no **single point of failure** ( **SPOF** ) by design
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设计避免**单点故障**（**SPOF**）
- en: Exposes the HTTP REST API for common object management
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供用于常见对象管理的HTTP REST API
- en: Highly scalable and a good fit for workloads demanding boosted performance
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展，适用于要求提升性能的工作负载
- en: Designed for eventual consistency
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计时考虑最终一致性
- en: Highly available by design and can scale easily horizontally through inexpensive
    hardware
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设计实现高可用性，并且可以通过廉价硬件轻松实现水平扩展
- en: 'Now, let’s look at the next storage optio n: the Cinder service.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看下一个存储选项：Cinder服务。
- en: Cinder – the block storage service
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cinder —— 块存储服务
- en: The other storage offering in the OpenStack ecosystem and counted as a core
    service is the block storage service named **Cinder** . Cinder has been developed
    to maintain independent life cycle operations of instance volumes (as virtual
    disks). The Cinder API provides an exhaustive list of different volume and snapshot
    operations, including create, delete, attach, detach, extend, clone, create images
    from volumes, create volumes from images, and create volumes from snapshots. With
    recent OpenStack releases, more operational capabilities have been developed to
    support backups, restoration , and volume migration.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack生态系统中的另一项存储服务，并被视为核心服务的是名为**Cinder**的块存储服务。Cinder被开发用于保持实例卷（虚拟磁盘）的独立生命周期操作。Cinder
    API提供了完整的不同卷和快照操作列表，包括创建、删除、附加、分离、扩展、克隆、从卷创建镜像、从镜像创建卷以及从快照创建卷。随着最近的OpenStack版本发布，更多的操作功能已经被开发出来，以支持备份、恢复和卷迁移。
- en: Manila – the shared filesystems service
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Manila —— 共享文件系统服务
- en: File-sharing solutions have been an emerging storage option for enterprises.
    The need to have a collaborative central storage brain for access facility and
    simplicity of management has grown in the latest decade in IT infrastructure either
    on-premises or in cloud environments. The OpenStack community did not miss the
    opportunity since the *Liberty* release, introducing the **Manila Distributed
    and Shared File Systems** solution. Coming from the same logical workflow as Cinder,
    Manila has not been included as a core OpenStack project due to its limited capabilities.
    As per the *Kilo* release until *the latest releases* , Manila had proven itself
    as a stable component within the OpenStack infrastructure, allowing enterprises
    to gain access to self-service file-sharing for different hungry resources and
    clients.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 文件共享解决方案已经成为企业日益增长的存储选择。在过去十年中，IT基础设施中无论是本地部署还是云环境，对一个可以提供便捷访问和简单管理的协作中央存储大脑的需求不断增加。自*Liberty*版本以来，OpenStack社区抓住了这一机会，推出了**Manila分布式和共享文件系统**解决方案。Manila与Cinder有着相同的逻辑工作流，但由于功能有限，它没有被纳入核心OpenStack项目。从*Kilo*版本到*最新版本*，Manila已经证明自己是OpenStack基础设施中的一个稳定组件，使企业能够访问自助文件共享服务，满足不同资源和客户端的需求。
- en: 'Once again, several storage vendors have kept the momentum of contributing
    to the Manila service by developing more drivers to support their storage backend
    hardware. Following the same steps as Cinder, we might find an exhaustive list
    of supported file share drivers by Manila, including CephFS, LVM, Hadoop HDFS,
    EMC, IBM, and many others. A full list of supported drivers can be found on the
    Manila OpenStack web page: [https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html](https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html)
    . More interesting updates have been included in the latest releases for the Manila
    service, including security, backup share integration, and increased level of
    accessibility, which will be discussed more in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Storage – Block , Object, and* *File Shares* .'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提到，多个存储供应商通过开发更多的驱动程序来支持其存储后端硬件，从而持续推动Manila服务的发展。沿用Cinder的做法，我们可能会发现Manila支持的完整文件共享驱动程序列表，包括CephFS、LVM、Hadoop
    HDFS、EMC、IBM等。完整的支持驱动程序列表可以在Manila的OpenStack网页上找到：[https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html](https://docs.openstack.org/manila/latest/configuration/shared-file-systems/drivers.html)。在Manila服务的最新版本中，还包括了一些更有趣的更新，如安全性、备份共享集成以及更高的可访问性，这些将在[*第5章*](B21716_05.xhtml#_idTextAnchor146)
    *OpenStack存储——块存储、对象存储和文件共享*中详细讨论。
- en: Neutron – the networking service
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Neutron——网络服务
- en: The previous citation might be arguable if we consider **nova-network** still
    being used! On the other hand, since the *Grizzly* release, the **Neutron** project
    has become a more **Networking as a Service** ( **NaaS** ) pillar in the OpenStack
    ecosystem. That makes sense due to the approach taken by the community to provide
    an independent networking service and not embedded, as is the case with the Nova
    compute service. The transition from legacy **nova-network** to Neutron has been
    tougher for companies willing to migrate from the old days to the new network
    era led by Neutron. Although we will keep the light on Neutron in the next parts
    of the book, a section will be dedicated in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* , to discuss
    the more advanced features of Neutron.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑到**nova-network**仍在使用中，前面的引用可能会有争议！另一方面，自*Grizzly*版本以来，**Neutron**项目已经成为OpenStack生态系统中的一个更重要的**网络即服务**（**NaaS**）支柱。这是有道理的，因为社区采取了提供独立网络服务而非嵌入式服务的方法（Nova计算服务就是一个例子）。从传统的**nova-network**到Neutron的过渡，对于那些希望从旧时代迁移到由Neutron主导的新网络时代的公司来说，过程更为艰难。尽管我们将在本书的后续部分继续关注Neutron，[*第6章*](B21716_06.xhtml#_idTextAnchor159)
    *OpenStack网络——连接性与托管服务选项*中将专门讨论Neutron的更多高级功能。
- en: 'The motivation to adopt Neutron is fairly obvious considering the rise of this
    project to solve many **nova-network** limitations and open new networking capabilities.
    Compared to **nova-network** , which provides basic networking in the OpenStack
    ecosystem (mainly limited to interaction with the compute service), Neutron brings
    several aspects and features that can summarized as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 采用Neutron的动机非常明显，因为该项目的崛起解决了许多**nova-network**的局限性，并开放了新的网络能力。与仅提供基础网络功能的**nova-network**（主要限于与计算服务的交互）相比，Neutron带来了几个方面和功能，可以总结如下：
- en: Simplified self-service for projects to provision ports, subnets, networks,
    and router objects
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目简化自助服务，用于配置端口、子网、网络和路由器对象
- en: Advanced networking features can be deployed in no time, such as firewalls,
    **virtual private networks** ( **VPNs** ), and load balancers
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级网络功能可以在短时间内部署，例如防火墙、**虚拟私人网络**（**VPN**）和负载均衡器
- en: More support for complex network topologies
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对复杂网络拓扑的更多支持
- en: Vast ways of third-party network solution integration
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方网络解决方案集成的多种方式
- en: A standalone service making architecture less complex to support **high** **availability**
    ( **HA** )
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个独立的服务使架构支持**高可用性**（**HA**）变得更简单
- en: Effective service for all sizes of deployments
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对各种规模部署的高效服务
- en: With the emergence of hardware and software based on **software-defined networking**
    ( **SDN** ) solutions, the OpenStack Neutron team has taken the lead to engage
    more support on the SDN part, resulting in the development of Neutron drivers
    to integrate some SDN famous implementations such as OpenContrail and VMware NSX.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于 **软件定义网络** (**SDN**) 解决方案的硬件和软件的出现，OpenStack Neutron 团队率先在 SDN 部分提供更多支持，从而开发了
    Neutron 驱动程序，以集成一些著名的 SDN 实现，如 OpenContrail 和 VMware NSX。
- en: Since 2013, within each OpenStack release, the Neutron project has been considered
    one of the services with the most number of commits and feature development. [*Chapter
    6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking – Connectivity and
    Managed Service Options* , will iterate in depth about more Neutron capabilities
    and connect the pieces with SDNs within th e latest OpenStack releases.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2013 年以来，在每个 OpenStack 版本中，Neutron 项目一直被认为是提交次数和功能开发最多的服务之一。[*第6章*](B21716_06.xhtml#_idTextAnchor159)，*OpenStack
    网络 – 连接性和托管服务选项*，将深入探讨更多 Neutron 功能，并将这些功能与最新 OpenStack 版本中的 SDN 连接起来。
- en: Ceilometer – the telemetry service
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ceilometer – 遥测服务
- en: 'The telemetry service was introduced in the OpenStack ecosystem in the *Havana*
    release. Code-named **Ceilometer** , the telemetry service has emerged with a
    pertinent mission: record, gather, and monitor resource utilization metrics across
    the OpenStack infrastructure. Unlike older OpenStack releases, the latest Ceilometer
    versions include only metric collection features, leaving alarming and notification
    functions to anoth er dedicated project code-named Aodh.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 遥测服务是在 OpenStack 生态系统中引入的，出现在 *Havana* 版本中。代号为 **Ceilometer** 的遥测服务带着一个明确的使命：记录、收集和监控
    OpenStack 基础设施中的资源利用度度量数据。与旧版本的 OpenStack 不同，最新的 Ceilometer 版本仅包含度量数据收集功能，将报警和通知功能交给了另一个名为
    Aodh 的专用项目。
- en: Aodh – the alerting service
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Aodh – 警报服务
- en: As part of the monitoring services chain, enabling users to trigger custom notifications
    based on telemetry events has become a required addition to keeping an eye on
    deployed project resources. As mentioned in the *Ceilometer – the telemetry service*
    section, the alarming feature has been separated from Ceilometer, and **Aodh**
    is dedicated to firing alarms based on configured rules and set thresholds.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作为监控服务链的一部分，允许用户根据遥测事件触发自定义通知，已成为监控部署项目资源的必备功能。如 *Ceilometer – 遥测服务* 部分所述，报警功能已从
    Ceilometer 中分离出来，**Aodh** 专门用于根据配置的规则和设定的阈值触发警报。
- en: Important note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Ceilometer is designed to send collected metric data to other destinations,
    including a non-OpenStack open source project named *Gnocchi* . The Gnocchi project
    formulated the time-series **Database as a Service** ( **DBaaS** ) term. In this
    case, a dedicated database interface is no lo nger required to store and query
    metrics.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Ceilometer 旨在将收集到的度量数据发送到其他目的地，包括一个名为 *Gnocchi* 的非 OpenStack 开源项目。Gnocchi 项目提出了时间序列
    **数据库即服务** (**DBaaS**) 这一术语。在这种情况下，不再需要专用的数据库接口来存储和查询度量数据。
- en: Horizon – the dashboard service
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Horizon – 仪表盘服务
- en: Having a **graphical user interface** ( **GUI** ) to operate your OpenStack
    environment would definitely make your administrative tasks much simpler. Surprisingly,
    the OpenStack first releases did not bring a GUI initiative to fire a few clicks
    from a dashboard to assist in managing the infrastructure resources. It was only
    in the *Essex* release that **Horizon** was born to accommodate the first core
    project’s operations, such as the creation of object containers, operating a life
    cycle of instances, managing images, and a basic layout on project users’ management,
    formerly named tenant users. The need to include more control from the dashboard
    has been felt with the extensive inclusion of more services, and, within the latest
    releases, we might notice that the Horizon experience is much more sophisticated.
    Built with the Django framework and supporting most of the OpenStack APIs, the
    majority of the core services’ resources can be operated through Horizon. On the
    other hand, more advanced operations such as customizing networking configuration
    can be fired only using the OpenStack CLI. Additionally, do not expect that Horizon
    will automatically include an extra service beyond the core ones. It is not a
    limitation, but, for modular design reasons, installing a new service would require
    the addition of a new Horizon module associated with it and defined as a *panel*
    . Horizon, with a good reference of the CLI, would be sufficient to administer
    your cloud if running a small setup. Talking about the wider infrastructure, with
    dozens of projects and hundreds of resources, administering this size of setup
    might not be ideal using Horizon. Agile thinking would require more automation
    and scripting on a large-scale size. In [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right Way (DevSecOps)* , we will discuss
    in detail the automation of the OpenStack setup and how to use the same approach
    to manage r esources within project resources themselves.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个**图形用户界面**（**GUI**）来操作您的OpenStack环境，肯定会让您的管理任务变得更加简便。令人惊讶的是，OpenStack的首次发布并未带来图形界面支持，无法通过仪表板的几次点击来帮助管理基础设施资源。直到*Essex*版本，**Horizon**才诞生，用于支持第一个核心项目的操作，如创建对象容器、操作实例生命周期、管理镜像，以及基本的项目用户管理布局，之前这些用户被称为租户用户。随着更多服务的广泛集成，仪表板需要包含更多控制功能，这一需求逐渐显现。在最新的版本中，我们可能会注意到Horizon的使用体验变得更加复杂。Horizon是基于Django框架构建的，支持大多数OpenStack
    API，核心服务的大部分资源都可以通过Horizon进行操作。另一方面，像自定义网络配置这样的高级操作只能通过OpenStack CLI来执行。此外，不要指望Horizon会自动包含核心服务之外的其他服务。这并不是限制，而是出于模块化设计的考虑，安装新服务需要增加与之关联的Horizon模块，并将其定义为*面板*。对于运行小型环境的云管理而言，Horizon配合CLI的良好参考足以胜任。谈到更大规模的基础设施，拥有数十个项目和数百个资源的管理，使用Horizon可能并不是理想的选择。敏捷思维在大规模环境下要求更多的自动化和脚本支持。在[*第2章*](B21716_02.xhtml#_idTextAnchor089)，*正确启动OpenStack设置
    - DevSecOps方法*中，我们将详细讨论OpenStack设置的自动化，以及如何使用相同的方法管理项目资源中的资源。
- en: Non-OpenStack services
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非OpenStack服务
- en: 'As part of the OpenStack control plane, we might find, as per any software
    architecture, other essential core components that all OpenStack services rely
    on: **Advanced Messaging Queuing Protocol** ( **AMQP** ) and database services.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为OpenStack控制平面的一部分，我们可能会发现，与任何软件架构一样，所有OpenStack服务依赖的其他核心组件：**高级消息队列协议**（**AMQP**）和数据库服务。
- en: Passing the message – AMQP
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传递消息 – AMQP
- en: AMQP, the messaging queue service, is one of the other ingredients for making
    a modular architecture design in the OpenStack ecosystem.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: AMQP，消息队列服务，是构建OpenStack生态系统模块化架构设计的另一种重要组件。
- en: For each request hitting a specific OpenStack service API, the message queue
    server will make sure to communicate messages between all engaged components (processes)
    to accomplish the request asynchronously. A variety of open source AMQP software
    can be used throughout your OpenStack deployment if you are biased about using
    one of them; it can just be simply specified during the setup to have it runn
    ing and communicating with different components. RabbitMQ, Qpid, and ZeroMQ are
    supported message queue solutions; RabbitMQ is the most used one. Being an expert
    in AMQP operations is not a requirement for a successful OpenStack operation day,
    but a few main points should be taken into consideration when dealing with a message
    queue in OpenStack. We will see in later chapters how OpenStack services depend
    on the message queue from an architectural perspective, giving importance to such
    services running on the highest possible availability. The other aspect, having
    some adequate familiarity with how messaging concepts work, would be more than
    helpful for troubleshooting days. [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane – Shared Services* , and [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , will highlight in sequence
    the internal workflow of AMQP in OpenStack and how to maximize its availability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个请求命中特定的 OpenStack 服务 API，消息队列服务器将确保在所有参与的组件（进程）之间异步地传递消息，以完成该请求。如果你偏向于使用某一款开源
    AMQP 软件，那么在 OpenStack 部署过程中，你可以在设置时简单地指定它进行运行，并与不同的组件进行通信。RabbitMQ、Qpid 和 ZeroMQ
    是支持的消息队列解决方案，其中 RabbitMQ 是最常用的。虽然成为 AMQP 操作的专家并不是成功运营 OpenStack 的必备条件，但在处理 OpenStack
    中的消息队列时，有几个关键点是需要考虑的。我们将在后续章节中从架构角度了解 OpenStack 服务如何依赖于消息队列，并强调这些服务运行在尽可能高的可用性上的重要性。另一个方面是，对于消息概念的基本了解，对于排查问题将是非常有帮助的。[*第3章*](B21716_03.xhtml#_idTextAnchor108)，*OpenStack
    控制平面 – 共享服务*，以及 [*第7章*](B21716_07.xhtml#_idTextAnchor174)，*运行高可用云 – 满足 SLA*，将依次介绍
    AMQP 在 OpenStack 中的内部工作流程，并阐述如何最大化其可用性。
- en: Storing the state – the database
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储状态 – 数据库
- en: 'The other fundamental OpenStack control plane service is the database. Similar
    to the AMQP service, each OpenStack service will require a database connection
    to store its different states. The database in the OpenStack ecosystem defines
    persistent storage that can be consulted and updated by all services upon each
    request. The database topic from an architectural perspective has been a *hot*
    topic mostly due to security concerns. As mentioned earlier, in the *Nova – the
    compute service* section, there is a tendency from that learning curve to reduce
    any possible insecure access to the database or services running in different
    hosts that could potentially present a security risk. The other side of the database
    sensitivity topic, similar to the AMQP service, is the availability dilemma. Besides
    the risk of failures, OpenStack architects and administrators will need to reiterate
    through the other traditional database anomaly once it starts growing: performance.
    [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108) , *OpenStack Control Plane – Shared
    Services* , and [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly
    Available Cloud – Meeting the SLA* , will cover how to ensure a healthy start
    of the database for larger OpenStack deployments. [*Chapter 9*](B21716_09.xhtml#_idTextAnchor204)
    , *Benchmarking the Infrastructure – Evaluating Resource Capacity and Optimization*
    , will bring the motivation behind the database bench marking routine for a better
    state of performance.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基础的 OpenStack 控制平面服务是数据库。与 AMQP 服务类似，每个 OpenStack 服务都需要数据库连接来存储其不同的状态。OpenStack
    生态系统中的数据库定义了持久存储，所有服务在每个请求时都可以查询和更新它。数据库这一话题从架构角度来看一直是一个*热点*话题，主要是由于安全问题。如前所述，在*Nova
    – 计算服务*部分，由于学习曲线的影响，存在减少对数据库或在不同主机上运行的服务的不安全访问的趋势，这些服务可能会带来安全风险。数据库敏感性话题的另一面，类似于
    AMQP 服务，是可用性困境。除了故障的风险，OpenStack 架构师和管理员还需要在数据库开始增长时重新审视传统数据库异常问题：性能。[*第3章*](B21716_03.xhtml#_idTextAnchor108)，*OpenStack
    控制平面 – 共享服务*，以及 [*第7章*](B21716_07.xhtml#_idTextAnchor174)，*运行高可用云 – 满足 SLA*，将介绍如何确保大型
    OpenStack 部署中数据库的健康启动。[*第9章*](B21716_09.xhtml#_idTextAnchor204)，*基准测试基础设施 – 评估资源容量和优化*，将揭示数据库基准测试例行程序背后的动机，以提升性能状态。
- en: Other services
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他服务
- en: 'The latest OpenStack releases have demonstrated goodwill to OpenStack users
    to deploy and operate an OpenStack private cloud infrastructure not only with
    very stable core services but also with additional ones. Compared to older releases,
    several incubated projects have been in on-off mode, making the decision to adopt
    some of them in your existing deployment insecure. Different insights from different
    project initiatives have sculpted more confident software within the latest OpenStack
    releases ready to support more features and services in your cloud environment.
    Some of the extra stable services are listed in the following table in the order
    of integration through different OpenStack cycle releases:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的OpenStack发布版展示了OpenStack对用户的良好意图，不仅提供了非常稳定的核心服务，还增加了其他服务，帮助用户部署和运营OpenStack私有云基础设施。与较旧的版本相比，一些孵化项目处于间歇模式，决定是否采用其中一些项目来集成到现有部署中会存在不安全因素。来自不同项目倡议的不同见解在最新的OpenStack发布版中塑造了更自信的软件，这些软件已准备好在您的云环境中支持更多功能和服务。一些额外的稳定服务按不同OpenStack周期发布的集成顺序列出在下表中：
- en: '| **Service** | **Release** | **Description** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **服务** | **发布** | **描述** |'
- en: '| --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Heat | Havana | Orchestration PaaS extension on top of the IaaS provided
    by OpenStack. Heat abstracts cloud resources into code – also known as **domain-specific
    language** ( **DSL** ). That comes when cloud operators leverage the power of
    code templating to automate things, reduce human error, and increase the agility
    of resource management. Heat uses **Heat Orchestration Templates** ( **HOTs**
    ), in YAML or JSON format, allowing users to define the whole infrastructure to
    run their workloads from code. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Heat | Havana | 基于OpenStack提供的IaaS扩展的编排PaaS。Heat将云资源抽象成代码——也称为**领域特定语言**（**DSL**）。当云操作员利用代码模板的力量来自动化任务、减少人为错误并提高资源管理的敏捷性时，Heat便发挥作用。Heat使用**Heat编排模板**（**HOTs**），支持YAML或JSON格式，允许用户通过代码定义运行工作负载所需的整个基础设施。
    |'
- en: '| Trove | Icehouse | DBaaS allows users to automate the provisioning of relational
    and non-relational scalable databases with the minimum overhead of database tasks
    such as patching, maintenance, and backup through provided automation. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Trove | Icehouse | DBaaS允许用户自动化提供关系型和非关系型可扩展数据库，且数据库任务（如打补丁、维护和备份）通过提供的自动化进行最小化管理。
    |'
- en: '| Sahara | Juno (incubated in Icehouse) | **Elastic Data Processing as a Service**
    ( **EDPaaS** ) provides a means of orchestrating structured and unstructured data
    processing and analysis software and infrastructure on top of OpenStack, such
    as Hadoop and Spark clusters. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Sahara | Juno（在Icehouse中孵化） | **弹性数据处理即服务**（**EDPaaS**）提供了一种在OpenStack上编排结构化和非结构化数据处理与分析软件及基础设施的方式，如Hadoop和Spark集群。
    |'
- en: '| Ironic | Kilo | **Bare Metal as a Service** ( **BMaaS** ) allows users to
    provision infrastructure directly on a physical machine, thus no hypervisor layer
    is involved. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Ironic | Kilo | **裸金属即服务**（**BMaaS**）允许用户直接在物理机上提供基础设施，因此不涉及虚拟化层。 |'
- en: '| Murano | Kilo | **Application as a Service** ( **AaaS** ) enables developers
    to speed up application deployment and publish into an application catalog that
    can be browsable and ready for deployment in an automated fashion. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Murano | Kilo | **应用即服务**（**AaaS**）使开发人员能够加速应用程序的部署，并将其发布到一个可以浏览并准备以自动化方式部署的应用程序目录中。
    |'
- en: '| Designate | Liberty | **DNS as a Service** ( **DNSaaS** ) handles **Domain
    Name Service** ( **DNS** ) entries such as DNS records and zones. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Designate | Liberty | **DNS即服务**（**DNSaaS**）处理**域名服务**（**DNS**）条目，如DNS记录和区域。
    |'
- en: '| Barbican | Liberty | **Secret Manager as a Service** ( **SMaaS** ) is designed
    to centrally store different types of secrets, such as passwords, keys, and certificates,
    to be used by other OpenStack services in a secure way. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Barbican | Liberty | **秘密管理即服务**（**SMaaS**）旨在集中存储不同类型的秘密，如密码、密钥和证书，以便其他OpenStack服务能够以安全的方式使用。
    |'
- en: '| Zaqar | Liberty | **Messaging as a Service** ( **MSaaS** ) allows users to
    provision and manage multi-tenant messaging and notification queues. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Zaqar | Liberty | **消息即服务**（**MSaaS**）允许用户提供和管理多租户消息和通知队列。 |'
- en: '| Magnum | Liberty | **Container as a Service** ( **CaaS** ) orchestrates a
    set of containers through a supported **container orchestration engine** ( **COE**
    ) such as Docker Swarm, Kubernetes, and Mesos. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Magnum | Liberty | **容器即服务**（**CaaS**）通过支持的**容器编排引擎**（**COE**）如Docker Swarm、Kubernetes和Mesos来编排一组容器。
    |'
- en: '| Octavia | Liberty | **Load Balancer as a Service** ( **LBaaS** ) provides
    a load balancing function. Octavia can be counted as an enterprise-class load
    balancer solution. This service has become an attractive feature that can be used
    much more easily compared to the Neutron LBaaS driver. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Octavia | Liberty | **作为服务的负载均衡器**（**LBaaS**）提供了负载均衡功能。Octavia 可被视为一个企业级负载均衡解决方案。与
    Neutron LBaaS 驱动程序相比，这项服务已经变得更加易于使用，成为了一个吸引人的功能。 |'
- en: Table 1.2 – Additional OpenStack services
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.2 – 其他 OpenStack 服务
- en: After reviewing the core and incubated OpenStack services, we can now walk through
    different functional requirements that would construct what a private cloud can
    offer as services.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了核心和孵化中的 OpenStack 服务后，我们现在可以逐一浏览不同的功能需求，这些需求将构建出私有云可以提供的服务。
- en: Constructing the motivation
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建动机
- en: Before building a picture of your future OpenStack environment, it is necessary
    to have business goals that align with the outcome of the OpenStack solution.
    As we have seen briefly in the previous sections with the latest updates in the
    OpenStack glossary, you might feel how overwhelming it could be with newly developed
    services and features. Obviously, the new image of OpenStack looks more robust
    than ever, with many hands of **Anything as a Service** ( **XaaS** ) opportunities.
    The array of choices might prevent architects and operators from starting a proper
    OpenStack infrastructure exercise if the scope of your business is not defined
    from the first day of drafting the design. As with existing major public providers
    such as AWS, Azure, or GCP, the OpenStack deployment should be considered as a
    way of investment in the long run. Expecting operational and cost savings can
    be felt only at later stages. Being strategic is essential to take advantage of
    the OpenStack solution if used properly from the early days. Hundreds of enterprises
    have switched gears to use OpenStack in their environment, but not all of them
    had a successful journey and soon dropped it. For this reason, understanding in
    which use case your business will fall ensures a safe start to your OpenStack
    journey.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建未来的 OpenStack 环境之前，必须确保商业目标与 OpenStack 解决方案的结果相一致。正如我们在前面章节中简要提到的，随着 OpenStack
    词汇表的最新更新，你可能会感到新开发的服务和功能令人应接不暇。显然，OpenStack 的新形象比以往更加稳健，拥有众多**服务即一切**（**XaaS**）的机会。众多选择可能会阻碍架构师和运维人员，如果从设计草图的第一天起，商业目标没有明确，便无法启动一个合适的
    OpenStack 基础设施建设。像 AWS、Azure 或 GCP 等现有的大型公有云提供商一样，OpenStack 部署应被视为一种长期投资。期望在运营和成本节约方面的效果，只能在后期阶段显现出来。如果从一开始就正确使用
    OpenStack，具备战略眼光至关重要。成百上千的企业已转向在其环境中使用 OpenStack，但并非所有企业都取得了成功，许多企业很快就放弃了它。因此，理解你的业务将适用于哪个使用案例，能够确保
    OpenStack 之旅的安全起步。
- en: Since 2010, OpenStack usage has taken different facets whereby companies have
    invested in the OpenStack ecosystem, followed the milestone generation, and adopted
    it. After 14 years, we can see a plethora of deployments where OpenStack shines
    with a respectful history of success. The next sections will broadly list a few
    common use cases of implementations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2010 年以来，OpenStack 的使用呈现出多样化的面貌，企业们投资于 OpenStack 生态系统，跟随其里程碑的演进并采纳它。14 年后，我们可以看到大量的部署案例，OpenStack
    以其辉煌的成功历史闪耀着光辉。接下来的章节将广泛列出一些常见的实施使用案例。
- en: Application development accelerator
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用开发加速器
- en: 'The modern **software development life cycle** ( **SDLC** ) takes a more mature
    direction by enhancing the stages of its release and automating the whole chain,
    from simple code commits to different types of testing. Although **Continuous
    Integration/Continuous Delivery** ( **CI/CD** ) methods and tools emerged slightly
    before the rise of the cloud era, delivering a well-tested software product has
    faced a new challenge: *resource limitations* . Although the wording *limitation*
    is not exactly the term that would describe this challenge, from a developer’s
    perspective, it means there are no available resources to continue or perform
    specific unit, smoke, or integration testing. Preproduction environments are even
    more challenging to validate in all stages if a business will not allow the product
    to go live without provisioning exactly the same environment as in production
    for the sake of testing. Resource instrumentation and virtualization have resolved
    this issue, but only partially. With a lack of automation and an engine of operation,
    the whole chain still does not save costs and reduce the **time to market** (
    **TTM** ). That is where OpenStack has been engaged for several hundred enterprises:
    accelerate product releases by running their CI/CD pipelines in a multi-tenant
    OpenStack environm ent.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现代**软件开发生命周期**（**SDLC**）通过增强发布阶段并自动化整个链条，从简单的代码提交到不同类型的测试，朝着更加成熟的方向发展。尽管**持续集成/持续交付**（**CI/CD**）方法和工具在云时代兴起之前就出现了，但交付一个经过充分测试的软件产品面临着一个新挑战：*资源限制*。尽管*限制*这一措辞并不完全描述这个挑战，但从开发者的角度来看，这意味着没有可用的资源来继续或执行特定的单元、冒烟或集成测试。如果企业不允许产品在没有为测试提供与生产环境完全相同的环境的情况下上线，那么预生产环境在所有阶段的验证将更加具有挑战性。资源仪表化和虚拟化解决了这一问题，但仅仅是部分解决。由于缺乏自动化和操作引擎，整个链条仍然无法节省成本并减少**市场时间**（**TTM**）。这就是OpenStack为几百家企业所做的工作：通过在多租户的OpenStack环境中运行他们的CI/CD流水线，加速产品发布。
- en: Cloud application enabler
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云应用启用器
- en: If you are planning to provide what most developers dream of (developing from
    scratch!), then you are in the right corner. Whether the developed application
    is a web pet store, online reservation assistant, or flight reservation engine,
    wrapping the application in its proper infrastructure can be done in a fast and
    clean way using Heat, for example. As the application will be defined as *code*
    in a HOT file, you will not need to worry about the application configuration
    consistency as well as its degree of scalability if autoscaling is declared in
    the template. As OpenStack services will take care of all the automation overhead,
    application owners can focus on the next level of the application business to
    find more spots for improvements rather than wasting effort on resource management
    and operation overhead.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划提供大多数开发者梦寐以求的功能（从零开始开发！），那么你来对地方了。无论开发的应用程序是一个网页宠物商店、在线预订助手，还是航班预订引擎，都可以通过例如Heat这样的工具，在合适的基础设施中快速、干净地封装应用程序。由于应用程序将在HOT文件中定义为*代码*，你无需担心应用程序配置的一致性以及如果模板中声明了自动扩展的话，它的可扩展性问题。由于OpenStack服务将处理所有的自动化开销，应用程序所有者可以专注于应用程序业务的下一个层级，寻找更多改进的空间，而不是浪费精力在资源管理和运营开销上。
- en: The other trait of using OpenStack, especially within the survey from the *Kilo*
    release, is the trending cloud-ready application. OpenStack enables a fast application
    publishing process in different ways. As container technologies adoption increases,
    OpenStack has included container services to support a few famous container orchestration
    engines, such as Docker Swarm and Kubernetes, via the OpenStack Magnum service.
    76% of users in the *User Survey* from the *Juno* release showed interest in the
    OpenStack and containerization marriage. A well-known example of a vast OpenStack
    environment is CERN. Based on OpenStack, CERN is running over 300,000 cores, with
    a big chunk of it being Kubernetes workloads (over 500 clusters) within the Magnum
    space.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenStack的另一个特点，特别是在*Kilo*版本的调查中，是云就绪应用的趋势。OpenStack通过多种方式加速应用程序的发布过程。随着容器技术的采用增加，OpenStack已经包括了容器服务，以通过OpenStack
    Magnum服务支持一些著名的容器编排引擎，如Docker Swarm和Kubernetes。在*Kilo*版本的*用户调查*中，76%的用户表示对OpenStack和容器化结合的兴趣。一个广为人知的OpenStack大规模环境示例是CERN。基于OpenStack，CERN运行着超过300,000个核心，其中很大一部分是Kubernetes工作负载（超过500个集群），这些都在Magnum空间内运行。
- en: Important note
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More details on the conducted OpenStack survey can be found at [https://www.openstack.org/user-survey/2022-user-survey-report](https://www.openstack.org/user-survey/2022-user-survey-report)
    .
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有关OpenStack调查的更多详细信息，请参见[https://www.openstack.org/user-survey/2022-user-survey-report](https://www.openstack.org/user-survey/2022-user-survey-report)。
- en: The Murano application catalog service is another OpenStack service that facilitates
    application publishing in no time based on contai ners.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Murano应用目录服务是另一个OpenStack服务，它可以快速基于容器发布应用。
- en: HPC supporter
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HPC支持者
- en: It might be confusing to mix the *HPC* term in the context of OpenStack. Eventually,
    OpenStack does not access the hardware to configure compute power and increase
    hardware performance. On the other hand, it enables designing your commodity hardware
    in a way to grow horizontally when hitting more compute, storage, and network
    demand. In the HPC context, the main focus is on augmenting the underlying hardware
    capacity, and the cloud will secure the multi-tenancy to access resources in the
    most optimized way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenStack的背景下混合使用*HPC*术语可能会让人感到困惑。最终，OpenStack并不直接访问硬件以配置计算能力和提升硬件性能。另一方面，它使得你可以设计商品化硬件，以便在遇到更多计算、存储和网络需求时实现横向扩展。在HPC的背景下，主要关注的是增强底层硬件容量，而云计算将确保多租户能够以最优化的方式访问资源。
- en: Surprisingly, in the early days of OpenStack when only Nova and Swift existed,
    the **National Aeronautics and Space Administration** ( **NASA** ) and Rackspace
    were the initiators of the first OpenStack production environment running HPC.
    More research organizations have since adopted the OpenStack ecosystem to deliver
    compute and storage at a large scale to researchers and scientists, but again,
    selecting the hardware layout and planning ahead is still required to feel the
    added value of OpenStack either via virtualized infrastructure, bare metal, or
    both. OpenStack enables all methods of orchestration for HPC environment depl
    oyments.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，在OpenStack的早期阶段，当时只有Nova和Swift存在时，**美国国家航空航天局**（**NASA**）和Rackspace是首批启动HPC运行的OpenStack生产环境的发起者。此后，更多的研究机构开始采用OpenStack生态系统，为研究人员和科学家大规模提供计算和存储，但同样，选择硬件布局并进行提前规划仍然是必需的，才能真正感受到OpenStack通过虚拟化基础设施、裸金属或两者结合所带来的附加价值。OpenStack支持HPC环境部署的所有编排方法。
- en: Network functions virtualization moderator
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络功能虚拟化协调者
- en: Only a few enterprises in the telecommunication industry realized the opportunities
    of virtualizing network services in the early days. Following the same steps as
    the server virtualization experience, enterprises started investing in the new
    trend of running their edge network services on standard machines and providing
    functions such as routers, proxies, load balancers, and firewalls to consumers
    as VMs. This became a major asset in responding quickly to network changes and
    feature demands. **Network functions virtualization** ( **NFV** ) relies heavily
    on compute power that can run on commodity hardware and virtual environments,
    whereas in the traditional way, physical appliances running each network function
    must exist. In the OpenStack context, NFV will need only a compute service to
    expose different functions. As per the *Kilo User Survey* , more than 12% of organizations
    use OpenStack in the networking and telecommunication industry. As Telco adopts
    OpenStack for such a purpose, Nova will be your assistant but with a strategic
    eye on compute resources and placement to make i t happen.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在电信行业，只有少数企业在早期意识到虚拟化网络服务的机会。按照服务器虚拟化经验的相同步骤，企业开始投资于将其边缘网络服务运行在标准机器上的新趋势，并以虚拟机的形式向消费者提供路由器、代理、负载均衡器和防火墙等功能。这成为了快速响应网络变化和功能需求的重要资产。**网络功能虚拟化**（**NFV**）在很大程度上依赖于可以在商品化硬件和虚拟环境中运行的计算能力，而在传统方式中，每个网络功能必须依赖物理设备来运行。在OpenStack的背景下，NFV只需要一个计算服务来暴露不同的功能。根据*Kilo用户调查*，超过12%的组织在网络和电信行业中使用OpenStack。随着电信行业采用OpenStack来实现这一目的，Nova将成为你的助手，但它会从计算资源和部署位置的战略角度来帮助实现这一目标。
- en: Big data facilitator
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据促进者
- en: Volkswagen AG and BMW have shared their feedback on using OpenStack for big
    data and analytics, and the outcome was valuable assistance of the ecosystem orchestrating
    all the different services and resources to accomplish some large data analytics
    tasks. Before the *Juno* release and the unleashing of the Sahara – EDPaaS – project,
    big data resources could be orchestrated using HOTs to provision a cluster in
    no time. Sahara was unleashed to simplify the deployment of such resources in
    more granular and ready-to-go ways, considering the type of the data processing
    framework, versions, size of the cluster, desired topology, and so on. The big
    data scope in OpenStack can be wider when thinking out loud by using some advanced
    features of the OpenStack services, such as storage and networking. Companies
    keep showing interest in automating their **Extract-Transform-Load** ( **ETL**
    ) pipelines internally by leveraging the OpenStack big data service. If you consider
    your business to be bound to a Hadoop provider, for example, make sure that performance
    is a key specification for your underlying hardware that No va will use.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 大众汽车（Volkswagen AG）和宝马（BMW）分享了他们使用 OpenStack 进行大数据和分析的反馈，结果是生态系统的宝贵支持，协同所有不同的服务和资源来完成一些大数据分析任务。在
    *Juno* 版本发布之前以及 Sahara – EDPaaS – 项目的发布之前，大数据资源可以通过 HOTs 快速编排来配置一个集群。Sahara 的发布旨在简化以更细粒度、更即插即用的方式部署此类资源，考虑到数据处理框架的类型、版本、集群的规模、所需拓扑等。通过使用
    OpenStack 服务的一些高级功能（例如存储和网络），思考大数据的范围可以更广阔。公司们继续表现出对通过利用 OpenStack 大数据服务自动化他们
    **提取-转换-加载**（**ETL**）管道的兴趣。如果你认为你的业务依赖于某个 Hadoop 提供商，例如，确保性能是底层硬件的关键规格，Nova 会使用这些硬件。
- en: Private cloud service provider
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 私有云服务提供商
- en: Private cloud environments could be the first main motivation when reading the
    definition of OpenStack for the first time in the old releases’ documentation.
    Being private will limit the services offered to end users within the perimeter
    of your organization or, technically, *behind* *the firewall* .
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 私有云环境可能是你第一次阅读旧版本 OpenStack 文档时理解其定义的主要动机。作为私有云将限制在你组织的边界内提供给最终用户的服务，或者从技术角度讲，*在防火墙*
    *后面*。
- en: The term *private cloud* might sum up all the use cases previously. The only
    difference is that previous OpenStack implementations can stay isolated within
    your perimeter or serve on -demand needs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*私有云* 这个术语可能涵盖了之前提到的所有使用案例。唯一的区别是，之前的 OpenStack 实现可以保持在你的边界内隔离，或者根据需求提供服务。'
- en: Public cloud service provider
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公有云服务提供商
- en: 'Going public! Why not? Back in the early 2000s, Amazon internally launched
    a private elastic compute cloud to enable infrastructure agility for developers
    and users running the giant online e-commerce website and other services selling
    millions of items worldwide. **Simple Queue Service** ( **SQS** ) was launched
    and exposed to the public as the first cloud service, then later S3, followed
    by the EC2 service. The idea started with object storage named S3 and the EC2
    service. Originally, NASA and Rackspace initiated and immensely contributed to
    software code to create software operating a private cloud initially for elastic
    computation and object storage capabilities. That is how Nova and Swift came to
    be. The first release attracted early-bird companies seeking innovations and started
    contributing to the new open source and soon cultivating more experience on market
    models for XaaS products. Rackspace launched its *public cloud* version offering
    compute and object storage services initially. This was followed by many others
    adopting a public cloud powered by OpenStack, such as Open Telekom Cloud, Cleura,
    VEXXHOST, and many more. Lately, it feels like the world runs on OpenStack with
    the hashtag **#RunOnOpenStack** . With its widespread implementation with big
    names around the world, the OpenStack community has initiated the concept of *OpenStack
    public cloud passports* . The idea behind it is to act as a major public cloud
    provider by providing OpenStack services in different regions and locations worldwide.
    The program ensures a massive hub of collaboration between different OpenStack
    public cloud providers, enabling users to roam between more than 60 **availability
    zones** ( **AZs** ) worldwide. The [openstack.org](http://openstack.org) website
    keeps updating the public cloud locations that can be filtered through the OpenStack
    public cloud marketplace page: [https://www.openstack.org/marketpla ce/public-clouds/](https://www.openstack.org/marketplace/public-clouds/)
    .'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上市！为什么不呢？早在2000年代初期，亚马逊内部推出了一个私有弹性计算云，以便为开发者和用户提供基础设施敏捷性，支持运行巨大的在线电子商务网站和其他销售全球数百万商品的服务。**简单队列服务**（**SQS**）作为第一个云服务发布并对外开放，随后是S3，紧接着是EC2服务。这个想法最初源于名为S3的对象存储和EC2服务。最初，NASA和Rackspace启动并大力贡献了软件代码，创建了一个最初用于弹性计算和对象存储能力的私有云软件。这就是Nova和Swift诞生的背景。第一次发布吸引了寻求创新的先行公司，这些公司开始为新的开源项目做出贡献，并很快在XaaS产品的市场模型上积累了更多经验。Rackspace最初推出了其*公共云*版本，提供计算和对象存储服务。随后，许多其他公司采用了由OpenStack支持的公共云，如Open
    Telekom Cloud、Cleura、VEXXHOST等。最近，感觉全世界都在使用OpenStack，伴随着**#RunOnOpenStack**的标签。随着OpenStack在全球大品牌中的广泛应用，OpenStack社区提出了*OpenStack公共云护照*的概念。其背后的理念是，通过在全球不同地区和位置提供OpenStack服务，作为一个主要的公共云提供商。该计划确保了不同OpenStack公共云提供商之间的大规模合作枢纽，使用户能够在全球超过60个**可用区**（**AZs**）之间自由漫游。[openstack.org](http://openstack.org)网站不断更新可以通过OpenStack公共云市场页面筛选的公共云位置：[https://www.openstack.org/marketpla
    ce/public-clouds/](https://www.openstack.org/marketplace/public-clouds/)。
- en: Picking up the pieces
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拼凑碎片
- en: 'Independent of which use case your future OpenStack deployment will start with,
    mastering the art of its ecosystem is a vital step before investigating any further
    design and architecture options. OpenStack covers more than 1 million lines of
    Python code! At this stage, there is no need to worry about learning how the code
    works: far more important is to take a reverse engineering approach: understanding
    the ecosystem interactions. Simply understanding the interrelation of core services
    gives you the keys to the kingdom of the art of design. Starting with the basics,
    the foundational components will open more doors to the next ones if we master
    the core ones correctly. So far, we have revisited core services of OpenStack
    that exist with each release and had a brief scan of the incubated projects, particularly
    within the latest releases. Next, we will select a common workflow that illustrates
    the different steps and pieces to spawn a VM. This type of workflow will demonstrate
    how different OpenStack core services interact and will involve almost all the
    components.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 无论未来的OpenStack部署从哪个用例开始，掌握其生态系统的艺术是进一步研究任何设计和架构选项之前至关重要的一步。OpenStack包含超过100万行Python代码！此时不需要担心如何学习代码的工作原理：更重要的是采取逆向工程的方法：理解生态系统的相互作用。仅仅理解核心服务之间的相互关系，就能为你打开设计艺术的大门。基础知识掌握得当，基础组件将为进一步的学习打开更多的门。所以，到目前为止，我们回顾了OpenStack每个版本中存在的核心服务，并简要扫描了孵化项目，特别是在最新的版本中。接下来，我们将选择一个常见的工作流程，展示生成虚拟机的不同步骤和部分。这个工作流程将展示不同的OpenStack核心服务如何交互，并涉及几乎所有组件。
- en: Important note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The listing of which services are counted as core ones might differ between
    different resources and documentation. Telemetry, scheduling, and dashboard services
    could be considered optional services. The current assumption is based on real-world
    deployment that leverages the usage of those services from the start of the journey.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的资源和文档中，列出哪些服务算作核心服务可能有所不同。遥测、调度和仪表盘服务可能被视为可选服务。目前的假设是基于现实世界中的部署，从旅程开始就利用这些服务。
- en: 'The first iteration demonstrates a high level of the building blocks of different
    OpenStack core services that include compute, imaging, identity, block storage,
    monitoring, scheduling, dashboard, and networking services:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮演示展示了不同OpenStack核心服务的构建模块，包括计算、镜像、身份验证、块存储、监控、调度、仪表盘和网络服务：
- en: '![Figure 1.2 – High-level services interaction spawning instance workflow](img/B21716_01_02.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 高级服务交互生成实例工作流程](img/B21716_01_02.jpg)'
- en: Figure 1.2 – High-level services interaction spawning instance workflow
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 高级服务交互生成实例工作流程
- en: 'It is important to take note of the main blocks before jumping into the granular
    presentation of the spawning process as illustrated in the previous diagram. By
    providing the Horizon dashboard or via the CLI (similarly, via APIs), an authentication
    process will be fired first by challenging the user request with login credentials
    that will be handled by the identity service, Keystone. Once validated (an authentication
    token will be provided for the whole session and subsequent requests), the request
    will reach the Nova API to take the next steps. Nova will interact with each of
    the following services counted as *mandatory* to successfully spawn an instance:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳入生成过程的详细展示之前，注意了解主要模块是非常重要的，正如前面的图所示。通过提供Horizon仪表盘或通过CLI（同样通过API），首先会触发身份验证过程，通过登录凭证挑战用户请求，该过程由身份服务Keystone处理。一旦验证通过（将为整个会话和后续请求提供身份验证令牌），请求将到达Nova
    API，继续下一步。Nova将与以下服务进行交互，这些服务被视为*必需*，以成功生成实例：
- en: The imaging service, Glance, queries an image to start the instance.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镜像服务Glance查询启动实例所需的镜像。
- en: The networking service, Neutron, acquires networking resources.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络服务Neutron获取网络资源。
- en: The block storage service, Cinder, allocates storage volumes.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块存储服务Cinder分配存储卷。
- en: The scheduling service, Placement, allocates available compute provider resources.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度服务Placement分配可用的计算提供者资源。
- en: Note that the whole chain of interactions passes through the API of each component.
    For each inter-component request, the core database will be consulted for read
    and object updates. As per spawning an instance, a record of a new VM instance
    will be created in the instance and used for reference with other services in
    the subsequent steps. As per the instance request, Nova will interact with the
    queuing message service as well – for example, to request an instance launch (for
    the Nova compute process).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，整个交互链通过每个组件的 API 进行传递。对于每个组件间的请求，核心数据库将被查询以进行读取和对象更新。根据实例的创建，一个新的虚拟机实例记录将在实例中创建，并在后续步骤中与其他服务进行参考。根据实例请求，Nova
    还将与队列消息服务进行交互——例如，请求启动实例（对于 Nova 计算进程）。
- en: As part of this workflow, any API endpoint, scheduling, imaging, identity, and
    shared components such as database and queuing services are counted as part of
    the OpenStack control plane.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在此工作流中，任何 API 端点、调度、影像、身份验证以及共享组件，如数据库和队列服务，都视为 OpenStack 控制平面的一部分。
- en: Important note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The control plane and data plane are ways of conceptual separation in complex
    system architectures. The terms will be defined in more detail in [*Chapter 3*](B21716_03.xhtml#_idTextAnchor108)
    , *OpenStack Control Plane –* *Shared Services* .
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面和数据平面是复杂系统架构中概念性分离的方式。术语将在 [*第 3 章*](B21716_03.xhtml#_idTextAnchor108) 中更详细地定义，*OpenStack
    控制平面 –* *共享服务*。
- en: 'Now, let’s zoom in and explore the workflow in a more granular view so that
    we can build a whole spawning instance picture. As demonstrated in the previous
    part of the chapter, each OpenStack service includes subcomponents working together
    to serve a specific request. Obviously, the API part exists in each service. Remember
    that is the *gate* of each module in the OpenStack ecosystem. Each exposed API
    service (providing an HTTP-based RESTful API) reaches other components and, subsequently,
    subcomponents via the queuing message service bus, as shown in the following workflow
    diagram:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们放大并以更细致的视角来探讨工作流，以便构建完整的生成实例的图景。如本章前部分所示，每个 OpenStack 服务包括多个子组件，它们共同工作以服务特定的请求。显然，每个服务中都存在
    API 部分。记住，它是 OpenStack 生态系统中每个模块的*门户*。每个暴露的 API 服务（提供基于 HTTP 的 RESTful API）通过队列消息服务总线与其他组件以及随后与子组件进行通信，如下方工作流图所示：
- en: '![Figure 1.3 – Low-level services interaction spawning instance workflow](img/B21716_01_03.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 低级服务交互生成实例工作流](img/B21716_01_03.jpg)'
- en: Figure 1.3 – Low-level services interaction spawning instance workflow
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 低级服务交互生成实例工作流
- en: Starting with the identity service, Keystone should be aware of the available
    services in the architecture setup to authorize any further interaction. Keystone
    holds a service catalog containing all API endpoints of different services.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 从身份服务开始，Keystone 应该知道架构中可用的服务，以授权任何进一步的交互。Keystone 持有一个服务目录，包含所有不同服务的 API 端点。
- en: 'The previous diagram illustrates each step by including different components
    and their respective subcomponents:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表通过包括不同的组件及其各自的子组件来说明每一步：
- en: A REST API call will be initiated by using Horizon or the CLI and reaches the
    identity service Keystone endpoint.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 Horizon 或 CLI 发起一个 REST API 调用，达到身份服务 Keystone 端点。
- en: Keystone challenges the request, gets the user credentials (a form of username
    and password), and validates the authentication via its API.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keystone 挑战请求，获取用户凭证（用户名和密码的形式），并通过其 API 验证身份。
- en: The validated authentication generates an authentication token (an **auth**
    token) back to the requester user. The **auth** token will be cached for the specific
    user to be used for subsequent API REST calls between the rest of the services.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证后的身份认证会生成一个身份认证令牌（**auth** 令牌）并返回给请求的用户。该 **auth** 令牌将被缓存，以供特定用户在后续的 API REST
    调用中使用，涉及其他服务之间的交互。
- en: As our request is a *compute* one, the next station that will be reached is
    the compute service API, **nova-api** . Again, the form of the request is a REST
    API, forwarded by Keystone from its catalog. You can think of the catalog as a
    *service map* where Keystone locates each service via the registered endpoint.
    In this case, the REST API encapsulated with the **auth** token will be sent to
    the compute service endpoint.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的请求是一个*计算*请求，接下来将达到的站点是计算服务 API，**nova-api**。同样，请求的形式是一个 REST API，由 Keystone
    从其目录中转发。你可以将目录视为一个*服务地图*，在该地图上，Keystone 通过已注册的端点定位每个服务。在这种情况下，包含 **auth** 令牌的
    REST API 将发送到计算服务端点。
- en: The compute API, the **nova-api** subcomponent, checks the **auth** token coming
    with a request reaching the compute endpoint. At this stage, the **auth** token
    will be validated in return by the identity service, Keystone. The purpose of
    this secondary interaction with Keystone is to inform the compute API in the request
    which permissions and roles users will have and for how long the **auth** token
    will stay alive before generating a new one (noted as the token expiry period).
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算API，即**nova-api**子组件，会检查随请求一起到达计算端点的**auth**令牌。在这一阶段，**auth**令牌将通过身份服务Keystone进行验证。与Keystone的这一次二次交互的目的是告知计算API请求中的用户拥有哪些权限和角色，以及**auth**令牌将在多长时间内有效，直到生成新的令牌（此过程被标记为令牌过期时间）。
- en: Once validated, the first database interaction will take place by **nova-api**
    , which creates a new VM object that specifies the different parameters in the
    request. Bear in mind that each service will have its own logical database to
    update the state of different resources. In the current scenario, the Nova service
    holds its dedicated database schema for different state updates and read and write
    operations.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦验证通过，**nova-api**将进行第一次数据库交互，创建一个新的虚拟机对象，指定请求中的不同参数。请记住，每个服务都会有自己独立的逻辑数据库来更新不同资源的状态。在当前场景中，Nova服务拥有自己的数据库模式，用于不同状态更新以及读写操作。
- en: 'Now, we come to the next interaction of another subcomponent of the Nova service,
    **nova-scheduler** , to specify which compute node will host the new instance.
    The **nova-api** subcomponent will contact the scheduler with such a request via
    the communication hub: the messaging queue service. This nature of the operation
    is processed via a **Remote Procedure Call** ( **RPC** ) call, noted as **rpc.call**
    , which will be published in the message queue and remain there till **nova-scheduler**
    picks it up.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们进入Nova服务的另一个子组件**nova-scheduler**的下一次交互，用于指定哪个计算节点将承载新的实例。**nova-api**子组件将通过通信枢纽——消息队列服务，向调度器发出此类请求。这项操作通过**远程过程调用**（**RPC**）进行处理，标记为**rpc.call**，该请求会发布到消息队列中并保持在那里，直到**nova-scheduler**将其取出。
- en: The **nova-scheduler** subcomponent takes the lead on fetching the most convenient
    compute node to host the new instance. In our highlighted basic core design, the
    Placement service will be engaged in the process of selecting the best compute
    node(s).
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-scheduler**子组件负责获取最合适的计算节点来承载新的实例。在我们突出的基础核心设计中，Placement服务将参与选择最佳计算节点的过程。'
- en: The **nova-scheduler** subcomponent reaches **placement-api** to query its compute
    resource provider. Note that the Placement service is still considered a new service
    as an extension of the other services for tracking their resources, such as compute,
    IP pools, and storage, expressed as resource providers. Since the *Stein* release,
    it has been extracted from the Nova ecosystem and runs on its own. In our workflow,
    we can consider the usage of Placement in the *prefiltering* stage before the
    final selection at the Nova scheduling level. The Placement service also exposes
    its own endpoint in the Keystone catalog. Hence, it will validate the **auth**
    token via the identity service, which updates the token header and returns it
    to the **placement-api** process.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-scheduler**子组件通过查询其计算资源提供者来联系**placement-api**。请注意，Placement服务仍被视为一个新服务，作为其他服务的扩展，用于跟踪它们的资源，如计算、IP池和存储，表现为资源提供者。自*Stein*版本发布以来，它已从Nova生态系统中提取并独立运行。在我们的工作流中，我们可以将Placement用作最终选择之前的*预过滤*阶段。Placement服务还在Keystone目录中公开了自己的端点。因此，它将通过身份服务验证**auth**令牌，更新令牌头并将其返回给**placement-api**进程。'
- en: The selection of hosts is determined via the Placement service by querying its
    own inventory database and, at the first iteration, returns a selection of compute
    nodes and any traits associated with them.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主机的选择是通过Placement服务来确定的，该服务查询其自身的库存数据库，在第一次迭代时返回一组计算节点及其相关的特性。
- en: By applying its filtering and weighing parameters at the returned result by
    the **placement-api** component, the **nova-scheduler** process will come up with
    the right compute node and reach **placement-api** in the second iteration to
    claim the resources for the instance.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在返回结果中应用**placement-api**组件的过滤和加权参数，**nova-scheduler**进程将选出合适的计算节点，并在第二次迭代时通过**placement-api**获取资源，以供实例使用。
- en: The **placement-api** component adjusts its state records in the database, and
    **nova-scheduler** updates the state of the VM object in its database with the
    compute node ID and its hostname and then submits a message in the form of **rpc.cast**
    to the messaging hub. This message will be picked up by **nova-compute** to launch
    the instance in the designated hypervisor host ID.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**placement-api** 组件调整其数据库中的状态记录，**nova-scheduler** 更新其数据库中虚拟机对象的状态，并添加计算节点
    ID 及其主机名，然后以 **rpc.cast** 形式将消息提交到消息中枢。此消息将被 **nova-compute** 获取，用于在指定的虚拟化主机 ID
    上启动实例。'
- en: To communicate with the next subcomponent, **nova-conductor** , **nova-compute**
    will publish the next **rpc.call** call in the message queue to start the launch
    of the instance with details for preparation, such as CPU, disk, and RAM specs.
    Note that **nova-compute** can be running in different process occurrences in
    different compute nodes. Only one particular **nova-compute** occurrence will
    publish **rpc.call** to **nova-conductor** based on the compute node chosen in
    *step 8* .
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了与下一个子组件 **nova-conductor** 进行通信，**nova-compute** 将在消息队列中发布下一个 **rpc.call**
    调用，以启动实例的启动过程，并包括准备细节，如 CPU、磁盘和内存规格。请注意，**nova-compute** 可以在不同的计算节点中的不同进程实例中运行。只有一个特定的
    **nova-compute** 实例会根据 *第 8 步* 选择的计算节点发布 **rpc.call** 到 **nova-conductor**。
- en: If we recall the **nova-conductor** addition in the Nova service architecture,
    the main purpose of this service is to play a *gate role* between the **nova-compute**
    process and the database to minimize the database exposure and hence reduce its
    attack blast radius. As we are aiming for a large OpenStack deployment, the **nova-compute**
    component will be detached from the controller node (holding the main control
    plane services) and will be dedicated to different physical hypervisor nodes.
    If one of the compute nodes has been compromised, spreading that security issue
    to the database will be more difficult due to the logical and physical separation
    when processes interact with the database. The **nova-conductor** component will
    be the one interacting directly with the Nova database to read the instance details,
    such as compute node ID, its requested flavors, memory, CPU, and disk claims.
    The state will be reported back to the messaging queue service by an RPC publish
    call.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们回顾 **nova-conductor** 在 Nova 服务架构中的作用，其主要目的是充当 **nova-compute** 进程和数据库之间的
    *网关角色*，以最小化数据库暴露，从而减少其攻击范围。由于我们的目标是大规模部署 OpenStack，**nova-compute** 组件将与控制节点（托管主要控制平面服务）分离，并将专门部署在不同的物理虚拟化节点上。如果某个计算节点被攻陷，通过逻辑和物理的分离，当进程与数据库交互时，会使得将安全问题传播到数据库变得更加困难。**nova-conductor**
    组件将直接与 Nova 数据库进行交互，读取实例详细信息，例如计算节点 ID、请求的规格、内存、CPU 和磁盘需求。状态将通过 RPC 发布调用返回到消息队列服务。
- en: 'The **nova-compute** component picks up the published message and subsequently
    reaches the next service component by sending a RESTful API to the image service,
    Glance: the **glance-api** process. The request contains a form of **GET API**
    request for the image details encapsulated in the requested image ID information.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-compute** 组件获取已发布的消息，并随后通过向镜像服务 Glance 发送 RESTful API 请求来访问下一个服务组件：**glance-api**
    进程。该请求包含一个 **GET API** 请求，用于获取请求的镜像 ID 信息。'
- en: Glance will provide the requested image ID through its image-registered URL.
    But first, as we are reaching out to another service in the ecosystem, Glance
    will contact the identity service, Keystone, to validate the **auth** token again.
    Assuming the token session has not expired yet, Keystone will validate the token,
    and in the same way as Nova in *step 5* , the identity service will check the
    user role and permissions before it updates the token header and sends it back
    to the Image API.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Glance 将通过其注册的镜像 URL 提供请求的镜像 ID。但首先，由于我们正在访问生态系统中的另一个服务，Glance 将联系身份服务 Keystone，重新验证
    **auth** 令牌。如果令牌会话尚未过期，Keystone 将验证令牌，并且像 *第 5 步* 中的 Nova 一样，身份服务将在更新令牌头并将其返回给镜像
    API 之前检查用户角色和权限。
- en: The Glance component is considered much simpler than Nova, as there is no interaction
    with a message queue in this step. The **glance-api** component simply checks
    the Glance database (running its own dedicated database schema), retrieves the
    metadata of the requested image ID, and reports back to **nova-compute** with
    the image URL format.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Glance 组件被认为比 Nova 简单，因为在此步骤中没有与消息队列的交互。**glance-api** 组件仅检查 Glance 数据库（运行其专用的数据库模式），获取请求的镜像
    ID 的元数据，并以镜像 URL 格式反馈给 **nova-compute**。
- en: The **nova-compute** component reaches the image store and starts loading the
    provided image via the returned URL.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-compute** 组件访问镜像存储，并通过返回的 URL 开始加载提供的镜像。'
- en: 'We now come to the next service to interact with: networking. The **nova-compute**
    component sends a RESTful API request to the Neutron service, via **neutron-server**
    . As discussed earlier, Neutron (by naming convention in its own architecture)
    does not involve an explicit subcomponent name with the **api** suffix. The **neutron-server**
    subcomponent is the *API gate* that will deal with all API requests. Running the
    same identity cycle and assuming that the token has not expired yet, **neutron-server**
    will forward the API request to Keystone, where it gets validated, updates the
    token header with different roles and permissions, and sends it back to the **neutron-server**
    process.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要与之交互的服务是网络服务。**nova-compute** 组件通过 **neutron-server** 向 Neutron 服务发送一个
    RESTful API 请求。如前所述，Neutron（根据其架构的命名惯例）并不涉及带有 **api** 后缀的明确子组件名称。**neutron-server**
    子组件是处理所有 API 请求的 *API 网关*。在执行相同的身份验证周期并假设令牌尚未过期的情况下，**neutron-server** 会将 API
    请求转发给 Keystone，经过验证后更新令牌头部的不同角色和权限，并将其返回给 **neutron-server** 进程。
- en: The **neutron-server** component checks the networking item’s status and respectively
    creates the requested resources encapsulated in the network parameters’ API call.
    Note that the Neutron service varies between different types of requests and hence
    exposes an array of workflows depending on the request complexity. We will cover
    other workflow scenarios dealing with other network controllers, agents, and plugins,
    such as **Open vSwitch** ( **OVS** ), in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* . For example,
    once a port is created for an instance, **neutron-server** will publish an RPC
    call in the message queue reaching out to the **Dynamic Host Configuration Protocol**
    ( **DHCP** ) agent. The **neutron-dhcp-agent** process will invoke its associated
    DHCP driver to reload its host’s file entries via the **dnsmasq** process. Once
    the instance is booted, **dnsmasq** is ready to pick up the instance request and
    send a DHCP offer for the final instance network configuration. This scenario
    also assumes a network and subnet are created where the instance will be attached,
    and that is specified in the instance creation request.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**neutron-server** 组件检查网络项的状态，并根据网络参数的 API 调用分别创建请求的资源。请注意，Neutron 服务会根据请求的不同类型有所不同，因此会根据请求的复杂性暴露不同的工作流。我们将在[*第六章*](B21716_06.xhtml#_idTextAnchor159)《OpenStack
    网络 – 连接性与管理服务选项》中讨论涉及其他网络控制器、代理和插件的其他工作流场景，例如 **Open vSwitch** (**OVS**)。例如，一旦为实例创建了端口，**neutron-server**
    会在消息队列中发布一个 RPC 调用，联系 **动态主机配置协议** (**DHCP**) 代理。**neutron-dhcp-agent** 进程将调用其相关的
    DHCP 驱动程序，通过 **dnsmasq** 进程重新加载主机的文件条目。一旦实例启动，**dnsmasq** 就准备好接收实例请求并发送 DHCP 提供以进行最终的实例网络配置。此场景还假定已创建网络和子网，实例将在其中连接，这一点在实例创建请求中已指定。'
- en: Once networking parameters are acquired for the new instance request, **nova-compute**
    reaches the database and updates the VM object network state record.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦获取到新实例请求的网络参数，**nova-compute** 会访问数据库并更新虚拟机对象的网络状态记录。
- en: The last service to accomplish the instance creation is to reach out to the
    block storage service. Following the same way as previous services, **nova-compute**
    sends its RESTful API request to the Cinder API, the **cinder-api** subcomponent.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成实例创建的最后一个服务是与块存储服务进行交互。与之前的服务类似，**nova-compute** 向 Cinder API 的 **cinder-api**
    子组件发送 RESTful API 请求。
- en: In turn, the Cinder API process will get to the identity service where Keystone
    validates the **auth** token. Assuming that it has not expired, Keystone returns
    the updated token header once validated with the roles and permissions set for
    the requester.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，Cinder API 进程会进入身份服务，Keystone 验证 **auth** 令牌。如果令牌未过期，Keystone 会返回经验证后的更新令牌头，并附带请求者的角色和权限。
- en: As with Nova and Neutron, Cinder has more subcomponents that would take a few
    steps further before the final admission of the request back to the **nova-compute**
    process. Cinder involves its own scheduler as well by publishing an RPC call to
    the messaging queue service.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与 Nova 和 Neutron 一样，Cinder 还拥有更多的子组件，它们会在最终将请求提交给 **nova-compute** 进程之前，再执行一些额外的步骤。Cinder
    通过向消息队列服务发布 RPC 调用，也涉及到自己的调度程序。
- en: The **cinder-scheduler** subcomponent picks up the request and locates available
    resources for a new volume candidate list based on the requested volume specifications
    (volume size and type). Note that Cinder also provides another process, **cinder-volume**
    , that interacts with **cinder-scheduler** whenever a new read or write request
    is initiated within a specific storage provider. In this case, **cinder-volume**
    will be interfacing with backend driver methods to generate a candidate list that
    will be posted in the message queue and picked up later by the **cinder-scheduler**
    subcomponent.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**cinder-scheduler** 子组件会接收请求，并根据请求的卷规格（卷大小和类型）为新卷候选项列出可用资源。请注意，Cinder 还提供了另一个进程
    **cinder-volume**，该进程会在特定存储提供商内启动新的读写请求时与 **cinder-scheduler** 进行交互。在这种情况下，**cinder-volume**
    会与后端驱动程序方法接口，生成一个候选列表，并将其发布到消息队列，稍后由 **cinder-scheduler** 子组件取回。'
- en: The **cinder-scheduler** subcomponent updates the state of the volume ID with
    its associated returned metadata in the Cinder database.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**cinder-scheduler** 子组件会更新 Cinder 数据库中与其关联的返回元数据的卷 ID 状态。'
- en: The **cinder-api** component picks up the published message from **cinder-volume**
    and responds to **nova-compute** via a RESTful call.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**cinder-api** 组件会从 **cinder-volume** 获取发布的消息，并通过 RESTful 调用响应 **nova-compute**。'
- en: The **nova-compute** component receives the created volume metadata in the API
    request. By default, if no other specific hypervisor virtualization driver is
    being configured, Nova will execute the **libvirt** daemon and proceed by creating
    the instance on the designated compute node.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**nova-compute** 组件接收到 API 请求中的已创建卷的元数据。默认情况下，如果未配置其他特定的虚拟化驱动程序，Nova 将执行 **libvirt**
    守护进程，并继续在指定的计算节点上创建实例。'
- en: For the final stage of the instance spawning workflow, the created volume should
    be mapped by the **cinder-volume** process. That is when the virtualization driver,
    in this case, **libvirt** , will make the volume available to the instance by
    firing a **mount** operation. The way it is performed depends on the storage provider
    and protocol, but a common case is passing the volume path to the hypervisor,
    which will mount it to the instance as a virtual block device.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实例创建工作流的最后阶段，创建的卷应由 **cinder-volume** 进程映射。这时，虚拟化驱动程序（在此情况下是 **libvirt**）会通过触发
    **mount** 操作将卷提供给实例。执行的方式取决于存储提供商和协议，但常见的做法是将卷路径传递给虚拟机监控程序（hypervisor），它会将其挂载为虚拟块设备。
- en: 'The last piece of our initial workflow is the ingestion of the instance metrics
    in the Ceilometer service. It is vital to watch what has been spawned by checking
    the status of the created instance. Ceilometer provides two ways of collecting
    data metrics for instances: *polling agents* or *notification agents* . Collected
    metrics will pass through a Ceilometer pipeline transformation for further manipulation
    and prepare this data to be published. The latest releases come with an array
    of choices to store the published data in the Ceilometer database, which can be
    accessed through the Ceilometer API, Gnocchi, or even a simple file store.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初步工作流的最后一步是将实例指标传输到 Ceilometer 服务。监控已创建实例的状态至关重要。Ceilometer 提供了两种收集实例数据指标的方式：*轮询代理*
    或 *通知代理*。收集的指标将通过 Ceilometer 流水线转换以进行进一步操作，并准备发布这些数据。最新的版本提供了多种存储发布数据的选择，可以存储在
    Ceilometer 数据库中，也可以通过 Ceilometer API、Gnocchi，甚至是简单的文件存储访问。
- en: Zooming in on the communication between the different core components is essential
    before drafting a first design layout of your private cloud, which will be detailed
    in the next section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在为您的私有云草拟首个设计布局之前，深入了解不同核心组件之间的通信至关重要，这将在下一节详细说明。
- en: Architecting the cloud
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构云计算
- en: Identifying the initial use case of your OpenStack environment is a major key
    to a successful start of the cloud journey. With a vast array of features, especially
    within the latest releases, the cloud management platform has become more mature
    than ever, but that might be overwhelming. Sidetracking the purpose of your OpenStack
    implementation would increase management complexity and potentially put your business
    at risk if such investment is not carefully accompanied by a vision. For this
    purpose, drafting your design into different iterations will help you avoid being
    blocked by *the paradox of choices* and unleash a clear short-term vision toward
    the longer one. As the first design iteration, we will simply identify our conceptual
    OpenStack model, followed by the logical architecture. Once drafted, we will walk
    through the practical implementation and put some numbers together to reflect
    our first deplo yment picture in the hardware.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 确定 OpenStack 环境的初始使用案例是云计算旅程成功启动的关键之一。随着功能的广泛增加，尤其是在最新发布中，云管理平台比以往任何时候都更加成熟，但这也可能让人感到压倒。偏离
    OpenStack 实现的目的会增加管理复杂性，并可能在缺乏清晰愿景的情况下使业务面临风险。为此，将设计分为不同的迭代将有助于避免被*选择的悖论*所困扰，并释放出清晰的短期愿景，朝着更长远的目标迈进。作为第一次设计迭代，我们将简单地识别我们的概念性
    OpenStack 模型，然后是逻辑架构。一旦草拟完成，我们将进行实际的实施，并汇总一些数据，以反映我们第一次部署的硬件图景。
- en: Drafting the conceptual design
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 草拟概念设计
- en: 'Based on the latest launching instance workflow, we have settled on the core
    services that should exist in our first OpenStack implementation. Naturally, any
    services that will be extending our basic setup later, such as PaaS services,
    will rely on the services underway that construct mainly our IaaS layer. During
    the next design phases, we will consider a generic OpenStack use case that would
    be suitable for internal usage as a private cloud to support organizations with
    elastic IaaS resources, boosting cloud application development, enabling data
    analysis, and helping with public hosting workloads. Beyond the core services,
    other mature ones will be included in this design iteration, as summarized in
    the following table:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最新的启动实例工作流程，我们已经确定了在第一次 OpenStack 实现中应该包含的核心服务。自然地，任何后续扩展我们基础设置的服务，如 PaaS
    服务，都将依赖于当前正在建设的、主要构建我们 IaaS 层的服务。在接下来的设计阶段，我们将考虑一个通用的 OpenStack 使用案例，适用于作为私有云的内部使用，支持组织弹性的
    IaaS 资源，促进云应用开发，支持数据分析，并帮助处理公共托管工作负载。除了核心服务外，其他成熟的服务也将被纳入本次设计迭代，如下表所示：
- en: '| **Cloud service** | **Service role** |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **云服务** | **服务角色** |'
- en: '| Compute – Nova | Manages VM life cycleUI-enabled |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 计算 – Nova | 管理虚拟机生命周期UI启用 |'
- en: '| Imaging – Glance | Manages VM image filesUI-enabled |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 镜像 – Glance | 管理虚拟机镜像文件UI启用 |'
- en: '| Object storage – Swift | Manages objects for persistent storageUI-enabled
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 对象存储 – Swift | 管理持久化存储的对象UI启用 |'
- en: '| Block storage – Cinder | Manages VM volumes for persistent storageUI-enabled
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 块存储 – Cinder | 管理虚拟机卷的持久存储UI启用 |'
- en: '| Networking – Neutron | Manages network L2 and L3 resourcesUI-enabled |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 网络 – Neutron | 管理 L2 和 L3 网络资源UI启用 |'
- en: '| Data metrics – Ceilometer | Collects resource data metrics for monitoringUI-enabled
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 数据度量 – Ceilometer | 收集资源数据度量以供监控UI启用 |'
- en: '| File share – Manila | Manages scale-out file share systemsUI-enabled |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 文件共享 – Manila | 管理可扩展文件共享系统UI启用 |'
- en: '| Identity – Keystone | Authentication and authorization |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 身份验证 – Keystone | 身份认证与授权 |'
- en: '| Dashboard – Horizon | GUI |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 仪表板 – Horizon | GUI |'
- en: '| Scheduling – Placement | Prefiltering for resource providers and traits |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 调度 – Placement | 资源提供者及其特征的预过滤 |'
- en: Table 1.3 – Initial OpenStack services
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.3 – 初始 OpenStack 服务
- en: 'Summing up those services into one conceptual diagram will captivate our next
    logical design iteration and narrow down the distribution of the services across
    different cluster roles of the OpenStack implementation:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些服务总结成一个概念图，将引导我们下一次逻辑设计迭代，并缩小服务在 OpenStack 实现中不同集群角色之间的分布：
- en: '![Figure 1.4 – OpenStack services interaction workflow](img/B21716_01_04.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – OpenStack 服务交互工作流程](img/B21716_01_04.jpg)'
- en: Figure 1.4 – OpenStack services interaction workflow
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – OpenStack 服务交互工作流程
- en: The previous diagram illustrates a high-level presentation of our first OpenStack
    architecture draft that counts several services to host a variety of workloads
    for end users. We can proceed with our design by developing a logical layout in
    the next section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了我们第一个OpenStack架构草图的高层次呈现，包含多个服务，以托管为最终用户提供各种工作负载。我们可以在下一节中通过开发逻辑布局继续进行设计。
- en: Drafting the logical design
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 草拟逻辑设计
- en: 'The logical design iteration is a vital step that should be carried out with
    more attention. More brainstorming points will be raised at each level and within
    each service of the design to formulate an initial draft that can be implemented
    with more confidence. The other key consideration is to run our services for both
    the control and data planes in a minimum setup of HA. That can be empowered by
    clustering the controller and compute nodes in the subsequent iterations. While
    working on the cloud roadmap, it is essential to reflect on the growth aspect.
    As your business needs are identified, the first logical design should be mapped
    with the first explored requirements. Trying OpenStack even with the minimum installation
    can be helpful to familiarize yourself with its basic ecosystem components. On
    the other hand, a major consideration of the first deployment for future successful
    experiences is to think of *growth* . As mentioned earlier, OpenStack is developed
    with modular software architecture; reflecting the same approach in the logical
    design followed by the physical one will absolutely make your cloud shine. Consider
    the following guidelines to begin with:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑设计的迭代是一个至关重要的步骤，需要更加关注。在设计的每一层级和每个服务中都会提出更多的头脑风暴点，以制定一个更有信心能够实施的初步草稿。另一个关键考虑是为控制平面和数据平面运行我们的服务时，最小化HA设置。这可以通过在后续的迭代中集群控制节点和计算节点来实现。在进行云路线图规划时，必须考虑到增长方面。当你的业务需求被明确后，第一个逻辑设计应与首次探索的需求相匹配。即使是最小化安装的OpenStack，也能帮助你熟悉其基本的生态系统组件。另一方面，第一个部署时一个重要的考虑因素是考虑*增长*。正如前面提到的，OpenStack是以模块化软件架构开发的；在逻辑设计中体现相同的思路，然后在物理设计中遵循这一方法，绝对会让你的云架构大放异彩。请参考以下指南开始：
- en: Define roles for the OpenStack system in both the control and data planes
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为OpenStack系统在控制平面和数据平面中定义角色
- en: Isolate roles starting with the minimum number of nodes and sufficient hardware
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从最小节点数和足够的硬件开始，隔离角色
- en: Think ahead of the possibility of component services failing
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前考虑组件服务失败的可能性
- en: Use software that the cloud operations team is most familiar with, such as queuing
    message and database common services
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云运维团队最熟悉的软件，如消息排队和数据库常见服务
- en: 'Throughout the first iteration of our logical design, an incremental approach
    will be taken, starting with the following roles:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次逻辑设计迭代过程中，我们将采取增量方法，首先从以下角色开始：
- en: A pair of controller nodes running the control plane, including the database
    and queuing message.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对控制节点运行控制平面，包括数据库和消息排队。
- en: One compute node running the hypervisor layer based on **Kernel-based Virtual**
    **Machine** ( **KVM** ).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台计算节点运行基于**内核虚拟机**（**KVM**）的虚拟化层。
- en: MySQL with the MariaDB engine and RabbitMQ will be chosen as the database and
    messaging queue services, respectively. Most of the OpenStack-documented implementations
    use MySQL and RabbitMQ as almost a religious matter.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将选择使用MariaDB引擎的MySQL和RabbitMQ作为数据库和消息队列服务。大多数OpenStack的文档化实现几乎把MySQL和RabbitMQ当做“宗教”一样使用。
- en: HAProxy and Pacemaker will be running in each controller node for HA, load balancing,
    and clustering. Database HA will be implemented through Galera multi-master replication.
    RabbitMQ instances will run on their native clustering mode based on queue mirroring
    across both cloud controller hosts.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy和Pacemaker将在每个控制节点上运行，用于HA、负载均衡和集群。数据库HA将通过Galera多主复制实现。RabbitMQ实例将以其原生的集群模式运行，基于跨两个云控制主机的队列镜像。
- en: Important note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The first logical design should not be overwhelming as it is not a final draft
    ready for production yet. [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running
    a Highly Available Cloud – Meeting the SLA* , has been dedicated to going through
    each layer of the future implemented control and data planes in detail.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个逻辑设计不应过于复杂，因为它还不是准备投入生产的最终草稿。[*第7章*](B21716_07.xhtml#_idTextAnchor174)，*高可用云架构
    – 满足SLA要求*，专门详细介绍了未来实施的控制平面和数据平面每一层的内容。
- en: 'The first logical design proposal can be drafted by using the previously listed
    software tools and the cloud controller and compute roles, as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个逻辑设计提案可以通过使用先前列出的软件工具以及云控制器和计算角色来起草，如下所示：
- en: '![Figure 1.5 – High-level logical design of OpenStack components for deployment](img/B21716_01_05.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – OpenStack 组件部署的高级逻辑设计](img/B21716_01_05.jpg)'
- en: Figure 1.5 – High-level logical design of OpenStack components for deployment
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – OpenStack 组件部署的高级逻辑设计
- en: Once the logical setup of different components and roles has been drafted, we
    will need to identify how the OpenStack nodes should conne ct.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成了不同组件和角色的逻辑设置草案，我们将需要确定 OpenStack 节点的连接方式。
- en: Connecting the dots
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接点
- en: 'Another essential aspect during the logical design phase is to provide a consistent
    networking layout between the different roles of each OpenStack entity. There
    are gazillions of ways to connect different pieces in an OpenStack ecosystem;
    the following layout demonstrates a network segmentation approach:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑设计阶段的另一个重要方面是在每个 OpenStack 实体的不同角色之间提供一致的网络布局。在 OpenStack 生态系统中连接不同部分的方式多种多样；以下布局演示了一种网络分割方法：
- en: '![Figure 1.6 – Network segmentation OpenStack nodes](img/B21716_01_06.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – OpenStack 节点的网络分割](img/B21716_01_06.jpg)'
- en: Figure 1.6 – Network segmentation OpenStack nodes
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – OpenStack 节点的网络分割
- en: The isolation of different types of networks will increase security access through
    networking segmentation. The trade-off is the extra complexity, but security comes
    first. The other opinion on this choice is also the performance question, as dedicating
    a separate segment for each type of traffic would save bandwidth significantly.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网络分割不同类型的网络，将增加安全访问。这种折衷是额外的复杂性，但安全性放在首位。对于这种选择的另一个观点也是性能问题，因为为每种类型的流量专用一个单独的段将显著节省带宽。
- en: 'The previous diagram illustrates four types of networks denoted with the following
    conventions:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图表说明了四种网络类型，使用以下约定标记：
- en: '**External network** : Connecting to the outside world. OpenStack APIs can
    be reachable from the public as well as private backbone within a global organization
    network. The external network will provide routable IP addresses. This type of
    network will be part of the data plane as it will expose or direct traffic instances
    within the underlying infrastructure. It is essential to keep security configuration
    tied at this level by fronting the network with load balancer devices or appliances.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部网络** ：连接到外部世界。OpenStack 的 API 可以从公共以及全球组织网络内的私有骨干部分中访问。外部网络将提供路由 IP 地址。这种类型的网络将作为数据平面的一部分，因为它将公开或指示基础设施内实例的流量。通过在此级别前端化网络以负载均衡设备或设备来保持与安全配置相关是至关重要的。'
- en: '**Management network** : As part of the control plane, this provides interconnectivity
    between all the various nodes in the OpenStack environment. Services such as the
    database and queuing message will be plugged into this network to be reached by
    the compute nodes. The management network does not expose anything to the outside
    world and should be seen as a private or internal network (you might find the
    naming convention of *API network* in other sources).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理网络** ：作为控制平面的一部分，这提供了在 OpenStack 环境中所有不同节点之间的互连性。诸如数据库和排队消息服务将被插入到此网络中，以供计算节点访问。管理网络不向外部世界公开任何内容，应视为私有或内部网络（您可能会在其他来源中找到*API
    网络*的命名约定）。'
- en: '**Tenant network** : As users will require their virtual networks, the tenant
    network (referred to as a *guest* or *overlay network* in other sources) will
    be dedicated to handling instance traffic. There are a variety of options for
    this type of network that could be attached to SDN capabilities in the network
    node. [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159) , *OpenStack Networking
    – Connectivity and Managed Service Options* , will highlight the scope of virtual
    overlay networking in more detail. The tenant network can be considered as part
    of the data plane.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**租户网络** ：由于用户将需要其虚拟网络，租户网络（在其他来源中称为*客户*或*覆盖网络*）将专门处理实例流量。这种类型的网络可以附加到网络节点中的
    SDN 能力中。[*第 6 章*](B21716_06.xhtml#_idTextAnchor159)，*OpenStack 网络 – 连接和托管服务选项*，将更详细地介绍虚拟覆盖网络的范围。租户网络可以视为数据平面的一部分。'
- en: '**Storage network** : As part of the data plane, the storage network will connect
    compute, and storage cluster nodes.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储网络** ：作为数据平面的一部分，存储网络将连接计算和存储集群节点。'
- en: Important note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Instances can use a direct physical network instead of virtual ones empowered
    by Neutron through SDN, which would assign floating IPs to access the outside
    world. This model is referred to as a *provider network* that would connect network
    and compute. More details will be highlighted in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed* *Service Options* .
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 实例可以使用直接的物理网络，而不是通过 SDN 由 Neutron 提供的虚拟网络，后者会分配浮动 IP 来访问外部世界。这种模型被称为 *提供商网络*，它将网络和计算连接起来。更多细节将在
    [*第6章*](B21716_06.xhtml#_idTextAnchor159) 中详细说明， *OpenStack 网络 - 连接性与托管服务选项*。
- en: Enumerating the different types of networks earlier will help the selection
    and reservation of different ports and network cards of each host per role in
    the long-term setup. It is still possible to combine more than one network in
    the same physical connection. However, the continuous expansion of the underlying
    infrastructure will hit the limit of the physical capacity of the combined segments,
    leaving a variety of issues with bottlenecks and network performance anomalies.
    Planning and preparing in advance will save an immense amount of time and effort
    in modifying the networking layout once network performance starts boiling. If
    a network node is dedicated in the next iteration, for example, the controller
    node will not need to connect to the tenant and exter nal networks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 之前列举不同类型的网络将有助于在长期设置中选择并预留每个主机角色所需的不同端口和网卡。尽管仍然可以将多个网络合并到同一个物理连接中，但底层基础设施的持续扩展将达到物理容量的极限，从而带来瓶颈和网络性能异常等各种问题。提前规划和准备将节省大量的时间和精力，尤其是当网络性能开始出现问题时。如果在下一个迭代中某个网络节点是专用的，例如，控制节点就不需要连接到租户和外部网络。
- en: Drafting the physical design
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 起草物理设计
- en: The next section will assign values to our parameters defined in the previous
    logical design. However, understanding a few key concepts and practices beforehand
    will save plenty of effort and costs. Let’s start with a capacity-plannin g demonstration.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将为我们在前面的逻辑设计中定义的参数赋值。然而，提前理解一些关键概念和实践将节省大量的精力和成本。让我们从容量规划的演示开始。
- en: Preparing for capacity planning
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备容量规划
- en: Capacity planning comes at every stage of a new infrastructure project, either
    within in-house hosting or for the development of a simple application. The cosmos
    of capacity-planning practices is based on forecasting and predicting how many
    resources an IT infrastructure would require to respond to business needs. In
    the OpenStack context, once your business case has been defined, the process of
    capacity analysis is narrowed to a specific set of resources that should exist.
    If you are planning to host a mixture of generic web hosting applications and
    data analysis workloads, there are certain considerations to be planned in terms
    of hardware sizing and the technology that will run the workloads. NFV would require
    more attention when selecting the hardware as it can be a performance eater.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 容量规划出现在新基础设施项目的每个阶段，无论是在内部托管中还是用于开发简单应用程序。容量规划实践的核心是预测和预测 IT 基础设施在响应业务需求时需要多少资源。在
    OpenStack 的背景下，一旦定义了业务需求，容量分析的过程就会缩小到应该存在的特定资源集合。如果你计划托管混合的通用网站托管应用和数据分析工作负载，在硬件规格和运行工作负载的技术方面需要进行一定的规划。选择硬件时，NFV
    需要更多的关注，因为它可能会消耗大量性能。
- en: 'Building the capacity planning for the OpenStack case can be summarized as
    follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为 OpenStack 案例建立容量规划可以总结如下：
- en: '**Operate with elasticity** : Be able to respond and pull more ubiquitous compute
    resources when needed in the case of a failure or load increase using automation.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活操作**：能够在出现故障或负载增加时，通过自动化响应并拉取更多的普遍计算资源。'
- en: '**Expect to fail** : Underlying resources should be ready to be replaced immediately
    without the need to spend time on fixing and reconfiguration efforts during incidents.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预计失败**：底层资源应该随时准备好被替换，而无需在事件发生时浪费时间进行修复和重新配置。'
- en: '**Track for growth** : Available capacity over the course of production can
    be variable. As we are in an on-demand model, the expected growth is not forcibly
    linear. Regularly track the usage of your underlying infrastructure to plot your
    cloud usage and update the capacity roadmap.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪增长**：在生产过程中，实际可用容量可能会变化。由于我们处于按需模型中，预期的增长不一定是线性的。定期跟踪底层基础设施的使用情况，以绘制云使用情况并更新容量路线图。'
- en: Another key foundation that is highly appreciated by large infrastructure management
    is the adoption of **Information Technology Infrastructure Library** ( **ITIL**
    ) practices. By reflecting on the IT infrastructure that will run the OpenStack
    cloud environment, ITIL methodologies will definitely refine a strategic process
    to identify a complete cycle of your capacity management under the ITIL service
    design umbrella. If your organization has already rolled out ITIL practices, feel
    free to reuse and apply them in the cloud journey. From a **Cloud Service Provider**
    ( **CSP** ) perspective, having a tactical approach to managing the underlying
    IT infrastructure to align with business needs and user demand and take full control
    of the financial aspect is a *must-have* .
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个被大规模基础设施管理高度认可的关键基础是采纳**信息技术基础设施库**（**ITIL**）实践。通过反思将运行 OpenStack 云环境的 IT
    基础设施，ITIL 方法论无疑会完善一个战略流程，以识别在 ITIL 服务设计框架下的容量管理完整周期。如果您的组织已经实施了 ITIL 实践，可以随时在云旅程中重复使用并应用它们。从**云服务提供商**（**CSP**）的角度来看，采取一种战术方法来管理底层
    IT 基础设施，以满足业务需求和用户需求，并全面控制财务方面，是一个*必须具备*的能力。
- en: Important note
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: ITIL is a framework presented with a set of practices and methodologies to standardize,
    manage, and optimize IT services offered in a given business. ITIL has evolved
    into four different versions. All the versions emphasize common core concepts
    around the business service, such as the service design pillar targeting capacity
    management.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ITIL 是一个框架，提供了一套实践和方法论，用于标准化、管理和优化在特定业务中提供的 IT 服务。ITIL 已发展为四个不同版本。所有版本都强调围绕业务服务的共同核心概念，如服务设计支柱，专注于容量管理。
- en: Mapping the land
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射土地
- en: 'As a first iteration, it would be ideal to dig into a few parameters that would
    generate some numbers to start the journey. In order to estimate our hardware
    and configurations, we can think of a reverse approach. Instead of looking at
    how many instances the first deployment could accommodate, we should see this
    from the end user’s eyes: *Which instance flavors could be offered to run which
    specific workloads?* An instance flavor is a set of specifications defined as
    a characteristics template in the compute service, including the number of vCPU
    cores, amount of RAM (including swap), and root disk (including ephemeral size).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一次迭代，理想的做法是深入分析一些参数，以生成一些数据来开始旅程。为了估算我们的硬件和配置，我们可以考虑采用反向方法。与其考虑第一次部署能容纳多少实例，不如从最终用户的角度来看待这个问题：*哪些实例规格可以提供用于运行哪些特定工作负载？*
    实例规格是一组在计算服务中定义的特性模板，包括 vCPU 核心数量、RAM 容量（包括交换空间）和根磁盘（包括临时磁盘大小）。
- en: Important note
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More advanced flavor specs can be customized and created in OpenStack to deploy
    instances that require specific workloads by scheduling the Nova service to use
    a set of compute nodes, such as support of certain CPU architecture or intensive
    workloads for HPC.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 中，可以自定义和创建更高级的规格，以部署需要特定工作负载的实例，通过调度 Nova 服务使用一组计算节点，比如支持特定的 CPU
    架构或用于高性能计算（HPC）的密集型工作负载。
- en: A simple approach is to spread the population of compute resources into an average
    flavor model that could run generic workloads. As a start, an instance flavor
    could be suitable for testing tenant environments with a couple of vCPUs and 1024
    MB of RAM capacity. Defining the baseline flavor model will enable us to determine
    the next ones by doubling the size of capacity in the next iteration. Keep in
    mind that there are gazillions of ways to define the set of flavors, but, most
    importantly, identifying the starting one will help to classify the next sizing
    in the series.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是将计算资源分布到一个平均规格模型中，该模型可以运行通用工作负载。作为起点，一个实例规格可能适合测试租户环境，具有几个 vCPU 和 1024
    MB 的 RAM 容量。定义基线规格模型将使我们能够通过在下一次迭代中将容量大小加倍，来确定下一个规格。请记住，有无数种方法可以定义规格集，但最重要的是确定起始规格，它将帮助我们在系列中对下一个尺寸进行分类。
- en: Depending on your captured business needs, hence the type(s) of workload(s),
    you will have a clearer picture of how much hardware capacity will be required
    in each compute node you plan to invest in. With every combination of flavors,
    putting the density of resources in the compute box will help to measure whether
    any wasted room has been left behind so that it can be used within a new flavor.
    For example, if a compute node could support 40 medium and 10 small-sized instances
    and still have some room left, create a new flavor with the gapped size of 1 vCPU
    and 512 MB of RAM to be added to the compute node flavor catalog.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的商业需求和工作负载类型，您将更清楚地了解在每个计划投资的计算节点中所需的硬件容量。对于每种实例类型的组合，将资源密度放入计算节点中有助于衡量是否存在任何浪费空间，以便可以在新的实例类型中使用。例如，如果一个计算节点支持40个中型和10个小型实例，并且仍然有一些空间，可以创建一个新的实例类型，将1个vCPU和512MB的RAM添加到计算节点的实例类型目录中。
- en: 'The following use case will consider a business initiative to run instances
    with versatile types of generic workloads that include instance flavors:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 以下使用案例将考虑一个商业计划，运行具有多种类型通用工作负载的实例，包括实例类型：
- en: '| **Flavor** | **vCPU** | **RAM (MB)** | **Disk (GB)** |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| **实例类型** | **vCPU** | **内存 (MB)** | **磁盘 (GB)** |'
- en: '| Tiny | 1 | 512 | 10 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Tiny | 1 | 512 | 10 |'
- en: '| Small | 1 | 1024 | 20 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Small | 1 | 1024 | 20 |'
- en: '| Medium | 2 | 2048 | 40 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Medium | 2 | 2048 | 40 |'
- en: '| Large | 4 | 4096 | 80 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Large | 4 | 4096 | 80 |'
- en: Table 1.4 – Instance flavors list
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.4 – 实例类型列表
- en: The first capacity-planning roadmap will assume 200 VMs as a starting point
    with the first set of compute nodes. The following sections will conduct an estimation
    for different hardware specs, including CPU, RAM, storage, and network for each
    compute node.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个容量规划路线图将假设以200个虚拟机作为起点，并且是第一组计算节点。接下来的章节将对不同硬件规格进行估算，包括每个计算节点的CPU、RAM、存储和网络。
- en: CPU estimation
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CPU估算
- en: 'A proper calculation of the compute power will depend significantly on the
    technology and model supported by the hardware. For this reason, we will define
    a list of hardware assumptions for our use case as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 计算能力的正确计算将显著依赖于硬件所支持的技术和模型。因此，我们将定义一份硬件假设清单，以支持我们的使用案例，内容如下：
- en: GHz per physical core = 2.6 GHz
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个物理核心的GHz = 2.6 GHz
- en: Physical core hyperthreading support = use factor 2
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理核心超线程支持 = 使用系数 2
- en: GHz per VM (AVG compute units) = 2 GHz
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个虚拟机的GHz（平均计算单位）= 2 GHz
- en: GHz per VM (MAX compute units) = 16 GHz
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个虚拟机的GHz（最大计算单位）= 16 GHz
- en: Intel Xeon E5-2648L v2 core CPU = 10
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英特尔Xeon E5-2648L v2核心CPU = 10
- en: CPU sockets per server = 2
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个服务器的CPU插槽数 = 2
- en: CPU overcommit ratio = 16:1
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU过度承诺比率 = 16:1
- en: 'Oversubscription: Disabled'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超额订阅：禁用
- en: 'Hyperthreading: Disabled'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超线程：禁用
- en: 'Operating system overhead: 20%'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统开销：20%
- en: Important note
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: OpenStack compute allows the overcommitment of resources for CPU and RAM. The
    technique of overcommitment aims to maximize the usage of resources by leveraging
    sharing between VMs running in the same hypervisor machine. For example, running
    16 vCPUs per 1 physical CPU in a hypervisor host is denoted as a CPU overcommitment
    of 16:1.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack计算允许对CPU和RAM资源进行过度承诺。过度承诺的技术旨在通过利用在同一虚拟化宿主机上运行的虚拟机之间共享资源，最大化资源的使用。例如，在虚拟化宿主机中，每1个物理CPU运行16个vCPU，被称为16:1的CPU过度承诺。
- en: 'Let’s start with the number of virtual cores:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从虚拟核心数开始：
- en: '*(number of VMs x number of GHz per VM) / number of GHz* *per core*'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*(虚拟机数量 x 每个虚拟机的GHz) / 每个核心的GHz* '
- en: '**(200 * 2) / 2.6 =** **153.8 46**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**(200 * 2) / 2.6 =** **153.8 46**'
- en: A rounded decimal estimation will result in 154 vCPU cores.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 四舍五入后的估算结果为154个vCPU核心。
- en: 'Now, let’s take into account the operating system overhead running in the compute
    node of 20%, as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来考虑在计算节点上运行操作系统的开销为20%，如下所示：
- en: '**154 + ((154 * 20)/100) =** **184.4**'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**154 + ((154 * 20)/100) =** **184.4**'
- en: A rounded decimal estimation will be 185 vCPUs.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 四舍五入后的估算结果为185个vCPU。
- en: 'Adding the overcommitment ratio 16:1 parameter to estimate the actual physical
    CPU on the compute node can be calculated as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将16:1的过度承诺比率参数添加到计算节点中，估算实际的物理CPU可以按以下方式计算：
- en: '**185/16 =** **11.5625**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**185/16 =** **11.5625**'
- en: A rounded decimal estimation will result in 12 CPU cores.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 四舍五入后的估算结果为12个CPU核心。
- en: As a starting point, a compute node of 12 CPU cores would host 200 instances
    with a *small* model flavo r.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 作为起点，一个具有12个CPU核心的计算节点将托管200个*小型*模型实例。
- en: Memory estimation
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存估算
- en: 'The following assumptions would be required to align with the previous CPU
    estimation:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与前面的CPU估算对齐，需要以下假设：
- en: 1024 MB of RAM per instance for the small flavor
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个实例 1024 MB 的 RAM 用于小规格实例
- en: 8 GB of RAM maximum dynamic allocations per VM
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个虚拟机最大动态分配 8 GB 的 RAM
- en: Compute nodes supporting slots of 2, 4, 8, and 16 GB sticks
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 2、4、8 和 16 GB 内存条的计算节点
- en: RAM overcommit ratio of 1:1
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1:1 的内存超额承诺比例
- en: Operating system overhead of 20%
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统开销 20%
- en: 'For 200 VMs with small-sized flavor instances, a RAM estimation is given as
    follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 200 个小规格实例的虚拟机，RAM 估算如下：
- en: '**200 * 1024 MB =** **200 GB**'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**200 * 1024 MB =** **200 GB**'
- en: 'Adding the 20% operating system overhead gives us the following:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 增加 20% 操作系统开销后，我们得到以下结果：
- en: '**200 + ((200 * 20)/100) =** **240 GB**'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**200 + ((200 * 20)/100) =** **240 GB**'
- en: 'A 1:1 overcommitment ratio will determine the actual RAM size needed per compute
    node:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 1:1 的超额承诺比例将决定每个计算节点所需的实际 RAM 大小：
- en: '**240/1 =** **240 GB**'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**240/1 =** **240 GB**'
- en: That would require an amount of 240 GB of RAM in our compute node to accommodate
    200 small flavor instances.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这将要求我们的计算节点配备 240 GB 的 RAM 来容纳 200 个小规格实例。
- en: Important note
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The default Nova compute configuration comes with an overcommitment value of
    1:1.5. Be aware that, unlike CPU overcommitment behavior, memory overcommitment
    can affect the instance performance if not enough room for the swap memory is
    planned in advance. As a good practice, with 1:1.5, configure the swap memory
    to be at least double what is provid ed.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 默认 Nova 计算配置的超额承诺值为 1:1.5。需要注意的是，与 CPU 超额承诺行为不同，如果没有提前规划足够的交换内存空间，内存超额承诺可能会影响实例性能。作为良好的实践，使用
    1:1.5 时，交换内存应至少配置为提供的内存的两倍。
- en: Storage estimation
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存储估算
- en: As per storage, physical compute nodes should acquire enough storage capacity
    with a multitude of options by providing the instance’s root disk from the compute
    nodes’ physical disks themselves or via attached storage devices.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如同存储一样，物理计算节点应提供足够的存储容量，提供多种选择，例如通过计算节点的物理磁盘或通过附加存储设备为实例提供根磁盘。
- en: Considering 200 VMs and a small size flavor instance of 20 GB root disk, a compute
    node should acquire an amount of *200 * 40 =* *800 GB* .
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 200 个虚拟机和每个虚拟机 20 GB 根磁盘的小规格实例，一个计算节点应该获得 *200 * 40 =* *800 GB* 的存储。
- en: Depending on the operating system requirement for disk space and other factors
    with caching and swapping configuration, an estimation of between 900 GB and 1
    TB of storage will grant a secured disk allocation for all instances (think of
    swap and extra caching disk operation overheads).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 根据操作系统对磁盘空间的需求以及缓存和交换配置的其他因素，估算大约需要 900 GB 至 1 TB 的存储空间，以为所有实例提供安全的磁盘分配（考虑交换和额外的缓存磁盘操作开销）。
- en: Another aspect that can be considered for more flavor customization is the storage
    type specs. That was not mentioned in the previous flavor catalog table, but,
    as part of the capacity management exercise, when the business changes the scope
    of the nature of the workload, extra classes of flavors can be added to expand
    that list, such as high-performance storage and **input/output operations per
    second** ( **IOPS** ) specs. Storage devices have various specifications to deal
    with certain use cases and patterns; for highly intensive workloads that require
    fast reads and writes, **Solid-State Drive** ( **SSD** ) disks should be considered
    in the hardware list, followed by a new custom flavor presenting the SSD specifica
    tions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以考虑的用于更多规格定制的方面是存储类型规格。这个内容在之前的规格目录表中没有提及，但作为容量管理的一部分，当业务更改工作负载的性质范围时，可以添加额外的规格类以扩展该列表，比如高性能存储和
    **每秒输入/输出操作**（**IOPS**）规格。存储设备有各种规格以应对特定的用例和模式；对于需要快速读写的高强度工作负载，应该考虑将**固态硬盘**（**SSD**）列入硬件清单，并提供一个新定制规格的硬盘，显示
    SSD 规格。
- en: Networking estimation
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络估算
- en: 'Networking capacity planning comes in two categories:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 网络容量规划分为两类：
- en: Network topology, and switching and routing layers to provide sufficient IP
    addressing of the underlying cloud layer
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络拓扑结构，以及切换和路由层以提供足够的 IP 地址支持基础云层
- en: Instances and overlay networking functions running on top of the infrastructure
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基础设施之上运行的实例和覆盖网络功能
- en: The first pillar would require a consistent network layout that would reflect
    which node would acquire a private IP, a public one, or both. Obviously, when
    putting the nodes together, as per network design, the first step is to assign
    each host its private IP from a given private IP pool. Components that would require
    access to the outside world, as defined in the logical draft, would use public
    IPs. That can be granted via frontend devices or appliances such as load balancers
    or routers via **Source Network Address Translation** ( **SNAT** ) and **Destination
    Network Address Translation** ( **DNAT** ) mechanisms.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个支柱要求一个一致的网络布局，能够反映哪个节点将获得私有 IP、公有 IP 或两者。显然，根据网络设计，将节点放在一起的第一步是从给定的私有 IP
    池中为每个主机分配私有 IP。根据逻辑草案定义，需要访问外部世界的组件将使用公有 IP。可以通过前端设备或设备（如负载均衡器或路由器）通过 **源网络地址转换**（**SNAT**）和
    **目标网络地址转换**（**DNAT**）机制授予这些 IP。
- en: 'The second pillar captures a few factors of network capacity estimation targeting
    *undercloud* resources, which includes mainly the following assumptions:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个支柱捕获了针对 *undercloud* 资源的网络容量估算的几个因素，主要包括以下假设：
- en: Minimum bandwidth of 50 Mbits/sec per virtual interface per instance
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个实例每个虚拟接口的最低带宽为 50 Mbits/sec
- en: Association of one floating IP per instance
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个实例的浮动 IP 关联
- en: Association of one floating IP per NFV overlay function – virtual routers
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 NFV 覆盖功能的浮动 IP 关联——虚拟路由器
- en: 10% reserved floating IP for future use
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10% 的浮动 IP 保留供未来使用
- en: Another essential aspect that should be considered when planning for networking
    is the network interfaces that will be configured in each compute node. As we
    covered a high-level design in our logical architecture draft, a few networks
    will be created and assigned to each interface in each host. This architecture
    will involve switching L2 configuration, typically via **Virtual Local Area Network**
    ( **VLAN** ) aggregation. Once basic configurations are in place, running some
    benchmarking tools is necessary to gather a few metrics on the bandwidth capacity
    in each switch port and the compute host interface. That will help to get at-a-glance
    network performance information for the hop between the compute host and the main
    physical switch port. As the physical bandwidth will be shared between different
    virtual network instance interfaces, the first value of the switch port and compute
    node physical interface should be acceptable.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划网络时，另一个需要考虑的重要方面是将要在每个计算节点中配置的网络接口。如我们在逻辑架构草案中所讨论的那样，将创建并分配几个网络到每个主机中的每个接口。此架构将涉及
    L2 配置切换，通常通过 **虚拟局域网**（**VLAN**）聚合。一旦基本配置就绪，运行一些基准测试工具是必要的，以收集每个交换机端口和计算主机接口上的带宽容量的几个指标。这将帮助快速获取计算主机与主物理交换机端口之间跳数的网络性能信息。由于物理带宽将在不同的虚拟网络实例接口之间共享，交换机端口和计算节点物理接口的第一个值应是可接受的。
- en: The next consideration is taking into account three network interfaces cabled
    to each compute node. One of the network interfaces attached to the tenant network
    (associated with the VLAN in a dedicated switch port) will be cabled using a 10
    GB physical link that will serve 200 VMs, giving 50 MBits/sec for each.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个考虑因素是考虑到三条网络接口线缆连接到每个计算节点。连接到租户网络（与专用交换机端口中的 VLAN 关联）的网络接口将通过 10 GB 物理链路连接，服务
    200 台虚拟机，每台虚拟机提供 50 MBits/sec 的带宽。
- en: Important note
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: L2-specific vendors’ network performance varies from one device to another.
    Make sure to measure network traffic and supported **Maximum Transmission Unit**
    ( **MTU** ) sizes when designing your network segmentation and VLAN configurations.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: L2 特定厂商的网络性能因设备而异。在设计网络分段和 VLAN 配置时，确保测量网络流量和支持的 **最大传输单元**（**MTU**）大小。
- en: 'Physical network failure and performance should be highlighted as early as
    possible instead of waiting to be alerted. Including whole configurations and
    resetting cabling and interfaces when running in production does not sound like
    the most brilliant approach. One of the most popular networking techniques that
    would save cost and performance ahead is **network bonding** . Bonding supports
    two different modes: *active-backup* and *active-active* . The first will keep
    running only one network interface as active while the others are in the backup
    state, whereas the second involves the *link aggregation* concept – **Link Aggregation
    Control Protocol** ( **LACP** ) – where traffic will be load balanced through
    different interfaces.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 物理网络故障和性能问题应该尽早发现，而不是等到被警报通知。包括完整配置、重置布线和接口在生产环境中运行时听起来并不是最聪明的方法。一个节省成本和提高性能的流行网络技术是**网络绑定**。绑定支持两种不同模式：*主动-备份*和*主动-主动*。第一种模式将只保持一个网络接口为活动状态，而其他接口处于备份状态；第二种则涉及*链路聚合*概念——**链路聚合控制协议**（**LACP**）——其中流量将在不同接口之间进行负载均衡。
- en: To avoid network performance bottleneck surprises, the same approach can be
    taken with physical switches. A common well-known technology, **Multi-Chassis
    Link Aggregation** ( **MLAG** ), would transform several physical switches into
    one logical one, allowing FT and exposing the best-effort bandwidth to the connected
    node’s ports.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免网络性能瓶颈的意外发生，相同的方法也可以应用于物理交换机。一种常见且知名的技术是**多底盘链路聚合**（**MLAG**），它可以将多个物理交换机转化为一个逻辑交换机，从而允许冗余并将最佳努力带宽暴露给连接节点的端口。
- en: Empowering the network node’s performance without taking care of what capabilities
    the physical switching could provide would not guarantee a promising experience.
    Gathering the physical network capacity metrics in all the various layers is vital
    to avoid hitting unexpected performance bottlenecks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有考虑物理交换所能提供的能力的情况下提升网络节点性能，是无法保证优良体验的。收集各个层次的物理网络容量指标，对于避免遭遇意外性能瓶颈至关重要。
- en: 'The second pillar of our initial network capacity planning is the floating
    IP pool. As Neutron will be our network master in the OpenStack ecosystem, we
    are expecting the network node to interact with different network resources, such
    as instances, virtual routers, and load balancers, without mentioning advanced
    SDN configurations that would overwhelm our initial estimations. Floating IPs
    are publicly routable (public IPs are typically what you get from an **Internet
    Service Provider** ( **ISP** )). A floating IP request either from an instance
    or a network function resource such as a router or load balancer should not fail.
    Thus, we will estimate the pool as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初步网络容量规划的第二个支柱是浮动IP池。由于Neutron将在OpenStack生态系统中担任我们的网络主控，我们预计网络节点将与不同的网络资源进行交互，例如实例、虚拟路由器和负载均衡器，而不涉及可能会超出我们初步估算的高级SDN配置。浮动IP是公开可路由的（公共IP通常是从**互联网服务提供商**（**ISP**）获得的）。来自实例或网络功能资源（如路由器或负载均衡器）的浮动IP请求不应失败。因此，我们将按以下方式估算浮动IP池：
- en: '200 instances per compute node: 200 floating IPs'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个计算节点200个实例：200个浮动IP
- en: 20 tenants
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20个租户
- en: '10 routers per tenant: 200 floating IPs'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个租户10个路由器：200个浮动IP
- en: '10 load balancers per tenant: 200 floating IPs'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个租户10个负载均衡器：200个浮动IP
- en: Adding 10% for future use will generate a pool of 660 floating IPs.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了未来的使用增加10%，将生成一个660个浮动IP的池。
- en: Mixing the land
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合局域网
- en: The previous capacity-planning exercise considers a mono-compute density prototype
    targeting a *small* flavor. The capacity-planning study could involve more than
    one prototype of distributing the compute density, depending on your strategic
    business needs. The target layout can be extended to involve more flavors, supported
    in either heterogeneous or homogeneous compute density form. The **nova-scheduler**
    component can be configured, as we highlighted in a previous section, with advanced
    scheduling that works hand in hand with the Placement service to find the most
    optimal compute node. The same resource estimation for vCPU, RAM, and root disk
    can be followed to iterate through the next flavor to determine the next set of
    compute node groups. We will cover the filtering mechanisms to respond to compute
    requests in the most adequate way in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity* *and Flavors* .
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的容量规划练习考虑了一个针对*小型*口味的单一计算密度原型。容量规划研究可能涉及多个分布计算密度的原型，这取决于您的战略业务需求。目标布局可以扩展，以支持更多口味，并且可以是异构或同构计算密度形式。**nova-scheduler**
    组件可以进行配置，正如我们在前面一节中强调的那样，使用先进的调度功能，与放置服务协同工作，找到最优的计算节点。相同的 vCPU、RAM 和根磁盘的资源估算可以用于迭代下一个口味，以确定下一组计算节点。我们将在[*第
    4 章*](B21716_04.xhtml#_idTextAnchor125)中讨论响应计算请求的过滤机制，*OpenStack Compute – 计算容量*
    *和口味*。
- en: 'As illustrated in the following diagram, one way to set future boxes in the
    most predictable fashion is to assign each available compute node a homogenous
    flavor:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，设置未来计算节点最可预测的一种方式是为每个可用计算节点分配一个同质化的口味：
- en: '![Figure 1.7 – Compute node placements per flavor and workload trait](img/B21716_01_07.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 按口味和工作负载特征排列的计算节点](img/B21716_01_07.jpg)'
- en: Figure 1.7 – Compute node placements per flavor and workload trait
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 按口味和工作负载特征排列的计算节点
- en: As per the scheduling configuration together with the defined traits in the
    Placement service, a Nova request will reach the right available compute node
    to accommodate a specific workload. This model layout would classify compute nodes
    per types of workloads defined in your business requirements, so the compute nodes’
    capacity would be prepared in advance to expect the initial forecasted demand.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 根据调度配置和放置服务中定义的特征，Nova 请求将到达合适的可用计算节点，以容纳特定的工作负载。此模型布局将根据您的业务需求中定义的工作负载类型对计算节点进行分类，因此计算节点的容量将提前准备，以应对最初预测的需求。
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Being strategic is a fundamental key to making the right decisions when it comes
    to how to architect a complex ecosystem such as OpenStack. This chapter should
    lower the entry barrier to start an effective plan that meets your organization’s
    needs. The approach taken should help you identify different stages to start your
    cloud journey. There is no exact rule of thumb on how to design an operational
    OpenStack environment but templating the design patterns for each core service
    and sticking to the initially collected requirements will definitely enhance the
    journey. This chapter went through the newest updates on core services in the
    OpenStack ecosystem within *Antelope* and later releases, and a few more projects
    were considered to be offered once the private cloud is up and running. From an
    architecture perspective, this chapter should be revisited during the next stages
    to align and update your design draft in each step. As we used an iterative and
    incremental approach for our future OpenStack cloud environment, the next chapter
    will take you to the next deployment stage from what we have in draft, spiced
    up with best practices for the setup process.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 战略思维是做出正确决策的关键，特别是在如何设计像 OpenStack 这样复杂的生态系统时。本章旨在降低入门门槛，帮助您制定符合组织需求的有效计划。采取的方法应该帮助您识别启动云计算之旅的不同阶段。设计一个操作性的
    OpenStack 环境没有确切的经验法则，但为每个核心服务制定设计模板并遵循最初收集的需求，肯定能增强您的云计算之旅。本章回顾了在 *Antelope*
    及之后版本中 OpenStack 生态系统的核心服务的最新更新，并考虑了在私有云上线后可以提供的其他项目。从架构角度来看，本章应在下一个阶段重新审视，以便在每个步骤中调整和更新您的设计草稿。由于我们为未来的
    OpenStack 云环境采用了迭代和增量的方法，下一章将带您进入从草稿到部署阶段的下一步，并结合最佳实践进行设置过程的优化。
