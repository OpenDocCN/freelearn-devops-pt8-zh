- en: Chapter 1. Introduction to Linux Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章. Linux 容器简介
- en: Nowadays, deploying applications inside some sort of a Linux container is a
    widely adopted practice, primarily due to the evolution of the tooling and the
    ease of use it presents. Even though Linux containers, or operating-system-level
    virtualization, in one form or another, have been around for more than a decade,
    it took some time for the technology to mature and enter mainstream operation.
    One of the reasons for this is the fact that hypervisor-based technologies such
    as KVM and Xen were able to solve most of the limitations of the Linux kernel
    during that period and the overhead it presented was not considered an issue.
    However, with the advent of kernel namespaces and **control groups** (**cgroups**)
    the notion of a *light-weight virtualization* became possible through the use
    of containers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，在某种形式的 Linux 容器中部署应用程序已成为一种广泛采用的做法，主要得益于工具的演变和其提供的易用性。尽管 Linux 容器或操作系统级虚拟化在某种形式上已经存在超过十年，但这项技术需要一段时间才能成熟并进入主流应用。其原因之一是，基于虚拟机监控程序的技术（如
    KVM 和 Xen）在那段时间内能够解决 Linux 内核的大多数限制，且它所带来的开销并未被视为一个问题。然而，随着内核命名空间和**控制组**（**cgroups**）的出现，通过使用容器实现的*轻量级虚拟化*才变得可行。
- en: 'In this chapter, I''ll cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Evolution of the OS kernel and its early limitations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统内核的演变及其早期的限制
- en: Differences between containers and platform virtualization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器与平台虚拟化之间的区别
- en: Concepts and terminology related to namespaces and cgroups
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与命名空间和控制组相关的概念和术语
- en: An example use of process resource isolation and management with network namespaces
    and cgroups
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络命名空间和控制组进行进程资源隔离与管理的示例
- en: The OS kernel and its early limitations
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作系统内核及其早期的限制
- en: The current state of Linux containers is a direct result of the problems that
    early OS designers were trying to solve – managing memory, I/O, and process scheduling
    in the most efficient way.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 Linux 容器的状态是早期操作系统设计者试图解决的问题的直接结果——以最有效的方式管理内存、I/O 和进程调度。
- en: In the past, only a single process could be scheduled for work, wasting precious
    CPU cycles if blocked on an I/O operation. The solution to this problem was to
    develop better CPU schedulers, so more work can be allocated in a *fair* way for
    maximum CPU utilization. Even though the modern schedulers, such as the **Completely
    Fair Scheduler** (**CFS**) in Linux do a great job of allocating fair amounts
    of time to each process, there's still a strong case for being able to give higher
    or lower priority to a process and its subprocesses. Traditionally, this can be
    accomplished by the `nice()` system call, or real-time scheduling policies, however,
    there are limitations to the level of granularity or control that can be achieved.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，只有单个进程可以被调度执行，如果在进行 I/O 操作时发生阻塞，会浪费宝贵的 CPU 周期。解决这个问题的方法是开发更好的 CPU 调度器，以便能够以*公平*的方式分配更多的工作，从而最大限度地利用
    CPU。尽管现代的调度器，比如 Linux 中的**完全公平调度器**（**CFS**），在为每个进程分配公平的时间方面做得非常好，但仍然有很强的需求能够对进程及其子进程进行更高或更低的优先级控制。传统上，这可以通过
    `nice()` 系统调用或者实时调度策略来实现，但这也存在粒度或控制程度上的限制。
- en: Similarly, before the advent of virtual memory, multiple processes would allocate
    memory from a shared pool of physical memory. The virtual memory provided some
    form of memory isolation per process, in the sense that processes would have their
    own address space, and extend the available memory by means of a swap, but still
    there wasn't a good way of limiting how much memory each process and its children
    can use.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在虚拟内存出现之前，多个进程会从共享的物理内存池中分配内存。虚拟内存提供了一种每个进程的内存隔离形式，意味着进程会有自己的地址空间，并通过交换空间扩展可用内存，但仍然没有一个好的方式来限制每个进程及其子进程可以使用的内存量。
- en: To further complicate the matter, running different workloads on the same physical
    server usually resulted in a negative impact on all running services. A memory
    leak or a kernel panic could cause one application to bring the entire operating
    system down. For example, a web server that is mostly memory bound and a database
    service that is I/O heavy running together became problematic. In an effort to
    avoid such scenarios, system administrators would separate the various applications
    between a pool of servers, leaving some machines underutilized, especially at
    certain times during the day, when there was not much work to be done. This is
    a similar problem as a single running process blocked on I/O operation is a waste
    of CPU and memory resources.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 更加复杂的是，在同一物理服务器上运行不同的工作负载通常会对所有正在运行的服务产生负面影响。内存泄漏或内核恐慌可能导致某个应用程序使整个操作系统崩溃。例如，内存密集型的Web服务器与I/O密集型的数据库服务一起运行时，会变得非常有问题。为了避免这种情况，系统管理员通常会将不同的应用程序分配到一组服务器上，使得一些机器处于低利用率状态，尤其是在一天中的某些时段，当工作量不大时。这与单个运行中的进程因I/O操作阻塞而浪费CPU和内存资源是类似的问题。
- en: The solution to these problems is the use of hypervisor based virtualization,
    containers, or the combination of both.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的方法是使用基于虚拟机监控程序的虚拟化、容器技术，或者两者的结合。
- en: The case for Linux containers
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux容器的案例
- en: The hypervisor as part of the operating system is responsible for managing the
    life cycle of virtual machines, and has been around since the early days of mainframe
    machines in the late 1960s. Most modern virtualization implementations, such as
    Xen and KVM, can trace their origins back to that era. The main reason for the
    wide adoption of these virtualization technologies around 2005 was the need to
    better control and utilize the ever-growing clusters of compute resources. The
    inherited security of having an extra layer between the virtual machine and the
    host OS was a good selling point for the security minded, though as with any other
    newly adopted technology there were security incidents.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机监控程序作为操作系统的一部分，负责管理虚拟机的生命周期，自上世纪60年代末期的主机时代起便存在。大多数现代虚拟化实现，如Xen和KVM，都可以追溯到那个时期。大约在2005年，这些虚拟化技术的广泛应用的主要原因是需要更好地控制和利用日益增长的计算资源集群。虚拟机与宿主操作系统之间增加一层安全性也是一个好的卖点，尤其对注重安全性的人来说，尽管像所有新技术一样，仍然出现了一些安全事件。
- en: Nevertheless, the adoption of full virtualization and paravirtulization significantly
    improved the way servers are utilized and applications provisioned. In fact, virtualization
    such as KVM and Xen is still widely used today, especially in multitenant clouds
    and cloud technologies such as OpenStack.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，完全虚拟化和半虚拟化的采用显著改善了服务器的利用方式和应用的配置方式。事实上，像KVM和Xen这样的虚拟化技术至今仍被广泛使用，尤其是在多租户云和像OpenStack这样的云技术中。
- en: 'Hypervisors provide the following benefits, in the context of the problems
    outlined earlier:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前所述问题的背景下，虚拟机监控程序提供了以下好处：
- en: Ability to run different operating systems on the same physical server
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在同一物理服务器上运行不同的操作系统
- en: More granular control over resource allocation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更精细的资源分配控制
- en: Process isolation – a kernel panic on the virtual machine will not effect the
    host OS
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程隔离——虚拟机上的内核恐慌不会影响宿主操作系统
- en: Separate network stack and the ability to control traffic per virtual machine
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立的网络栈以及按虚拟机控制流量的能力
- en: Reduce capital and operating cost, by simplification of data center management
    and better utilization of available server resources
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过简化数据中心管理并更好地利用可用服务器资源，从而降低资本和运营成本。
- en: Arguably the main reason against using any sort of virtualization technology
    today is the inherited overhead of using multiple kernels in the same OS. It would
    be much better, in terms of complexity, if the host OS can provide this level
    of isolation, without the need for hardware extensions in the CPU, or the use
    of emulation software such as QEMU, or even kernel modules such as KVM. Running
    an entire operating system on a virtual machine, just to achieve a level of confinement
    for a single web server, is not the most efficient allocation of resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 今天反对使用任何形式的虚拟化技术的主要原因可以说是同一操作系统中使用多个内核所带来的开销。如果宿主操作系统能够提供这种隔离级别，而不需要CPU的硬件扩展，或不使用如QEMU之类的仿真软件，甚至不需要像KVM这样的内核模块，那么从复杂性角度来看，效果会更好。仅为了为单个Web服务器实现隔离而运行整个操作系统在虚拟机上，并不是最有效的资源分配方式。
- en: Over the last decade, various improvements to the Linux kernel were made to
    allow for similar functionality, but with less overhead – most notably the kernel
    namespaces and cgroups. One of the first notable technologies to leverage those
    changes was LXC, since kernel 2.6.24 and around the 2008 time frame. Even though
    LXC is not the oldest container technology, it helped fuel the container revolution
    we see today.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年里，Linux 内核进行了多次改进，以实现类似功能，但减少了开销——最显著的变化就是内核命名空间和 cgroups。LXC 是首批利用这些变化的技术之一，自内核版本
    2.6.24 和 2008 年左右开始。尽管 LXC 并不是最早的容器技术，但它帮助推动了我们今天所看到的容器革命。
- en: 'The main benefits of using LXC include:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LXC 的主要优点包括：
- en: Lesser overheads and complexity than running a hypervisor
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比运行虚拟机监控程序的开销和复杂性要小
- en: Smaller footprint per container
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个容器占用更小的系统资源
- en: Start times in the millisecond range
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动时间在毫秒范围内
- en: Native kernel support
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原生内核支持
- en: It is worth mentioning that containers are not inherently as secure as having
    a hypervisor between the virtual machine and the host OS. However, in recent years,
    great progress has been made to narrow that gap using **Mandatory Access Control**
    (**MAC**) technologies such as SELinux and AppArmor, kernel capabilities, and
    cgroups, as demonstrated in later chapters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，容器的安全性本身并不如在虚拟机和宿主操作系统之间使用虚拟机监控程序那样强。然而，近年来，通过使用 **强制访问控制** (**MAC**)
    技术，如 SELinux 和 AppArmor、内核能力和 cgroups，已经取得了显著进展，大大缩小了这一差距，后续章节将展示这些技术的应用。
- en: Linux namespaces – the foundation of LXC
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux 命名空间——LXC 的基础
- en: 'Namespaces are the foundation of lightweight process virtualization. They enable
    a process and its children to have different views of the underlying system. This
    is achieved by the addition of the `unshare()` and `setns()` system calls, and
    the inclusion of six new constant flags passed to the `clone()`, `unshare()`,
    and `setns()` system calls:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间是轻量级进程虚拟化的基础。它们使得进程及其子进程可以对底层系统有不同的视图。这是通过增加 `unshare()` 和 `setns()` 系统调用，以及为
    `clone()`、`unshare()` 和 `setns()` 系统调用传递的六个新的常量标志来实现的：
- en: '`clone()`: This creates a new process and attaches it to a new specified namespace'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clone()`：这会创建一个新进程并将其附加到一个新的指定命名空间'
- en: '`unshare()`: This attaches the current process to a new specified namespace'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unshare()`：这将当前进程附加到一个新的指定命名空间'
- en: '`setns()`: This attaches a process to an already existing namespace'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setns()`：这将一个进程附加到已存在的命名空间'
- en: 'There are six namespaces currently in use by LXC, with more being developed:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 LXC 使用六种命名空间，并且还在开发更多：
- en: Mount namespaces, specified by the `CLONE_NEWNS` flag
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挂载命名空间，通过 `CLONE_NEWNS` 标志指定
- en: UTS namespaces, specified by the `CLONE_NEWUTS` flag
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UTS 命名空间，通过 `CLONE_NEWUTS` 标志指定
- en: IPC namespaces, specified by the `CLONE_NEWIPC` flag
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPC 命名空间，通过 `CLONE_NEWIPC` 标志指定
- en: PID namespaces, specified by the `CLONE_NEWPID` flag
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PID 命名空间，通过 `CLONE_NEWPID` 标志指定
- en: User namespaces, specified by the `CLONE_NEWUSER` flag
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户命名空间，通过 `CLONE_NEWUSER` 标志指定
- en: Network namespaces, specified by the `CLONE_NEWNET` flag
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络命名空间，通过 `CLONE_NEWNET` 标志指定
- en: Let's have a look at each in more detail and see some userspace examples, to
    help us better understand what happens under the hood.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下每个优点，并通过一些用户空间的示例帮助我们更好地理解系统背后的运作原理。
- en: Mount namespaces
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挂载命名空间
- en: Mount namespaces first appeared in kernel 2.4.19 in 2002 and provided a separate
    view of the filesystem mount points for the process and its children. When mounting
    or unmounting a filesystem, the change will be noticed by all processes because
    they all share the same default namespace. When the `CLONE_NEWNS` flag is passed
    to the `clone()` system call, the new process gets a copy of the calling process
    mount tree that it can then change without affecting the parent process. From
    that point on, all mounts and unmounts in the default namespace will be visible
    in the new namespace, but changes in the per-process mount namespaces will not
    be noticed outside of it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 挂载命名空间首次出现在 2002 年的内核 2.4.19 版本中，为进程及其子进程提供了一个独立的文件系统挂载点视图。当挂载或卸载文件系统时，所有进程都会察觉到变化，因为它们共享同一个默认命名空间。当
    `CLONE_NEWNS` 标志传递给 `clone()` 系统调用时，新进程会获得调用进程挂载树的副本，并可以对其进行修改，而不会影响父进程。从那时起，默认命名空间中的所有挂载和卸载操作将会在新命名空间中可见，但在每个进程的挂载命名空间中的变化不会被外部进程察觉。
- en: 'The `clone()` prototype is as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`clone()` 原型如下：'
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'An example call that creates a child process in a new mount namespace looks
    like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例调用，在新的挂载命名空间中创建一个子进程，代码如下：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When the child process is created, it executes the `childFunc` function, which
    will perform its work in the new mount namespace.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当子进程被创建时，它会执行 `childFunc` 函数，该函数将在新的挂载命名空间中执行其工作。
- en: The `util-linux` package provides userspace tools that implement the `unshare()`
    call, which effectively unshares the indicated namespaces from the parent process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`util-linux` 包提供了实现 `unshare()` 调用的用户空间工具，这些工具有效地将指定的命名空间从父进程中分离出来。'
- en: 'To illustrate this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子：
- en: 'First open a terminal and create a directory in `/tmp` as follows:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先打开一个终端，并在 `/tmp` 下创建一个目录，如下所示：
- en: '[PRE2]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, move the current `bash` process to its own mount namespace by passing
    the mount flag to `unshare`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过向 `unshare` 传递挂载标志，将当前的 `bash` 进程移动到自己的挂载命名空间中：
- en: '[PRE3]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `bash` process is now in a separate namespace. Let''s check the associated
    inode number of the namespace:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`bash` 进程现在处于一个独立的命名空间中。让我们检查该命名空间的关联 inode 编号：'
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, create a temporary mount point:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个临时的挂载点：
- en: '[PRE5]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Also, make sure you can see the mount point from the newly created namespace:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，确保你能在新创建的命名空间中看到挂载点：
- en: '[PRE6]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As expected, the mount point is visible because it is part of the namespace
    we created and the current `bash` process is running from.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如预期的那样，挂载点可见，因为它是我们创建的命名空间的一部分，当前的 `bash` 进程正是从这个命名空间中运行的。
- en: 'Next, start a new terminal session and display the namespace inode ID from
    it:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，启动一个新的终端会话并显示该会话中的命名空间 inode ID：
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice how it's different from the mount namespace on the other terminal.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意它与另一个终端上的挂载命名空间的区别。
- en: 'Finally, check if the mount point is visible in the new terminal:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，检查新的终端中挂载点是否可见：
- en: '[PRE8]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Not surprisingly, the mount point is not visible from the default namespace.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，挂载点在默认命名空间中不可见。
- en: Note
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In the context of LXC, mount namespaces are useful because they provide a way
    for a different filesystem layout to exist inside the container. It's worth mentioning
    that before the mount namespaces, a similar process confinement could be achieved
    with the `chroot()` system call, however `chroot` does not provide the same per-process
    isolation as mount namespaces do.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LXC 的上下文中，挂载命名空间非常有用，因为它们提供了一种不同文件系统布局的方式，允许它存在于容器内部。值得一提的是，在挂载命名空间之前，可以使用
    `chroot()` 系统调用实现类似的进程隔离，但 `chroot` 并没有像挂载命名空间那样提供每个进程的独立隔离。
- en: UTS namespaces
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UTS 命名空间
- en: '**Unix Timesharing** (**UTS**) namespaces provide isolation for the hostname
    and domain name, so that each LXC container can maintain its own identifier as
    returned by the `hostname -f` command. This is needed for most applications that
    rely on a properly set hostname.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**Unix 时间共享**（**UTS**）命名空间为主机名和域名提供隔离，使得每个 LXC 容器都能保持其自己的标识符，正如 `hostname -f`
    命令返回的那样。大多数依赖于正确设置主机名的应用程序都需要这个功能。'
- en: 'To create a `bash` session in a new UTS namespace, we can use the `unshare`
    utility again, which uses the `unshare()` system call to create the namespace
    and the `execve()` system call to execute `bash` in it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要在新的 UTS 命名空间中创建一个 `bash` 会话，可以再次使用 `unshare` 工具，该工具使用 `unshare()` 系统调用来创建命名空间，并使用
    `execve()` 系统调用来执行 `bash`：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As the preceding output shows, the hostname inside the namespace is now `uts-namespace`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的输出所示，命名空间中的主机名现在是 `uts-namespace`。
- en: 'Next, from a different terminal, check the hostname again to make sure it has
    not changed:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从另一个终端检查主机名，确保它没有发生变化：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As expected, the hostname only changed in the new UTS namespace.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，主机名仅在新的 UTS 命名空间中发生了变化。
- en: 'To see the actual system calls that the `unshare` command uses, we can run
    the `strace` utility:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看 `unshare` 命令实际使用的系统调用，可以运行 `strace` 工具：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: From the output we can see that the `unshare` command is indeed using the `unshare()`
    and `execve()` system calls and the `CLONE_NEWUTS` flag to specify new UTS namespace.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，`unshare` 命令确实使用了 `unshare()` 和 `execve()` 系统调用，并使用 `CLONE_NEWUTS`
    标志来指定新的 UTS 命名空间。
- en: IPC namespaces
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IPC 命名空间
- en: The **Interprocess Communication** (**IPC**) namespaces provide isolation for
    a set of IPC and synchronization facilities. These facilities provide a way of
    exchanging data and synchronizing the actions between threads and processes. They
    provide primitives such as semaphores, file locks, and mutexes among others, that
    are needed to have true process separation in a container.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**进程间通信**（**IPC**）命名空间为一组 IPC 和同步设施提供隔离。这些设施提供了一种在线程和进程之间交换数据和同步操作的方式。它们提供诸如信号量、文件锁和互斥锁等原语，是容器中实现真正进程隔离所必需的。'
- en: PID namespaces
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PID 命名空间
- en: The **Process ID** (**PID**) namespaces provide the ability for a process to
    have an ID that already exists in the default namespace, for example an ID of
    `1`. This allows for an init system to run in a container with various other processes,
    without causing a collision with the rest of the PIDs on the same OS.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**进程 ID**（**PID**）命名空间为进程提供了一个 ID，允许该 ID 在默认命名空间中已存在，例如 ID 为 `1`。这使得初始化系统可以在容器中运行，并与其他进程一起运行，而不会与同一操作系统上的其他
    PID 冲突。'
- en: 'To demonstrate this concept, open up `pid_namespace.c`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个概念，打开 `pid_namespace.c` 文件：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, we include the headers and define the `childFunc` function that the
    `clone()` system call will use. The function prints out the child PID using the
    `getpid()` system call:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们包含头文件并定义`childFunc`函数，`clone()`系统调用将使用该函数。该函数通过`getpid()`系统调用打印子进程的 PID：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the `main()` function, we specify the stack size and call `clone()`, passing
    the child function `childFunc`, the stack pointer, the `CLONE_NEWPID` flag, and
    the `SIGCHLD` signal. The `CLONE_NEWPID` flag instructs `clone()` to create a
    new PID namespace and the `SIGCHLD` flag notifies the parent process when one
    of its children terminates. The parent process will block on `waitpid()` if the
    child process has not terminated.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `main()` 函数中，我们指定栈的大小并调用 `clone()`，传递子函数 `childFunc`、栈指针、`CLONE_NEWPID` 标志和
    `SIGCHLD` 信号。`CLONE_NEWPID` 标志指示 `clone()` 创建一个新的 PID 命名空间，`SIGCHLD` 标志通知父进程其子进程何时终止。如果子进程尚未终止，父进程将会阻塞在
    `waitpid()` 调用上。
- en: 'Compile and then run the program with the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并使用以下命令运行程序：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From the output, we can see that the child process has a PID of `1` inside its
    namespace and `17705` otherwise.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到子进程在其命名空间内的 PID 为 `1`，在其他地方为 `17705`。
- en: Note
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that error handling has been omitted from the code examples for brevity.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，代码示例中省略了错误处理以简化内容。
- en: User namespaces
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户命名空间
- en: The user namespaces allow a process inside a namespace to have a different user
    and group ID than that in the default namespace. In the context of LXC, this allows
    for a process to run as `root` inside the container, while having a non-privileged
    ID outside. This adds a thin layer of security, because braking out for the container
    will result in a non-privileged user. This is possible because of kernel 3.8,
    which introduced the ability for non-privileged processes to create user namespaces.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 用户命名空间允许一个命名空间中的进程拥有与默认命名空间中不同的用户和组 ID。在 LXC 的上下文中，这使得一个进程可以在容器内以`root`身份运行，而在外部拥有一个非特权
    ID。这增加了一层薄薄的安全性，因为一旦从容器中逃逸，进程将变为一个非特权用户。这是可能的，因为内核 3.8 引入了非特权进程创建用户命名空间的能力。
- en: 'To create a new user namespace as a non-privileged user and have `root` inside,
    we can use the `unshare` utility. Let''s install the latest version from source:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要在非特权用户下创建一个新的用户命名空间并在其中拥有`root`，我们可以使用`unshare`工具。让我们从源代码安装最新版本：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also use the `clone()` system call with the `CLONE_NEWUSER` flag to
    create a process in a user namespace, as demonstrated by the following program:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用带有 `CLONE_NEWUSER` 标志的 `clone()` 系统调用，在用户命名空间中创建一个进程，如下程序所示：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After compilation and execution, the output looks similar to this when run
    as `root` - UID of `0`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 编译和执行后，作为`root`运行时，输出看起来类似于这个 - UID 为 `0`：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Network namespaces
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络命名空间
- en: Network namespaces provide isolation of the networking resources, such as network
    devices, addresses, routes, and firewall rules. This effectively creates a logical
    copy of the network stack, allowing multiple processes to listen on the same port
    from multiple namespaces. This is the foundation of networking in LXC and there
    are quite a lot of other use cases where this can come in handy.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 网络命名空间提供网络资源的隔离，例如网络设备、地址、路由和防火墙规则。这有效地创建了网络栈的逻辑副本，允许多个进程在多个命名空间中监听同一个端口。这是
    LXC 网络的基础，并且在很多其他用例中也能派上用场。
- en: The `iproute2` package provides very useful userspace tools that we can use
    to experiment with the network namespaces, and is installed by default on almost
    all Linux systems.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`iproute2` 包提供了非常有用的用户空间工具，我们可以用它来实验网络命名空间，并且几乎所有 Linux 系统默认都安装了它。'
- en: 'There''s always the default network namespace, referred to as the root namespace,
    where all network interfaces are initially assigned. To list the network interfaces
    that belong to the default namespace run the following command:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总是存在默认的网络命名空间，称为根命名空间，所有网络接口最初都会分配到该命名空间。要列出属于默认命名空间的网络接口，可以运行以下命令：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this case, there are two interfaces – `lo` and `eth0`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有两个接口——`lo` 和 `eth0`。
- en: 'To list their configuration, we can run the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出它们的配置，我们可以运行以下命令：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Also, to list the routes from the root network namespace, execute the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，要列出根网络命名空间中的路由，可以执行以下命令：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s create two new network namespaces called `ns1` and `ns2` and list them:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建两个新的网络命名空间，分别命名为 `ns1` 和 `ns2`，并列出它们：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that we have the new network namespaces, we can execute commands inside
    them:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了新的网络命名空间，可以在其中执行命令：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The preceding output shows that in the `ns1` namespace, there's only one network
    interface, the loopback - `lo` interface, and it's in a `DOWN` state.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示，在 `ns1` 命名空间中，只有一个网络接口——回环接口 `lo`，且处于 `DOWN` 状态。
- en: 'We can also start a new `bash` session inside the namespace and list the interfaces
    in a similar way:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在命名空间内启动一个新的 `bash` 会话，并以类似的方式列出接口：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is more convenient for running multiple commands than specifying each,
    one at a time. The two network namespaces are not of much use if not connected
    to anything, so let's connect them to each other. To do this we'll use a software
    bridge called Open vSwitch.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这样比逐个指定每个命令要更方便。两个网络命名空间如果没有连接到任何东西，将不会有太多用处，所以让我们将它们连接起来。为此，我们将使用一个名为 Open
    vSwitch 的软件桥接器。
- en: Open vSwitch works just as a regular network bridge and then it forwards frames
    between virtual ports that we define. Virtual machines such as KVM, Xen, and LXC
    or Docker containers can then be connected to it.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch 的工作原理就像一个普通的网络桥接器，然后它会在我们定义的虚拟端口之间转发帧。虚拟机，如 KVM、Xen、LXC 或 Docker
    容器，可以通过它进行连接。
- en: 'Most Debian-based distributions such as Ubuntu provide a package, so let''s
    install that:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于 Debian 的发行版（如 Ubuntu）都提供该软件包，所以让我们安装它：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This installs and starts the Open vSwitch daemon. Time to create the bridge;
    we''ll name it `OVS-1`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装并启动 Open vSwitch 守护进程。现在是时候创建桥接器了，我们将它命名为 `OVS-1`：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you would like to experiment with the latest version of Open vSwitch, you
    can download the source code from [http://openvswitch.org/download/](http://openvswitch.org/download/)
    and compile it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想尝试最新版本的 Open vSwitch，可以从 [http://openvswitch.org/download/](http://openvswitch.org/download/)
    下载源代码并编译。
- en: 'The newly created bridge can now be seen in the root namespace:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的桥接器现在可以在根命名空间中看到：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In order to connect both network namespaces, let''s first create a virtual
    pair of interfaces for each namespace:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了连接这两个网络命名空间，我们首先为每个命名空间创建一对虚拟接口：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding two commands create four virtual interfaces `eth1-ns1`, `eth1-ns2`
    and `veth-ns1`, `veth-ns2`. The names are arbitrary.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个命令创建了四个虚拟接口——`eth1-ns1`、`eth1-ns2` 和 `veth-ns1`、`veth-ns2`。这些名称是任意的。
- en: 'To list all interfaces that are part of the root network namespace, run:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出属于根网络命名空间的所有接口，请运行：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s assign the `eth1-ns1` and `eth1-ns2` interfaces to the `ns1` and `ns2`
    namespaces:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `eth1-ns1` 和 `eth1-ns2` 接口分配到 `ns1` 和 `ns2` 命名空间：
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Also, confirm they are visible from inside each network namespace:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，确认它们是否能从每个网络命名空间内部看到：
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Notice, how each network namespace now has two interfaces assigned – `loopback`
    and `eth1-ns*`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在每个网络命名空间都分配了两个接口——`loopback` 和 `eth1-ns*`。
- en: 'If we list the devices from the root namespace, we should see that the interfaces
    we just moved to `ns1` and `ns2` namespaces are no longer visible:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从根命名空间列出设备，应该会看到我们刚刚移动到 `ns1` 和 `ns2` 命名空间的接口不再可见：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'It''s time to connect the other end of the two virtual pipes, the `veth-ns1`
    and `veth-ns2` interfaces to the bridge:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将两个虚拟管道的另一端——`veth-ns1` 和 `veth-ns2` 接口连接到桥接器了：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: From the preceding output, it's apparent that the bridge now has two ports,
    `veth-ns1` and `veth-ns2`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述输出中可以看出，桥接器现在有两个端口——`veth-ns1` 和 `veth-ns2`。
- en: 'The last thing left to do is bring the network interfaces up and assign IP
    addresses:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要做的就是启用网络接口并分配 IP 地址：
- en: '[PRE33]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Similarly for the `ns2` namespace:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于 `ns2` 命名空间：
- en: '[PRE34]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Network namespaces](img/image_01_001.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![Network namespaces](img/image_01_001.jpg)'
- en: 'With this, we established a connection between both `ns1` and `ns2` network
    namespaces through the Open vSwitch bridge. To confirm, let''s use `ping`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们通过 Open vSwitch 桥接器建立了 `ns1` 和 `ns2` 网络命名空间之间的连接。为了确认，让我们使用 `ping` 命令：
- en: '[PRE35]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Open vSwitch allows for assigning VLAN tags to network interfaces, resulting
    in traffic isolation between namespaces. This can be helpful in a scenario where
    you have multiple namespaces and you want to have connectivity between some of
    them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch 允许为网络接口分配 VLAN 标签，从而实现命名空间之间的流量隔离。这在你有多个命名空间并希望它们之间实现连接时非常有用。
- en: 'The following example demonstrates how to tag the virtual interfaces on the
    `ns1` and `ns2` namespaces, so that the traffic will not be visible from each
    of the two network namespaces:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示如何为 `ns1` 和 `ns2` 命名空间标记虚拟接口，以便这两个网络命名空间之间的流量不可见：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Both the namespaces should now be isolated in their own VLANs and `ping` should
    fail:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，两个命名空间应当被隔离在各自的 VLAN 中，且 `ping` 命令应当失败：
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can also use the `unshare` utility that we saw in the mount and UTC namespaces
    examples to create a new network namespace:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用我们在挂载和 UTC 命名空间示例中看到的 `unshare` 工具来创建一个新的网络命名空间：
- en: '[PRE38]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Resource management with cgroups
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 cgroups 进行资源管理
- en: Cgroups are kernel features that allows fine-grained control over resource allocation
    for a single process, or a group of processes, called **tasks**. In the context
    of LXC this is quite important, because it makes it possible to assign limits
    to how much memory, CPU time, or I/O, any given container can use.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Cgroups 是内核特性，允许对单个进程或一组进程（称为 **任务**）的资源分配进行细粒度控制。在 LXC 环境中，这非常重要，因为它使得为任何给定容器分配内存、CPU
    时间或 I/O 限制成为可能。
- en: 'The cgroups we are most interested in are described in the following table:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最关心的 cgroups 在下表中描述：
- en: '| **Subsystem** | **Description** | **Defined in** |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| **子系统** | **描述** | **定义在** |'
- en: '| `cpu` | Allocates CPU time for tasks | `kernel/sched/core.c` |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `cpu` | 为任务分配 CPU 时间 | `kernel/sched/core.c` |'
- en: '| `cpuacct` | Accounts for CPU usage | `kernel/sched/core.c` |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `cpuacct` | 记录 CPU 使用情况 | `kernel/sched/core.c` |'
- en: '| `cpuset` | Assigns CPU cores to tasks | `kernel/cpuset.c` |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset` | 为任务分配 CPU 核心 | `kernel/cpuset.c` |'
- en: '| `memory` | Allocates memory for tasks | `mm/memcontrol.c` |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| `memory` | 为任务分配内存 | `mm/memcontrol.c` |'
- en: '| `blkio` | Limits the I/O access to devices | `block/blk-cgroup.c` |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `blkio` | 限制设备的 I/O 访问 | `block/blk-cgroup.c` |'
- en: '| `devices` | Allows/denies access to devices | `security/device_cgroup.c`
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `devices` | 允许/拒绝对设备的访问 | `security/device_cgroup.c` |'
- en: '| `freezer` | Suspends/resumes tasks | `kernel/cgroup_freezer.c` |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `freezer` | 挂起/恢复任务 | `kernel/cgroup_freezer.c` |'
- en: '| `net_cls` | Tags network packets | `net/sched/cls_cgroup.c` |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| `net_cls` | 标记网络数据包 | `net/sched/cls_cgroup.c` |'
- en: '| `net_prio` | Prioritizes network traffic | `net/core/netprio_cgroup.c` |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `net_prio` | 优先级网络流量 | `net/core/netprio_cgroup.c` |'
- en: '| `hugetlb` | Limits the HugeTLB | `mm/hugetlb_cgroup.c` |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `hugetlb` | 限制 HugeTLB | `mm/hugetlb_cgroup.c` |'
- en: Cgroups are organized in hierarchies, represented as directories in a **Virtual
    File System** (**VFS**). Similar to process hierarchies, where every process is
    a descendent of the `init` or `systemd` process, cgroups inherit some of the properties
    of their parents. Multiple cgroups hierarchies can exist on the system, each one
    representing a single or group of resources. It is possible to have hierarchies
    that combine two or more subsystems, for example, memory and I/O, and tasks assigned
    to a group will have limits applied on those resources.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Cgroups 以层次结构的形式组织，表示为**虚拟文件系统**（**VFS**）中的目录。类似于进程层次结构，每个进程都是 `init` 或 `systemd`
    进程的后代，cgroups 会继承其父级的一些属性。系统上可以存在多个 cgroups 层次结构，每个层次表示单个或一组资源。可以拥有组合两个或更多子系统的层次结构，例如，内存和
    I/O，分配给某个组的任务将对这些资源应用限制。
- en: Note
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you are interested in how the different subsystems are implemented in the
    kernel, install the kernel source and have a look at the C files, shown in the
    third column of the table.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对内核中不同子系统的实现感兴趣，可以安装内核源代码并查看表格第三列中显示的 C 文件。
- en: 'The following diagram helps visualize a single hierarchy that has two subsystems—CPU
    and I/O—attached to it:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示有助于可视化一个包含两个子系统——CPU 和 I/O——的单一层次结构：
- en: '![Resource management with cgroups](img/image_01_002.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用 cgroups 进行资源管理](img/image_01_002.jpg)'
- en: 'Cgroups can be used in two ways:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Cgroups 可以通过两种方式使用：
- en: By manually manipulating files and directories on a mounted VFS
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过手动操作挂载的 VFS 上的文件和目录
- en: Using userspace tools provided by various packages such as `cgroup-bin` on Debian/Ubuntu
    and `libcgroup` on RHEL/CentOS
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种软件包提供的用户空间工具，如 Debian/Ubuntu 上的 `cgroup-bin` 和 RHEL/CentOS 上的 `libcgroup`
- en: Let's have a look at few practical examples on how to use cgroups to limit resources.
    This will help us get a better understanding of how containers work.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几个实际的例子，了解如何使用 cgroups 来限制资源。这将帮助我们更好地理解容器是如何工作的。
- en: Limiting I/O throughput
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制 I/O 吞吐量
- en: Let's assume we have two applications running on a server that are heavily I/O
    bound: `app1` and `app2`. We would like to give more bandwidth to `app1` during
    the day and to `app2` during the night. This type of I/O throughput prioritization
    can be accomplished using the `blkio` subsystem.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个应用程序在一台服务器上运行，且它们是高度 I/O 绑定的：`app1` 和 `app2`。我们希望白天将更多的带宽分配给 `app1`，而晚上则分配给
    `app2`。这种 I/O 吞吐量优先级调整可以通过使用 `blkio` 子系统来实现。
- en: 'First, let''s attach the `blkio` subsystem by mounting the `cgroup` VFS:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们通过挂载 `cgroup` VFS 来附加 `blkio` 子系统：
- en: '[PRE39]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, create two priority groups, which will be part of the same `blkio` hierarchy:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建两个优先级组，这些组将属于同一个 `blkio` 层级：
- en: '[PRE40]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We need to acquire the PIDs of the `app1` and `app2` processes and assign them
    to the `high_io` and `low_io` groups:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要获取 `app1` 和 `app2` 进程的 PID，并将它们分配到 `high_io` 和 `low_io` 组：
- en: '[PRE41]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Limiting I/O throughput](img/image_01_003.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![限制 I/O 吞吐量](img/image_01_003.jpg)'
- en: The blkio hierarchy we've created
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的 blkio 层级
- en: The `tasks` file is where we define what processes/tasks the limit should be
    applied on.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`tasks` 文件是我们定义应应用限制的进程/任务的地方。'
- en: 'Finally, let''s set a ratio of 10:1 for the `high_io` and `low_io` cgroups.
    Tasks in those cgroups will immediately use only the resources made available
    to them:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们为 `high_io` 和 `low_io` cgroups 设置 10:1 的比例。这些 cgroups 中的任务将立即仅使用分配给它们的资源：
- en: '[PRE42]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `blkio.weight` file defines the weight of I/O access available to a process
    or group of processes, with values ranging from 100 to 1,000\. In this example,
    the values of `1000` and `100` create a ratio of 10:1.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`blkio.weight` 文件定义了进程或进程组可用的 I/O 访问权重，值的范围从 100 到 1000。在这个例子中，`1000` 和 `100`
    的值创建了 10:1 的比例。'
- en: With this, the low priority application, `app2` will use only about 10 percent
    of the I/O operations available, whereas the high priority application, `app1`,
    will use about 90 percent.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样设置，低优先级的应用程序 `app2` 将仅使用大约 10% 的 I/O 操作，而高优先级的应用程序 `app1` 将使用大约 90%。
- en: 'If you list the contents of the `high_io` directory on Ubuntu you will see
    the following files:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 Ubuntu 上列出 `high_io` 目录的内容，你将看到以下文件：
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: From the preceding output you can see that only some files are writeable. This
    depends on various OS settings, such as what I/O scheduler is being used.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出可以看到，只有某些文件是可写的。这取决于各种操作系统设置，例如正在使用的 I/O 调度器。
- en: 'We''ve already seen what the `tasks` and `blkio.weight` files are used for.
    The following is a short description of the most commonly used files in the `blkio`
    subsystem:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到 `tasks` 和 `blkio.weight` 文件的用途。以下是 `blkio` 子系统中最常用文件的简短描述：
- en: '| **File** | **Description** |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **文件** | **描述** |'
- en: '| `blkio.io_merged` | Total number of reads/writes, sync, or async merged into
    requests |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.io_merged` | 总共合并到请求中的读/写、同步或异步操作数 |'
- en: '| `blkio.io_queued` | Total number of read/write, sync, or async requests queued
    up at any given time |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.io_queued` | 在任何给定时间排队的读/写、同步或异步请求的总数 |'
- en: '| `blkio.io_service_bytes` | The number of bytes transferred to or from the
    specified device |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.io_service_bytes` | 传输到或从指定设备的字节数 |'
- en: '| `blkio.io_serviced` | The number of I/O operations issued to the specified
    device |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.io_serviced` | 发往指定设备的 I/O 操作数量 |'
- en: '| `blkio.io_service_time` | Total amount of time between request dispatch and
    request completion in nanoseconds for the specified device |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.io_service_time` | 请求调度与请求完成之间的总时间（单位：纳秒），针对指定设备 |'
- en: '| `blkio.io_wait_time` | Total amount of time the I/O operations spent waiting
    in the scheduler queues for the specified device |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.io_wait_time` | 指定设备的 I/O 操作在调度器队列中等待的总时间 |'
- en: '| `blkio.leaf_weight` | Similar to `blkio.weight` and can be applied to the
    **Completely Fair Queuing** (**CFQ**) I/O scheduler |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.leaf_weight` | 类似于 `blkio.weight`，可以应用于 **完全公平队列** (**CFQ**) I/O 调度器
    |'
- en: '| `blkio.reset_stats` | Writing an integer to this file will reset all statistics
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.reset_stats` | 向此文件写入一个整数将重置所有统计信息 |'
- en: '| `blkio.sectors` | The number of sectors transferred to or from the specified
    device |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.sectors` | 传输到或从指定设备的扇区数 |'
- en: '| `blkio.throttle.io_service_bytes` | The number of bytes transferred to or
    from the disk |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.throttle.io_service_bytes` | 传输到或从磁盘的字节数 |'
- en: '| `blkio.throttle.io_serviced` | The number of I/O operations issued to the
    specified disk |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.throttle.io_serviced` | 向指定磁盘发出的I/O操作次数 |'
- en: '| `blkio.time` | The disk time allocated to a device in milliseconds |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.time` | 分配给设备的磁盘时间（以毫秒为单位） |'
- en: '| `blkio.weight` | Specifies weight for a cgroup hierarchy |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.weight` | 为cgroup层级指定权重 |'
- en: '| `blkio.weight_device` | Same as `blkio.weight`, but specifies a block device
    to apply the limit on |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `blkio.weight_device` | 与`blkio.weight`相同，但指定一个块设备来应用限制 |'
- en: '| `tasks` | Attach tasks to the cgroup |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| `tasks` | 将任务附加到cgroup |'
- en: Tip
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: One thing to keep in mind is that writing to the files directly to make changes
    will not persist after the server restarts. Later in this chapter, you will learn
    how to use the userspace tools to generate persistent configuration files.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，直接写入这些文件进行更改不会在服务器重启后保持不变。在本章的后面，您将学习如何使用用户空间工具生成持久化配置文件。
- en: Limiting memory usage
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制内存使用
- en: The `memory` subsystem controls how much memory is presented to and available
    for use by processes. This can be particularly useful in multitenant environments
    where better control over how much memory a user process can utilize is needed,
    or to limit memory hungry applications. Containerized solutions like LXC can use
    the `memory` subsystem to manage the size of the instances, without needing to
    restart the entire container.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory`子系统控制分配给进程的内存量以及进程可用的内存。这在多租户环境中尤其有用，因为在这种环境下，需要更好的控制每个用户进程可以使用多少内存，或者限制内存消耗较大的应用程序。像LXC这样的容器化解决方案可以使用`memory`子系统来管理实例的大小，而无需重启整个容器。'
- en: 'The `memory` subsystem performs resource accounting, such as tracking the utilization
    of anonymous pages, file caches, swap caches, and general hierarchical accounting,
    all of which presents an overhead. Because of this, the `memory` cgroup is disabled
    by default on some Linux distributions. If the following commands below fail you''ll
    need to enable it, by specifying the following GRUB parameter and restarting:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory`子系统执行资源会计，例如跟踪匿名页面、文件缓存、交换缓存和一般层级会计的利用率，这些都带来了一定的开销。因此，某些Linux发行版默认禁用`memory`
    cgroup。如果下面的命令失败，您需要通过指定以下GRUB参数并重启来启用它：'
- en: '[PRE44]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'First, let''s mount the `memory` cgroup:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们挂载`memory` cgroup：
- en: '[PRE45]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then set the `app1` memory to 1 GB:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将`app1`的内存设置为1GB：
- en: '[PRE46]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![Limiting memory usage](img/image_01_004.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![限制内存使用](img/image_01_004.jpg)'
- en: The memory hierarchy for the app1 process
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: app1进程的内存层级
- en: Similar to the `blkio` subsystem, the `tasks` file is used to specify the PID
    of the processes we are adding to the cgroup hierarchy, and the `memory.limit_in_bytes`
    specifies how much memory is to be made available in bytes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`blkio`子系统，`tasks`文件用于指定我们要添加到cgroup层级中的进程的PID，`memory.limit_in_bytes`指定要分配的内存大小（以字节为单位）。
- en: 'The `app1` memory hierarchy contains the following files:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`app1`的内存层级包含以下文件：'
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The files and their function in the memory subsystem are described in the following
    table:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 内存子系统中的文件及其功能如下表所示：
- en: '| **File** | **Description** |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| **文件** | **描述** |'
- en: '| `memory.failcnt` | Shows the total number of memory limit hits |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `memory.failcnt` | 显示内存限制命中的总次数 |'
- en: '| `memory.force_empty` | If set to `0`, frees memory used by tasks |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `memory.force_empty` | 如果设置为`0`，则释放任务占用的内存 |'
- en: '| `memory.kmem.failcnt` | Shows the total number of kernel memory limit hits
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.failcnt` | 显示内核内存限制命中的总次数 |'
- en: '| `memory.kmem.limit_in_bytes` | Sets or shows kernel memory hard limit |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.limit_in_bytes` | 设置或显示内核内存硬限制 |'
- en: '| `memory.kmem.max_usage_in_bytes` | Shows maximum kernel memory usage |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.max_usage_in_bytes` | 显示最大内核内存使用量 |'
- en: '| `memory.kmem.tcp.failcnt` | Shows the number of TCP buffer memory limit hits
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.tcp.failcnt` | 显示TCP缓冲区内存限制命中的次数 |'
- en: '| `memory.kmem.tcp.limit_in_bytes` | Sets or shows hard limit for TCP buffer
    memory |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.tcp.limit_in_bytes` | 设置或显示TCP缓冲区内存的硬限制 |'
- en: '| `memory.kmem.tcp.max_usage_in_bytes` | Shows maximum TCP buffer memory usage
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.tcp.max_usage_in_bytes` | 显示最大TCP缓冲区内存使用量 |'
- en: '| `memory.kmem.tcp.usage_in_bytes` | Shows current TCP buffer memory |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.tcp.usage_in_bytes` | 显示当前TCP缓冲区内存 |'
- en: '| `memory.kmem.usage_in_bytes` | Shows current kernel memory |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `memory.kmem.usage_in_bytes` | 显示当前的内核内存 |'
- en: '| `memory.limit_in_bytes` | Sets or shows memory usage limit |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `memory.limit_in_bytes` | 设置或显示内存使用限制 |'
- en: '| `memory.max_usage_in_bytes` | Shows maximum memory usage |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `memory.max_usage_in_bytes` | 显示最大内存使用量 |'
- en: '| `memory.move_charge_at_immigrate` | Sets or shows controls of moving charges
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `memory.move_charge_at_immigrate` | 设置或显示移动费用的控制 |'
- en: '| `memory.numa_stat` | Shows the number of memory usage per NUMA node |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `memory.numa_stat` | 显示每个 NUMA 节点的内存使用情况 |'
- en: '| `memory.oom_control` | Sets or shows the OOM controls |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `memory.oom_control` | 设置或显示 OOM 控制 |'
- en: '| `memory.pressure_level` | Sets memory pressure notifications |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `memory.pressure_level` | 设置内存压力通知 |'
- en: '| `memory.soft_limit_in_bytes` | Sets or shows soft limit of memory usage |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `memory.soft_limit_in_bytes` | 设置或显示内存使用的软限制 |'
- en: '| `memory.stat` | Shows various statistics |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `memory.stat` | 显示各种统计信息 |'
- en: '| `memory.swappiness` | Sets or shows swappiness level |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `memory.swappiness` | 设置或显示交换的级别 |'
- en: '| `memory.usage_in_bytes` | Shows current memory usage |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `memory.usage_in_bytes` | 显示当前内存使用情况 |'
- en: '| `memory.use_hierarchy` | Sets memory reclamation from child processes |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `memory.use_hierarchy` | 设置从子进程回收内存 |'
- en: '| `tasks` | Attaches tasks to the cgroup |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `tasks` | 将任务附加到 cgroup 中 |'
- en: 'Limiting the memory available to a process might trigger the **Out of Memory**
    (**OOM**) killer, which might kill the running task. If this is not the desired
    behavior and you prefer the process to be suspended waiting for memory to be freed,
    the OOM killer can be disabled:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 限制进程可用的内存可能会触发 **内存不足**（**OOM**）杀手，进而杀死正在运行的任务。如果这不是期望的行为，并且你希望进程被挂起，等待内存释放，则可以禁用
    OOM 杀手：
- en: '[PRE48]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `memory` cgroup presents a wide slew of accounting statistics in the `memory.stat`
    file, which can be of interest:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory` cgroup 在 `memory.stat` 文件中提供了广泛的记账统计信息，这些信息可能会引起关注：'
- en: '[PRE49]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'If you need to start a new task in the `app1` memory hierarchy you can move
    the current shell process into the `tasks` file, and all other processes started
    in this shell will be direct descendants and inherit the same cgroup properties:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要在`app1`内存层级中启动一个新任务，你可以将当前的 shell 进程移动到`tasks`文件中，所有在这个 shell 中启动的其他进程将成为其直接后代，并继承相同的
    cgroup 属性：
- en: '[PRE50]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The cpu and cpuset subsystems
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cpu 和 cpuset 子系统
- en: The `cpu` subsystem schedules CPU time to cgroup hierarchies and their tasks.
    It provides finer control over CPU execution time than the default behavior of
    the CFS.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpu` 子系统将 CPU 时间调度到 cgroup 层级及其任务中。与 CFS 的默认行为相比，它提供了对 CPU 执行时间的更细粒度控制。'
- en: The `cpuset` subsystem allows for assigning CPU cores to a set of tasks, similar
    to the `taskset` command in Linux.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpuset` 子系统允许将 CPU 核心分配给一组任务，类似于 Linux 中的 `taskset` 命令。'
- en: The main benefits that the `cpu` and `cpuset` subsystems provide are better
    utilization per processor core for highly CPU bound applications. They also allow
    for distributing load between cores that are otherwise idle at certain times of
    the day. In the context of multitenant environments, running many LXC containers,
    `cpu` and `cpuset` cgroups allow for creating different instance sizes and container
    flavors, for example exposing only a single core per container, with 40 percent
    scheduled work time.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpu` 和 `cpuset` 子系统提供的主要好处是为高度依赖 CPU 的应用提供更好的每个处理器核心利用率。它们还允许在某些时间段内将负载分配给通常空闲的核心。在多租户环境中，运行许多
    LXC 容器时，`cpu` 和 `cpuset` cgroup 允许创建不同的实例大小和容器类型，例如每个容器仅暴露一个核心，并分配 40% 的调度工作时间。'
- en: 'As an example, let''s assume we have two processes `app1` and `app2`, and we
    would like `app1` to use 60 percent of the CPU time and `app2` only 40 percent.
    We start by mounting the `cgroup` VFS:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，假设我们有两个进程 `app1` 和 `app2`，我们希望 `app1` 使用 60% 的 CPU 时间，`app2` 仅使用 40%。我们首先挂载
    `cgroup` VFS：
- en: '[PRE51]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then we create two child hierarchies:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建两个子层级：
- en: '[PRE52]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Also assign CPU shares for each, where `app1` will get 60 percent and `app2`
    will get 40 percent of the scheduled time:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 同时为每个任务分配 CPU 份额，其中 `app1` 将获得 60% 的调度时间，`app2` 将获得 40%：
- en: '[PRE53]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Finally, we move the PIDs in the `tasks` files:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将 PID 移动到`tasks`文件中：
- en: '[PRE54]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `cpu` subsystem contains the following control files:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpu` 子系统包含以下控制文件：'
- en: '[PRE55]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here''s a brief explanation of each:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个项的简要说明：
- en: '| **File** | **Description** |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| **文件** | **描述** |'
- en: '| `cpu.cfs_period_us` | CPU resource reallocation in microseconds |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| `cpu.cfs_period_us` | 以微秒为单位的 CPU 资源重新分配 |'
- en: '| `cpu.cfs_quota_us` | Run duration of tasks in microseconds during one `cpu.cfs_perious_us
    period` |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| `cpu.cfs_quota_us` | 任务在一个`cpu.cfs_period_us`周期内的运行时长，以微秒为单位 |'
- en: '| `cpu.shares` | Relative share of CPU time available to the tasks |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| `cpu.shares` | 任务可用的 CPU 时间的相对份额 |'
- en: '| `cpu.stat` | Shows CPU time statistics |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| `cpu.stat` | 显示 CPU 时间统计信息 |'
- en: '| `tasks` | Attaches tasks to the cgroup |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| `tasks` | 将任务附加到 cgroup 中 |'
- en: 'The `cpu.stat` file is of particular interest:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpu.stat` 文件特别重要：'
- en: '[PRE56]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To demonstrate how the `cpuset` subsystem works, let''s create `cpuset` hierarchies
    named `app1`, containing CPUs 0 and 1\. The `app2` cgroup will contain only CPU
    1:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示`cpuset`子系统的工作方式，假设我们创建一个名为`app1`的`cpuset`层次结构，其中包含CPU 0和1，`app2` cgroup将只包含CPU
    1：
- en: '[PRE57]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To check if the `app1` process is pinned to CPU 0 and 1, we can use:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查`app1`进程是否绑定到CPU 0和1，我们可以使用：
- en: '[PRE58]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The `cpuset app1` hierarchy contains the following files:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpuset app1`层次结构包含以下文件：'
- en: '[PRE59]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'A brief description of the control files is as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 控制文件的简要描述如下：
- en: '| **File** | **Description** |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| **文件** | **描述** |'
- en: '| `cpuset.cpu_exclusive` | Checks if other `cpuset` hierarchies share the settings
    defined in the current group |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.cpu_exclusive` | 检查是否其他`cpuset`层次结构共享当前组中定义的设置 |'
- en: '| `cpuset.cpus` | List of the physical numbers of the CPUs on which processes
    in that `cpuset` are allowed to execute |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.cpus` | 允许在该`cpuset`中的进程执行的CPU物理编号列表 |'
- en: '| `cpuset.mem_exclusive` | Should the `cpuset` have exclusive use of its memory
    nodes |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.mem_exclusive` | `cpuset`是否应独占其内存节点 |'
- en: '| `cpuset.mem_hardwall` | Checks if each tasks'' user allocation be kept separate
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.mem_hardwall` | 检查每个任务的用户分配是否保持独立 |'
- en: '| `cpuset.memory_migrate` | Checks if a page in memory should migrate to a
    new node if the values in `cpuset.mems` change |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.memory_migrate` | 检查当`cpuset.mems`中的值发生变化时，内存中的页是否应迁移到新的节点 |'
- en: '| `cpuset.memory_pressure` | Contains running average of the memory pressure
    created by the processes |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.memory_pressure` | 包含由进程创建的内存压力的运行平均值 |'
- en: '| `cpuset.memory_spread_page` | Checks if filesystem buffers should spread
    evenly across the memory nodes |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.memory_spread_page` | 检查文件系统缓冲区是否应均匀分布在内存节点之间 |'
- en: '| `cpuset.memory_spread_slab` | Checks if kernel slab caches for file I/O operations
    should spread evenly across the `cpuset` |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.memory_spread_slab` | 检查内核的文件I/O操作的slab缓存是否应均匀分布在`cpuset`中 |'
- en: '| `cpuset.mems` | Specifies the memory nodes that tasks in this cgroup are
    permitted to access |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.mems` | 指定此cgroup中的任务可以访问的内存节点 |'
- en: '| `cpuset.sched_load_balance` | Checks if the kernel balance should load across
    the CPUs in the `cpuset` by moving processes from overloaded CPUs to less utilized
    CPUs |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.sched_load_balance` | 检查内核是否应通过将进程从过载的CPU迁移到较少使用的CPU来平衡`cpuset`中的CPU负载
    |'
- en: '| `cpuset.sched_relax_domain_level` | Contains the width of the range of CPUs
    across which the kernel should attempt to balance loads |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| `cpuset.sched_relax_domain_level` | 包含内核应尝试平衡负载的CPU范围的宽度 |'
- en: '| `notify_on_release` | Checks if the hierarchy should receive special handling
    after it is released and no process are using it |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| `notify_on_release` | 检查释放后层次结构是否应接收特殊处理，且没有进程在使用它 |'
- en: '| `tasks` | Attaches tasks to the cgroup |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| `tasks` | 将任务附加到cgroup |'
- en: The cgroup freezer subsystem
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cgroup冻结子系统
- en: The `freezer` subsystem can be used to suspend the current state of running
    tasks for the purposes of analyzing them, or to create a checkpoint that can be
    used to migrate the process to a different server. Another use case is when a
    process is negatively impacting the system and needs to be temporarily paused,
    without losing its current state data.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`freezer`子系统可用于挂起当前运行任务的状态，以便对其进行分析，或创建一个可以用于将进程迁移到其他服务器的检查点。另一个使用场景是当某个进程对系统产生负面影响时，需要暂时暂停该进程，同时不丢失当前的状态数据。'
- en: The next example shows how to suspend the execution of the top process, check
    its state, and then resume it.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例展示了如何挂起`top`进程的执行，检查其状态，然后恢复它。
- en: 'First, mount the `freezer` subsystem and create the new hierarchy:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，挂载`freezer`子系统并创建新的层次结构：
- en: '[PRE60]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In a new terminal, start the `top` process and observe how it periodically
    refreshes. Back in the original terminal, add the PID of `top` to the `frozen_group`
    task file and observe its state:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个新的终端中，启动`top`进程并观察它如何定期刷新。回到原始终端，将`top`的PID添加到`frozen_group`任务文件中，并观察其状态：
- en: '[PRE61]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'To freeze the process, echo the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 要冻结进程，请执行以下命令：
- en: '[PRE62]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Notice how the top process output is not refreshing anymore, and upon inspection
    of its status file, you can see that it is now in the blocked state.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`top`进程的输出不再刷新，并且在检查其状态文件时，您可以看到它现在处于阻塞状态。
- en: 'To resume it, execute the following:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 要恢复执行，执行以下命令：
- en: '[PRE63]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Inspecting the `frozen_group` hierarchy yields the following files:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`frozen_group`层次结构会得到以下文件：
- en: '[PRE64]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The few files of interest are described in the following table:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格描述了几个感兴趣的文件：
- en: '| **File** | **Description** |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| **文件** | **描述** |'
- en: '| `freezer.parent_freezing` | Shows the parent-state. Shows `0` if none of
    the cgroup''s ancestors is `FROZEN`; otherwise, `1`. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| `freezer.parent_freezing` | 显示父状态。如果cgroup的祖先中没有一个是`FROZEN`则显示`0`；否则显示`1`。
    |'
- en: '| `freezer.self_freezing` | Shows the self-state. Shows `0` if the self-state
    is `THAWED`; otherwise, `1`. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| `freezer.self_freezing` | 显示自身状态。如果自身状态为`THAWED`则显示`0`；否则显示`1`。 |'
- en: '| `freezer.state` | Sets the self-state of the cgroup to either `THAWED` or
    `FROZEN`. |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| `freezer.state` | 将cgroup的自身状态设置为`THAWED`或`FROZEN`。 |'
- en: '| `tasks` | Attaches tasks to the cgroup. |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| `tasks` | 将任务附加到cgroup。 |'
- en: Using userspace tools to manage cgroups and persist changes
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用用户空间工具管理cgroups并持久化更改
- en: Working with the cgroups subsystems by manipulating directories and files directly
    is a fast and convenient way to prototype and test changes, however, this comes
    with few drawbacks, namely the changes made will not persist a server restart
    and there's not much error reporting or handling.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通过直接操作目录和文件来处理cgroup子系统是原型设计和测试更改的快速便捷方式，但这也有一些缺点，即所做的更改不会在服务器重启后持续，且错误报告或处理不多。
- en: To address this, there are packages that provide userspace  tools and daemons
    that are quite easy to use. Let's see a few examples.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，有一些提供用户空间工具和守护程序的软件包非常易于使用。让我们看几个例子。
- en: 'To install the tools on Debian/Ubuntu, run the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Debian/Ubuntu上安装工具，请运行以下命令：
- en: '[PRE65]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'On RHEL/CentOS, execute the following:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在RHEL/CentOS上，执行以下操作：
- en: '[PRE66]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'To mount all subsystems, run the following:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 要挂载所有子系统，请运行以下命令：
- en: '[PRE67]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Notice from the preceding output the location of the cgroups - `/sys/fs/cgroup`.
    This is the default location on many Linux distributions and in most cases the
    various subsystems have already been mounted.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 注意从前面的输出中cgroups的位置 - `/sys/fs/cgroup`。这是许多Linux发行版上的默认位置，在大多数情况下各个子系统已经被挂载。
- en: 'To verify what cgroup subsystems are in use, we can check with the following
    commands:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证正在使用的cgroup子系统，可以使用以下命令检查：
- en: '[PRE68]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Next, let''s create a `blkio` hierarchy and add an already running process
    to it with `cgclassify`. This is similar to what we did earlier, by creating the
    directories and the files by hand:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个`blkio`层级，并使用`cgclassify`将一个已运行的进程添加到其中。这与之前手动创建目录和文件的操作类似：
- en: '[PRE69]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now that we have defined the `high_io` and `low_io` cgroups and added a process
    to them, let''s generate a configuration file that can be used later to reapply
    the setup:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经定义了`high_io`和`low_io` cgroups，并向它们添加了一个进程，让我们生成一个配置文件，以便稍后重新应用这个设置：
- en: '[PRE70]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'To start a new process in the `high_io` group, we can use the `cgexec` command:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要在`high_io`组中启动一个新进程，我们可以使用`cgexec`命令：
- en: '[PRE71]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: In the preceding example, we started a new `bash` process in the `high_io cgroup`,
    as confirmed by looking at the `tasks` file.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们在`high_io cgroup`中启动了一个新的`bash`进程，通过查看`tasks`文件确认。
- en: 'To move an already running process to the `memory` subsystem, first we create
    the `high_prio` and `low_prio` groups and move the task with `cgclassify`:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 要将一个已运行的进程移动到`memory`子系统中，首先我们创建`high_prio`和`low_prio`组，并使用`cgclassify`移动任务：
- en: '[PRE72]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'To set the memory and CPU limits, we can use the `cgset` command. In contrast,
    remember that we used the `echo` command to manually move the PIDs and memory
    limits to the `tasks` and the `memory.limit_in_bytes` files:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置内存和CPU限制，可以使用`cgset`命令。与此相反，记住我们使用`echo`命令手动将PID和内存限制移动到`tasks`和`memory.limit_in_bytes`文件中：
- en: '[PRE73]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'To see how the cgroup hierarchies look, we can use the `lscgroup` utility:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看cgroup层级的外观，可以使用`lscgroup`实用工具：
- en: '[PRE74]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The preceding output confirms the existence of the `blkio`, `memory`, and `cpu`
    hierarchies and their children.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出确认了存在`blkio`、`memory`和`cpu`层级及其子层级。
- en: 'Once finished, you can delete the hierarchies with `cgdelete`, which deletes
    the respective directories on the VFS:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，可以使用`cgdelete`删除层级结构，这将删除VFS上相应的目录：
- en: '[PRE75]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'To completely clear the cgroups, we can use the `cgclear` utility, which will
    unmount the cgroup directories:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全清除cgroups，可以使用`cgclear`实用工具，它将卸载cgroup目录：
- en: '[PRE76]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Managing resources with systemd
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用systemd管理资源
- en: With the increased adoption of `systemd` as an init system, new ways of manipulating
    cgroups were introduced. For example, if the cpu controller is enabled in the
    kernel, `systemd` will create a cgroup for each service by default. This behavior
    can be changed by adding or removing cgroup subsystems in the configuration file
    of `systemd`, usually found at `/etc/systemd/system.conf`.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 `systemd` 作为初始化系统的广泛采用，引入了新的方法来操作 cgroups。例如，如果内核启用了 CPU 控制器，`systemd` 会默认为每个服务创建一个
    cgroup。通过在 `systemd` 配置文件中添加或移除 cgroup 子系统，可以更改这种行为，配置文件通常位于 `/etc/systemd/system.conf`。
- en: If multiple services are running on the server, the CPU resources will be shared
    equally among them by default, because `systemd` assigns equal weights to each.
    To change this behavior for an application, we can edit its service file and define
    the CPU shares, allocated memory, and I/O.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务器上运行着多个服务，默认情况下，CPU 资源会被平均分配给它们，因为 `systemd` 对每个服务分配相等的权重。要改变这种行为，我们可以编辑其服务文件，定义
    CPU 配额、分配的内存和 I/O。
- en: 'The following example demonstrates how to change the CPU shares, memory, and
    I/O limits for the nginx process:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何为 nginx 进程更改 CPU 配额、内存和 I/O 限制：
- en: '[PRE77]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'To apply the changes first reload `systemd`, then nginx:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这些更改，首先重新加载 `systemd`，然后重新启动 nginx：
- en: '[PRE78]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: This will create and update the necessary control files in `/sys/fs/cgroup/systemd`
    and apply the limits.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建并更新 `/sys/fs/cgroup/systemd` 中所需的控制文件，并应用限制。
- en: Summary
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The advent of kernel namespaces and cgroups made it possible to isolate groups
    of processes in a self-confined lightweight virtualization package; we call them
    containers. In this chapter, we saw how containers provide the same features as
    other full-fledged hypervisor-based virtualization technologies such as KVM and
    Xen, without the overhead of running multiple kernels in the same operating system.
    LXC takes full advantage of Linux cgroups and namespaces to achieve this level
    of isolation and resource control.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 内核命名空间和 cgroups 的出现使得将进程组隔离到一个自包含的轻量级虚拟化包中成为可能；我们称之为容器。在本章中，我们看到容器提供了与其他基于完整虚拟化技术（如
    KVM 和 Xen）相同的功能，而无需在同一操作系统中运行多个内核。LXC 充分利用了 Linux 的 cgroups 和命名空间，达到了这种隔离和资源控制的水平。
- en: With the foundation gained from this chapter, you'll be able to understand better
    what's going on under the hood, which will make it much easier to troubleshoot
    and support the full life cycle of Linux containers, as we'll do in the next chapters.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章所获得的基础知识，你将能更好地理解底层的运作原理，这将使得排查故障和支持 Linux 容器的整个生命周期变得更加容易，正如我们在接下来的章节中所做的那样。
