- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Containerize on Google Cloud – Building Solutions with GKE
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上容器化 – 使用 GKE 构建解决方案
- en: In the previous chapter, we built and automated our solution on Google Cloud
    by utilizing **Google Compute Engine** (**GCE**). We built **virtual machine**
    (**VM**) images with Packer and provisioned our VM using Terraform. In this chapter,
    we’ll follow a similar path, but instead of working with VMs, we’ll look at hosting
    our application in containers within a Kubernetes cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过利用 **Google Compute Engine**（**GCE**）在 Google Cloud 上构建并自动化了解决方案。我们使用
    Packer 构建了 **虚拟机**（**VM**）镜像，并通过 Terraform 配置了我们的虚拟机。在本章中，我们将采取类似的路径，但我们不再使用虚拟机，而是将重点放在在
    Kubernetes 集群中托管我们的应用程序。
- en: To achieve this, we’ll need to alter our approach by ditching Packer and replacing
    it with Docker to create a deployable artifact for our application. Once again,
    we’ll be using the `google` provider for Terraform and revisiting the `kubernetes`
    provider for Terraform that we looked at when we took the same step while on our
    journey with AWS and Azure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们需要通过放弃 Packer 并用 Docker 替代它来创建可部署的应用程序工件。我们将再次使用 Terraform 的`google`提供程序，并重新审视我们在使用
    AWS 和 Azure 进行相同操作时所用的 Terraform `kubernetes` 提供程序。
- en: Since the overwhelming majority of this remains the same when we move to Google
    Cloud, we won’t revisit these topics at the same length in this chapter. However,
    I would encourage you to bookmark [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402)
    and refer to it frequently.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 由于当我们迁移到 Google Cloud 时，大部分内容保持不变，因此我们在本章中不会对这些话题进行重复深入的讨论。然而，我建议你收藏 [*第 8 章*](B21183_08.xhtml#_idTextAnchor402)
    并经常参考。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Laying the foundation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打下基础
- en: Designing the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: Building the solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: Automating the deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化部署
- en: Laying the foundation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打下基础
- en: Our story continues through the lens of Söze Enterprises, founded by the enigmatic
    Turkish billionaire Keyser Söze. Our team has been hard at work building the next-generation
    autonomous vehicle orchestration platform. Previously, we had hoped to leapfrog
    the competition by leveraging Google Cloud’s rock-solid platform, leveraging our
    team’s existing skills, and focusing on feature development. The team was just
    getting into their groove when a curveball came out of nowhere.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的故事通过 Söze Enterprises 的视角继续，Söze Enterprises 是由神秘的土耳其亿万富翁 Keyser Söze 创立的。我们的团队一直在努力构建下一代自动驾驶汽车编排平台。之前，我们希望通过利用
    Google Cloud 的强大平台、发挥团队现有技能并专注于功能开发，来超越竞争对手。团队刚刚找到节奏时，一个意外的变故突然出现。
- en: Over the weekend, our elusive executive was influenced by a rendezvous with
    Sundar Pichai, the CEO of Alphabet and Google’s parent company, in Singapore.
    Keyser was seen gobbling down street food with Sundar on Satay Street. During
    this brief but enjoyable encounter, Sundar extolled the virtues and prowess of
    Kubernetes and Google’s unique position as the original developers of the open
    source technology. Keyser was enchanted by the prospect of more efficient resource
    utilization, leading to improved cost optimization and faster deployment and rollback
    times, and he was hooked. His new autonomous vehicle platform needed to harness
    the power of the cloud, and container-based architecture was the way to do it.
    So, he decided to accelerate his plans to adopt cloud-native architecture!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在周末，我们神秘的高管受到与 Alphabet 和 Google 母公司 CEO Sundar Pichai 在新加坡会面的影响。据称，Keyser 和
    Sundar 一起在 Satay Street 街头品尝街头小吃。在这次简短而愉快的会面中，Sundar 大力赞扬了 Kubernetes 的优点和强大功能，以及
    Google 作为开源技术的原始开发者的独特地位。Keyser 被 Kubernetes 所带来的更高效资源利用的前景所吸引，从而改善了成本优化、加快了部署和回滚速度，他深深地被打动了。他的新自动驾驶平台需要利用云的力量，而基于容器的架构正是实现这一目标的方式。因此，他决定加速推进采纳云原生架构的计划！
- en: The news of transitioning to a container-based architecture means reevaluating
    their approach, diving into new technologies, and possibly even reshuffling team
    dynamics. For the team, containers were always the long-term plan, but now, things
    need to be sped up, which will require a significant investment in time, resources,
    and training.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 转向基于容器的架构的消息意味着需要重新评估他们的方法，深入研究新技术，甚至可能需要重新调整团队动态。对于团队来说，容器一直是长期计划，但现在，事情需要加速，这将需要大量的时间、资源和培训投入。
- en: As the team scrambles to adjust their plans, they can’t help but feel a mix
    of excitement and apprehension. They know that they are part of something groundbreaking
    under Keyser’s leadership. His vision for the future of autonomous vehicles is
    bold and transformative. And while his methods may be unconventional, they have
    learned that his instincts are often correct. In this chapter, we’ll explore this
    transformation from VMs to containers using Google Cloud.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当团队匆忙调整计划时，他们不禁感到一阵兴奋与忧虑的混合情绪。他们知道，在凯泽的领导下，他们正参与一项具有突破性的工作。他对自动驾驶未来的愿景既大胆又具变革性。尽管他的方法可能不同寻常，但他们已经学会了，他的直觉通常是正确的。在本章中，我们将探讨如何利用Google
    Cloud将解决方案从虚拟机转向容器。
- en: Designing the solution
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: 'As we saw in the previous chapter, where we built our solution using VMs on
    Google Cloud, we had full control over the operating system configuration through
    the VM images we provisioned with Packer. Just as we did when we went through
    the same process on our journey with AWS and Azure in *Chapters 8* and *11*, we’ll
    need to introduce a new tool to replace VM images with container images – **Docker**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中看到的，我们在Google Cloud上构建的解决方案，通过我们用Packer提供的虚拟机镜像，完全控制操作系统配置。就像我们在*第8章*和*第11章*的AWS和Azure之旅中所做的那样，我们现在需要引入一个新工具，用容器镜像替代虚拟机镜像——**Docker**：
- en: '![Figure 14.1 – Logical architecture for the autonomous vehicle platform](img/B21183_14_1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – 自动驾驶平台的逻辑架构](img/B21183_14_1.jpg)'
- en: Figure 14.1 – Logical architecture for the autonomous vehicle platform
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – 自动驾驶平台的逻辑架构
- en: 'Our application architecture, comprising a frontend, a backend, and a database,
    will remain the same, but we will need to provision different resources with Terraform
    and harness new tools from Docker and Kubernetes to automate the deployment of
    our solution to this new infrastructure:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用架构，包括前端、后端和数据库，保持不变，但我们需要通过Terraform配置不同的资源，并借助Docker和Kubernetes的新工具来自动化部署到这一新基础设施上：
- en: '![Figure 14.2 – Source control structure of our repository](img/B21183_14_2.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2 – 我们仓库的源代码控制结构](img/B21183_14_2.jpg)'
- en: Figure 14.2 – Source control structure of our repository
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – 我们仓库的源代码控制结构
- en: This solution will have seven parts. We still have the application code and
    Dockerfiles (replacing the Packer-based VM images) for both the frontend and backend.
    We also still have GitHub Actions to implement our CI/CD process, but we now have
    two Terraform code bases – one for provisioning the underlying infrastructure
    to Google Cloud and another for provisioning our application to the Kubernetes
    cluster hosted on GKE. Then, we have the two code bases for our application’s
    frontend and backend.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 该解决方案将包括七个部分。我们依然拥有前端和后端的应用代码和Dockerfile（取代基于Packer的虚拟机镜像）。我们依然使用GitHub Actions来实现CI/CD流程，但现在我们有两个Terraform代码库——一个用于在Google
    Cloud上配置基础设施，另一个用于将我们的应用部署到托管在GKE上的Kubernetes集群中。然后，我们有两个代码库，分别用于前端和后端的应用。
- en: Cloud architecture
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云架构
- en: '**Google Kubernetes Engine** (**GKE**) is a sophisticated offering that allows
    you to provision a managed Kubernetes cluster in a multitude of ways, depending
    on your objectives, whether that is to maximize simplicity of operations or highly
    customized configurations.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Kubernetes Engine**（**GKE**）是一个复杂的产品，它允许你以多种方式提供管理的Kubernetes集群，具体取决于你的目标，无论是最大化操作简便性，还是高度自定义的配置。'
- en: Autopilot
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动驾驶
- en: One of the simplest ways of operating a Kubernetes cluster on Google Cloud is
    using the Autopilot feature of GKE. Turning on the Autopilot feature abstracts
    much of the complexity of operating a Kubernetes cluster. This option changes
    the operating model radically, so much so that it is probably more akin to some
    of the container-based serverless options on other clouds than it does the managed
    Kubernetes offerings that we’ve delved into in previous chapters. As a result,
    it is outside the scope of this book. However, if this approach appeals to you,
    I suggest that you investigate further in Google’s documentation ([https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview)).
    I’m pointing this out because, unlike AWS and Azure, which have separately branded
    services that abstract away container orchestration, **Google Cloud Platform**
    (**GCP**) has this capability coupled with its managed Kubernetes offering.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上操作 Kubernetes 集群最简单的方式之一是使用 GKE 的 Autopilot 功能。开启 Autopilot
    功能可以将 Kubernetes 集群管理中的许多复杂性抽象化。这个选项彻底改变了操作模式，甚至可以说，它更像是其他云提供的基于容器的无服务器选项，而不是我们在前几章中深入探讨的托管
    Kubernetes 服务。因此，这超出了本书的范围。然而，如果这种方式对你有吸引力，我建议你在 Google 的文档中进一步研究 ([https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview))。我之所以提到这一点，是因为与
    AWS 和 Azure 提供的独立品牌的服务不同，**Google Cloud Platform** (**GCP**) 将这种功能与其托管 Kubernetes
    服务结合在一起。
- en: Regional versus zonal
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域集群与单区集群
- en: 'GKE supports two primary cluster types: regional and zonal. The cluster type
    affects how the cluster’s underlying physical infrastructure is provisioned across
    GCP, which subsequently affects the resiliency of the Kubernetes cluster:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 支持两种主要的集群类型：区域集群和单区集群。集群类型会影响集群底层物理基础设施在 GCP 中的部署方式，这进而影响 Kubernetes 集群的弹性：
- en: '![Figure 14.3 – The GKE zonal cluster hosts the control plane and all nodes
    within a single Availability Zone](img/B21183_14_3.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3 – GKE 区域集群托管控制平面和所有节点在单一可用区内](img/B21183_14_3.jpg)'
- en: Figure 14.3 – The GKE zonal cluster hosts the control plane and all nodes within
    a single Availability Zone
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 – GKE 区域集群托管控制平面和所有节点在单一可用区内
- en: 'A zonal cluster is deployed within a single Availability Zone within a given
    region. As we know, each region has a name, such as `us-west1`. To reference a
    specific zone, we append the zone number to the end of the region name. For example,
    to reference Availability Zone A in the West US 1 region, we can refer to it by
    its name – that is, `us-west1-a`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 单区集群部署在给定区域内的单一可用区内。如我们所知，每个区域都有一个名称，例如 `us-west1`。为了引用特定的可用区，我们将在区域名称后面加上可用区的编号。例如，要引用西美国
    1 区的 A 可用区，我们可以通过其名称来引用——即 `us-west1-a`：
- en: '![Figure 14.4 – The GKE regional cluster replicates the control plane and nodes
    across all zones within the region](img/B21183_14_4.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4 – GKE 区域集群在该区域的所有可用区内复制控制平面和节点](img/B21183_14_4.jpg)'
- en: Figure 14.4 – The GKE regional cluster replicates the control plane and nodes
    across all zones within the region
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 – GKE 区域集群在该区域的所有可用区内复制控制平面和节点
- en: A regional cluster is deployed across Availability Zones within a given region.
    When you deploy a regional cluster, by default, your cluster is deployed across
    three Availability Zones within that region. This approach results in higher availability
    and resiliency in case of an Availability Zone outage.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 区域集群部署在给定区域内的多个可用区中。当你部署区域集群时，默认情况下，集群会在该区域的三个可用区内部署。这种方式可以提高高可用性和弹性，以防某个可用区发生故障。
- en: Virtual network
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: As we discussed in the previous chapter, when we set up our VM-based solution
    on Google Cloud, we will need a virtual network to host our GKE cluster. This
    will allow us to configure a private GKE cluster so that the Kubernetes control
    plane and the node have private IP addresses and are not directly accessible from
    the internet.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中讨论的，当我们在 Google Cloud 上设置基于虚拟机的解决方案时，我们需要一个虚拟网络来托管我们的 GKE 集群。这将允许我们配置一个私有的
    GKE 集群，使得 Kubernetes 控制平面和节点拥有私有 IP 地址，并且无法直接从互联网访问。
- en: 'In the previous chapter, where we set up our VM-based solution, we set up two
    subnets: one for the frontend and one for the backend. However, when using a Kubernetes
    cluster to host our solution, both the frontend and backend will be hosted on
    the same Kubernetes nodes.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章节中，我们设置了基于 VM 的解决方案，配置了两个子网：一个用于前端，一个用于后端。然而，在使用 Kubernetes 集群托管我们的解决方案时，前端和后端将托管在相同的
    Kubernetes 节点上。
- en: This straightforward approach, where multiple node pools share one subnet, can
    suffice for less complex configurations. However, while this setup simplifies
    network management, it can potentially limit the scalability of individual node
    pools due to shared network resources and address space constraints.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的方法，其中多个节点池共享一个子网，适用于较简单的配置。然而，尽管这种设置简化了网络管理，但由于共享网络资源和地址空间的限制，它可能会限制单个节点池的可扩展性。
- en: For more scalable and flexible architectures, especially in larger or more dynamic
    environments, it’s often advantageous to allocate separate subnets for different
    node pools. This method allows each node pool to scale independently and optimizes
    network organization, providing better resource allocation and isolation. This
    kind of structured subnetting becomes increasingly important as the complexity
    and scale of the Kubernetes deployments grow, making it a key consideration in
    GKE network planning and configuration.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更具可扩展性和灵活性的架构，特别是在较大或动态变化的环境中，通常会为不同的节点池分配单独的子网。这种方法使得每个节点池可以独立扩展，优化了网络组织，提供了更好的资源分配和隔离。随着
    Kubernetes 部署的复杂性和规模的增加，这种结构化的子网划分变得越来越重要，成为 GKE 网络规划和配置中的关键考虑因素。
- en: Container registry
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器镜像仓库
- en: Like the other cloud platforms we’ve been delving into in this book, Google
    Cloud also offers a robust container registry service known as **Google Artifact
    Registry**, which is a private registry for hosting and managing container images
    and Helm charts. Artifact Registry supports many other formats besides container
    images but we’ll only be using it in this capacity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们在本书中深入探讨的其他云平台一样，Google Cloud 也提供了一个强大的容器镜像仓库服务，称为 **Google Artifact Registry**，它是一个用于托管和管理容器镜像和
    Helm 图表的私有仓库。Artifact Registry 除了支持容器镜像之外，还支持许多其他格式，但我们只会在这个功能下使用它。
- en: Google Artifact Registry is set up pretty similarly to other cloud providers.
    It resembles **Azure Container Registry** a bit more though because it can host
    multiple repositories, allowing you to host multiple container images in the same
    Artifact Registry.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Google Artifact Registry 的设置方式与其他云提供商非常相似。然而，它与 **Azure Container Registry**
    更为相似，因为它可以托管多个仓库，允许你在同一个 Artifact Registry 中托管多个容器镜像。
- en: Load balancing
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: GKE has a very similar experience to other managed Kubernetes offerings that
    we have looked at in this book. By default, when a Kubernetes service is provisioned
    to a private cluster, GKE will automatically provision an internal load balancer
    for this service. This will make the Kubernetes service available within the virtual
    network but not to the outside world.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 与我们在本书中查看的其他托管 Kubernetes 服务提供商有非常相似的体验。默认情况下，当 Kubernetes 服务被部署到私有集群时，GKE
    会自动为该服务配置一个内部负载均衡器。这将使 Kubernetes 服务在虚拟网络内可用，但无法访问外部网络。
- en: 'This works well for our backend REST API but doesn’t work for our public web
    application, which is intended to be accessible from the public Internet. Like
    on AWS and Azure, to make the frontend service accessible to the internet, we
    need to configure an ingress controller on the cluster and a public load balancer
    that has a public IP address and will route traffic to the ingress controller
    on the GKE cluster:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于我们的后端 REST API 很有效，但对于我们的公共 Web 应用程序则不适用，因为该应用程序旨在可以从公共互联网访问。像在 AWS 和 Azure
    上一样，要使前端服务能够访问互联网，我们需要在集群上配置一个入口控制器，并配置一个具有公共 IP 地址的公共负载均衡器，该负载均衡器将流量路由到 GKE 集群上的入口控制器：
- en: '![Figure 14.5 – The GKE cluster with an NGINX ingress controller automating
    a Google Cloud load balancer](img/B21183_14_5.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5 – 配置了 NGINX 入口控制器的 GKE 集群，自动化 Google Cloud 负载均衡器](img/B21183_14_5.jpg)'
- en: Figure 14.5 – The GKE cluster with an NGINX ingress controller automating a
    Google Cloud load balancer
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 – 配置了 NGINX 入口控制器的 GKE 集群，自动化 Google Cloud 负载均衡器
- en: As we did in previous chapters, we’ll set up an NGINX ingress controller and
    configure it to automatically provision the necessary external load balancer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中所做的那样，我们将设置一个 NGINX 入口控制器，并配置它以自动配置所需的外部负载均衡器。
- en: Network security
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络安全
- en: When working with GKE, network security is managed in a manner akin to the practices
    described in [*Chapter 13*](B21183_13.xhtml#_idTextAnchor569) for VMs, leveraging
    similar concepts and tools within the Google Cloud ecosystem. GKE clusters are
    typically deployed within a virtual network, allowing them to seamlessly integrate
    with other Google Cloud services.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 GKE 时，网络安全的管理方式类似于[**第 13 章**](B21183_13.xhtml#_idTextAnchor569)中描述的虚拟机实践，利用
    Google Cloud 生态系统中的相似概念和工具。GKE 集群通常部署在虚拟网络中，允许它们与其他 Google Cloud 服务无缝集成。
- en: Similar to the other managed Kubernetes offerings, the virtual network acts
    as the primary boundary for network security, within which GKE has its internal
    network where pods and services communicate. Google Cloud firewalls are used to
    define security rules at the subnet level, controlling inbound and outbound traffic
    similar to how they are employed with VMs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他托管的 Kubernetes 服务类似，虚拟网络作为网络安全的主要边界，GKE 在其中拥有其内部网络，Pods 和服务在该网络内进行通信。Google
    Cloud 防火墙用于在子网级别定义安全规则，控制进出流量，类似于在虚拟机中使用的方式。
- en: Additionally, GKE takes advantage of native Kubernetes network policies for
    finer-grained control within the cluster, allowing administrators to define how
    Pods communicate with each other and with other resources in the virtual network.
    This dual-layered approach, combining the external security controls of the virtual
    network with the internal mechanisms of GKE, creates a comprehensive and robust
    network security environment for Kubernetes deployments.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GKE 利用原生 Kubernetes 网络策略对集群内部进行更细粒度的控制，允许管理员定义 Pods 如何相互通信以及如何与虚拟网络中的其他资源通信。这种双层方法将虚拟网络的外部安全控制与
    GKE 的内部机制结合起来，为 Kubernetes 部署创建了一个全面且强大的网络安全环境。
- en: Workload identity
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作负载身份
- en: As we did with AWS and Azure in previous chapters, we’ll be setting up a workload
    identity to allow our application’s pods to authenticate with other Google Cloud
    services using a Google Cloud identity provider. This will allow us to use the
    built-in role-based access control to grant access for Kubernetes service accounts
    to other Google Cloud resources.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在前几章中对 AWS 和 Azure 的处理方式一样，我们将设置工作负载身份，使我们的应用程序的 Pods 能够通过 Google Cloud
    身份提供者与其他 Google Cloud 服务进行身份验证。这将使我们能够使用内置的基于角色的访问控制，授予 Kubernetes 服务帐户访问其他 Google
    Cloud 资源的权限。
- en: Secrets management
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密钥管理
- en: GKE does not have direct integration with Google Secrets Manager like other
    cloud platforms. Instead, the options available to you are to leverage native
    Kubernetes secrets or to access Google Secrets Manager from your application code
    itself. This approach does have some security advantages but it is less ideal
    as it tightly couples your application to GCP SDKs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他云平台不同，GKE 没有与 Google Secrets Manager 进行直接集成。相反，你可以选择利用原生 Kubernetes 密钥，或通过应用程序代码本身访问
    Google Secrets Manager。这种方式确实具有一些安全优势，但由于它将应用程序与 GCP SDK 紧密耦合，因此并不理想。
- en: Kubernetes cluster
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 集群
- en: Building a Kubernetes cluster using GKE involves a few key decisions that determine
    the modality of your cluster. As we’ve discussed in this book, we will omit the
    use of Autopilot to maintain congruency with the other managed Kubernetes offerings
    from the other cloud platforms we’ve looked at in this book. So, we will focus
    on building a private Kubernetes cluster with its own virtual network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GKE 构建 Kubernetes 集群涉及一些关键决策，这些决策决定了集群的模式。正如我们在本书中讨论的那样，为了与本书中讨论的其他云平台的托管
    Kubernetes 服务保持一致，我们将省略使用 Autopilot。所以，我们将专注于构建一个拥有自己虚拟网络的私有 Kubernetes 集群。
- en: Like other managed Kubernetes offerings, GKE provides flexibility to configure
    node pools based on workload types, but unlike those offerings, you don’t need
    to set up node pools for running core Kubernetes services. GKE handles all that
    on your behalf! This abstraction greatly simplifies cluster design. Overall, GKE’s
    simplicity and robust feature set allow us to build highly scalable Kubernetes
    clusters with minimal effort.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他托管的 Kubernetes 服务类似，GKE 提供了根据工作负载类型配置节点池的灵活性，但与这些服务不同的是，你无需为运行核心 Kubernetes
    服务设置节点池。GKE 会为你处理所有这些！这种抽象极大地简化了集群设计。总体而言，GKE 的简单性和强大的功能集使我们能够以最小的努力构建高度可扩展的 Kubernetes
    集群。
- en: Deployment architecture
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署架构
- en: As we saw with the cloud architecture, there were many similarities between
    our work in *Chapters 8* and *11* with AWS and Microsoft Azure. The deployment
    architecture will mirror what we saw in those chapters as well. In the previous
    chapter, we saw the differences in the Terraform provider when we configured the
    `google` provider to provision our solution to VMs using GCE.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在云架构中所看到的那样，在使用 AWS 和微软 Azure 时，我们在[*第 8 章*](B21183_08.xhtml#_idTextAnchor402)和[*第
    11 章*](B21183_08.xhtml#_idTextAnchor402)的工作有许多相似之处。部署架构也将与我们在这些章节中看到的架构相一致。在上一章中，我们看到了在配置
    `google` 提供程序时，Terraform 提供程序的差异，用于将我们的解决方案通过 GCE 部署到虚拟机。
- en: In the context of container-based architecture, the only significant difference
    from our deployment in the previous chapters with AWS and Azure will be the way
    we authenticate with the container registry and the Kubernetes cluster. It’s important
    to recall the deployment architectural approach outlined in the corresponding
    section of [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402). In the next section,
    we’ll build the same solution on GCP, ensuring we don’t repeat the same information.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于容器的架构中，与我们之前在 AWS 和 Azure 上的部署相比，唯一显著的区别将是我们如何进行容器注册表和 Kubernetes 集群的身份验证。回顾我们在[*第
    8 章*](B21183_08.xhtml#_idTextAnchor402)中概述的部署架构方法是非常重要的。在接下来的部分，我们将在 GCP 上构建相同的解决方案，确保我们不会重复相同的信息。
- en: In this section, we reviewed the key changes in our architecture as we transitioned
    from VM-based architecture to container-based architecture. We were careful not
    to retread the ground we covered in [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402),
    where we first went through this transformation on the AWS platform. In the next
    section, we’ll get tactical in building the solution, but again, we’ll be careful
    to build on the foundations we built in the previous chapter when we first set
    up our solution on GCP using VMs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了从基于虚拟机的架构过渡到基于容器的架构时，架构中的关键变化。我们小心避免重复[*第 8 章*](B21183_08.xhtml#_idTextAnchor402)中的内容，该章节中我们首次在
    AWS 平台上进行了这一转型。在接下来的部分，我们将具体实施解决方案，但仍会小心地基于我们在上一章中首次使用虚拟机在 GCP 上搭建解决方案时打下的基础。
- en: Building the solution
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建解决方案
- en: In this section, we’ll be taking our theoretical knowledge and applying it to
    a tangible, functioning solution while harnessing the power of Docker, Terraform,
    and Kubernetes on GCP. Some parts of this process will require significant change,
    such as when we provision our Google Cloud infrastructure using Terraform; other
    parts will have minor changes, such as the Kubernetes configuration that we use
    to deploy our application to our Kubernetes cluster, and some will have almost
    no change whatsoever, such as when we build a push our Docker images to our container
    registry.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把理论知识应用到一个具体且可操作的解决方案上，同时在 GCP 上利用 Docker、Terraform 和 Kubernetes 的强大功能。这个过程中有些部分将需要显著的变化，例如当我们使用
    Terraform 配置 Google Cloud 基础设施时；其他部分则会有较小的变化，比如我们用来将应用程序部署到 Kubernetes 集群的 Kubernetes
    配置，而有些部分几乎不会有任何变化，例如当我们构建并推送 Docker 镜像到容器注册表时。
- en: Docker
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker
- en: In this section, we’ll go into great detail on how we can implement our Dockerfile,
    which installs our .NET application code and runs the service in a container.
    If you skipped *Chapters 7* through *9* due to a lack of interest in AWS, I can’t
    hold that against you – particularly if your primary interest in reading this
    book is working on GCP. However, I would encourage you to review the corresponding
    section within [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402) to see how we can
    use Docker to configure a container with our .NET application code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细介绍如何实现我们的 Dockerfile，它安装我们的 .NET 应用程序代码并在容器中运行服务。如果你因为对 AWS 缺乏兴趣而跳过了[*第
    7 章*](B21183_08.xhtml#_idTextAnchor402)到[*第 9 章*](B21183_08.xhtml#_idTextAnchor402)的内容，我不能怪你——特别是如果你主要是为了在
    GCP 上工作而阅读本书的话。不过，我还是建议你查看[*第 8 章*](B21183_08.xhtml#_idTextAnchor402)中的相关部分，看看我们如何使用
    Docker 来配置带有 .NET 应用程序代码的容器。
- en: Infrastructure
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础设施
- en: As we know, Terraform is not a write-once, run-anywhere solution. It is a highly
    extensible **Infrastructure as Code** (**IaC**) tool that uses a well-defined
    strategy pattern to facilitate the management of multiple cloud platforms. This
    yields very similar conceptually structured solutions but with significant variations
    embedded within the differing implementation details and nomenclature of each
    corresponding cloud platform.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，Terraform 不是一个一次写入、到处运行的解决方案。它是一个高度可扩展的 **基础设施即代码** (**IaC**) 工具，采用定义良好的策略模式来促进多个云平台的管理。这提供了概念上非常相似的解决方案，但在每个云平台的不同实现细节和术语中嵌入了显著的差异。
- en: As we discussed in the previous section, the virtual network configuration will
    largely be identical and the load balancer will be automatically provisioned by
    GKE via the NGINX ingress controller. Therefore, in this section, we will only
    focus on the new resources that we need to replace our VMs with a Kubernetes cluster.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的，虚拟网络配置基本相同，负载均衡器将通过 GKE 的 NGINX ingress 控制器自动配置。因此，在本节中，我们将只关注需要替换我们的虚拟机（VM）以构建
    Kubernetes 集群的新资源。
- en: Container registry
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器注册表
- en: 'The first thing we need is a Google Cloud Artifact Registry that we can push
    Docker images to. We’ll use this as part of our Docker build process later when
    we build and push Docker images to be used by our GKE cluster:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的第一件事是一个 Google Cloud Artifact Registry，用于将 Docker 镜像推送到其中。我们将在后续的 Docker
    构建过程中使用它，当我们构建并推送 Docker 镜像以供 GKE 集群使用时：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Service account
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务账户
- en: 'To grant our applications and services the ability to implicitly authenticate
    with Google Cloud and access other services and resources hosted therein, we need
    to set up a service account that we can associate with the workloads running on
    our cluster. This is similar to the IAM role and managed identity we specified
    on AWS and Azure, respectively:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了授予我们的应用程序和服务能够隐式地与 Google Cloud 进行身份验证并访问其中托管的其他服务和资源的能力，我们需要设置一个服务账户，并将其与集群上运行的工作负载关联起来。这类似于我们在
    AWS 和 Azure 上指定的 IAM 角色和托管身份：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Kubernetes cluster
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 集群
- en: 'This Terraform code creates a GKE cluster with a customized name – that is,
    Google Cloud Region. The `location` attribute is extremely critical as its value
    can determine if the cluster is regional or zonal. Simply making a tiny change
    from `us-west1` to `us-west1-a` has this effect:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这段 Terraform 代码创建了一个带有自定义名称的 GKE 集群——也就是 Google Cloud 区域。`location` 属性非常关键，因为它的值可以决定集群是区域性的还是按可用区划分的。仅仅将
    `us-west1` 更改为 `us-west1-a` 就能产生如下效果：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By default, GKE will automatically provision a default node pool. This is a
    common practice that, unfortunately, prioritizes the graphical user experience
    via the Google Cloud console over the IaC experience. This problem is not unique
    to Google Cloud; both AWS and Azure have similar areas of friction where automation
    is an afterthought. As a result, we are at least left with attributes that allow
    us to circumvent this behavior. By setting `remove_default_node_pool` to `true`,
    we can ensure that this default behavior is eliminated. Furthermore, setting `initial_node_count`
    to `1` can further speed up this process.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GKE 将自动配置一个默认节点池。这是一种常见做法，不幸的是，它优先考虑了通过 Google Cloud 控制台提供的图形用户体验，而忽视了基础设施即代码（IaC）体验。这个问题并非
    Google Cloud 独有；AWS 和 Azure 也有类似的摩擦点，自动化往往是事后的考虑。结果，我们至少拥有了一些属性，可以让我们绕过这种行为。通过将
    `remove_default_node_pool` 设置为 `true`，我们可以确保消除这种默认行为。此外，将 `initial_node_count`
    设置为 `1` 可以进一步加速这个过程。
- en: 'As we discussed previously, GKE abstracts the Kubernetes master services from
    us so that we don’t need to worry about deploying a node pool for these Kubernetes
    system components. Therefore, we are left with defining our node pools for our
    applications and services to run on:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，GKE 抽象化了 Kubernetes 主节点服务，这样我们就不需要担心为这些 Kubernetes 系统组件部署节点池。因此，我们只需要为我们的应用程序和服务定义节点池以运行：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The basic configuration of a node pool resource connects it to the corresponding
    cluster and specifies a `node_count` value. The `node_config` block is where we
    configure more details for the nodes within the pool. The node pool configuration
    should look similar to what we saw in *Chapters 8* and *11* when we configured
    the managed Kubernetes offerings of AWS and Azure. Node pools have a count that
    controls how many VMs we can spin up and a VM size that specifies how many CPU
    cores and memory each node gets. We also need to specify the service account under
    which the node pool will operate:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 节点池资源的基本配置将其连接到相应的集群，并指定一个`node_count`值。`node_config`块用于配置节点池中节点的更多细节。节点池配置应类似于我们在*第8章*和*第11章*中配置AWS和Azure的托管Kubernetes服务时所看到的。节点池有一个计数，控制我们可以启动多少个虚拟机，以及一个虚拟机大小，指定每个节点的CPU核心数和内存。我们还需要指定节点池将运行的服务账户：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, `oauth_scopes` is used to specify what permissions the nodes should have
    access to. To enable Google Cloud logging and monitoring, we need to add scopes
    to allow the nodes to tap into these existing Google Cloud services.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`oauth_scopes`用于指定节点应具有的访问权限。为了启用Google Cloud日志记录和监控，我们需要添加范围，以允许节点访问这些现有的Google
    Cloud服务。
- en: Workload identity
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作负载身份
- en: 'To enable a workload identity, we need to modify both our cluster and node
    pool configuration. The cluster needs to have the `workload_identity_config` block
    defined with `workload_pool` set with a specific magic string that will provision
    the GKE metadata service within the cluster:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用工作负载身份，我们需要修改集群和节点池的配置。集群需要定义`workload_identity_config`块，并设置`workload_pool`，它将使用特定的魔法字符串在集群内配置GKE元数据服务：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the GKE metadata service is made available within the cluster, we need
    to configure our node pools so that they integrate with it using the `workload_metadata_config`
    block. We can do this by specifying `GKE_METADATA` as the mode:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦GKE元数据服务在集群内可用，我们需要配置我们的节点池，使其通过`workload_metadata_config`块与该服务进行集成。我们可以通过将`GKE_METADATA`指定为模式来实现这一点：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Kubernetes
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes
- en: In *Chapters 8* and *11*, we built out the Kubernetes deployments using the
    Terraform provider for Kubernetes on AWS and Azure, respectively. We’ll follow
    the same approach here, building on the infrastructure we provisioned in the previous
    section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8章*和*第11章*中，我们分别使用AWS和Azure上的Kubernetes Terraform提供程序构建了Kubernetes部署。在这里，我们将沿用相同的方法，基于上一节中配置的基础设施继续构建。
- en: Provider setup
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提供程序设置
- en: As we saw in [*Chapter 11*](B21183_11.xhtml#_idTextAnchor509), there is not
    much that changes when executing Terraform using the Kubernetes provider to provision
    resources to the Kubernetes control plane. We still authenticate against our target
    cloud platform, follow Terraform’s core workflow, and pass in additional input
    parameters for platform-specific resources that we need to reference. Most notably,
    information about the cluster and other GCP services such as Secrets Manager and
    other details that might need to be put into Kubernetes ConfigMaps can be used
    by the pods to point them at the endpoint of their database.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第11章*](B21183_11.xhtml#_idTextAnchor509)中看到的，当使用Kubernetes提供程序执行Terraform以向Kubernetes控制平面提供资源时，几乎没有什么变化。我们仍然会对目标云平台进行身份验证，遵循Terraform的核心工作流，并传入额外的输入参数，以引用我们需要的特定平台资源。最重要的是，关于集群和其他GCP服务的信息，例如Secrets
    Manager等，可能需要放入Kubernetes的ConfigMaps中，供Pod使用并指向其数据库端点。
- en: 'As we saw in *Chapters 8* and *11*, when we accomplished the same task on AWS
    and Azure, I am using a layered approach to provision the infrastructure first
    and then provision to Kubernetes. As a result, we can reference the Kubernetes
    cluster using the data source from the Terraform workspace that provisions the
    Google Cloud infrastructure. This allows us to access important connectivity details
    without exporting them outside of Terraform and passing them around during deployment:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第8章*和*第11章*中所看到的，当我们在AWS和Azure上完成相同的任务时，我采用了一种分层的方法，先配置基础设施，然后再部署到Kubernetes。因此，我们可以使用Terraform工作空间中的数据源引用Kubernetes集群，该工作空间提供Google
    Cloud基础设施。这使我们能够访问重要的连接详细信息，而无需将其导出到Terraform外部并在部署过程中传递。
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see, in the preceding code, when using the data source, we only
    need to specify the cluster name and its target region. Using this data source,
    we can then initialize the `kubernetes` provider:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，在前面的代码中，当使用数据源时，我们只需要指定集群名称及其目标区域。使用这个数据源后，我们可以初始化`kubernetes`提供程序：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This configuration varies slightly from the provider initialization techniques
    we used with AWS and Azure in previous chapters with the addition of `token`.
    Similar to how we initialized the `helm` provider on other cloud platforms, we
    can pass the same inputs to set up the Helm provider.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置与我们在之前章节中与 AWS 和 Azure 使用的提供者初始化技术略有不同，增加了 `token`。类似于我们如何在其他云平台上初始化 `helm`
    提供者，我们可以传递相同的输入来设置 Helm 提供者。
- en: Workload identity
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作负载身份验证
- en: 'As we discussed in *Chapters 8* and *11*, where we implemented a workload identity
    on both AWS and Azure, we need a way for our Kubernetes workloads to be able to
    implicitly authenticate with Google Cloud services and resources. To do so, we
    need an identity provisioned within Google Cloud, which we saw in the previous
    section of this chapter, but we also need something provisioned within Kubernetes
    that will connect our pod specifications to the Google Cloud service account:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第8章*和*第11章*中讨论的，在 AWS 和 Azure 上实现工作负载身份验证，我们需要一种方法让我们的 Kubernetes 工作负载能够隐式地与
    Google Cloud 的服务和资源进行身份验证。为此，我们需要在 Google Cloud 中设置一个身份，在本章的前一节中已经看到，但我们还需要在 Kubernetes
    中设置一些内容，将我们的 Pod 规范连接到 Google Cloud 的服务账号：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding code will provision the Kubernetes service account that will complete
    the linkage with the Google Cloud configuration that we provisioned in the previous
    section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将会配置 Kubernetes 服务账号，完成与我们在前一节中配置的 Google Cloud 配置的连接。
- en: Now that we’ve built out the three components of our architecture, in the next
    section, we’ll move on to how we can automate the deployment using Docker so that
    we can build and publish the container images. We’ll also look at doing this using
    Terraform so that we can provision our infrastructure and deploy our solution
    to Kubernetes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了架构的三个组件，在下一节中，我们将继续讨论如何使用 Docker 自动化部署，以便构建和发布容器镜像。我们还将探讨如何使用 Terraform
    来实现这一点，以便为 Kubernetes 部署基础设施和解决方案。
- en: Automating the deployment
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化部署
- en: In this section, we’ll look at how we can automate the deployment process for
    container-based architectures. We’ll employ similar techniques we saw in [*Chapter
    8*](B21183_08.xhtml#_idTextAnchor402) when we took this same journey down the
    Amazon. As a result, we’ll focus on what changes we need to make when we want
    to deploy to Microsoft Azure and the Azure Kubernetes Service.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将看看如何自动化容器化架构的部署过程。我们将采用类似的技术，就像我们在[*第8章*](B21183_08.xhtml#_idTextAnchor402)中采取了相同的方法时，这一旅程是在亚马逊上。因此，我们将关注当我们希望部署到
    Microsoft Azure 和 Azure Kubernetes 服务时需要做哪些改变。
- en: Docker
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker
- en: In [*Chapter 8*](B21183_08.xhtml#_idTextAnchor402), we covered each step of
    the GitHub Actions workflow that causes Docker to build, tag, and push our Docker
    container images. Thanks to the nature of Docker’s cloud-agnostic architecture,
    this overwhelmingly stays the same.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B21183_08.xhtml#_idTextAnchor402)中，我们详细介绍了 GitHub Actions 工作流程的每个步骤，导致
    Docker 构建、标记和推送我们的 Docker 容器镜像。由于 Docker 的云无关架构的特性，这一过程基本上保持不变。
- en: The only thing that changes is that Google Cloud encapsulates a service account’s
    credentials into a JSON file that is downloaded from the Google Cloud console
    rather than a secret string like on AWS or Azure. As a result, much of the Google
    Cloud tooling is set up to look for this file at a specific path location.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一变化的是 Google Cloud 将服务账号的凭据封装到一个 JSON 文件中，该文件从 Google Cloud 控制台下载，而不是像 AWS
    或 Azure 上的密钥字符串。因此，大部分 Google Cloud 的工具设置是要在特定路径位置查找此文件。
- en: 'Therefore, we need to use a special username `_json_key` and reference the
    value of the JSON file stored in a GitHub Actions secret:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要使用一个特殊的用户名 `_json_key` 并引用存储在 GitHub Actions 秘密中的 JSON 文件的值：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The only thing that changes is the way we must configure Docker so that it targets
    Google Artifact Registry.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一变化的是我们必须配置 Docker 以便它能够对准 Google Artifact Registry。
- en: Terraform
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Terraform
- en: In [*Chapter 13*](B21183_13.xhtml#_idTextAnchor569), we comprehensively covered
    the process of creating a Terraform GitHub Action that authenticates with GCP
    using a service account. Therefore, we won’t be delving into it any further. I
    encourage you to refer back to [*Chapter 10*](B21183_10.xhtml#_idTextAnchor474)
    to review the process.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第13章*](B21183_13.xhtml#_idTextAnchor569)中，我们全面讲解了创建一个 Terraform GitHub 动作的过程，该动作使用服务账号在
    GCP 上进行身份验证。因此，我们不会进一步深入讨论它。我鼓励您参考[*第10章*](B21183_10.xhtml#_idTextAnchor474)以回顾这个过程。
- en: Kubernetes
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes
- en: When we automate Kubernetes with Terraform, we are just running `terraform apply`
    again with a different root module. This time, the root module will configure
    the `kubernetes` and `helm` providers in addition to the `google` provider. However,
    we won’t ever create new resources with the `google` provider; we will only obtain
    data sources to existing resources we provisioned in the previous `terraform apply`
    command that provisioned the infrastructure to Google Cloud.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 Terraform 自动化 Kubernetes 时，只需使用不同的根模块再次运行 `terraform apply`。这一次，根模块将配置
    `kubernetes` 和 `helm` 提供程序，除了 `google` 提供程序之外。然而，我们不会使用 `google` 提供程序创建新的资源；我们只会获取先前
    `terraform apply` 命令中提供的基础设施的数据源。
- en: As a result, the GitHub Action that executes this process will look strikingly
    similar to how we executed Terraform with Google Cloud. Some of the variables
    might change to include things such as the container image details and cluster
    information.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，执行此流程的 GitHub Action 看起来与我们如何在 Google Cloud 中执行 Terraform 非常相似。一些变量可能会改变，包括容器映像详细信息和集群信息。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we designed, built, and automated the deployment of a complete
    and end-to-end solution using container-based architecture. We built onto the
    foundations from [*Chapter 13*](B21183_13.xhtml#_idTextAnchor569), where we worked
    with the foundational infrastructure of Google Cloud networking but layered on
    GKE to host our application in containers. In the next and final step in our GCP
    journey, we’ll be looking at serverless architecture, thus moving beyond the underlying
    infrastructure and letting the platform itself take our solution to new heights.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用基于容器的架构设计、构建和自动化部署了一个完整的端到端解决方案。我们在 [*第13章*](B21183_13.xhtml#_idTextAnchor569)
    的基础上构建，那里我们处理了 Google Cloud 网络的基础设施，但在 GKE 上承载我们的应用程序容器。在我们的 GCP 旅程的下一步和最后一步中，我们将探讨无服务器架构，从而超越底层基础设施，让平台本身将我们的解决方案推向新的高度。
