- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: OpenStack Control Plane – Shared Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 控制平面 – 共享服务
- en: “The value of an idea lies in the using of it.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “一个想法的价值在于它的应用。”
- en: – Thomas A. Edison
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 托马斯·爱迪生
- en: 'Addressing the challenge of business continuity within the OpenStack private
    cloud comes down to the traits and capabilities of its control plane. As highlighted
    in the first chapter, a variety of core services, including compute, imaging,
    storage, networking, and identity, should be designed for deployment with scalability
    and security in mind at an early stage. The other facet of such a requirement
    is to treat other non-native OpenStack services the same way as core ones by designing
    for failure and scalability. Such services include messaging queues and databases.
    As we consider best practices for deploying to production, it is essential to
    iterate through each service that would be part of the OpenStack control plane.
    Another challenge that arises during the deployment is the way OpenStack services
    are built and deployed. As we saw in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right Way (DevSecOps)* , containers to
    run our OpenStack services, breaking the control plane services into smaller chunks
    of units, will increase the reliability and scalability of each running service.
    In this chapter, we will extend our designed layout to a production-ready environment
    by going through the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 OpenStack 私有云中的业务连续性挑战归结为其控制平面的特性和能力。如第一章所述，多个核心服务，包括计算、镜像、存储、网络和身份，应在早期阶段设计以适应可扩展性和安全性。此类需求的另一个方面是将非原生
    OpenStack 服务与核心服务同等对待，设计时考虑故障和可扩展性。此类服务包括消息队列和数据库。在我们考虑生产环境部署的最佳实践时，必须逐一迭代每个将成为
    OpenStack 控制平面一部分的服务。部署过程中出现的另一个挑战是 OpenStack 服务的构建和部署方式。正如我们在[*第二章*](B21716_02.xhtml#_idTextAnchor089)《启动
    OpenStack 设置 – 正确的方式（DevSecOps）》中看到的，容器用于运行 OpenStack 服务，将控制平面服务拆分成更小的单元，将提高每个运行服务的可靠性和可扩展性。在本章中，我们将通过以下主题将设计的布局扩展到生产就绪环境：
- en: Defining the perimeter of the OpenStack control plane
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 OpenStack 控制平面的边界
- en: Identifying different OpenStack services taking up part of the control plane
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别占用部分控制平面的不同 OpenStack 服务
- en: Preparing a first production deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备首次生产部署
- en: Deploying the next OpenStack environment using kolla-ansible
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 kolla-ansible 部署下一个 OpenStack 环境
- en: Uncovering additional custom options for the OpenStack control plane deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭示 OpenStack 控制平面部署的其他自定义选项
- en: The OpenStack control plane
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack 控制平面
- en: When we started designing our first iteration layout, we encountered a variety
    of OpenStack services, each composed of several components. One or several components
    can be designed to run in separate physical machines assigned with roles for logical
    operations and management reasons. As highlighted in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Considerations* , separating the roles logically
    will support the orchestration of the OpenStack deployment and operation. The
    term *control plane* is widely used in the network glossary. It dictates how to
    process and run requests. As an abstract concept, the control plane can be formed
    of one or many system components that centrally manage and describe the way a
    request can be fulfilled. When referring to the OpenStack logical layout, a cloud
    controller is designated to run most of the pieces of the control plane.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始设计首次迭代布局时，遇到了各种 OpenStack 服务，每个服务由多个组件组成。一个或多个组件可以设计成在不同的物理机器上运行，这些机器被分配了用于逻辑操作和管理的角色。正如[*第一章*](B21716_01.xhtml#_idTextAnchor014)《重新审视
    OpenStack – 设计考虑》所强调的，逻辑上分离角色将支持 OpenStack 部署和操作的编排。术语 *控制平面* 在网络词汇中广泛使用，它决定了如何处理和运行请求。作为一个抽象概念，控制平面可以由一个或多个系统组件组成，这些组件集中管理和描述如何满足请求。在
    OpenStack 逻辑布局中，云控制器被指定为运行控制平面的大部分组件。
- en: Running the control plane – cloud controller
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行控制平面 – 云控制器
- en: 'The OpenStack APIs, scheduler services (including compute, storage, and file
    share schedulers), API endpoints, databases, and messaging queues are counted
    as part of the OpenStack control plane. This section iterates through each service
    that is part of the cloud controller setup and summarizes them into two main categories,
    OpenStack and infrastructure services, as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack API、调度服务（包括计算、存储和文件共享调度器）、API 端点、数据库和消息队列都被视为 OpenStack 控制平面的一部分。本节将逐一讲解作为云控制器设置一部分的每个服务，并将其总结为两大类：OpenStack
    服务和基础设施服务，如下所示：
- en: '![Figure 3.1 – An overview of the OpenStack control plane](img/B21716_03_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – OpenStack 控制平面概览](img/B21716_03_01.jpg)'
- en: Figure 3.1 – An overview of the OpenStack control plane
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – OpenStack 控制平面概览
- en: We will walk through the control plane in the following subsections and examine
    each service, its subcomponents, and the latest updates.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下小节中逐步讲解控制平面，并检查每个服务、其子组件和最新的更新。
- en: Identity service
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 身份服务
- en: The identity service, as explored in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Considerations* , presents a vital component
    to authorize and authenticate service communication to accomplish a specific request,
    such as storage creation or instance provisioning. As part of the control plane,
    Keystone consolidates a catalog of all existing OpenStack services to be reached
    through its API.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[*第 1 章*](B21716_01.xhtml#_idTextAnchor014)《重温 OpenStack – 设计考虑》中所探讨的，身份服务是一个重要组件，用于授权和验证服务通信，以完成特定请求，如存储创建或实例配置。作为控制平面的一部分，Keystone
    整合了所有现有 OpenStack 服务的目录，用户可以通过其 API 访问这些服务。
- en: 'It is important to provide a glossary of key terms of Keystone when running
    the control plane services. This is to ensure compliance and governance policies
    when dealing with different sets of domains, projects, and users as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行控制平面服务时，提供 Keystone 的关键术语词汇表非常重要。这是为了确保在处理不同域、项目和用户时符合合规性和治理政策，如下所示：
- en: '**Project** : The concept of a *project* has replaced that of a *tenant* within
    version 3 of the Identity API. The concept did not change, as a project is a logical
    construction of a set of resources isolating users and groups.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目**：在身份 API 的版本 3 中，*项目*的概念取代了*租户*的概念。概念并没有变化，项目是资源集合的逻辑构建，用于隔离用户和组。'
- en: '**Domain** : Going up a layer, a domain contains a set of users, groups, and
    projects. This abstract isolation could be very helpful for large enterprises
    with different sets of departments, for example. Each department can be allocated
    a domain made up of different projects and users.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**域**：上移一层，域包含一组用户、组和项目。这种抽象的隔离对于拥有不同部门的大型企业非常有帮助。例如，每个部门可以分配一个由不同项目和用户组成的域。'
- en: '**Role** : One of the key mechanisms of the Keystone service since its early
    days is the concept of the *role* . A user or group of users can be assigned to
    different projects with different authorization layouts without the need to create
    multiple users with different assignments and switch manually to a different environment
    to access specific resources through different projects.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**角色**：自 Keystone 服务初期以来，*角色*的概念一直是其关键机制之一。可以将一个用户或一组用户分配到不同的项目，并使用不同的授权布局，而无需创建多个具有不同分配的用户，也不需要手动切换到不同的环境来通过不同的项目访问特定资源。'
- en: '**Catalog** : Keystone grants access to different services through its discovery
    service. Those services, from a Keystone perspective, are exposed as OpenStack
    service endpoints and registered in the service catalog.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目录**：Keystone 通过其发现服务授予访问不同服务的权限。从 Keystone 的角度来看，这些服务作为 OpenStack 服务端点公开，并注册在服务目录中。'
- en: '**Token** : Accessing a specific service must be done through obtaining a token.
    It can be considered as an information card that validates the user, as well as
    the expiration date and time. It also checks which project(s) the token has been
    associated with, and to which endpoints. The token concept itself has passed through
    different versions and ways of working. When expanding a production OpenStack
    environment, it is necessary to have a basic understanding of the scoping mechanism
    for the Keystone token. As a large number of users will be competing to access
    services and resources, scoping will help to better organize the access boundaries
    in a Keystone request, which can be categorized into two groups:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**令牌**：访问特定服务必须通过获取令牌来完成。它可以被视为验证用户的身份证明卡，包括过期日期和时间。它还检查令牌与哪些项目关联，并指向哪些端点。令牌概念本身经历了不同的版本和工作方式。当扩展生产环境中的
    OpenStack 时，了解 Keystone 令牌的作用范围机制是非常必要的。由于大量用户会竞争访问服务和资源，作用范围将有助于更好地组织 Keystone
    请求中的访问边界，通常可以分为两类：'
- en: '**Scoped token** : Once a token request is created, the Identity API will specify
    which OpenStack services (endpoints) will be used by the requester and its associated
    roles. It mainly contains information such as the reference to the project and
    domain ID.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有范围令牌**：一旦创建令牌请求，身份 API 将指定请求者将使用哪些 OpenStack 服务（端点）及其关联的角色。它主要包含诸如项目和域 ID
    的引用信息。'
- en: '**Unscoped token** : Unlike the scoped token, the created token request will
    not contain any specific authorization scope to access services. When users present
    credentials for the first time, the usage of unscoped tokens avoids an excessive
    loop of authentication flow. At a later time, the unscoped token can be narrowed
    down to a scoped one to limit authorization to a specific service component(s).'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无范围令牌**：与有范围令牌不同，创建的令牌请求不会包含任何特定的授权范围来访问服务。当用户首次提交凭证时，使用无范围令牌可以避免过度的身份验证循环。稍后，無範圍令牌可以转化为有范围令牌，以将授权限制在特定的服务组件上。'
- en: '**User** : This is an API requester to consume OpenStack services and resources.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户**：这是一个 API 请求者，用于消耗 OpenStack 服务和资源。'
- en: '**Group** : This is a collection of users or API requesters belonging to the
    same domain, as explained previously.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组**：这是属于同一域的用户或 API 请求者的集合，如前所述。'
- en: The Keystone API has been improved by providing more options for authentication
    mechanisms aside from the traditional password way, such as SAML2 and OpenID Connect
    based on OAuth. Identity federation support has been a big milestone for the Keystone
    implementation, enabling pluggable **Identity Providers** ( **IdP** ) through
    different backends, including SQL, LDAP, and even a federated IdP. Within the
    latest OpenStack releases, including Antelope and Bobcat, several backends have
    been supported, allowing enterprises to leverage their existing authentication/authorization
    systems for more consolidated management.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Keystone API 已通过提供更多认证机制选项得到改进，除了传统的密码方式，还支持基于 OAuth 的 SAML2 和 OpenID Connect。身份联合支持是
    Keystone 实现的重要里程碑，它通过不同的后端启用了可插拔的**身份提供者**（**IdP**），包括 SQL、LDAP，甚至是联合的 IdP。在最新的
    OpenStack 版本中，包括 Antelope 和 Bobcat，已经支持了多个后端，允许企业利用现有的认证/授权系统，实现更为集中化的管理。
- en: 'A brief summary of the current supported authentication backends is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当前支持的认证后端的简要总结如下：
- en: '**LDAP** : Many organizations might have existing LDAP instances and require
    a set of security policies to be followed for different compliance reasons. In
    this setup, Keystone will need to grant access to the LDAP servers to read and
    write and will not handle an IdP role internally.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LDAP**：许多组织可能已经有现有的 LDAP 实例，并要求遵循一套安全策略以满足不同的合规要求。在这种配置下，Keystone 需要授予访问
    LDAP 服务器的权限，以进行读写操作，并且不会在内部处理身份提供者（IdP）角色。'
- en: '**SQL** : Most of the default OpenStack deployments come with SQL configuration
    to store user information in a SQL database and provide direct interaction with
    the Keystone service.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SQL**：大多数默认的 OpenStack 部署都配置了 SQL，以将用户信息存储在 SQL 数据库中，并提供与 Keystone 服务的直接交互。'
- en: '**Federated IdP** : This is one of the most common setups, allowing organizations
    to leverage a complete consolidated identity point where Keystone will establish
    a trust relationship and act as a service provider. The OpenStack community has
    developed more stable releases around this layout, offering more ways of authentication
    against multiple backends, such as SQL, LDAP, and Active Directory, through SAML
    assertions. Lately, there has been more work on OpenID Connect and OAuth in its
    second version. This even allows authentication using public social login platforms
    such as Google and Facebook.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联合身份提供者（IdP）**：这是最常见的设置之一，允许组织利用一个完整的整合身份点，Keystone将在其中建立信任关系，并充当服务提供者。OpenStack社区围绕这种布局开发了更稳定的发布版本，提供了通过SAML断言对多个后端（如SQL、LDAP和Active
    Directory）进行身份验证的更多方法。最近，在OpenID Connect和OAuth第二版方面也有了更多的工作。这甚至允许使用Google和Facebook等公共社交登录平台进行身份验证。'
- en: Important note
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The Keystone Identity API supports the full **Create, Read, Update, and Delete**
    ( **CRUD** ) RESTful operations API. Commands are fired using the OpenStack CLI
    and not the Keystone CLI as the latter is no longer supported within Antelope
    and later releases, and the latest OpenStack releases that have adopted Identity
    API version 3.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Keystone身份API支持完整的**创建、读取、更新和删除**（**CRUD**）RESTful操作API。命令通过OpenStack CLI发出，而非Keystone
    CLI，因为后者在Antelope及后续版本中不再支持，并且最新的OpenStack版本已采用身份API版本3。
- en: Depending on which authentication and authorization strategy you adopt, the
    mechanism of Keystone is the same when dealing with pure OpenStack ecosystem workflows.
    The Identity API’s interaction with the rest of the services might be overwhelming
    to tackle when dealing with a large infrastructure layout. At this point, once
    we have iterated through the latest state of the art in Keystone, a brief overview
    of how interactions work will be helpful before listing the next services. We
    should remember that Keystone will turn out to be the security hub of AA in the
    OpenStack ecosystem where default configurations would not be ideal to avoid possible
    security gaps and further boost your compliance and governance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您采用的身份验证和授权策略，当处理纯OpenStack生态系统工作流时，Keystone的机制是相同的。在处理大型基础设施布局时，身份API与其他服务的交互可能会让人感到难以应对。此时，在我们梳理完Keystone的最新技术状态后，简要概述交互如何工作将有助于在列出下一个服务之前理解。我们应当记住，Keystone将成为OpenStack生态系统中AA的安全中心，默认配置不理想，应避免可能的安全漏洞，并进一步提升合规性和治理。
- en: Important note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Keystone configuration can be customized through the Keystone **kolla-ansible**
    playbook located at **/kolla-ansible/ansible/roles/keystone** . At the time of
    writing this book, the default **kolla-ansible** configuration within the Antelope
    and later releases uses Keystone fernet tokens to authenticate uses instead of
    traditional **Public Key Infrastructure** ( **PKI** )-based tokens.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Keystone配置可以通过位于**/kolla-ansible/ansible/roles/keystone**的Keystone **kolla-ansible**剧本进行自定义。编写本书时，Antelope及后续版本中的默认**kolla-ansible**配置使用Keystone
    fernet令牌进行身份验证，而非传统的基于**公钥基础设施**（**PKI**）的令牌。
- en: 'The following workflow briefly demonstrates typical Keystone request and response
    interactions between an API requester (user or system) and identity service to
    access a specific OpenStack service:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下工作流简要演示了API请求者（用户或系统）与身份服务之间的典型Keystone请求和响应交互，以访问特定的OpenStack服务：
- en: A user or API requester authenticates against Keystone service using credentials.
    Identity API receives the request and grants a token to the requester once authenticated.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户或API请求者使用凭证通过Keystone服务进行身份验证。身份API接收请求，并在验证成功后向请求者授予一个令牌。
- en: The user or API requester presents the request with the received token to access
    the demanded service.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户或API请求者使用接收到的令牌提出请求，以访问所需的服务。
- en: The demanded service validates the eligibility of the presented token with the
    identity service.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所需的服务通过身份服务验证所提供令牌的有效性。
- en: Once validated, the requested service replies with a response to the API requester.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦验证成功，请求的服务将向API请求者回复响应。
- en: 'This workflow is depicted in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该工作流如下图所示：
- en: '![Figure 3.2 – Keystone API request processing](img/B21716_03_02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – Keystone API请求处理](img/B21716_03_02.jpg)'
- en: Figure 3.2 – Keystone API request processing
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – Keystone API请求处理
- en: As noted previously, any request to access a specific service or resource will
    engage the identity service and later interactions will be handled by API requests.
    Taking a more complex scenario, such as virtual machine creation, more interactions
    will be involved. This would engage more services to be contacted, such as image,
    network, and storage, forming the essential resources to run an instance. Hence,
    Keystone will validate each request by using the original authentication token.
    A new desired service request will require a newly generated authenticated request
    using the first authentication token.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，任何请求访问特定服务或资源将涉及身份验证服务，后续的交互将通过 API 请求处理。以更复杂的场景为例，例如虚拟机创建，将涉及更多的交互。这将触发更多服务的调用，如镜像、网络和存储，构成运行实例的基本资源。因此，Keystone
    将使用原始认证令牌验证每个请求。新的服务请求将需要使用首次认证令牌生成新的经过认证的请求。
- en: Important note
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Keystone now has OAuth 2.0 Mutual TLS support. Keystone Auth has a new plugin
    for the OAuth 2.0 Device Authorization Grant.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Keystone 现在支持 OAuth 2.0 双向 TLS。Keystone Auth 具有一个新的插件，用于 OAuth 2.0 设备授权授予。
- en: Compute service
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算服务
- en: 'Our control plane includes all compute service components except **nova-compute**
    , which will be running in dedicated hosts. As covered in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Considerations* , the composition of different
    compute daemons constructs the main capability of the OpenStack ecosystem. The
    following Nova services will run as part of the cloud controller stack:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的控制平面包括所有计算服务组件，除了**nova-compute**，它将在专用主机上运行。如[*第1章*](B21716_01.xhtml#_idTextAnchor014)《重访
    OpenStack - 设计考虑》中所述，不同计算守护进程的组成构建了 OpenStack 生态系统的主要能力。以下 Nova 服务将作为云控制器堆栈的一部分运行：
- en: '**nova-api** : This is the primary interface to handle compute requests, including
    the creation, listing, deletion, and management of compute resources.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nova-api**：这是处理计算请求的主要接口，包括计算资源的创建、列出、删除和管理。'
- en: '**nova-scheduler** : This is a set of filters and weights to determine the
    most suitable compute hosts to run an instance based on custom or default characteristics
    such as host location, memory, or CPU architecture.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nova-scheduler**：这是一个过滤器和加权集合，用于根据自定义或默认特征（如主机位置、内存或 CPU 架构）来确定最适合运行实例的计算主机。'
- en: '**nova-conductor** : This is considered a novel addition to the Nova architecture
    prior to the latest OpenStack release, **nova-conductor** insulates the compute
    process from contacting the database directly (counted as not trusted from a security
    perspective) and connects the database directly instead.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nova-conductor**：这被认为是 Nova 架构中的一项新颖补充，在最新的 OpenStack 版本之前，**nova-conductor**将计算过程与直接联系数据库（从安全角度看，被认为是不受信的）隔离开来，转而直接连接数据库。'
- en: '**nova-novncproxy** : This provides VNC console access to the instances.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nova-novncproxy**：此服务为实例提供 VNC 控制台访问。'
- en: Important note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Nova playbook is located at **/kolla-ansible/ansible/roles/nova** in the
    **kolla-ansible** repository. Note that the playbook does not separate each compute
    component on its own.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Nova 剧本位于 **/kolla-ansible/ansible/roles/nova** 目录下，位于 **kolla-ansible** 仓库中。请注意，剧本没有单独区分每个计算组件。
- en: Scheduling service
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度服务
- en: 'The Placement service, as highlighted in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Consideration* , is considered a great addition
    to the compute service to conduct fine-grained prefiltering and achieve advanced
    instance scheduling not only by checking compute resources but also for networking
    and storage pools. The Placement service was introduced as part of the Nova architecture,
    and due to its evolution, it has been turned into a separate project. Some important
    constructs that use this service are as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第1章*](B21716_01.xhtml#_idTextAnchor014)《重访 OpenStack - 设计考虑》中所强调的，Placement
    服务被认为是对计算服务的极大补充，通过检查计算资源、网络和存储池，不仅实现精细化的预筛选，还实现了先进的实例调度。Placement 服务作为 Nova 架构的一部分被引入，并随着其发展，已成为一个独立的项目。使用此服务的一些重要构件如下：
- en: '**Resource providers** : The Placement service tracks the underlying resources
    (types and occurrences), such as compute, storage pools, and networking, in the
    form of abstracted data model objects.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源提供者**：Placement 服务以抽象数据模型对象的形式跟踪底层资源（类型和实例），例如计算、存储池和网络。'
- en: '**Resource class** : Tracked resources are classified by types that are categorized
    into two main sets of resource classes:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源类别**：跟踪的资源按类型分类，分为两大类资源类别：'
- en: '**Standard** : **MEMORY_MB** , **VCPU** , **DISK_GB** , **IPV4_ADDRESS** ,
    **PCI_DEVICE** , **SRIOV_NET_VF** , **NUMA_CORE** , **NUMA_SOCKET** , **VGPU**
    , and so on.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准**：**MEMORY_MB**、**VCPU**、**DISK_GB**、**IPV4_ADDRESS**、**PCI_DEVICE**、**SRIOV_NET_VF**、**NUMA_CORE**、**NUMA_SOCKET**、**VGPU**等。'
- en: '**Custom** : Starting with the **CUSTOM_** prefix, custom resources can be
    created and added to the resource classes of the Placement service.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义**：以**CUSTOM_**前缀开始，可以创建并将自定义资源添加到放置服务的资源类别中。'
- en: '**Inventories** : A collection of a set of resource classes provided by a given
    resource provider. For example, a resource provider, **RP01** , is associated
    with four resource classes: **16 VCPU** , **4096 MEMORY_MB** , **200 DISK_GB**
    , and **50 IPV4_ADDRESS** .'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库存**：由特定资源提供者提供的一组资源类别集合。例如，一个资源提供者**RP01**，与四个资源类别相关联：**16 VCPU**、**4096
    MEMORY_MB**、**200 DISK_GB** 和 **50 IPV4_ADDRESS**。'
- en: '**Traits** : These represent the characteristics of a resource provider. For
    example, vCPU provided by a resource provider, **RP01** , can be **HW_CPU_X86_SVM**
    (SVM x86 hardware architecture) and **DISK_GB** can be **Solid-State Drive** (
    **SSD** ). Both traits assigned to **RP01** will be noted as **is_X86_SVM** and
    **is_SSD** for both **VPCU** and **DISK_GB** resources, respectively.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特性**：这些表示资源提供者的特征。例如，资源提供者**RP01**提供的vCPU可以是**HW_CPU_X86_SVM**（SVM x86硬件架构），**DISK_GB**可以是**固态硬盘**（**SSD**）。分配给**RP01**的这两个特性将分别记为**is_X86_SVM**和**is_SSD**，适用于**VCPU**和**DISK_GB**资源。'
- en: Important note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The Placement service provides standard traits; however, operators can still
    create custom traits by using the placement API CLI. Traits can be specified in
    the Glance image characteristics, or the extra specs of the instance flavor used
    for the instance launch.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 放置服务提供标准特性；然而，操作员仍然可以通过放置API CLI创建自定义特性。特性可以在Glance镜像特征中指定，或在实例启动时使用的实例规格的额外规格中指定。
- en: '**Allocation** : An instance that occupies resources in a compute node is considered
    a consumer for a resource provider. This occupation of resources is represented
    as *allocation* stored in a data model record.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分配**：占用计算节点资源的实例被视为资源提供者的消费者。这种资源的占用表示为*分配*，存储在数据模型记录中。'
- en: '**Allocation candidates** : After processing each request, the Placement service
    reshuffles the existing resource providers and returns the latest available group
    to be available for the next suitable requests.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分配候选**：在处理每个请求后，放置服务会重新排列现有的资源提供者，并返回最新的可用资源组，以便为下一个合适的请求提供资源。'
- en: 'Through the Placement service, more advanced filtering options can be achieved.
    The following workflow demonstrates how the networking service interacts with
    the compute service through placement. The desired scenario is to schedule instance
    provisioning in a compute node based on the network bandwidth:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过放置服务，可以实现更高级的筛选选项。以下工作流展示了网络服务如何通过放置与计算服务进行交互。期望的场景是基于网络带宽在计算节点中调度实例配置：
- en: An inventory is created through the compute service.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算服务创建一个库存。
- en: A resource provider is created by the compute node.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 资源提供者由计算节点创建。
- en: Under the compute resource provider, a networking resource provider is created
    by the networking service.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算资源提供者下，网络服务创建一个网络资源提供者。
- en: The networking service reports the bandwidth inventories.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络服务报告带宽库存。
- en: The networking service provides a resource request for a port.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络服务提供一个端口的资源请求。
- en: The compute service receives the resource request from the port and adds it
    to the API request in a **GET /** **allocation_candidates** request.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算服务从端口接收资源请求，并将其添加到**GET /** **allocation_candidates**请求的API请求中。
- en: The exposed candidates will be filtered by the Placement service and the compute
    service will pick up the most suitable one.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 曝露的候选项将通过放置服务进行筛选，计算服务将选择最合适的一个。
- en: The compute service claims the resources in the Placement service.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算服务在放置服务中申请资源。
- en: The compute service exchanges the allocation update with the networking service.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算服务与网络服务交换分配更新。
- en: The Placement service updates its allocation candidates for the next request.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 放置服务更新其下一次请求的分配候选。
- en: The placement API provides additional capabilities by assisting the Nova scheduling
    process. Prior to the Newton release, only computing resource usage (such as CPU
    and RAM) was tracked and managed. With the addition of multiple resource providers
    (storage and network services), the placement API simplifies resource recording
    and scheduling through a unified management method.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 API 提供了额外的功能，通过帮助 Nova 调度过程。Newton 版本之前，仅跟踪和管理计算资源的使用情况（如 CPU 和 RAM）。随着多个资源提供者（存储和网络服务）的加入，部署
    API 通过统一的管理方法简化了资源的记录和调度。
- en: Image service
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 镜像服务
- en: The Glance API is part of the control plane. However, as was highlighted in
    [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack – Design
    Consideration* , storing the images can be handled by different data store types,
    such as Swift object storage, AWS S3, the Ceph storage system, VMware, filesystems,
    and, recently, the Cinder block storage service. The evolution of the Glance service
    has been marked mostly by the flexibility of the storage image backend support
    within the latest OpenStack releases. Within Antelope and later releases, the
    Glance API service can be configured to support a list of backends at the same
    time, where the Glance API will process the disk image operation using the default
    data store; otherwise, it will go through the defined backend list.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Glance API 是控制平面的一部分。然而，正如在[*第 1 章*](B21716_01.xhtml#_idTextAnchor014)中强调的，*重访
    OpenStack – 设计考虑*，存储镜像可以通过不同的数据存储类型来处理，例如 Swift 对象存储、AWS S3、Ceph 存储系统、VMware、文件系统以及最近的
    Cinder 块存储服务。Glance 服务的发展主要体现在支持最新 OpenStack 版本中存储镜像后端的灵活性。在 Antelope 版本及之后的版本中，Glance
    API 服务可以配置为同时支持多个后端列表，Glance API 将使用默认的数据存储来处理磁盘镜像操作；否则，它将使用定义的后端列表。
- en: Important note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: By default, the imaging service playbook supports file, HTTP, VMware, Cinder,
    Swift, and Ceph backends located in the **/kolla-ansible/ansible/roles/glance/defaults/main.yml**
    file. Enabling or disabling additional image storage backends can be done from
    the **/kolla-ansible/etc/kolla/globals.yml** file by setting **yes** or **no**
    to **glance_backend_ceph** , **glance_backend_file** , **glance_backend_swift**
    , and **glance_backend_vmware** for Ceph, file, Swift and VMware storage backends,
    respectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，影像服务剧本支持文件、HTTP、VMware、Cinder、Swift 和 Ceph 后端，这些后端位于**/kolla-ansible/ansible/roles/glance/defaults/main.yml**文件中。启用或禁用额外的镜像存储后端可以通过在**/kolla-ansible/etc/kolla/globals.yml**文件中设置**glance_backend_ceph**、**glance_backend_file**、**glance_backend_swift**和**glance_backend_vmware**的值为**yes**或**no**来实现，分别对应
    Ceph、文件、Swift 和 VMware 存储后端。
- en: 'The **glance-registry** process running in the control plane will handle the
    metadata updates for each associated image in the database, including the image
    location defined in the storage backend path, as illustrated in the following
    figure:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制平面中运行的**glance-registry**进程将处理数据库中每个关联镜像的元数据更新，包括存储后端路径中定义的镜像位置，如下图所示：
- en: '![Figure 3.3 – The Glance control plane components](img/B21716_03_03.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – Glance 控制平面组件](img/B21716_03_03.jpg)'
- en: Figure 3.3 – The Glance control plane components
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – Glance 控制平面组件
- en: The other great addition to this Glance storage support capability is that the
    control plane can be customized to support multiple block storage backends for
    Glance. If two Cinder storage volumes are created, a property can be assigned
    to a volume, for example, SSD or HDD, where the Glance API will be instructed
    to use the desired Cinder volume based on the cloud operator’s desired configuration.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的增强功能是，控制平面可以定制为支持多个块存储后端。如果创建了两个 Cinder 存储卷，可以将一个属性分配给某个卷，例如 SSD 或 HDD，在这种情况下，Glance
    API 将根据云操作员的配置指示使用所需的 Cinder 卷。
- en: Networking service
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络服务
- en: Unlike many other OpenStack services, the Neutron service is considered more
    complex due to the nature of its evolution through the OpenStack releases as well
    as its advanced virtual networking features and extensions. As demonstrated in
    [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack – Design
    Consideration* , the networking capaci ty and segmentation should be planned in
    advance not only to accommodate more circulating traffic but also to take advantage
    of virtual networking scalability. For this, a network node is dedicated to handling
    different types of tenants and providers by design. Meanwhile, the main Neutron
    API will run on the controller node as part of the control plane. The number of
    Neutron drivers and plugins is increasing with each OpenStack release; within
    the Antelope release, most of the well-known SDN driver mechanisms are supported,
    including **openvswitch** , **OVN** , **linux bridge** , **l2population** , and
    **baremetal** . Advanced service plugins providing virtual network features such
    as VPNs, firewalling, and routing run on a separate network node. It is strongly
    recommended to consider hardware network bonding for both cloud controllers and
    network nodes for performance and resiliency reasons.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他OpenStack服务不同，Neutron服务被认为更为复杂，原因在于其通过OpenStack版本的演变及其先进的虚拟网络功能和扩展。如[*第1章*](B21716_01.xhtml#_idTextAnchor014)《重新审视OpenStack
    – 设计考虑》所示，网络容量和分段应提前规划，不仅要容纳更多的流量，还要利用虚拟网络的可扩展性。为此，设计上专门设置了网络节点来处理不同类型的租户和提供商。同时，主要的Neutron
    API将作为控制平面的一部分运行在控制节点上。随着每个OpenStack版本的发布，Neutron驱动程序和插件的数量也在增加；在Antelope版本中，支持大多数知名的SDN驱动机制，包括**openvswitch**、**OVN**、**linux
    bridge**、**l2population**和**baremetal**。提供虚拟网络功能的高级服务插件，如VPN、防火墙和路由，将在单独的网络节点上运行。强烈建议考虑对云控制器和网络节点进行硬件网络绑定，以提高性能和可靠性。
- en: Important note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Networking configuration through the **kolla-ansible** infrastructure code repository
    can take different forms depending on the networking features to be added. All
    mechanisms, drivers, and service plugins are listed in the Neutron file ( **/kolla-ansible/ansible/roles/neutron/defaults/main.yml**
    ). Neutron drivers such as Open vSwitch and OVN run on their own playbooks located
    in the same folder path, **/kolla-ansible/ansible/roles/openvswitch** and **/**
    **kolla-ansible/ansible/roles/ovn-controller** , respectively
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**kolla-ansible**基础设施代码库进行的网络配置，形式会根据要添加的网络功能而有所不同。所有机制、驱动程序和服务插件都列在Neutron文件中（**/kolla-ansible/ansible/roles/neutron/defaults/main.yml**）。如Open
    vSwitch和OVN等Neutron驱动程序运行在它们自己的playbook中，位于相同的文件夹路径下，分别是**/kolla-ansible/ansible/roles/openvswitch**和**/kolla-ansible/ansible/roles/ovn-controller**。
- en: Block storage service
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块存储服务
- en: 'Cinder’s main mission since its incubation within the OpenStack ecosystem has
    been to provide persistent storage to instances. Unlike ephemeral storage, the
    block storage concept has been included in OpenStack since Bexar’s early release
    and kept as part of the Nova project. However, since the Folsom release, it has
    been changed to its own service. There are many reasons for this separation; the
    main reason is the development of the Cinder architecture. Some of the block storage
    subcomponents are considered part of a control plane, including **cinder-api**
    and **cinder-scheduler** , as illustrated in the following figure:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Cinder自从在OpenStack生态系统中孵化以来，主要使命就是为实例提供持久化存储。与临时存储不同，块存储概念自Bexar版本早期发布以来就已包含在OpenStack中，并作为Nova项目的一部分持续存在。然而，自Folsom版本起，它已被独立为一个服务。造成这种分离的原因有很多，主要原因是Cinder架构的发展。一些块存储的子组件被视为控制平面的一部分，包括**cinder-api**和**cinder-scheduler**，如以下图所示：
- en: '![Figure 3.4 – Cinder control plane components](img/B21716_03_04.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – Cinder控制平面组件](img/B21716_03_04.jpg)'
- en: Figure 3.4 – Cinder control plane components
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – Cinder控制平面组件
- en: We will cover the block storage extensions in more depth in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Storage – Block, Object, and* *File Shares* .
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第5章*](B21716_05.xhtml#_idTextAnchor146)《OpenStack存储 – 块存储、对象存储与文件共享》中更深入地介绍块存储扩展。
- en: Important note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Block storage is not enabled by default in the current **kolla-ansible** infrastructure
    code repository. Setting **enable_cinder** to **'yes'** in the **/kolla-ansible/etc/kolla/globals.yml**
    file will enable created instances to store their disks in the associated volume.
    Supported Cinder storage backends are listed in the Cinder playbook defined in
    the **/kolla-ansible/ansible/roles/cinder/default/main.yml** file and referenced
    with the **cinder_backend** configuration stanza code. Make sure to prepare an
    initial LVM volume named **cinder-volumes** as the **kolla-ansible** wrapper will
    expect an existing volume group with the mentioned name as stated in the **globals.yml**
    file.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 **kolla-ansible** 基础架构代码库中默认未启用块存储。通过在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中将 **enable_cinder** 设置为 **'yes'**，可以启用已创建的实例将其磁盘存储在关联的卷中。支持的 Cinder 存储后端在 **/kolla-ansible/ansible/roles/cinder/default/main.yml**
    文件中定义的 Cinder 剧本中列出，并通过 **cinder_backend** 配置段代码引用。确保准备一个名为 **cinder-volumes**
    的初始 LVM 卷，因为 **kolla-ansible** 包装器将期望存在一个具有该名称的卷组，如 **globals.yml** 文件中所述。
- en: Object storage service
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对象存储服务
- en: Swift is made up of several subcomponents to manage objects, including accounts,
    containers, and objects. At the highest level, the main interface to interact
    with the rest of the object storage ring is the **swift-proxy-server** daemon,
    which will be running on the cloud controller instance(s).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Swift 由多个子组件组成，用于管理对象，包括账户、容器和对象。在最高层级，与其他对象存储环交互的主要接口是 **swift-proxy-server**
    守护进程，它将在云控制实例上运行。
- en: Important note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Swift is disabled by default in the current **kolla-ansible** infrastructure
    code repository. It can be enabled by setting **enable_swift** to **yes** in the
    **/** **kolla-ansible/etc/kolla/globals.yml** file.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 **kolla-ansible** 基础架构代码库中默认禁用 Swift。可以通过在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中将 **enable_swift** 设置为 **yes** 来启用它。
- en: File-sharing service
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件共享服务
- en: As we have aimed to provide a managed shared file service in our initial production
    draft, a few components of the Manila service will be part of the control plane,
    including **manila-api** and **manila-scheduler** . Like Cinder, the backend storage
    flavors of the file-sharing service can vary. We will cover more configuration
    details of the file-sharing service in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Storage – Block, Object, and* *File Shares* .
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们旨在提供一种托管的共享文件服务，在我们的初始生产草案中，Manila 服务的几个组件将成为控制平面的一部分，包括 **manila-api**
    和 **manila-scheduler**。与 Cinder 类似，文件共享服务的后端存储类型可能会有所不同。我们将在[*第5章*](B21716_05.xhtml#_idTextAnchor146)《OpenStack
    存储 – 块、对象和文件共享》中讨论更多关于文件共享服务的配置细节。
- en: Important note
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The file-sharing service is disabled by default in the current **kolla-ansible**
    infrastructure code repository. Enabling the Manila service can be done by adjusting
    the **enable_manila** setting to **yes** in the **/** **kolla-ansible/etc/kolla/globals.yml**
    file.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 **kolla-ansible** 基础架构代码库中默认禁用文件共享服务。通过调整 **/kolla-ansible/etc/kolla/globals.yml**
    文件中的 **enable_manila** 设置为 **yes**，可以启用 Manila 服务。
- en: Monitoring
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控
- en: As stated in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack
    – Design Considerations* , Ceilometer has been changed to act solely as a metric
    collector of resources running under the OpenStack command. The **ceilometer-api**
    component is part of the control plane. Considering the way Ceilometer functions,
    its agent-based architecture involves the existence of data collection agents
    that collect metric data via REST APIs and poll resources. In addition to **ceilometer-api**
    , a ceilometer polling agent also runs in the controller node with a central agent
    to poll all resources created by various tenants under the OpenStack infrastructure.
    A notification agent listening to the message bus to consume notification data
    emitted by agents can optionally run on the cloud controller node.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[*第1章*](B21716_01.xhtml#_idTextAnchor014)《重新审视OpenStack – 设计考虑》中所述，Ceilometer
    已被更改为仅作为运行在 OpenStack 命令下的资源的度量收集器。**ceilometer-api** 组件是控制平面的一部分。考虑到 Ceilometer
    的功能，它基于代理的架构涉及数据收集代理的存在，这些代理通过 REST API 收集度量数据并轮询资源。除了**ceilometer-api**，一个 ceilometer
    轮询代理还在控制节点上运行，并且有一个中央代理用于轮询由不同租户在 OpenStack 基础设施下创建的所有资源。一个监听消息总线以消费代理发出的通知数据的通知代理可以选择性地在云控制节点上运行。
- en: Important note
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Ceilometer service is disabled by default in the current **kolla-ansible**
    infrastructure code repository. Set **enable_ceilometer** to **yes** in the **/kolla-ansible/etc/kolla/globals.yml**
    file. The default playbook configuration of Ceilometer is located under the **/**
    **kolla-ansible/ansible/roles/ceilometer** directory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 **kolla-ansible** 基础设施代码仓库默认禁用 Ceilometer 服务。在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中将 **enable_ceilometer** 设置为 **yes**。Ceilometer 的默认 playbook 配置位于 **/** **kolla-ansible/ansible/roles/ceilometer**
    目录下。
- en: Alarming
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 报警
- en: Alarming was originally designed as part of the Ceilometer features and was
    decoupled to run on its own project with the code name **Aodh** . The alarming
    components can live within the cloud controller node, which includes additional
    components for alarm evaluation, a notifier, and a state database.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 报警最初作为 Ceilometer 功能的一部分设计，后来被解耦并独立运行，代号为 **Aodh**。报警组件可以存在于云控制节点内，包含用于报警评估、通知以及状态数据库的附加组件。
- en: Important note
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Aodh service is disabled by default in the current **kolla-ansible** infrastructure
    code repository. Set **enable_aodh** to **yes** in the **/kolla-ansible/etc/kolla/globals.yml**
    file. The default playbook configuration of Aodh is located under the **/** **kolla-ansible/ansible/roles/aodh**
    directory.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 **kolla-ansible** 基础设施代码仓库默认禁用 Aodh 服务。在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中将 **enable_aodh** 设置为 **yes**。Aodh 的默认 playbook 配置位于 **/** **kolla-ansible/ansible/roles/aodh**
    目录下。
- en: Dashboard service
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仪表盘服务
- en: Horizon runs behind an Apache web server instance that serves as a frontend
    to expose OpenStack services in a dashboard, but not all non-core services are
    reflected in the dashboard by default. Horizon needs to be configured by enabling
    directives for certain services and features, such as VPN as a service. Horizon
    is based on the Python Django framework, which could be overutilized if the number
    of end users keeps increasing and requests are performed excessively. Scaling
    the dashboard is something one should expect, especially if most of the supported
    directives for additional services are enabled and exposed to a wider audience.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Horizon 在一个 Apache Web 服务器实例后端运行，作为前端暴露 OpenStack 服务的仪表盘，但默认情况下并不是所有非核心服务都会反映在仪表盘中。Horizon
    需要通过启用特定服务和功能的指令来进行配置，例如 VPN 服务。Horizon 基于 Python Django 框架，如果终端用户数量不断增加且请求过多，可能会出现过度利用的情况。特别是当启用并将大多数附加服务的指令暴露给更多用户时，仪表盘的扩展性是一个必须考虑的问题。
- en: Important note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Horizon is counted as a core service in the **kolla-ansible** infrastructure
    code repository by default. The **enable_openstack_core** stanza configuration
    is set to **yes** by default, which includes the Horizon service in the **/kolla-ansible/etc/kolla/globals.yml**
    file. In the same file, you can enable additional Horizon directives to be managed
    through the dashboard by commenting out the **enable_horizon_service** configuration
    line where the service refers to the additional OpenStack service. By default,
    all additional services supported by the dashboard are enabled in the Horizon
    playbook, as reflected in the **/** **kolla-ansible/ansible/roles/horizon/default/main.yml**
    file.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Horizon 被视为 **kolla-ansible** 基础设施代码仓库中的核心服务。**enable_openstack_core**
    配置项默认设置为 **yes**，这意味着 Horizon 服务会被包含在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中。在同一个文件中，你可以通过注释掉 **enable_horizon_service** 配置行，来启用额外的 Horizon 指令，从而通过仪表盘管理该服务，该服务指向附加的
    OpenStack 服务。默认情况下，仪表盘支持的所有附加服务都已启用，并在 Horizon playbook 中反映，具体内容见 **/** **kolla-ansible/ansible/roles/horizon/default/main.yml**
    文件。
- en: As we have covered most of the OpenStack services that will be part of the control
    plane, the next section will iterate through the non-OpenStack services counted
    as shared services.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经涵盖了大部分将作为控制平面一部分的 OpenStack 服务，接下来的部分将介绍那些被视为共享服务的非 OpenStack 服务。
- en: Shared services – infrastructure services
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享服务 – 基础设施服务
- en: In [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting OpenStack –
    Cloud Design Considerations* , we identified shared services as being non-OpenStack
    services. Shared services, such as queuing messages, databases, and, optionally,
    the caching layer, are the most critical ones besides other core services.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 1 章*](B21716_01.xhtml#_idTextAnchor014)《重新审视 OpenStack - 云设计考虑》中，我们将共享服务定义为非
    OpenStack 服务。共享服务，例如消息队列、数据库和（可选的）缓存层，除了其他核心服务外，是最关键的服务之一。
- en: Message queue
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消息队列
- en: As part of the control plane, RabbitMQ is the most commonly used messaging service
    in OpenStack. Dedicating separate nodes for the queue is highly recommended but
    is not a *must-have* . Vast deployment based on multi-regional OpenStack environments
    is a suitable case to run in a dedicated RabbitMQ cluster due to the meshed requests
    of different services, including compute, storage, and networks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为控制平面的一部分，RabbitMQ 是 OpenStack 中最常用的消息服务。强烈建议为队列专门分配单独的节点，但这不是必须的。基于多区域 OpenStack
    环境的广泛部署是运行专用 RabbitMQ 集群的合适案例，因为它需要处理不同服务（包括计算、存储和网络）的网状请求。
- en: The other overlooked aspect of the queuing message is the security requirement.
    RabbitMQ sits between most OpenStack services as a communication hub. It is important
    to keep message exchanges secured. RabbitMQ supports TLS, which protects subscribers
    and clients from tampered-with messages and forces encryption in transit.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 队列消息的另一个被忽视的方面是安全需求。RabbitMQ 作为大多数 OpenStack 服务之间的通信中心存在。保持消息交换的安全性至关重要。RabbitMQ
    支持 TLS，可以保护订阅者和客户端免受篡改的消息，并强制在传输中进行加密。
- en: Important note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The RabbitMQ TLS option is disabled by default in the **kolla-ansible** infrastructure
    code repository. It can be enabled by setting **rabbitmq_enable_tls** to **yes**
    in the **/kolla-ansible/etc/kolla/globals.yml** file. Additional RabbitMQ configurations
    can be adjusted in its associated playbook, such as message and queue expiration,
    which are located in the **/** **kolla-ansible/ansible/roles/rabbitmq/default/main.yml**
    file.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ TLS 选项在 **kolla-ansible** 基础设施代码库中默认为禁用状态。可以通过在 **/kolla-ansible/etc/kolla/globals.yml**
    文件中将 **rabbitmq_enable_tls** 设置为 **yes** 来启用它。其他 RabbitMQ 配置可以在其关联的 playbook 中进行调整，例如消息和队列过期时间，在
    **kolla-ansible/ansible/roles/rabbitmq/default/main.yml** 文件中设置。
- en: Database
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据库
- en: The other vital piece of the OpenStack puzzle ecosystem is the database holding
    updates of all service states and user-tracking information. MariaDB is considered
    the database engine as part of the control plane, made better with the Galera
    wrapper for clustering in multi-master cluster mode.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 拼图生态系统的另一个重要组成部分是存储所有服务状态和用户跟踪信息更新的数据库。作为控制平面的一部分，MariaDB 被视为数据库引擎，并通过
    Galera 包装器在多主集群模式下提供更好的性能。
- en: Important note
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: MariaDB clustering relies on HAProxy for database instance switching. More details
    will be discussed in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running
    a Highly Available Cloud – Meeting the SLA* . The default database engine is configured
    with MariaDB in the **kolla-ansible** infrastructure code repository.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: MariaDB 集群依赖于 HAProxy 进行数据库实例切换。更多细节将在 [*第 7 章*](B21716_07.xhtml#_idTextAnchor174)，*运行高可用云
    – 满足 SLA* 中讨论。默认数据库引擎配置为 MariaDB 在 **kolla-ansible** 基础设施代码库中。
- en: Caching
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓存
- en: Caching comes as an optional piece of our control plane. However, it is strongly
    recommended to begin with caching due to its light footprint in cloud controller
    utilization. Memcached is becoming a standard to provide ephemeral data between
    different OpenStack services, such as API interaction information between the
    identity service and the rest of the services demanding token validation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存作为我们控制平面的可选部分。然而，由于其在云控制器利用率中的轻量级足迹，强烈建议从缓存开始。Memcached 正在成为提供身份验证令牌验证服务间
    API 交互信息等 OpenStack 服务之间的临时数据的标准。
- en: Important note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Memcached is enabled in the **kolla-ansible** infrastructure code repository
    by default. The maximum cached memory size and number of connections can be set
    in the Memcached playbook located in the **/** **kolla-ansible/ansible/roles/memcached/default/main.yml**
    file.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Memcached 在 **kolla-ansible** 基础设施代码库中默认启用。可以在位于 **kolla-ansible/ansible/roles/memcached/default/main.yml**
    文件中的 Memcached playbook 中设置最大缓存内存大小和连接数。
- en: Identifying the control plane of OpenStack is an essential step before starting
    the deployment of the private cloud. That will help to identify which design layout
    will fit in your environment to ensure high availability as the highest priority.
    The next section will iterate through the next design draft scoped around the
    OpenStack control plane.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始部署私有云之前，识别 OpenStack 的控制平面是一个重要的步骤。这将有助于确定哪种设计布局适合您的环境，以确保高可用性作为最高优先级。接下来的部分将围绕
    OpenStack 控制平面的设计草案进行迭代。
- en: Arming the control plane – prepare for failure
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 武装控制平面 – 为故障做准备
- en: The composition of different components around the OpenStack control plane requires
    more effort to keep them up and running with maximum resiliency. There are a variety
    of ways to achieve maximum availability for the control plane, but they should
    be prepared and tested in advance. Within the latest releases of OpenStack, several
    tools have been adopted to ensure a scalable and highly available control plane.
    Another approach is simply reusing some patterns that already exist, such as highly
    available database layouts or messaging bus clustering. We will discuss each of
    the OpenStack layers’ high availability in more depth in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* . It is essential, at this
    level, to define a strategy to empower the control plane from both scalability
    and availability perspectives.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 控制平面周围不同组件的组合需要更多的努力以保持它们正常运行并具有最大韧性。有多种方法可以实现控制平面的最大可用性，但这些方法应提前准备并测试。在
    OpenStack 的最新版本中，已采用多个工具以确保控制平面的可扩展性和高可用性。另一种方法是简单地重用一些已存在的模式，例如高可用的数据库布局或消息总线集群。在[*第
    7 章*](B21716_07.xhtml#_idTextAnchor174)，《运行高可用云——满足 SLA》中，我们将更深入地讨论 OpenStack
    各个层级的高可用性。在这一层次，定义一种策略来从可扩展性和可用性角度增强控制平面是至关重要的。
- en: 'Both terms should be considered from the first iteration of the design plan
    by addressing the following questions:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个术语应该从设计计划的第一阶段开始考虑，通过回答以下问题：
- en: How can a control plane respond to a large number of API requests?
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面如何应对大量的 API 请求？
- en: How can hardware failure be faced and resolved in the least amount of time?
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何以最短的时间面对和解决硬件故障？
- en: Which design patterns can be used to ensure a stable and resilient control plane?
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些设计模式可以确保控制平面的稳定性和韧性？
- en: How can we handle the growth of infrastructure services such as databases and
    messaging buses?
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何处理基础设施服务（如数据库和消息总线）的增长？
- en: 'The OpenStack ecosystem has been designed to scale massively. Armed with its
    modular architecture, designing the control plane for resiliency is an open door
    for different ways of implementation. A summary of possible known design options
    to prepare for a successful control plane deployment is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 生态系统已经设计成可以大规模扩展。凭借其模块化架构，设计具有韧性的控制平面为不同实现方式提供了一个开放的门。为成功部署控制平面做好准备的已知设计选项总结如下：
- en: '**Active/active** : All cluster nodes participate in processing requests. This
    pattern is mainly associated with an *asymmetric clustering* layout whereby an
    entry point such as a load balancer distributes the requests across active control
    plane services. An active/active setup increases the performance and scalability
    of the cloud infrastructure by distributing the load across an army of nodes.
    Automating the subscriptions or discarding a node can be orchestrated with a load
    balancer and does not necessarily require a resource manager depending on the
    load balancer’s capabilities.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动/主动**：所有集群节点参与处理请求。这个模式主要与*非对称集群*布局相关，其中一个入口点（例如负载均衡器）将请求分发到活动的控制平面服务。主动/主动配置通过将负载分配到多个节点上，增加了云基础设施的性能和可扩展性。通过负载均衡器自动化订阅或丢弃节点的操作可以通过负载均衡器进行编排，根据负载均衡器的能力，可能不需要资源管理器。'
- en: '**Active/passive** : Unlike the previous mode, active nodes serve requests
    while standby nodes take over only when one or several active nodes fail. Passive
    control plane nodes are referred to as *sleepy watchers* and do not require an
    additional frontend point such as a load balancer, unlike in active/active mode.
    Such designs are associated mainly with *symmetric clustering* whereby resource
    manager software evaluates and ensures that the active control plane component
    is the only one serving the requests at any point in time.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动/被动**：与之前的模式不同，活动节点处理请求，而备用节点仅在一个或多个活动节点失败时接管。被动控制平面节点被称为*懒惰观察者*，不像主动/主动模式中那样需要额外的前端点（如负载均衡器）。这种设计主要与*对称集群*相关，在这种模式下，资源管理器软件评估并确保在任何时候只有一个活动的控制平面组件在处理请求。'
- en: Important note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The **kolla-ansible** infrastructure code repository comes with predefined roles
    for a highly available control plane defined in the **globals.yml** file with
    the **'hacluster'** stanza configuration. Most of the adopted clustering software
    tools within the OpenStack control plane services, including the Antelope release,
    use a few **HAProxy** instances for load balancing and Keepalived for dynamic
    cluster node health checks based on the concept of **Virtual IP** ( **VIP** )
    addresses. The HAProxy playbook comes with the latest customizable configuration
    settings located under the **/kolla-ansible/ansible/roles/loadbalancer/** directory.
    [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly Available
    Cloud – Meeting the SLA* , will cover the HAProxy and Keepalived setup in more
    detail to deploy a highly available OpenStack control plane.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible** 基础设施代码库带有预定义的角色，用于高可用控制平面的配置，这些角色定义在 **globals.yml** 文件中的
    **''hacluster''** 配置段落中。大多数采用的 OpenStack 控制平面服务集群软件工具，包括 Antelope 版本，都使用若干 **HAProxy**
    实例进行负载均衡，并基于 **虚拟 IP**（**VIP**）地址的概念使用 Keepalived 进行动态集群节点健康检查。HAProxy 的剧本带有最新的可自定义配置设置，位于
    **/kolla-ansible/ansible/roles/loadbalancer/** 目录下。[*第 7 章*](B21716_07.xhtml#_idTextAnchor174)《运行高可用云–满足
    SLA》将更详细地介绍 HAProxy 和 Keepalived 的设置，以部署高可用的 OpenStack 控制平面。'
- en: Designing for control plane failure would depend on the target **Service-Level
    Agreement** ( **SLA** ) and available options within your infrastructure. Active/active
    mode could be a *best-effort* scalable and highly available implementation but
    would cost extra by dedicating either a hardware or software appliance to assign
    the load-balancing role of the cluster. The choice of database is another dilemma
    that most cloud architects and operators pay special attention to due to complexity
    and performance concerns. For example, an organization’s policy dictates the usage
    of the Oracle database engine. Setting an active-active database setup might not
    be straightforward to address control plane high-availability requirements because
    that might require more specific tools and additional integration, unlike using
    other engines such as MySQL Galera.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面故障设计将依赖于目标**服务级别协议**（**SLA**）以及在您的基础设施内可用的选项。主动/主动模式可能是一种*尽力而为*的可扩展和高可用性实现，但通过专用硬件或软件设备来分配集群的负载均衡角色会产生额外成本。数据库的选择是另一个大多数云架构师和运维人员特别关注的难题，因为其复杂性和性能问题。例如，某组织的政策要求使用
    Oracle 数据库引擎。设置主动-主动数据库配置可能无法直接解决控制平面高可用性要求，因为这可能需要更多的特定工具和额外的集成，而使用其他引擎如 MySQL
    Galera 则可能会更简单。
- en: Planning for more
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多的规划
- en: Relying on the CI/CD tool as a source of truth is a best practice to keep a
    consistent control plane services cluster. As we will cover in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting the SLA* , adding a new cloud controller
    node should be done transparently without disturbing the running API services.
    That is another valid reason to develop more integration tests of service consistency
    during a new control plane node deployment in a separate environment. A monitoring
    system should feed back the capacity, service availability, and resource usage
    to ensure a successful production day if all is as expected.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于 CI/CD 工具作为真实来源是一种最佳实践，可以保持控制平面服务集群的一致性。正如我们在[*第 7 章*](B21716_07.xhtml#_idTextAnchor174)《运行高可用云–满足
    SLA》中所述，添加新的云控制器节点应该是透明进行的，不会打扰到正在运行的 API 服务。这也是在独立环境中开发更多服务一致性集成测试的有效理由，以确保新控制平面节点部署的顺利进行。监控系统应反馈容量、服务可用性和资源使用情况，以确保如果一切如预期那样，生产环境的成功运行。
- en: The process of deploying an additional cloud controller node to empower the
    control plane can be applied the same way when joining additional control plane
    services. Deploying more OpenStack services will expose more APIs to interact
    with the other nodes and handle more user or service requests. This can increase
    the load by hundreds of API requests per minute, and undoubtedly more CPU, memory,
    networking, and disk will be consumed. Raising performance and scalability concerns
    for such matters should not limit the growth of your OpenStack cloud services
    portfolio as there are several ways to address such a challenge. As long as automation
    is preserved throughout all operations tasks, the deployment of a new OpenStack
    component that counts as a new control plane service should pass through the CI/CD
    stages. The new component should pass through the testing and development environments
    and send feedback with a few metrics such as capacity and usage limits before
    promoting it to the production environment. A design decision can be made afterward,
    for example, by running the new component in the same cloud controller cluster
    or dedicating a separate physical or virtual node to run.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 部署额外的云控制器节点以增强控制平面的过程可以在加入额外控制平面服务时以相同方式应用。部署更多的 OpenStack 服务将暴露更多的 API 以与其他节点交互，并处理更多的用户或服务请求。这可能会导致每分钟数百个
    API 请求的负载增加，毫无疑问，更多的 CPU、内存、网络和磁盘将被消耗。在这种情况下，关于性能和可扩展性的担忧不应限制 OpenStack 云服务组合的增长，因为有多种方式可以解决这一挑战。只要在所有操作任务中保持自动化，作为新控制平面服务的
    OpenStack 组件部署应通过 CI/CD 阶段。新组件应通过测试和开发环境，并在提升到生产环境之前发送包含容量和使用限制等一些指标的反馈。之后可以做出设计决策，例如，将新组件运行在同一个云控制器集群中，或将其分配给一个单独的物理或虚拟节点运行。
- en: The addition of a new API service as part of the control plane should be reflected
    across the nodes running the specific service role. That is where the CI/CD system
    will shine and ensure consistency when dealing with the exponential growth of
    the infrastructure in terms of nodes and services.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 作为控制平面一部分的新 API 服务的添加应在运行特定服务角色的节点上反映出来。CI/CD 系统在此时发挥作用，确保在节点和服务的基础设施呈指数级增长时保持一致性。
- en: Important note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A few other OpenStack services will be covered and integrated in the next chapters
    as part of the control plane, such as **Magnum** and **Zun** in [*Chapter 4*](B21716_04.xhtml#_idTextAnchor125)
    , *OpenStack Compute – Compute Capacity and Flavors* ; LBaaSv2 in [*Chapter 6*](B21716_06.xhtml#_idTextAnchor159)
    , *OpenStack Networking – Connectivity and Managed Service Options* ; and Watcher
    in [*Chapter 9*](B21716_09.xhtml#_idTextAnchor204) , *Benchmarking the Infrastructure
    – Evaluating Resource Capacity* *and Optimization* .
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，将涵盖并集成一些其他的 OpenStack 服务，作为控制平面的一部分，例如 [*第 4 章*](B21716_04.xhtml#_idTextAnchor125)中的
    **Magnum** 和 **Zun**，*OpenStack 计算 – 计算能力与规格*；[*第 6 章*](B21716_06.xhtml#_idTextAnchor159)中的
    LBaaSv2，*OpenStack 网络 – 连接性与托管服务选项*；以及 [*第 9 章*](B21716_09.xhtml#_idTextAnchor204)中的
    Watcher，*基础设施基准测试 – 评估资源容量* *和优化*。
- en: Extending the deployment
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展部署
- en: 'Once different services that construct the control plane have been identified,
    it is a good time to update the initial design draft. An ideal way to start deployment
    with the upcoming production stage in mind is to assign each physical node a set
    of roles to run different OpenStack services. That brings us to the following
    configuration:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦识别出构成控制平面的不同服务，便是更新初始设计草稿的好时机。为了在即将到来的生产阶段中理想地开始部署，最佳方式是为每个物理节点分配一组角色来运行不同的
    OpenStack 服务。这将引导我们进入以下配置：
- en: '**Cloud controller node** : This runs the control plane services including
    OpenStack APIs, identity, imaging, file sharing, telemetry, and dashboard services.
    Shared services, including a database with a MySQL Galera cluster, the RabbitMQ
    messaging service, and Memcached, will be part of the assigned cloud controller
    node role.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云控制器节点**：该节点运行包括 OpenStack API、身份认证、镜像、文件共享、遥测和仪表盘服务在内的控制平面服务。共享服务，包括 MySQL
    Galera 集群数据库、RabbitMQ 消息服务和 Memcached，将成为分配给云控制器节点角色的一部分。'
- en: '**Compute node** : This runs KVM as the main hypervisor with Nova compute and
    networking agent services.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算节点**：该节点将 KVM 作为主要的虚拟化平台，运行 Nova 计算和网络代理服务。'
- en: '**Network node** : This runs Neutron agent services, including **layer3** ,
    **lbaas** , **dhcp** , and plugin agents.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络节点**：该节点运行 Neutron 代理服务，包括**Layer3**、**LBaaS**、**DHCP** 和插件代理。'
- en: '**Storage node** : This runs Cinder volume services interfacing with the Cinder
    scheduler to provide block storage resources.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储节点** : 运行 Cinder 卷服务，与 Cinder 调度器交互，提供块存储资源。'
- en: '**Deployer node** : This runs the orchestration deployment toolchain, including
    Ansible, the Kolla builder, and the CI server. The deployer node will optionally
    host a local private Docker registry.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署节点** : 运行编排部署工具链，包括 Ansible、Kolla 构建器和 CI 服务器。部署节点还可以选择托管本地私有 Docker 注册表。'
- en: 'The updated design draft exposes a multi-node OpenStack environment, as shown
    in the following diagram:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的设计草案展示了一个多节点 OpenStack 环境，如下图所示：
- en: '![Figure 3.5 – A multi-node OpenStack deployment layout](img/B21716_03_05.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 多节点 OpenStack 部署布局](img/B21716_03_05.jpg)'
- en: Figure 3.5 – A multi-node OpenStack deployment layout
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 多节点 OpenStack 部署布局
- en: Important note
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Depending on the availability of your hardware, the network and storage roles
    can be assigned to the cloud controller node. Use the monitoring metrics data
    to decide in advance the right moment when those services should be installed
    in separate physical nodes. Splitting storage and network services is recommended
    when dealing with the exponential growth of the OpenStack infrastructure usage.
    If you are planning to run the nodes in a virtual environment using tools such
    as VMware ESXi or VirtualBox, make sure to have the hardware virtualization extensions
    installed in addition to the virtual networking configuration.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 根据硬件的可用性，网络和存储角色可以分配给云控制器节点。使用监控指标数据提前决定在何时将这些服务安装到独立的物理节点上。当处理 OpenStack 基础设施使用量的指数增长时，建议分离存储和网络服务。如果计划在虚拟环境中运行节点，使用
    VMware ESXi 或 VirtualBox 等工具时，确保安装硬件虚拟化扩展，并配置虚拟网络。
- en: Initializing the network
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化网络
- en: As depicted in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014) , *Revisiting
    OpenStack – Design Considerations* , a few networks should exist and be prepared
    in advance. Preparing the physical networking beforehand is essential as part
    of the extension of the production layout in the next iterations. The proposed
    standard networks in our initial draft include management, tenant, storage, and
    external networks connecting different nodes in our initial cluster layout.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[*第 1 章*](B21716_01.xhtml#_idTextAnchor014)中所示，*重新审视 OpenStack – 设计考虑*，应该提前准备一些网络。提前准备物理网络是必需的，作为下一个迭代中生产布局扩展的一部分。我们初步草案中提出的标准网络包括管理、租户、存储和外部网络，连接我们初始集群布局中的不同节点。
- en: 'The following interfaces will be considered:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下接口将被考虑：
- en: '**eth0** : The management interface'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eth0** : 管理接口'
- en: '**eth1** : The overlay and tenant interface'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eth1** : 覆盖和租户接口'
- en: '**eth2** : The external interface'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eth2** : 外部接口'
- en: '**eth3** : The storage interface'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eth3** : 存储接口'
- en: 'As shown in the following diagram, four distinct types of traffic are considered
    for the OpenStack networking setup, including management, tenant and overlay,
    storage, and external. Note that a network node is dedicated to managing various
    OpenStack networking services that would increase the performance with fewer chances
    of network saturation at the cloud controller node level:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，OpenStack 网络设置考虑了四种不同类型的流量，包括管理、租户和覆盖、存储以及外部网络。请注意，一个网络节点专门用于管理各种 OpenStack
    网络服务，这样可以提高性能，并减少云控制器节点层级的网络饱和风险。
- en: '![Figure 3.6 – Physical network segmentation layout](img/B21716_03_06.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 物理网络分段布局](img/B21716_03_06.jpg)'
- en: Figure 3.6 – Physical network segmentation layout
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 物理网络分段布局
- en: 'At the switching layer, networks are organized logically through the VLANs,
    where each network is assigned a VLAN ID coupled with the assigned network IP
    subnetwork address pool. The following table provides an example of initial network
    mapping given a reserved IP pool of **10.0.0.0/8** :'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在交换层，网络通过 VLAN 进行逻辑组织，每个网络都分配了一个 VLAN ID，并与分配的网络 IP 子网地址池关联。下表提供了给定保留 IP 池 **10.0.0.0/8**
    的初始网络映射示例：
- en: '| **Network name** | **IP Pool** | **VLAN ID** |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **网络名称** | **IP 池** | **VLAN ID** |'
- en: '| Management | **10.0.0.0/24** | **100** |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 管理 | **10.0.0.0/24** | **100** |'
- en: '| Overlay and tenant | **10.10.0.0/24** | **200** |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 覆盖和租户 | **10.10.0.0/24** | **200** |'
- en: '| External | **10.20.0.0/24** | **300** |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 外部 | **10.20.0.0/24** | **300** |'
- en: '| Storage | **10.30.0.0/24** | **400** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 存储 | **10.30.0.0/24** | **400** |'
- en: Table 3.1 – The OpenStack network IP addresses
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 – OpenStack 网络 IP 地址
- en: Important note
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: When managing OpenStack packages internally, the management interface does not
    require outbound access to the external network to reach the internet to download
    packages. The deployer instance can be connected to the management network within
    the same VLAN to install the required packages directly. External network access
    can be limited to the deployer instance and tenant network for future instances
    of external access.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '在内部管理 OpenStack 包时，管理界面不需要外部网络的出站访问来下载包。部署实例可以连接到相同 VLAN 内的管理网络，以便直接安装所需的包。外部网络访问可以限制为部署实例和租户网络，以便未来需要外部访问的实例。  '
- en: As we are dealing with OpenStack services running in Docker containers, it is
    essential to take the created virtual network components under the hood into consideration.
    Once the OpenStack environment is deployed, Kolla will use the Neutron networking
    model to create a few bridges, ports, and namespaces across each node in the cluster.
    As we will see in the next configuration section, Kolla maps each network interface
    to a Neutron physical network denoted as **physnet** . Each container bridge created
    by Neutron is defined in the **neutron_bridge_name** stanza code configuration
    to set the default Kolla naming convention stated as **br_ex** . As mentioned
    earlier, the **kolla-ansible** deployment mode is highly customizable, so we can
    assign meaningful names for each bridge mapped to each network interface, respectively.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '由于我们处理的是在 Docker 容器中运行的 OpenStack 服务，因此必须考虑底层创建的虚拟网络组件。一旦部署了 OpenStack 环境，Kolla
    将使用 Neutron 网络模型在集群中的每个节点上创建一些桥接、端口和命名空间。正如我们将在下一部分配置中看到的，Kolla 会将每个网络接口映射到一个
    Neutron 物理网络，称为 **physnet**。Neutron 创建的每个容器桥接在 **neutron_bridge_name** 配置段中定义，以设置默认的
    Kolla 命名约定，命名为 **br_ex**。如前所述，**kolla-ansible** 部署模式高度可定制，因此我们可以为每个桥接分配有意义的名称，分别映射到每个网络接口。  '
- en: Important note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '重要提示  '
- en: The assigned network interface will be used by Neutron to access the public
    network to allow tenants to create routers and assign floating IPs to workloads
    requiring internet access defined in the Kolla **/etc/kolla/gloabls.yml** configuration
    file by assigning the **neutron_external_interface** setting to the dedicated
    interface, **eth2** , on the compute node.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '分配的网络接口将由 Neutron 用于访问公共网络，以便允许租户创建路由器并为需要互联网访问的工作负载分配浮动 IP，配置文件 **/etc/kolla/globals.yml**
    中定义的 **neutron_external_interface** 设置将被分配到计算节点的专用接口 **eth2** 上。  '
- en: The network bonding layout is highly recommended to be configured in advance
    for each host. As discussed earlier, the bonding configuration depends on the
    tools and running software when using Linux as an operating system that varies
    from one distribution to another. Commonly used bonding configurations are the
    native Linux bonds and Open vSwitch.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '强烈建议为每个主机预先配置网络绑定布局。如前所述，绑定配置取决于工具和运行软件，当使用 Linux 操作系统时，不同的发行版有所不同。常用的绑定配置包括本地
    Linux 绑定和 Open vSwitch。  '
- en: Important note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '重要提示  '
- en: 'Bonding configuration is out of the scope of this book. A good source of bonding
    configuration for Ubuntu using the kernel module can be found here: [https://help.ubuntu.com/community/UbuntuBonding](https://help.ubuntu.com/community/UbuntuBonding
    )'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '本书不涉及绑定配置。关于使用内核模块在 Ubuntu 上进行绑定配置的良好参考资料可以在此找到：[https://help.ubuntu.com/community/UbuntuBonding](https://help.ubuntu.com/community/UbuntuBonding)  '
- en: Listing the cluster specs
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '列出集群规格  '
- en: 'The initial draft of the physical layout in [*Chapter 1*](B21716_01.xhtml#_idTextAnchor014)
    , *Revisiting OpenStack – Design Consideration* , can be considered as a starting
    point to pick up our hardware specs for each host. With slight differences between
    host roles, the following table summarizes a typical production hardware and software
    setup for each host:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 1 章*](B21716_01.xhtml#_idTextAnchor014)《重新审视 OpenStack——设计考虑》中的物理布局初稿可以作为我们为每个主机选择硬件规格的起点。尽管主机角色之间存在细微差异，以下表格总结了每个主机的典型生产硬件和软件设置：'
- en: '| **Hostname** | **CPU** **cores** | **RAM** | **Disk** | **Network** | **Software**
    | **Role** |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **主机名** | **CPU** **核心数** | **内存** | **磁盘** | **网络** | **软件** | **角色** |  '
- en: '| **cc01.os** | 8 | 128 GB | 250 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH, Python
    2.7 or greater, NTP client | Cloud controller |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| **cc01.os** | 8 | 128 GB | 250 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH, Python
    2.7 或更高版本, NTP 客户端 | 云控制器 |  '
- en: '| **cn01.os** | 12 | 240 GB | 500 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH,
    Python 2.7 or greater, NTP client | Compute node |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| **cn01.os** | 12 | 240 GB | 500 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH,
    Python 2.7 或更高版本, NTP 客户端 | 计算节点 |  '
- en: '| **net01.os** | 4 | 32 GB | 250 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH, Python2.7
    or greater, NTP client | Network node |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| **net01.os** | 4 | 32 GB | 250 GB | 4 x 10 GB | Ubuntu 22 LTS, OpenSSH, Python2.7
    或更高版本, NTP 客户端 | 网络节点 |'
- en: '| **storage01.os** | 4 | 32 GB | 1 TB | 4x10 GB | Ubuntu 22 LTS, OpenSSH, python2.7
    or greater, NTP client | Storage Node |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| **storage01.os** | 4 | 32 GB | 1 TB | 4x10 GB | Ubuntu 22 LTS, OpenSSH, python2.7
    或更高版本, NTP 客户端 | 存储节点 |'
- en: Table 3.2 – OpenStack hosts configurations
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2 – OpenStack 主机配置
- en: Note that OpenStack is designed to run on commodity hardware. The table did
    not mention any precise hardware model that would depend on the set budget and
    resources to kick off a first setup. As we planned, in the first chapter, to accommodate
    200 instances with the *small* flavor during our first deployment, the production
    setup should be capable of being expanded with more hardware and scale horizontally
    for each layer of the OpenStack ecosystem. The **undercloud** monitoring tools
    should regularly gather data on the performance and usage of each node to help
    in deciding the right sizing and additional resources needed to be plugged in.
    The first hosts and hardware listings should not welcome any production workload
    before arming the cluster with a layer of redundancy that will be discussed in
    detail in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174) , *Running a Highly
    Available Cloud – Meeting* *the SLA* .
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OpenStack 是设计用于运行在普通硬件上的。表格中没有提到任何具体的硬件型号，这取决于启动第一个配置时的预算和资源。如我们所计划，在第一章中，为了容纳
    200 个实例，我们在首次部署时将使用 *小型* 配置，生产环境的设置应该能够通过增加更多硬件来扩展，并且每一层 OpenStack 生态系统都能水平扩展。**undercloud**
    监控工具应定期收集每个节点的性能和使用情况数据，以帮助决定所需的正确配置和需要添加的资源。首次主机和硬件列表在为集群添加冗余层之前不应承担任何生产负载，冗余层将在
    [*第七章*](B21716_07.xhtml#_idTextAnchor174) 中详细讨论，*构建高可用云 – 满足 SLA*。
- en: Preparing the infrastructure code
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备基础设施代码
- en: Using the same **kolla-ansible** repository, we can customize the setup of different
    OpenStack services, including the control plane services, in the same way as highlighted
    in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking Off the OpenStack
    Setup – The Right* *Way (DevSecOps)* .
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的 **kolla-ansible** 仓库，我们可以像 [*第二章*](B21716_02.xhtml#_idTextAnchor089) 中所述，*启动
    OpenStack 配置 – 正确的方法（DevSecOps）* 一样，自定义不同 OpenStack 服务（包括控制平面服务）的设置。
- en: As a best practice, use a separate branch before merging to master and then
    start the deployment into the production environment.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最佳实践，在合并到主分支之前使用一个独立的分支，然后开始部署到生产环境。
- en: Configuring the host groups
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置主机组
- en: 'The **kolla-ansible** infrastructure code defines the target hosts and associated
    parameters such as hostnames and network IP addresses in the **/ansible/inventory**
    directory. The listing of host groups can be customized based on roles associated
    with the initial draft. The format of an inventory file is straightforward as
    highlighted in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking Off
    the OpenStack Setup – The Right Way (DevSecOps)* . The following table defines
    the different host groups in our next deployment iterations:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible** 基础设施代码定义了目标主机和相关参数，如主机名和网络 IP 地址，这些信息存储在 **/ansible/inventory**
    目录下。主机组的列出可以根据与初步草稿相关的角色进行定制。清单文件的格式简单明了，如 [*第二章*](B21716_02.xhtml#_idTextAnchor089)
    中所述，*启动 OpenStack 配置 – 正确的方法（DevSecOps）*。下表定义了我们在下一次部署迭代中使用的不同主机组：'
- en: '| **OpenStack Kolla – Ansible** **host groups** | **OpenStack** **node role**
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **OpenStack Kolla – Ansible** **主机组** | **OpenStack** **节点角色** |'
- en: '| **control** | Runs most of the OpenStack control plane, including core service
    APIs, database, and messaging queue services |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **control** | 运行大部分 OpenStack 控制平面服务，包括核心服务 API、数据库和消息队列服务 |'
- en: '| **compute** | Hosts running the **nova-compute** service |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| **compute** | 运行 **nova-compute** 服务的主机 |'
- en: '| **network** | Dedicated to network nodes running Neutron plugins and agents
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| **network** | 专用于运行 Neutron 插件和代理的网络节点 |'
- en: '| **haproxy** | Target hosts running HAProxy |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| **haproxy** | 运行 HAProxy 的目标主机 |'
- en: '| **storage** | Runs the Cinder volume service |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **storage** | 运行 Cinder 卷服务 |'
- en: '| **monitoring** | Dedicated host(s) for monitoring OpenStack servers running
    Prometheus and Grafana |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **monitoring** | 专用主机，用于监控运行 Prometheus 和 Grafana 的 OpenStack 服务器 |'
- en: '| **logging** | Runs and processes logs for different OpenStack services |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **logging** | 运行并处理不同 OpenStack 服务的日志 |'
- en: '| **deployment** | Runs the deployment of the infrastructure code using Ansible
    and Kolla |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **deployment** | 使用 Ansible 和 Kolla 运行基础设施代码的部署 |'
- en: '| **object** | Runs the Swift object storage server’s instances |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **对象** | 运行Swift对象存储服务器的实例 |'
- en: Table 3.3 – OpenStack Ansible host groups
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3 – OpenStack Ansible主机组
- en: The inventory file is highly customizable and can be adapted to your needs depending
    on whether you need to accommodate different services in a new role or separate
    more core services in dedicated hosts. Our next step will consider the deployment
    of a multi-node setup, including one dedicated host for each cloud controller,
    compute, storage, and network role, respectively. This deployment should not be
    counted as production-ready as each role will be deployed in a single node that
    does not yet meet the high-availability aspect. We will cover high availability
    and fault tolerance in OpenStack deployment in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud – Meeting* *the SLA* .
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 库存文件高度可定制，可以根据需要适应不同的服务，既可以为新角色容纳不同服务，也可以将更多的核心服务分配到专用主机上。我们的下一步将考虑多节点设置的部署，包括分别为每个云控制器、计算、存储和网络角色部署一个专用主机。此部署不应被视为生产就绪，因为每个角色将部署在单个节点上，该节点尚未满足高可用性的要求。我们将在[*第7章*](B21716_07.xhtml#_idTextAnchor174)中介绍OpenStack部署的高可用性和容错，*运行高可用云
    – 满足* *SLA*。
- en: 'Starting with the source of truth, the deployer node where the CI/CD chain
    lives, create a new branch dedicated to a staging environment:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 从真相源开始，在CI/CD链所在的部署节点上，创建一个新的分支，专用于暂存环境：
- en: '[PRE0]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Create a new file named **multi_packtpub_prod** in the cloned repository, **/ansible/inventory**
    . A simple way to assign different host groups from a default multi-node configuration
    file can be found in the same directory, **/ansible/inventory** , named **multinode**
    .
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在克隆的仓库中创建一个名为**multi_packtpub_prod**的新文件，路径为**/ansible/inventory**。可以在同一目录**/ansible/inventory**中找到一个简单的方法，用于从默认的多节点配置文件中分配不同的主机组，文件名为**multinode**。
- en: 'The configuration for each host group is formatted as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主机组的配置格式如下所示：
- en: '[PRE1]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The inventory file can be customized in different ways by assigning explicitly
    a single or several OpenStack services to run on specific hosts. That can be accomplished
    by adding the following configuration stanza:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 库存文件可以通过明确分配一个或多个OpenStack服务在特定主机上运行的方式进行不同的定制。这可以通过添加以下配置段来实现：
- en: '[PRE2]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For example, the following configuration instructs **kolla-ansible** to install
    the Swift service on the cloud control node group assigned as control:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下配置指示**kolla-ansible**在分配为控制的云控制节点组上安装Swift服务：
- en: '[PRE3]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The previous configuration stanza will install and run all Swift components
    in the control host group. With a more granular configuration, it is possible
    to instruct which component will be installed in which host. If none is mentioned,
    the service will be installed in the control host group. The next example instructs
    **kolla-ansible** to install other components rather than **swift-proxy-server**
    in the host group referenced as **object_storage** :'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配置段将安装并运行所有Swift组件在控制主机组中。通过更细粒度的配置，可以指示哪些组件将在哪些主机上安装。如果没有特别提到，服务将安装在控制主机组中。下一个示例指示**kolla-ansible**在引用为**object_storage**的主机组中安装除**swift-proxy-server**之外的其他组件：
- en: '[PRE4]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For our next iteration, the inventory section pointing to the shared services
    such as the database caching layer and messaging queue is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的下一次迭代，指向共享服务（如数据库缓存层和消息队列）的库存部分如下所示：
- en: '[PRE5]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Core control plane services will be running on the cloud controller host group,
    as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 核心控制平面服务将在云控制器主机组上运行，如下所示：
- en: '[PRE6]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'More granular shared and control plane service configurations can be customized
    per service, as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 更细粒度的共享和控制平面服务配置可以按服务进行定制，如下所示：
- en: 'For the image service, Glance can be configured as follows:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于镜像服务，Glance可以按如下方式配置：
- en: '[PRE7]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'All compute components except **nova-compute** can be configured as follows:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除**nova-compute**外，所有计算组件可以按如下方式配置：
- en: '[PRE8]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The Placement service API can be configured as follows:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Placement服务API可以按如下方式配置：
- en: '[PRE9]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The block storage API and volume scheduler service can be configured as follows:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块存储API和卷调度器服务可以按如下方式配置：
- en: '[PRE10]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The object storage proxy component can be configured as follows:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象存储代理组件可以按如下方式配置：
- en: '[PRE11]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The telemetry core and notification components can be configured as follows:'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遥测核心和通知组件可以按如下方式配置：
- en: '[PRE12]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The shared services and control plane sections can be defined in different ways.
    More services and components would inherit the same configuration once the design
    of a new project has been drafted to run in the cloud controller nodes. As we
    are not covering all services in this chapter, we will stick to our second iteration
    defined earlier and more control plane services will be added throughout the next
    chapters of the book. The inventory file will be the source of truth that identifies
    the latest version of the services running in the environment.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 共享服务和控制平面部分可以以不同方式定义。一旦新项目的设计草案已完成并准备在云控制节点上运行，更多的服务和组件将继承相同的配置。由于本章没有涵盖所有服务，我们将坚持前面定义的第二次迭代，并将在本书的后续章节中添加更多的控制平面服务。清单文件将作为确立环境中运行服务最新版本的真实来源。
- en: 'The next class of the inventory section defines the compute target nodes and
    the respective components they will be running:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 清单部分的下一个类别定义了计算目标节点及其将运行的相应组件：
- en: '[PRE13]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Optionally, as each compute node would require the collection of metrics for
    further monitoring, **collectd** will run on the compute host group in addition
    to the **ceilometer-compute** and **ipmi** components:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，由于每个计算节点都需要收集指标以供进一步监控，**collectd** 将在计算主机组上运行，此外还有 **ceilometer-compute**
    和 **ipmi** 组件：
- en: '[PRE14]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As each compute node would interconnect with other nodes to serve tenant networks,
    a few Neutron agents, as designed in the previous section, will be running on
    the compute host group:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个计算节点都将与其他节点互联以服务租户网络，设计在前一部分中的一些 Neutron 代理将运行在计算主机组上：
- en: '[PRE15]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Important note
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: OVN will be discussed in detail in [*Chapter 5*](B21716_05.xhtml#_idTextAnchor146)
    , *OpenStack Networking – Connectivity and Managed Service Options* , as being
    the most commonly used networking service in OpenStack within the latest releases,
    including Antelope.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: OVN 将在 [*第 5 章*](B21716_05.xhtml#_idTextAnchor146) 中详细讨论，*OpenStack 网络 – 连接性和管理服务选项*，作为
    OpenStack 中最新版本（包括 Antelope）中最常用的网络服务。
- en: 'The OpenStack networking service is designed to run on its own host, except
    for its API, as part of the cloud control plane. The following code defines the
    Network node and associated components in the inventory file:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 网络服务设计为在其自己的主机上运行，除了其 API 作为云控制平面的一部分。以下代码在清单文件中定义了网络节点及相关组件：
- en: '[PRE16]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'All Neutron services and agents except for its API will run on the network
    node, as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 除其 API 外，所有 Neutron 服务和代理将在网络节点上运行，如下所示：
- en: '[PRE17]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Additionally, the **manila-share** component will run on the network host group
    that will use the Neutron plugin to provide network access to the file share resources:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**manila-share** 组件将运行在使用 Neutron 插件来提供文件共享资源网络访问的网络主机组上：
- en: '[PRE18]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The OVN controller was configured to run on the compute node as well as in
    the network host group, as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: OVN 控制器已配置为在计算节点和网络主机组中运行，如下所示：
- en: '[PRE19]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**ovn-controller-network** inherits the association with network host groups:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**ovn-controller-network** 继承与网络主机组的关联：'
- en: '[PRE20]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that the OVN mechanism uses its own database that can run on the cloud
    controller host group. The OVN database itself supports three different schemas
    that can be running on the same cloud controller host group, as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，OVN 机制使用其自己的数据库，该数据库可以运行在云控制器主机组上。OVN 数据库本身支持三种不同的架构，可以在同一云控制器主机组上运行，如下所示：
- en: '[PRE21]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next stanza of the inventory is the storage layer in which initially, a
    single storage node will be defined:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 清单的下一部分是存储层，最初将定义一个存储节点：
- en: '[PRE22]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The storage node will run the Cinder volume:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 存储节点将运行 Cinder 卷：
- en: '[PRE23]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The last part of the inventory file includes the deployer host itself, defined
    as localhost:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 清单文件的最后一部分包括部署主机本身，定义为 localhost：
- en: '[PRE24]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Other services can be counted as additional services to install if you count
    running a custom **Zabbix agent** , for example, in all nodes or specific scanning
    tools reporting to a central hub. A simple way to do this is by creating a new
    stanza referring to all host groups defined in the inventory. For example, the
    following code section instructs Ansible to install a few tools, such as **cron**
    , and services to manage and collect logs using **fluentd** for Kolla containers
    running our OpenStack services. Optionally, **kolla-ansible** comes with **kolla-toolbox**
    to quickly run specific command lines in a Docker container once installed:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 其他服务可以算作额外的安装服务，例如，如果你在所有节点或特定扫描工具上运行自定义**Zabbix 代理**并报告给中央集线器。实现这一目标的简单方法是创建一个新的段落，引用清单中定义的所有主机组。例如，以下代码部分指示
    Ansible 安装一些工具，如**cron**，以及使用**fluentd**来管理和收集日志的服务，适用于运行 OpenStack 服务的 Kolla
    容器。可选地，**kolla-ansible**附带了**kolla-toolbox**，可以在安装后快速在 Docker 容器中运行特定的命令行：
- en: '[PRE25]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Important note
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In a multi-node setup, make sure that the deployer machine can reach all other
    hosts to run the Ansible playbooks. It is possible to specify in the inventory
    file how Ansible should interact with each target host. In the current setup,
    SSH keys have been generated and pushed to each target host to run Ansible against
    during the deployment. Bear in mind that hostnames should be propagated in **/etc/hosts**
    in each node if IP addresses are not used in the inventory file.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在多节点配置中，确保部署机器能够访问所有其他主机以运行 Ansible playbook。可以在清单文件中指定 Ansible 如何与每个目标主机交互。在当前配置中，已经生成并推送了
    SSH 密钥到每个目标主机，以便在部署过程中使用 Ansible。请注意，如果清单文件中未使用 IP 地址，则主机名应在每个节点的**/etc/hosts**中进行传播。
- en: 'Save the file, commit it, and push it to the created branch:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 保存文件，提交并推送到创建的分支：
- en: '[PRE26]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once the host mapping and branching configuration are properly done, it is time
    to dive deep into the **kolla-ansible** code and customize the future OpenStack
    parameters for different services.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦主机映射和分支配置完成，便可以深入研究**kolla-ansible**代码，并为不同服务自定义未来的 OpenStack 参数。
- en: Parameterizing the environment
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数化环境
- en: The next key file is the **globals.yml** configuration file defined in the **/etc/kolla/**
    directory. This file acts as the central configuration hub for all OpenStack environments.
    The default file that comes with the community **kolla-ansible** repository defines
    most of the OpenStack services configuration for each service option. You can
    copy and comment out the desired configuration line or generate a new one holding
    the OpenStack configuration settings.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个关键文件是**globals.yml**配置文件，该文件位于**/etc/kolla/**目录中。这个文件充当所有 OpenStack 环境的中央配置中心。社区版**kolla-ansible**仓库附带的默认文件定义了每个服务选项的
    OpenStack 服务配置。你可以复制并注释掉所需的配置行，或者生成一个新的配置行来持有 OpenStack 配置设置。
- en: 'The **globals.yml** file in the current iteration includes the following configurations:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 当前迭代中的**globals.yml**文件包括以下配置：
- en: 'Use Ubuntu as the Kolla base container distribution:'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ubuntu 作为 Kolla 基础容器发行版：
- en: '[PRE27]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Install the latest stable OpenStack release:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装最新的稳定版 OpenStack 发行版：
- en: '[PRE28]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Assign an unused default VIP to one controller interface:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为一个控制器接口分配一个未使用的默认 VIP：
- en: '[PRE29]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use KVM as the default hypervisor driver:'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 KVM 作为默认的虚拟化驱动程序：
- en: '[PRE30]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Use the Neutron Open vSwitch plugin:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Neutron Open vSwitch 插件：
- en: '[PRE31]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Enable Neutron provider networks:'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用 Neutron 提供者网络：
- en: '[PRE32]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Assign a network interface for API services:'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 API 服务分配一个网络接口：
- en: '[PRE33]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Assign a network interface for the Neutron external port:'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Neutron 外部端口分配一个网络接口：
- en: '[PRE34]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Use the local Docker registry as created in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089)
    , *Kicking Off the OpenStack Setup – The Right* *Way (DevSecOps)* :'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在[*第2章*](B21716_02.xhtml#_idTextAnchor089)中创建的本地 Docker 镜像库，*启动 OpenStack
    设置——正确的* *方式（DevSecOps）*：
- en: '[PRE35]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Save the file, commit, and push it to the created branch:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存文件，提交并推送到创建的分支：
- en: '[PRE36]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Important note
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Some configuration parameters in the **kolla-ansible** community repository
    might be updated from one OpenStack release to another, such as new configuration
    parameters in the **globals.yml** file or the repository structure itself. It
    is highly encouraged to cast an eye on the repository documentation before deploying
    a specific release. The book uses the latest stable release. Make sure to check
    the branch name and its relevant tags for the first pull.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**kolla-ansible** 社区仓库中的一些配置参数可能会随着 OpenStack 版本的不同而更新，例如 **globals.yml** 文件中的新配置参数或仓库结构本身。强烈建议在部署特定版本之前查看仓库文档。本文使用的是最新的稳定版本。请确保在首次拉取时检查分支名称及其相关标签。'
- en: Advanced configuration
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级配置
- en: The **globals.yml** file does not provide all granular configuration settings
    by default. For example, the maximum connections of HAProxy and the maximum amount
    of memory of a Memcached instance are more specific settings that cannot be directly
    adjusted in the **globals.yml** file.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**globals.yml** 文件默认不提供所有详细的配置设置。例如，HAProxy 的最大连接数和 Memcached 实例的最大内存量是更具体的设置，无法直接在
    **globals.yml** 文件中调整。'
- en: One way of conducting more advanced configuration in an OpenStack production
    environment is by going through the Ansible playbooks. Keep in mind that the **globals.yml**
    file is a high-level representation on how to deploy an OpenStack environment
    from one single hub file.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenStack 生产环境中进行更高级配置的一种方式是通过查看 Ansible playbooks。请记住，**globals.yml** 文件是从一个单独的中心文件部署
    OpenStack 环境的高级表示。
- en: 'There are three different levels of configuring an OpenStack deployment using
    **kolla-ansible** , in the following order:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **kolla-ansible** 配置 OpenStack 部署有三个不同的级别，按照以下顺序：
- en: '**/etc/kolla/globals.yml** : This is the first hub of generic and custom OpenStack,
    including shared services settings.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**/etc/kolla/globals.yml** ：这是通用和自定义 OpenStack 的第一个中心，包含共享服务设置。'
- en: '**/kolla-ansible/ansible/group_vars/all.yml** : This reflects the global and
    specific settings of the OpenStack services. Once they are edited and running,
    the settings in the **all.yml** file will be merged with the **globals.yml** file.
    Settings that are not declared explicitly in the **globals.yml** file will use
    the default ones in **all.yml** .'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**/kolla-ansible/ansible/group_vars/all.yml** ：该文件反映了 OpenStack 服务的全局和特定设置。一旦它们被编辑并运行，**all.yml**
    文件中的设置将与 **globals.yml** 文件合并。在 **globals.yml** 文件中未显式声明的设置将使用 **all.yml** 中的默认值。'
- en: '**Ansible playbooks** : Few cases would require the introduction of an advanced
    configuration in a specific service. Each Ansible playbook should include all
    available options for a given service or component. Ansible playbooks are located
    under **/kolla-ansible/ansible/roles** , where each OpenStack component and shared
    service available within Antelope and later releases are listed. As highlighted
    in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking Off the OpenStack
    Setup – The Right Way (DevSecOps)* ,in the discussion of how an Ansible playbook
    is constructed, adjusting a setting for a specific service is straightforward.
    For example, changing the maximum amount of memory in Memcached can be performed
    by simply updating the **memcached_max_memory** configuration stanza in the **main.yml**
    file located under the **/** **roles/memcached/defaults/** folder.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ansible playbooks** ：少数情况下，可能需要为特定服务引入高级配置。每个 Ansible playbook 应包含给定服务或组件的所有可用选项。Ansible
    playbook 位于 **/kolla-ansible/ansible/roles** 目录下，其中列出了 Antelope 版本及更高版本中所有可用的
    OpenStack 组件和共享服务。正如在 [*第二章*](B21716_02.xhtml#_idTextAnchor089) *启动 OpenStack
    设置 – 正确的方式（DevSecOps）* 中强调的那样，在讨论如何构建 Ansible playbook 时，调整特定服务的设置非常简单。例如，通过简单更新位于
    **/** **roles/memcached/defaults/** 文件夹中的 **main.yml** 文件中的 **memcached_max_memory**
    配置项，可以更改 Memcached 的最大内存量。'
- en: Customizing the control plane configuration is pretty straightforward by simply
    looking at the Ansible role and playbook for a given OpenStack core service. **/kolla-ansible/ansible/roles/**
    includes all sets of OpenStack core services and other projects officially incubated
    within the latest stable release. Note that there is a direct way of reflecting
    a playbook setting in the **globals.yml** or **all.yml** file.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义控制平面配置相当简单，只需查看给定 OpenStack 核心服务的 Ansible 角色和 playbook 即可。**/kolla-ansible/ansible/roles/**
    包括所有 OpenStack 核心服务集和其他在最新稳定版本中官方孵化的项目。请注意，有一种直接的方法可以将 playbook 设置反映到 **globals.yml**
    或 **all.yml** 文件中。
- en: Deploying the environment
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署环境
- en: Dedicating a staging environment to run deployments before running in production
    is one of the development pillars that is most touted in discussions about code
    and artifact release stability. There is always a temptation to skip a second
    environment such as staging when deploying code, especially with resource-hungry
    environments such as OpenStack. Staging can be expensive, but thinking of the
    outcome provides a boost of confidence to mimic a production environment before
    merging that code to the master branch. A common solution to the cost challenge
    if you are not able to afford an exact copy of the production setup is to use
    a smaller version of the production environment but keep the same design layout.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个staging环境专门用于在生产环境之前进行部署，是在关于代码和工件发布稳定性的讨论中，最常被提及的开发支柱之一。在部署代码时，总是有跳过staging环境的诱惑，尤其是对于资源密集型的环境（如OpenStack）。Staging环境可能很昂贵，但考虑到最终结果，它能为在将代码合并到主分支之前模拟生产环境提供信心。如果你无法负担与生产环境完全相同的复制品，解决成本问题的常见方法是使用生产环境的较小版本，但保持相同的设计布局。
- en: 'In the following snippet, all required steps will be automated through a Jenkins
    pipeline targeting the **multi_packtpub_stg** branch. To do so, create a new Jenkins
    file targeting the staging branch:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，所有必需的步骤将通过一个针对**multi_packtpub_stg**分支的Jenkins流水线自动化实现。为此，创建一个新的Jenkins文件，目标是staging分支：
- en: '[PRE37]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Commit and push the created file to the source code repository with the new
    branch:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 提交并推送创建的文件到源代码仓库，并创建新分支：
- en: '[PRE38]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As demonstrated in [*Chapter 2*](B21716_02.xhtml#_idTextAnchor089) , *Kicking
    Off the OpenStack Setup – The Right Way (DevSecOps)* , create a new job by selecting
    **New Item** from the top-left menu in the Jenkins user interface. Select **Pipeline**
    as the type of our job with a desired name for running the staging pipeline. Follow
    the same steps for the staging pipeline by specifying the path of the Jenkins
    file from the repository. Different build stages are visible as coded in the pipeline,
    as shown in the following figure:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第2章*](B21716_02.xhtml#_idTextAnchor089)所示，*启动OpenStack设置 – 正确的方法（DevSecOps）*，通过选择Jenkins用户界面左上角的**新建项目**，创建一个新作业。选择**流水线**作为我们作业的类型，并为运行staging流水线指定一个所需的名称。通过指定Jenkins文件在仓库中的路径，按照相同的步骤设置staging流水线。流水线中不同的构建阶段可见，如下图所示：
- en: '![Figure 3.7 – An OpenStack staging environment deployment](img/B21716_03_07.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 一个OpenStack staging环境部署](img/B21716_03_07.jpg)'
- en: Figure 3.7 – An OpenStack staging environment deployment
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 一个OpenStack staging环境部署
- en: The first build run can take longer to install the required packages and dependencies
    on the Jenkins server. Automating the deployment pipeline is very handy as orchestrating
    all steps from one single pipeline file reduces the time to run specific steps
    manually in case an error occurs in one of the pipeline stages. A Jenkins build
    can be configured to pull source code on a regular basis ( **cron** , for example)
    or trigger a build when new code is pushed and merged to the development branch.
    Note that there are multiple ways to achieve more robust deployment pipelines
    by integrating more unit tests. We have, so far, built an initial deployment pipeline.
    From it, installing additional services or making changes can be performed from
    one single place and with more control. Throughout the next chapters, we will
    stick to the same deployment approach armed with best practices by keeping deployments
    in small batches, continuous, and test-driven.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次构建运行可能需要更长时间来安装Jenkins服务器上所需的软件包和依赖项。自动化部署流水线非常方便，因为通过一个单一的流水线文件来协调所有步骤，减少了在流水线某个阶段出现错误时手动执行特定步骤的时间。可以配置Jenkins构建定期拉取源代码（例如**cron**），或者在有新的代码推送并合并到开发分支时触发构建。请注意，通过集成更多的单元测试，有多种方法可以实现更强大的部署流水线。到目前为止，我们已经构建了一个初步的部署流水线。从中，可以从一个地方并且更有控制地安装额外的服务或进行更改。在接下来的章节中，我们将继续坚持这种部署方法，结合最佳实践，将部署保持为小批量、持续进行并且以测试驱动。
- en: Summary
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we went through a cornerstone of OpenStack deployment: the
    control plane. Identifying each component role within the control plane layer
    supports the preparation of a highly available and scalable orchestration layer
    that will we cover in more detail in [*Chapter 7*](B21716_07.xhtml#_idTextAnchor174)
    , *Running a Highly Available Cloud– Meeting the SLA* . Some of the core services,
    such as Keystone, Glance, and Placement, were covered in more detail to highlight
    different options that your next production iteration can support through identity
    consolidation or an imaging backend, for example. Note that the control plane
    is not limited only to the deployed services we have explored so far in this chapter.
    The OpenStack ecosystem is still growing with additional services; hence, more
    APIs can be part of the control plane. From an architectural perspective, the
    Designate API can be part of the control plane and run in the same cloud controller,
    whereas other management components can run on a dedicated host to enhance DNS
    request performance in a large-scale environment.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们回顾了 OpenStack 部署的基石：控制平面。识别控制平面层中每个组件的角色有助于为高度可用且可扩展的编排层做准备，我们将在[*第 7 章*](B21716_07.xhtml#_idTextAnchor174)
    *运行高可用云 - 满足 SLA* 中详细讨论。一些核心服务，如 Keystone、Glance 和 Placement，已被更详细地探讨，以突出您的下一次生产迭代可以支持的不同选项，例如通过身份合并或映像后端。请注意，控制平面不仅限于我们在本章中探讨的已部署服务。OpenStack
    生态系统仍在增长，随着更多服务的加入，更多的 API 可以成为控制平面的一部分。从架构的角度来看，Designate API 可以是控制平面的一部分，并与其他管理组件一起运行在相同的云控制器中，而其他管理组件则可以在专用主机上运行，以提高大规模环境中
    DNS 请求的性能。
- en: Most importantly, regardless of which option you choose, the control plane must
    be prepared for failure and proven to be scalable, as we learned in this chapter.
    Adding or removing a service unit within the control plane cluster should not
    be a disruptive operation. That was a valid reason to adopt a container-based
    deployment approach for our OpenStack production environment. The chapter extended
    the first deployment using **kolla-ansible** by going into more depth on a variety
    of control plane playbooks with additional possible advanced settings. Although
    the chapter did not cover the high-availability aspect of the control plane in
    detail, the design draft should be capable of enforcing the control plane with
    fault tolerance before exposing the cloud environment for consumption.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，无论您选择哪种选项，控制平面必须为故障做好准备，并且经过验证可以扩展，正如我们在本章中所学到的那样。向控制平面集群中添加或移除服务单元不应是破坏性的操作。这也是我们选择基于容器的部署方法来构建
    OpenStack 生产环境的合理原因。本章通过深入探讨各种控制平面 playbook 和附加的高级设置，扩展了使用 **kolla-ansible** 的首次部署。虽然本章没有详细讨论控制平面高可用性的方面，但设计草案应能够在将云环境开放供使用之前，通过容错来加强控制平面。
- en: The next chapter will extend our deployment by going through the compute layer.
    As we have separated our control plane and designed only the compute API and scheduler
    to be part of the cloud controller, the next step is to dive into the OpenStack
    compute service and uncover various options for deployment as well as features,
    including containerization options, within the latest OpenStack releases.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将通过计算层扩展我们的部署。由于我们已经将控制平面分离，并设计了只有计算 API 和调度器作为云控制器的一部分，下一步是深入探讨 OpenStack
    计算服务，揭示各种部署选项以及最新 OpenStack 版本中的功能，包括容器化选项。
